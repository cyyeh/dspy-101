{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The core concepts of DSPy\n",
    "2. Understanding the building blocks of DSPy\n",
    "3. How to use DSPy in practice and recommended workflow\n",
    "4. Enable to trace DSPy internals\n",
    "5. Future roadmap of DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Core Concepts\n",
    "- Building Blocks\n",
    "- Recommended Workflow\n",
    "- Examples\n",
    "- Roadmap\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l0gVphd9-YGBM_qmihKhow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "Notes:\n",
    "- Earlier versions of DSPy involved tens of clients for different LM providers.(deprecated, and will be removed in DSPy 2.6) Starting from 2.5, use `dspy.LM` instead(using litellm under the hood)\n",
    "- Inspecting history\n",
    "- Adapters\n",
    "    - DSPy 2.5 introduces **Adapters** as a layer between Signatures and LMs, responsible for formatting these pieces (Signature I/O fields, instructions, and examples) as well as generating and parsing the outputs.\n",
    "- Using `dspy.configure` and `dspy.context` is thread-safe!\n",
    "- By default LMs in DSPy are cached. If you repeat the same call, you will get the same outputs. But you can turn off caching by setting `cache=False` while declaring `dspy.LM` object\n",
    "- Any OpenAI-compatible endpoint is easy to set up with an `openai/` prefix as well.\n",
    "\n",
    "\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/building-blocks/1-language_models/\n",
    "- source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/clients/lm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"gpt-4o-mini\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directly calling the LLM(not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chat LLMs\n",
    "lm(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": \"What is 2+2?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the llm with DSPy signatures and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# Run with the default LM configured with `dspy.configure` above.\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using multiple LLMs at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with the default LM configured above, i.e. GPT-4o-mini\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print('gpt-4o-mini:', response.answer)\n",
    "\n",
    "gpt_4o = dspy.LM(model='gpt-4o', max_tokens=300)\n",
    "\n",
    "# Run with GPT-4o instead\n",
    "with dspy.context(lm=gpt_4o):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "    print('gpt-4o:', response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuring llm attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini = dspy.LM(\n",
    "\t'gpt-4o-mini',\n",
    "\ttemperature=0.9,\n",
    "\tmax_tokens=3000,\n",
    "\tstop=None,\n",
    "\tcache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using locally hosted LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_port = 11434 \n",
    "ollama_url = f\"http://localhost:{ollama_port}\"\n",
    "ollama_llm = dspy.LM(model=\"ollama/llama3.2:1b\", api_base=ollama_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting llm output and usage metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in lm.history[-1].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating custom LLM class (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom LM class is quite straightforward in DSPy. You can inherit from the dspy.LM class or create a new class with a similar interface. You'll need to implement/override these three methods:\n",
    "\n",
    "- `__init__`: Initialize the LM with the given model and other keyword arguments.\n",
    "- `__call__`: Call the LM with the given input prompt and return a list of string outputs.\n",
    "- `inspect_history`: The history of interactions with the LM. This is optional but is needed by some optimizers in DSPy.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import dspy\n",
    "import google.generativeai as genai\n",
    "\n",
    "class GeminiLM(dspy.LM):\n",
    "    def __init__(self, model, api_key=None, endpoint=None, **kwargs):\n",
    "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"] or api_key)\n",
    "\n",
    "        self.endpoint = endpoint\n",
    "        self.history = []\n",
    "\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "\n",
    "    def __call__(self, prompt=None, messages=None, **kwargs):\n",
    "        # Custom chat model working for text completion model\n",
    "        prompt = '\\n\\n'.join([x['content'] for x in messages] + ['BEGIN RESPONSE:'])\n",
    "\n",
    "        completions = self.model.generate_content(prompt)\n",
    "        self.history.append({\"prompt\": prompt, \"completions\": completions})\n",
    "\n",
    "        # Must return a list of strings\n",
    "        return [completions.candidates[0].content.parts[0].text]\n",
    "\n",
    "    def inspect_history(self):\n",
    "        for interaction in self.history:\n",
    "            print(f\"Prompt: {interaction['prompt']} -> Completions: {interaction['completions']}\")\n",
    "\n",
    "lm = GeminiLM(\"gemini-1.5-flash\", temperature=0)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "qa = dspy.ChainOfThought(\"question->answer\")\n",
    "qa(question=\"What is the capital of France?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Structured LLM output with Adapters (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signatures\n",
    "\n",
    "Notes:\n",
    "- inline-based signature prompt creation\n",
    "![](https://dspy-docs.vercel.app/deep-dive/signature/img/prompt_creation.png)\n",
    "- class-based signature prompt creation\n",
    "![](https://dspy-docs.vercel.app/deep-dive/signature/img/class_based_prompt_creation.png)\n",
    "\n",
    "References:\n",
    "- documentation\n",
    "    - https://dspy-docs.vercel.app/building-blocks/2-signatures/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/understanding-signatures/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/\n",
    "- source code: https://github.com/stanfordnlp/dspy/tree/main/dspy/signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.\n",
    "\n",
    "A signature is a declarative specification of input/output behavior of a DSPy module. Signatures allow you to tell the LM what it needs to do, rather than specify how we should ask the LM to do it.\n",
    "\n",
    "You're probably familiar with function signatures, which specify the input and output arguments and their types. DSPy signatures are similar, but the differences are that:\n",
    "\n",
    "- While typical function signatures just describe things, DSPy Signatures define and control the behavior of modules.\n",
    "- The field names matter in DSPy Signatures. You express semantic roles in plain English: a question is different from an answer, a sql_query is different from python_code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should I use a DSPy Signature?\n",
    "\n",
    "tl;dr For modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes).\n",
    "\n",
    "Long Answer: Most people coerce LMs to do tasks by hacking long, brittle prompts. Or by collecting/generating data for fine-tuning.\n",
    "\n",
    "Writing signatures is far more modular, adaptive, and reproducible than hacking at prompts or finetunes. The DSPy compiler will figure out how to build a highly-optimized prompt for your LM (or finetune your small LM) for your signature, on your data, and within your pipeline. In many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things and tune the metrics directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline DSPy Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signatures can be defined as a short string, with argument names that define semantic roles for inputs/outputs.\n",
    "\n",
    "1. `Question Answering: \"question -> answer\"`\n",
    "2. `Sentiment Classification: \"sentence -> sentiment\"`\n",
    "3. `Summarization: \"document -> summary\"`\n",
    "\n",
    "Your signatures can also have multiple input/output fields.\n",
    "\n",
    "1. `Retrieval-Augmented Question Answering: \"context, question -> answer\"`\n",
    "2. `Multiple-Choice Question Answering with Reasoning: \"question, choices -> reasoning, selection\"`\n",
    "\n",
    "Tip: For fields, any valid variable names work! Field names should be semantically meaningful, but start simple and don't prematurely optimize keywords! Leave that kind of hacking to the DSPy compiler. For example, for summarization, it's probably fine to say \"document -> summary\", \"text -> gist\", or \"long_context -> tldr\".\n",
    "\n",
    "Notes:\n",
    "- Many DSPy modules (except `dspy.Predict`) return auxiliary information by expanding your signature under the hood. For example, `dspy.ChainOfThought` also adds a rationale field that includes the LM's reasoning before it generates the output summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n",
    "\n",
    "classify = dspy.Predict('sentence -> sentiment')\n",
    "classify(sentence=sentence).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the XSum dataset.\n",
    "document = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\n",
    "\n",
    "summarize = dspy.ChainOfThought('document -> summary')\n",
    "response = summarize(document=document)\n",
    "\n",
    "print(response.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-based DSPy Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some advanced tasks, you need more verbose signatures. This is typically to:\n",
    "\n",
    "1. Clarify something about the nature of the task (expressed below as a docstring).\n",
    "2. Supply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.\n",
    "3. Supply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.\n",
    "\n",
    "Tips:\n",
    "- There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs).\n",
    "- How `Predict` works:\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/#how-predict-works\n",
    "    - source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/predict/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emotion(dspy.Signature):\n",
    "    \"\"\"Classify emotion among sadness, joy, love, anger, fear, surprise.\"\"\"\n",
    "\n",
    "    sentence = dspy.InputField()\n",
    "    sentiment = dspy.OutputField()\n",
    "\n",
    "sentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\n",
    "\n",
    "classify = dspy.Predict(Emotion)\n",
    "classify(sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using signatures to build modules & compiling them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While signatures are convenient for prototyping with structured inputs/outputs, that's not the main reason to use them!\n",
    "\n",
    "You should compose multiple signatures into bigger DSPy modules and compile these modules into optimized prompts and finetunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules\n",
    "\n",
    "Notes:\n",
    "- A DSPy module is a building block for programs that use LMs.\n",
    "    - Each built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any DSPy Signature.\n",
    "    - A DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\n",
    "    - Multiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\n",
    "- What other DSPy modules are there? How can I use them?\n",
    "    - The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
    "        - `dspy.Predict`: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
    "        - `dspy.ChainOfThought`: Teaches the LM to think step-by-step before committing to the signature's response.\n",
    "        - `dspy.ProgramOfThought`: Teaches the LM to output code, whose execution results will dictate the response.\n",
    "        - `dspy.ReAct`: An agent that can use tools to implement the given signature.\n",
    "        - `dspy.MultiChainComparison`: Can compare multiple outputs from ChainOfThought to produce a final prediction.\n",
    "    - We also have some function-style modules:\n",
    "        - `dspy.majority`: Can do basic voting to return the most popular response from a set of predictions.\n",
    "- How do I compose multiple modules into a bigger program?\n",
    "    - DSPy is just Python code that uses modules in any control flow you like. (There's some magic internally at `compile` time to trace your LM calls.)\n",
    "    - This means that, you can just call the modules freely. No weird abstractions for chaining calls.\n",
    "    - This is basically PyTorch's design approach for define-by-run / dynamic computation graphs. Refer to the intro tutorials for examples.\n",
    "\n",
    "References:\n",
    "- documentation:\n",
    "    - https://dspy-docs.vercel.app/building-blocks/3-modules/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/modules/guide\n",
    "- source code:\n",
    "    - https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/module.py\n",
    "    - https://github.com/stanfordnlp/dspy/tree/main/dspy/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What's something great about the ColBERT retrieval model?\"\n",
    "\n",
    "# 1) Declare with a signature, and pass some config.\n",
    "classify = dspy.ChainOfThought('question -> answer', n=5)\n",
    "\n",
    "# 2) Call with input argument.\n",
    "response = classify(question=question)\n",
    "\n",
    "# 3) Access the outputs.\n",
    "response.completions.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Notes:\n",
    "- DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.\n",
    "    - For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.\n",
    "- How much data do I need and how do I collect data for my task?\n",
    "    - Concretely, you can use DSPy optimizers usefully with as few as 10 example inputs, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\n",
    "    - How can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels, so it's not that hard.\n",
    "    - However, chances are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here.\n",
    "    - If there's data whose licenses are permissive enough, we suggest you use them. Otherwise, you can also start using/deploying/demoing your system and collect some initial data that way.\n",
    "- DSPy `Example` objects\n",
    "    - The core data type for data in DSPy is Example. You will use Examples to represent items in your training set and test set.\n",
    "    - DSPy Examples are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type Prediction, which is a special sub-class of Example.\n",
    "    - When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example\n",
    "- Loading Dataset from sources\n",
    "    - One of the most convenient way to import datasets in DSPy is by using `DataLoader`. The first step is to declare an object, this object can then be used to call utilities to load datasets in different formats:\n",
    "        - `DataLoader().from_csv(...)`\n",
    "        - `DataLoader().from_json(...)`\n",
    "        - `DataLoader().from_parquet(...)`\n",
    "        - `DataLoader().from_pandas(...)`\n",
    "        - `DataLoader().from_huggingface(...)`\n",
    "\n",
    "References:\n",
    "- documentation:\n",
    "    - https://dspy-docs.vercel.app/building-blocks/4-data/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/data-handling/built-in-datasets/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/data-handling/loading-custom-data/\n",
    "- source code:\n",
    "    - https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/example.py\n",
    "    - https://github.com/stanfordnlp/dspy/blob/main/dspy/datasets/dataset.py\n",
    "    - https://github.com/stanfordnlp/dspy/blob/main/dspy/datasets/dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pair = dspy.Example(question=\"This is a question?\", answer=\"This is an answer.\")\n",
    "\n",
    "print(qa_pair)\n",
    "print(qa_pair.question)\n",
    "print(qa_pair.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Input.\n",
    "print(qa_pair.with_inputs(\"question\"))\n",
    "\n",
    "# Multiple Inputs; be careful about marking your labels as inputs unless you mean it.\n",
    "print(qa_pair.with_inputs(\"question\", \"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_summary = dspy.Example(article= \"This is an article.\", summary= \"This is a summary.\").with_inputs(\"article\")\n",
    "\n",
    "input_key_only = article_summary.inputs()\n",
    "non_input_key_only = article_summary.labels()\n",
    "\n",
    "print(\"Example object with Input fields only:\", input_key_only)\n",
    "print(\"Example object with Non-Input fields only:\", non_input_key_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()\n",
    "\n",
    "blog_alpaca = dl.from_huggingface(\n",
    "    \"intertwine-expel/expel-blog\",\n",
    "    input_keys=(\"title\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced: Inside DSPy's `Dataset` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dspy-docs.vercel.app/deep-dive/data-handling/img/data-loading.png)\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from dspy.datasets.dataset import Dataset\n",
    "\n",
    "\n",
    "class HotPotQA(Dataset):\n",
    "    def __init__(self, *args, only_hard_examples=True, keep_details='dev_titles', unofficial_dev=True, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert only_hard_examples, \"Care must be taken when adding support for easy examples.\" \\\n",
    "                                   \"Dev must be all hard to match official dev, but training can be flexible.\"\n",
    "        \n",
    "        hf_official_train = load_dataset(\"hotpot_qa\", 'fullwiki', split='train', trust_remote_code=True)\n",
    "        hf_official_dev = load_dataset(\"hotpot_qa\", 'fullwiki', split='validation', trust_remote_code=True)\n",
    "\n",
    "        official_train = []\n",
    "        for raw_example in hf_official_train:\n",
    "            if raw_example['level'] == 'hard':\n",
    "                if keep_details is True:\n",
    "                    keys = ['id', 'question', 'answer', 'type', 'supporting_facts', 'context']\n",
    "                elif keep_details == 'dev_titles':\n",
    "                    keys = ['question', 'answer', 'supporting_facts']\n",
    "                else:\n",
    "                    keys = ['question', 'answer']\n",
    "\n",
    "                example = {k: raw_example[k] for k in keys}\n",
    "                \n",
    "                if 'supporting_facts' in example:\n",
    "                    example['gold_titles'] = set(example['supporting_facts']['title'])\n",
    "                    del example['supporting_facts']\n",
    "\n",
    "                official_train.append(example)\n",
    "\n",
    "        rng = random.Random(0)\n",
    "        rng.shuffle(official_train)\n",
    "\n",
    "        self._train = official_train[:len(official_train)*75//100]\n",
    "\n",
    "        if unofficial_dev:\n",
    "            self._dev = official_train[len(official_train)*75//100:]\n",
    "        else:\n",
    "            self._dev = None\n",
    "\n",
    "        for example in self._train:\n",
    "            if keep_details == 'dev_titles':\n",
    "                del example['gold_titles']\n",
    "        \n",
    "        test = []\n",
    "        for raw_example in hf_official_dev:\n",
    "            assert raw_example['level'] == 'hard'\n",
    "            example = {k: raw_example[k] for k in ['id', 'question', 'answer', 'type', 'supporting_facts']}\n",
    "            if 'supporting_facts' in example:\n",
    "                example['gold_titles'] = set(example['supporting_facts']['title'])\n",
    "                del example['supporting_facts']\n",
    "            test.append(example)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Notes:\n",
    "- What is a metric and how do I define a metric for my task?\n",
    "    - A metric is just a function that will take examples from your data and the output of your system and return a score that quantifies how good the output is.\n",
    "    - For simple tasks, this could be just \"accuracy\" or \"exact match\" or \"F1 score\". This may be the case for simple classification or short-form QA tasks. However, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/building-blocks/5-metrics/\n",
    "- source code: https://github.com/stanfordnlp/dspy/tree/main/dspy/evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(example, pred, trace=None):\n",
    "    return example.answer.lower() == pred.answer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Set up the evaluator, which can be re-used in your code.\n",
    "evaluator = Evaluate(devset=YOUR_DEVSET, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "# Launch evaluation.\n",
    "evaluator(YOUR_PROGRAM, metric=YOUR_METRIC)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate: Using AI feedback for your metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most applications, your system will output long-form outputs, so your metric should check multiple dimensions of the output using AI feedback from LMs.\n",
    "\n",
    "```python\n",
    "# Define the signature for automatic assessments.\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of a tweet along the specified dimension.\"\"\"\n",
    "\n",
    "    assessed_text = dspy.InputField()\n",
    "    assessment_question = dspy.InputField()\n",
    "    assessment_answer = dspy.OutputField(desc=\"Yes or No\")\n",
    "\n",
    "\n",
    "gpt4T = dspy.OpenAI(model='gpt-4-1106-preview', max_tokens=1000, model_type='chat')\n",
    "\n",
    "def metric(gold, pred, trace=None):\n",
    "    question, answer, tweet = gold.question, gold.answer, pred.output\n",
    "\n",
    "    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\n",
    "    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\n",
    "\n",
    "    with dspy.context(lm=gpt4T):\n",
    "        correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\n",
    "        engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\n",
    "\n",
    "    correct, engaging = [m.assessment_answer.lower() == 'yes' for m in [correct, engaging]]\n",
    "    score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\n",
    "\n",
    "    if trace is not None: return score >= 2\n",
    "    return score / 2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced: Using a DSPy program as your metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your metric is itself a DSPy program, one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is easy to define and optimize by collecting a few examples.\n",
    "\n",
    "When your metric is used during evaluation runs, DSPy will not try to track the steps of your program.\n",
    "\n",
    "But during compiling (optimization), DSPy will trace your LM calls. The trace will contain inputs/outputs to each DSPy predictor and you can leverage that to validate intermediate steps for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers(formerly Teleprompters)\n",
    "\n",
    "Notes:\n",
    "- A DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.\n",
    "    - There are many built-in optimizers in DSPy, which apply vastly different strategies. A typical DSPy optimizer takes three things:\n",
    "    - Your DSPy program. This may be a single module (e.g., dspy.Predict) or a complex multi-module program.\n",
    "    - Your metric. This is a function that evaluates the output of your program, and assigns it a score (higher is better).\n",
    "    - A few training inputs. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).\n",
    "- What does a DSPy Optimizer tune? How does it tune them?\n",
    "    - DSPy programs consist of multiple calls to LMs, stacked together as [DSPy modules]. Each DSPy module has internal parameters of three kinds: (1) the LM weights, (2) the instructions, and (3) demonstrations of the input/output behavior.\n",
    "    - Given a metric, DSPy can optimize all of these three with multi-stage optimization algorithms. These can combine gradient descent (for LM weights) and discrete LM-driven optimization, i.e. for crafting/updating instructions and for creating/validating demonstrations. DSPy Demonstrations are like few-shot examples, but they're far more powerful. They can be created from scratch, given your program, and their creation and selection can be optimized in many effective ways.\n",
    "    - In many cases, we found that compiling leads to better prompts than human writing. Not because DSPy optimizers are more creative than humans, but simply because they can try more things, much more systematically, and tune the metrics directly.\n",
    "- Which optimizer should I use?\n",
    "    - Ultimately, finding the ‘right’ optimizer to use & the best configuration for your task will require experimentation. Success in DSPy is still an iterative process - getting the best performance on your task will require you to explore and iterate.\n",
    "    - That being said, here's the general guidance on getting started:\n",
    "        - If you have very few examples (around 10), start with `BootstrapFewShot`.\n",
    "        - If you have more data (50 examples or more), try `BootstrapFewShotWithRandomSearch`.\n",
    "        - If you prefer to do instruction optimization only (i.e. you want to keep your prompt 0-shot), use `MIPROv2` configured for 0-shot optimization to optimize.\n",
    "        - If you’re willing to use more inference calls to perform longer optimization runs (e.g. 40 trials or more), and have enough data (e.g. 200 examples or more to prevent overfitting) then try `MIPROv2`.\n",
    "        - If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, finetune a small LM for your task with `BootstrapFinetune`.\n",
    "\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/building-blocks/6-optimizers/\n",
    "- source code: https://github.com/stanfordnlp/dspy/tree/main/dspy/teleprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Few-Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `LabeledFewShot`: Simply constructs few-shot examples (demos) from provided labeled input and output data points. Requires k (number of examples for the prompt) and trainset to randomly select k examples from.\n",
    "\n",
    "2. `BootstrapFewShot`: Uses a teacher module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in trainset. Parameters include max_labeled_demos (the number of demonstrations randomly selected from the trainset) and max_bootstrapped_demos (the number of additional examples generated by the teacher). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the \"compiled\" prompt. Advanced: Supports using a teacher program that is a different DSPy program that has compatible structure, for harder tasks.\n",
    "\n",
    "3. `BootstrapFewShotWithRandomSearch`: Applies BootstrapFewShot several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of BootstrapFewShot, with the addition of num_candidate_programs, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, LabeledFewShot optimized program, BootstrapFewShot compiled program with unshuffled examples and num_candidate_programs of BootstrapFewShot compiled programs with randomized example sets.\n",
    "\n",
    "4. `KNNFewShot`: Uses k-Nearest Neighbors algorithm to find the nearest training example demonstrations for a given input example. These nearest neighbor demonstrations are then used as the trainset for the BootstrapFewShot optimization process. See this notebook for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Instruction Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `COPRO`: Generates and refines new instructions for each step, and optimizes them with coordinate ascent (hill-climbing using the metric function and the trainset). Parameters include depth which is the number of iterations of prompt improvement the optimizer runs over.\n",
    "\n",
    "2. `MIPROv2`: Generates instructions and few-shot examples in each step. The instruction generation is data-aware and demonstration-aware. Uses Bayesian Optimization to effectively search over the space of generation instructions/demonstrations across your modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `BootstrapFinetune`: Distills a prompt-based DSPy program into weight updates (for smaller LMs). The output is a DSPy program that has the same steps, but where each step is conducted by a finetuned model instead of a prompted LM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Ensemble`: Ensembles a set of DSPy programs and either uses the full set or randomly samples a subset into a single program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading optimizer output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a program: The resulting file is in plain-text JSON format. It contains all the parameters and steps in the source program. You can always read it and see what the optimizer generated.\n",
    "\n",
    "You can add save_field_meta to additionally save the list of fields with the keys, name, field_type, description, and prefix with: `optimized_program.save(YOUR_SAVE_PATH, save_field_meta=True)`.\n",
    "\n",
    "```python\n",
    "optimized_program.save(YOUR_SAVE_PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a program:\n",
    "\n",
    "```python\n",
    "loaded_program = YOUR_PROGRAM_CLASS()\n",
    "loaded_program.load(path=YOUR_SAVE_PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions\n",
    "\n",
    "Notes:\n",
    "- Why and What is DSPy Assertions?\n",
    "    - Despite the growth of techniques like fine-tuning or “prompt engineering”, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy's modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints.\n",
    "    - To address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.\n",
    "- `dspy.Assert` and `dspy.Suggest` API\n",
    "    - when a constraint is not met:\n",
    "        - Backtracking Mechanism: An under-the-hood backtracking is initiated, offering the model a chance to self-refine and proceed, which is done through\n",
    "        - Dynamic Signature Modification: internally modifying your DSPy program’s Signature by adding the following fields:\n",
    "            - Past Output: your model's past output that did not pass the validation_fn\n",
    "            - Instruction: your user-defined feedback message on what went wrong and what possibly to fix\n",
    "        - If the error continues past the `max_backtracking_attempts`, then `dspy.Assert` will halt the pipeline execution, altering you with an `dspy.AssertionError`. This ensures your program doesn't continue executing with “bad” LM behavior and immediately highlights sample failure outputs for user assessment.\n",
    "    - `dspy.Suggest` vs. `dspy.Assert`:\n",
    "        - `dspy.Suggest` on the other hand offers a softer approach. It maintains the same retry backtracking as `dspy.Assert` but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the `max_backtracking_attempts`, `dspy.Suggest` will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a \"best-effort\" manner without halting execution.\n",
    "        - `dspy.Suggest` are best utilized as \"helpers\" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.\n",
    "        - `dspy.Assert` are recommended during the development stage as \"checkers\" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.\n",
    "- It is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/deep-dive/assertions/\n",
    "- source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/assertions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case: Including Assertions in DSPy Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SimplifiedBaleenAssertions(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=2, max_hops=2):\n",
    "        super().__init__()\n",
    "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        prev_queries = [question]\n",
    "\n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "\n",
    "            dspy.Suggest(\n",
    "                len(query) <= 100,\n",
    "                \"Query should be short and less than 100 characters\",\n",
    "                target_module=self.generate_query\n",
    "            )\n",
    "\n",
    "            dspy.Suggest(\n",
    "                validate_query_distinction_local(prev_queries, query),\n",
    "                \"Query should be distinct from: \"\n",
    "                + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(prev_queries)),\n",
    "                target_module=self.generate_query\n",
    "            )\n",
    "\n",
    "            prev_queries.append(query)\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        if all_queries_distinct(prev_queries):\n",
    "            self.passed_suggestions += 1\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        pred = dspy.Prediction(context=context, answer=pred.answer)\n",
    "        return pred\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dspy.primitives.assertions import assert_transform_module, backtrack_handler\n",
    "\n",
    "baleen_with_assertions = assert_transform_module(SimplifiedBaleenAssertions(), backtrack_handler)\n",
    "\n",
    "# backtrack_handler is parameterized over a few settings for the backtracking mechanism\n",
    "# To change the number of max retry attempts, you can do\n",
    "baleen_with_assertions_retry_once = assert_transform_module(SimplifiedBaleenAssertions(), \n",
    "    functools.partial(backtrack_handler, max_backtracks=1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assertion-Driven Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSPy Assertions work with optimizations that DSPy offers, particularly with `BootstrapFewShotWithRandomSearch`\n",
    "\n",
    "- Compilation with Assertions This includes assertion-driven example bootstrapping and counterexample bootstrapping during compilation. The teacher model for bootstrapping few-shot demonstrations can make use of DSPy Assertions to offer robust bootstrapped examples for the student model to learn from during inference. In this setting, the student model does not perform assertion aware optimizations (backtracking and retry) during inference.\n",
    "- Compilation + Inference with Assertions -This includes assertion-driven optimizations in both compilation and inference. Now the teacher model offers assertion-driven examples but the student can further optimize with assertions of its own during inference time.\n",
    "\n",
    "\n",
    "```python\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=validate_context_and_answer_and_hops,\n",
    "    max_bootstrapped_demos=max_bootstrapped_demos,\n",
    "    num_candidate_programs=6,\n",
    ")\n",
    "\n",
    "#Compilation with Assertions\n",
    "compiled_with_assertions_baleen = teleprompter.compile(student = baleen, teacher = baleen_with_assertions, trainset = trainset, valset = devset)\n",
    "\n",
    "#Compilation + Inference with Assertions\n",
    "compiled_baleen_with_assertions = teleprompter.compile(student=baleen_with_assertions, teacher = baleen_with_assertions, trainset=trainset, valset=devset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define your task\n",
    "\n",
    "- Expected input/output behavior\n",
    "- Quality and Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define your pipeline\n",
    "\n",
    "- simple chaing-of-thought/retrieval/tool use, like a calculator\n",
    "- is there a typical workflow for solving your problem in multiple well-defined steps or is it more open-ended?\n",
    "- start simple and let the next few steps guide any complexity you will add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explore a few examples\n",
    "\n",
    "- consider using a large and powerful LM or a couple of different LMs, just to understand what's possible\n",
    "- you're stil using your pipeline zero-shot, so it will be far from perfect\n",
    "- understanding where things go wrong in zero-shot usage will go a long way\n",
    "- record the interesting (both easy and hard) examples you try; even if you don't have labels, simply tracking the inputs you tried will be useful for DSPy optimizers below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define your data\n",
    "\n",
    "- Time to more formally declare your training and validation data for DSPy evaluation and optimization\n",
    "- You can use DSPy optimizers usefully with as few as 10 examples, but having 50-100 examples (or even better, 300-500 examples) goes a long way.\n",
    "- If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels, so it's not that hard.\n",
    "- chances are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define your metric\n",
    "\n",
    "- For simple tasks, this could be just \"accuracy\" or \"exact match\" or \"F1 score\". This may be the case for simple classification or short-form QA tasks.\n",
    "- However, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).\n",
    "-Getting this right on the first try is unlikely, but you should start with something simple and iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Collect preliminary \"zero-shot\" evaluations\n",
    "\n",
    "- Now that you have some data and a metric, run evaluation on your pipeline before any optimizer runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compile with a DSPy optimizer\n",
    "\n",
    "- In general, you don't need to have labels for your pipeline steps, but your data examples need to have input values and whatever labels your metric requires (e.g., no labels if your metric is reference-free, but final output labels otherwise in most cases).\n",
    "- Here's the general guidance on getting started:\n",
    "    - If you have very little data, e.g. 10 examples of your task, use `BootstrapFewShot`\n",
    "    - If you have slightly more data, e.g. 50 examples of your task, use `BootstrapFewShotWithRandomSearch`.\n",
    "    - If you have more data than that, e.g. 300 examples or more, use `MIPRO`.\n",
    "    - If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, compile that down to a small LM with `BootstrapFinetune`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Iterate\n",
    "\n",
    "- Some questions you can ask yourself:\n",
    "    - Did you define your task well?\n",
    "    - Do you need to collect (or find online) more data for your problem?\n",
    "    - Do you want to update your metric?\n",
    "    - And do you want to use a more sophisticated optimizer?\n",
    "    - Do you need to consider advanced features like DSPy Assertions?\n",
    "    - Or, perhaps most importantly, do you want to add some more complexity or steps in your DSPy program itself?\n",
    "    - Do you want to use multiple optimizers in a sequence?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "- [The ImageNet Moment of DSPy from Professor Bo Wang’s Lab](https://x.com/lateinteraction/status/1783990747257360779)\n",
    "    - [WangLab at MEDIQA-CORR 2024: Optimized LLM-based Programs for Medical Error Detection and Correction\n",
    "](https://arxiv.org/abs/2404.14544)\n",
    "- [STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking](https://github.com/stanford-oval/storm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "- reference: https://github.com/stanfordnlp/dspy/blob/main/docs/docs/roadmap.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://dspy-docs.vercel.app/intro/\n",
    "- DSPy cheatsheet: https://dspy-docs.vercel.app/cheatsheet\n",
    "- GitHub: https://github.com/stanfordnlp/dspy\n",
    "- Introduction by Author\n",
    "    - Video: https://www.youtube.com/live/JEMYuzrKLUw?si=iwAzhwobN52zgIZ_\n",
    "    - Slides: https://llmagents-learning.org/slides/dspy_lec.pdf\n",
    "- Intro to DSPy: Goodbye Prompting, Hello Programming!: https://towardsdatascience.com/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9\n",
    "- Papers\n",
    "    - [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)\n",
    "    - [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)\n",
    "    - [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)\n",
    "    - [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-101-bGSlOmMx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
