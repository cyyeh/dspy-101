{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Concepts](#concepts)\n",
    "- [Building Blocks](#building-blocks)\n",
    "    - [Language Models](#language-models)\n",
    "    - [Signatures](#signatures)\n",
    "    - [Modules](#modules)\n",
    "    - [Data](#data)\n",
    "    - [Metrics](#metrics)\n",
    "    - [Optimizers](#optimizers)\n",
    "    - [Assertions](#assertions)\n",
    "    - [Type Predictors](#type-predictors)\n",
    "- [Workflow](#workflow)\n",
    "- [Examples](#examples)\n",
    "- [Roadmap](#roadmap)\n",
    "- [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "Notes:\n",
    "- Earlier versions of DSPy involved tens of clients for different LM providers.(deprecated, and will be removed in DSPy 2.6) Starting from 2.5, use `dspy.LM` instead(using litellm under the hood)\n",
    "- Inspecting history\n",
    "- Adapters\n",
    "    - DSPy 2.5 introduces **Adapters** as a layer between Signatures and LMs, responsible for formatting these pieces (Signature I/O fields, instructions, and examples) as well as generating and parsing the outputs.\n",
    "- Using `dspy.configure` and `dspy.context` is thread-safe!\n",
    "- By default LMs in DSPy are cached. If you repeat the same call, you will get the same outputs. But you can turn off caching by setting `cache=False` while declaring `dspy.LM` object\n",
    "- Any OpenAI-compatible endpoint is easy to set up with an `openai/` prefix as well.\n",
    "\n",
    "\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/building-blocks/1-language_models/\n",
    "- source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/clients/lm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"gpt-4o-mini\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directly calling the LLM(not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! How can I assist you today?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chat LLMs\n",
    "lm(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": \"What is 2+2?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the llm with DSPy signatures and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient information to determine the number of floors in the castle David Gregory inherited.\n"
     ]
    }
   ],
   "source": [
    "# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# Run with the default LM configured with `dspy.configure` above.\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using multiple LLMs at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini: Insufficient information to determine the number of floors in the castle David Gregory inherited.\n",
      "gpt-4o: Unknown\n"
     ]
    }
   ],
   "source": [
    "# Run with the default LM configured above, i.e. GPT-4o-mini\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print('gpt-4o-mini:', response.answer)\n",
    "\n",
    "gpt_4o = dspy.LM(model='gpt-4o', max_tokens=300)\n",
    "\n",
    "# Run with GPT-4o instead\n",
    "with dspy.context(lm=gpt_4o):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "    print('gpt-4o:', response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuring llm attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini = dspy.LM(\n",
    "\t'gpt-4o-mini',\n",
    "\ttemperature=0.9,\n",
    "\tmax_tokens=3000,\n",
    "\tstop=None,\n",
    "\tcache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using locally hosted LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_port = 11434 \n",
    "ollama_url = f\"http://localhost:{ollama_port}\"\n",
    "ollama_llm = dspy.LM(model=\"ollama/llama3.2:1b\", api_base=ollama_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting llm output and usage metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: hello!\n",
      "messages: [{'role': 'user', 'content': 'hello!'}]\n",
      "kwargs: {'temperature': 0.0, 'max_tokens': 1000}\n",
      "response: ModelResponse(id='chatcmpl-AP1dLg4xby728IqZqLU9RzSRpNLsB', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant', tool_calls=None, function_call=None))], created=1730527843, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_0ba0d124f1', usage=Usage(completion_tokens=9, prompt_tokens=9, total_tokens=18, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), service_tier=None)\n",
      "outputs: ['Hello! How can I assist you today?']\n",
      "usage: {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), 'prompt_tokens_details': PromptTokensDetails(audio_tokens=None, cached_tokens=0)}\n",
      "cost: 6.75e-06\n",
      "timestamp: 2024-11-02T14:21:43.808549\n",
      "uuid: 3ebd4e0b-b76a-49f3-8240-ea5107064865\n",
      "model: gpt-4o-mini\n",
      "model_type: chat\n"
     ]
    }
   ],
   "source": [
    "for k, v in lm.history[-1].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://dspy-docs.vercel.app/intro/\n",
    "- GitHub: https://github.com/stanfordnlp/dspy\n",
    "- Introduction by Author\n",
    "    - Video: https://www.youtube.com/live/JEMYuzrKLUw?si=iwAzhwobN52zgIZ_\n",
    "    - Slides: https://llmagents-learning.org/slides/dspy_lec.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-101-bGSlOmMx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
