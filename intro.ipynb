{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Concepts](#concepts)\n",
    "- [Building Blocks](#building-blocks)\n",
    "    - [Language Models](#language-models)\n",
    "    - [Signatures](#signatures)\n",
    "    - [Modules](#modules)\n",
    "    - [Data](#data)\n",
    "    - [Metrics](#metrics)\n",
    "    - [Optimizers](#optimizers)\n",
    "    - [Assertions](#assertions)\n",
    "    - [Type Predictors](#type-predictors)\n",
    "- [Workflow](#workflow)\n",
    "- [Examples](#examples)\n",
    "- [Roadmap](#roadmap)\n",
    "- [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "Notes:\n",
    "- Earlier versions of DSPy involved tens of clients for different LM providers.(deprecated, and will be removed in DSPy 2.6) Starting from 2.5, use `dspy.LM` instead(using litellm under the hood)\n",
    "- Inspecting history\n",
    "- Adapters\n",
    "    - DSPy 2.5 introduces **Adapters** as a layer between Signatures and LMs, responsible for formatting these pieces (Signature I/O fields, instructions, and examples) as well as generating and parsing the outputs.\n",
    "- Using `dspy.configure` and `dspy.context` is thread-safe!\n",
    "- By default LMs in DSPy are cached. If you repeat the same call, you will get the same outputs. But you can turn off caching by setting `cache=False` while declaring `dspy.LM` object\n",
    "- Any OpenAI-compatible endpoint is easy to set up with an `openai/` prefix as well.\n",
    "\n",
    "\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/building-blocks/1-language_models/\n",
    "- source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/clients/lm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"gpt-4o-mini\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directly calling the LLM(not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! How can I assist you today?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chat LLMs\n",
    "lm(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": \"What is 2+2?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the llm with DSPy signatures and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient information to determine the number of floors in the castle David Gregory inherited.\n"
     ]
    }
   ],
   "source": [
    "# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# Run with the default LM configured with `dspy.configure` above.\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using multiple LLMs at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini: Insufficient information to determine the number of floors in the castle David Gregory inherited.\n",
      "gpt-4o: Unknown\n"
     ]
    }
   ],
   "source": [
    "# Run with the default LM configured above, i.e. GPT-4o-mini\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print('gpt-4o-mini:', response.answer)\n",
    "\n",
    "gpt_4o = dspy.LM(model='gpt-4o', max_tokens=300)\n",
    "\n",
    "# Run with GPT-4o instead\n",
    "with dspy.context(lm=gpt_4o):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "    print('gpt-4o:', response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuring llm attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini = dspy.LM(\n",
    "\t'gpt-4o-mini',\n",
    "\ttemperature=0.9,\n",
    "\tmax_tokens=3000,\n",
    "\tstop=None,\n",
    "\tcache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using locally hosted LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_port = 11434 \n",
    "ollama_url = f\"http://localhost:{ollama_port}\"\n",
    "ollama_llm = dspy.LM(model=\"ollama/llama3.2:1b\", api_base=ollama_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting llm output and usage metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: None\n",
      "messages: [{'role': 'system', 'content': 'Your input fields are:\\n1. `question` (str)\\n\\nYour output fields are:\\n1. `reasoning` (str)\\n2. `answer` (str)\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\n\\nIn adhering to this structure, your objective is: \\n        Given the fields `question`, produce the fields `answer`.'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nHow many floors are in the castle David Gregory inherited?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}]\n",
      "kwargs: {'temperature': 0.0, 'max_tokens': 1000}\n",
      "response: ModelResponse(id='chatcmpl-AP1gFYUpR7PAotKStTcGdeP89I0dM', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\\n\\n[[ ## answer ## ]]\\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None))], created=1730528023, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_0ba0d124f1', usage=Usage(completion_tokens=97, prompt_tokens=174, total_tokens=271, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), service_tier=None)\n",
      "outputs: ['[[ ## reasoning ## ]]\\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\\n\\n[[ ## answer ## ]]\\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\\n\\n[[ ## completed ## ]]']\n",
      "usage: {'completion_tokens': 97, 'prompt_tokens': 174, 'total_tokens': 271, 'completion_tokens_details': CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), 'prompt_tokens_details': PromptTokensDetails(audio_tokens=None, cached_tokens=0)}\n",
      "cost: 8.429999999999999e-05\n",
      "timestamp: 2024-11-02T14:31:31.584452\n",
      "uuid: 3772cf83-ec34-49e7-b8a6-ae90b7ab2992\n",
      "model: gpt-4o-mini\n",
      "model_type: chat\n"
     ]
    }
   ],
   "source": [
    "for k, v in lm.history[-1].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating custom LLM class (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom LM class is quite straightforward in DSPy. You can inherit from the dspy.LM class or create a new class with a similar interface. You'll need to implement/override these three methods:\n",
    "\n",
    "- `__init__`: Initialize the LM with the given model and other keyword arguments.\n",
    "- `__call__`: Call the LM with the given input prompt and return a list of string outputs.\n",
    "- `inspect_history`: The history of interactions with the LM. This is optional but is needed by some optimizers in DSPy.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import dspy\n",
    "import google.generativeai as genai\n",
    "\n",
    "class GeminiLM(dspy.LM):\n",
    "    def __init__(self, model, api_key=None, endpoint=None, **kwargs):\n",
    "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"] or api_key)\n",
    "\n",
    "        self.endpoint = endpoint\n",
    "        self.history = []\n",
    "\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "\n",
    "    def __call__(self, prompt=None, messages=None, **kwargs):\n",
    "        # Custom chat model working for text completion model\n",
    "        prompt = '\\n\\n'.join([x['content'] for x in messages] + ['BEGIN RESPONSE:'])\n",
    "\n",
    "        completions = self.model.generate_content(prompt)\n",
    "        self.history.append({\"prompt\": prompt, \"completions\": completions})\n",
    "\n",
    "        # Must return a list of strings\n",
    "        return [completions.candidates[0].content.parts[0].text]\n",
    "\n",
    "    def inspect_history(self):\n",
    "        for interaction in self.history:\n",
    "            print(f\"Prompt: {interaction['prompt']} -> Completions: {interaction['completions']}\")\n",
    "\n",
    "lm = GeminiLM(\"gemini-1.5-flash\", temperature=0)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "qa = dspy.ChainOfThought(\"question->answer\")\n",
    "qa(question=\"What is the capital of France?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Structured LLM output with Adapters (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signatures\n",
    "\n",
    "Notes:\n",
    "- inline-based signature prompt creation\n",
    "![](https://dspy-docs.vercel.app/deep-dive/signature/img/prompt_creation.png)\n",
    "- class-based signature prompt creation\n",
    "![](https://dspy-docs.vercel.app/deep-dive/signature/img/class_based_prompt_creation.png)\n",
    "\n",
    "References:\n",
    "- documentation\n",
    "    - https://dspy-docs.vercel.app/building-blocks/2-signatures/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/understanding-signatures/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/\n",
    "- source code: https://github.com/stanfordnlp/dspy/tree/main/dspy/signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.\n",
    "\n",
    "A signature is a declarative specification of input/output behavior of a DSPy module. Signatures allow you to tell the LM what it needs to do, rather than specify how we should ask the LM to do it.\n",
    "\n",
    "You're probably familiar with function signatures, which specify the input and output arguments and their types. DSPy signatures are similar, but the differences are that:\n",
    "\n",
    "- While typical function signatures just describe things, DSPy Signatures define and control the behavior of modules.\n",
    "- The field names matter in DSPy Signatures. You express semantic roles in plain English: a question is different from an answer, a sql_query is different from python_code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should I use a DSPy Signature?\n",
    "\n",
    "tl;dr For modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes).\n",
    "\n",
    "Long Answer: Most people coerce LMs to do tasks by hacking long, brittle prompts. Or by collecting/generating data for fine-tuning.\n",
    "\n",
    "Writing signatures is far more modular, adaptive, and reproducible than hacking at prompts or finetunes. The DSPy compiler will figure out how to build a highly-optimized prompt for your LM (or finetune your small LM) for your signature, on your data, and within your pipeline. In many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things and tune the metrics directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline DSPy Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signatures can be defined as a short string, with argument names that define semantic roles for inputs/outputs.\n",
    "\n",
    "1. `Question Answering: \"question -> answer\"`\n",
    "2. `Sentiment Classification: \"sentence -> sentiment\"`\n",
    "3. `Summarization: \"document -> summary\"`\n",
    "\n",
    "Your signatures can also have multiple input/output fields.\n",
    "\n",
    "1. `Retrieval-Augmented Question Answering: \"context, question -> answer\"`\n",
    "2. `Multiple-Choice Question Answering with Reasoning: \"question, choices -> reasoning, selection\"`\n",
    "\n",
    "Tip: For fields, any valid variable names work! Field names should be semantically meaningful, but start simple and don't prematurely optimize keywords! Leave that kind of hacking to the DSPy compiler. For example, for summarization, it's probably fine to say \"document -> summary\", \"text -> gist\", or \"long_context -> tldr\".\n",
    "\n",
    "Notes:\n",
    "- Many DSPy modules (except `dspy.Predict`) return auxiliary information by expanding your signature under the hood. For example, `dspy.ChainOfThought` also adds a rationale field that includes the LM's reasoning before it generates the output summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n",
    "\n",
    "classify = dspy.Predict('sentence -> sentiment')\n",
    "classify(sentence=sentence).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee, a 21-year-old footballer, made seven appearances for the Hammers, scoring once in a Europa League match. He had loan spells at Blackpool and Colchester United, scoring twice for Colchester, but could not prevent their relegation. His contract details with the Tykes remain undisclosed.\n"
     ]
    }
   ],
   "source": [
    "# Example from the XSum dataset.\n",
    "document = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\n",
    "\n",
    "summarize = dspy.ChainOfThought('document -> summary')\n",
    "response = summarize(document=document)\n",
    "\n",
    "print(response.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-based DSPy Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some advanced tasks, you need more verbose signatures. This is typically to:\n",
    "\n",
    "1. Clarify something about the nature of the task (expressed below as a docstring).\n",
    "2. Supply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.\n",
    "3. Supply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.\n",
    "\n",
    "Tips:\n",
    "- There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs).\n",
    "- How `Predict` works:\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/#how-predict-works\n",
    "    - source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/predict/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment='fear'\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Emotion(dspy.Signature):\n",
    "    \"\"\"Classify emotion among sadness, joy, love, anger, fear, surprise.\"\"\"\n",
    "\n",
    "    sentence = dspy.InputField()\n",
    "    sentiment = dspy.OutputField()\n",
    "\n",
    "sentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\n",
    "\n",
    "classify = dspy.Predict(Emotion)\n",
    "classify(sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using signatures to build modules & compiling them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While signatures are convenient for prototyping with structured inputs/outputs, that's not the main reason to use them!\n",
    "\n",
    "You should compose multiple signatures into bigger DSPy modules and compile these modules into optimized prompts and finetunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules\n",
    "\n",
    "Notes:\n",
    "- A DSPy module is a building block for programs that use LMs.\n",
    "    - Each built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any DSPy Signature.\n",
    "    - A DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\n",
    "    - Multiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\n",
    "- What other DSPy modules are there? How can I use them?\n",
    "    - The others are very similar. They mainly change the internal behavior with which your signature is implemented!\n",
    "        - `dspy.Predict`: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
    "        - `dspy.ChainOfThought`: Teaches the LM to think step-by-step before committing to the signature's response.\n",
    "        - `dspy.ProgramOfThought`: Teaches the LM to output code, whose execution results will dictate the response.\n",
    "        - `dspy.ReAct`: An agent that can use tools to implement the given signature.\n",
    "        - `dspy.MultiChainComparison`: Can compare multiple outputs from ChainOfThought to produce a final prediction.\n",
    "    - We also have some function-style modules:\n",
    "        - `dspy.majority`: Can do basic voting to return the most popular response from a set of predictions.\n",
    "- How do I compose multiple modules into a bigger program?\n",
    "    - DSPy is just Python code that uses modules in any control flow you like. (There's some magic internally at `compile` time to trace your LM calls.)\n",
    "    - This means that, you can just call the modules freely. No weird abstractions for chaining calls.\n",
    "    - This is basically PyTorch's design approach for define-by-run / dynamic computation graphs. Refer to the intro tutorials for examples.\n",
    "\n",
    "References:\n",
    "- documentation:\n",
    "    - https://dspy-docs.vercel.app/building-blocks/3-modules/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/modules/guide\n",
    "- source code:\n",
    "    - https://github.com/stanfordnlp/dspy/blob/main/dspy/primitives/module.py\n",
    "    - https://github.com/stanfordnlp/dspy/tree/main/dspy/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval approaches, allowing for high accuracy in document retrieval while maintaining fast performance through late interaction.',\n",
       " 'One great aspect of the ColBERT retrieval model is its use of late interaction, which allows it to efficiently combine the strengths of dense and traditional retrieval methods, resulting in fast and accurate search performance.',\n",
       " 'One great aspect of the ColBERT retrieval model is its efficient use of contextual embeddings, enabling high-quality information retrieval while maintaining fast performance through a late interaction mechanism.',\n",
       " 'One great thing about the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval techniques, allowing for fast and accurate document retrieval through late interaction mechanisms.',\n",
       " 'One great aspect of the ColBERT retrieval model is its ability to combine efficiency with high-quality relevance scoring through a late interaction mechanism, making it well-suited for fast and accurate information retrieval tasks.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What's something great about the ColBERT retrieval model?\"\n",
    "\n",
    "# 1) Declare with a signature, and pass some config.\n",
    "classify = dspy.ChainOfThought('question -> answer', n=5)\n",
    "\n",
    "# 2) Call with input argument.\n",
    "response = classify(question=question)\n",
    "\n",
    "# 3) Access the outputs.\n",
    "response.completions.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The ColBERT retrieval model stands out because it efficiently combines the strengths of dense and sparse retrieval methods. It uses late interaction for fast retrieval while preserving the contextual information from dense embeddings. This allows it to achieve high accuracy and relevance in document retrieval tasks while maintaining speed, making it suitable for large-scale applications. Additionally, its ability to leverage pre-trained language models enhances its performance in understanding and retrieving relevant documents based on complex queries.',\n",
       "    answer='One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval approaches, allowing for high accuracy in document retrieval while maintaining fast performance through late interaction.',\n",
       "    completions=Completions(...)\n",
       ") (4 completions omitted)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completions(\n",
       "    reasoning=['The ColBERT retrieval model stands out because it efficiently combines the strengths of dense and sparse retrieval methods. It uses late interaction for fast retrieval while preserving the contextual information from dense embeddings. This allows it to achieve high accuracy and relevance in document retrieval tasks while maintaining speed, making it suitable for large-scale applications. Additionally, its ability to leverage pre-trained language models enhances its performance in understanding and retrieving relevant documents based on complex queries.', 'The ColBERT retrieval model is notable for its ability to efficiently combine the benefits of both traditional dense retrieval and modern transformer-based architectures. One of the great features of ColBERT is its use of late interaction, allowing it to process queries and documents separately and then interact them at retrieval time. This approach significantly speeds up the retrieval process while maintaining high performance in terms of relevance and accuracy. Additionally, it leverages contextual embeddings, which helps in understanding the nuances of language better than earlier models. This balance of efficiency and effectiveness makes ColBERT a powerful tool for information retrieval tasks.', 'ColBERT is a retrieval model that enhances the efficiency and effectiveness of information retrieval tasks by utilizing a two-stage approach. One of the great aspects of ColBERT is its ability to efficiently leverage the power of contextual embeddings (like those from BERT) while maintaining fast retrieval speeds. It does this by using a late interaction mechanism, allowing it to compute relevance scores between query and document representations without requiring full pairwise comparison. This results in improved retrieval performance while still being scalable to large datasets, making it particularly well-suited for applications in search engines and large-scale information retrieval systems.', 'The ColBERT retrieval model is notable for its efficiency and effectiveness in document retrieval tasks. One of its key strengths is that it combines the benefits of dense and sparse retrieval methods. By using late interaction mechanisms, ColBERT allows for the fast retrieval of relevant documents while still capturing the rich semantic information of queries and documents in a compact manner. This results in improved retrieval speeds without sacrificing accuracy, making it particularly suitable for large-scale information retrieval tasks.', \"The ColBERT retrieval model stands out because it effectively balances efficiency and effectiveness in information retrieval tasks. It utilizes a late interaction mechanism that allows it to process large collections of documents quickly while still maintaining high-quality relevance scoring. This approach makes it suitable for applications requiring both speed and accuracy, such as search engines and recommendation systems. Additionally, ColBERT leverages BERT's contextual embeddings, enhancing its understanding of the semantic relationships between queries and documents.\"],\n",
       "    answer=['One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval approaches, allowing for high accuracy in document retrieval while maintaining fast performance through late interaction.', 'One great aspect of the ColBERT retrieval model is its use of late interaction, which allows it to efficiently combine the strengths of dense and traditional retrieval methods, resulting in fast and accurate search performance.', 'One great aspect of the ColBERT retrieval model is its efficient use of contextual embeddings, enabling high-quality information retrieval while maintaining fast performance through a late interaction mechanism.', 'One great thing about the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval techniques, allowing for fast and accurate document retrieval through late interaction mechanisms.', 'One great aspect of the ColBERT retrieval model is its ability to combine efficiency with high-quality relevance scoring through a late interaction mechanism, making it well-suited for fast and accurate information retrieval tasks.']\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://dspy-docs.vercel.app/intro/\n",
    "- GitHub: https://github.com/stanfordnlp/dspy\n",
    "- Introduction by Author\n",
    "    - Video: https://www.youtube.com/live/JEMYuzrKLUw?si=iwAzhwobN52zgIZ_\n",
    "    - Slides: https://llmagents-learning.org/slides/dspy_lec.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-101-bGSlOmMx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
