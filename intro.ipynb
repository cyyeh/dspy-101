{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Concepts](#concepts)\n",
    "- [Building Blocks](#building-blocks)\n",
    "    - [Language Models](#language-models)\n",
    "    - [Signatures](#signatures)\n",
    "    - [Modules](#modules)\n",
    "    - [Data](#data)\n",
    "    - [Metrics](#metrics)\n",
    "    - [Optimizers](#optimizers)\n",
    "    - [Assertions](#assertions)\n",
    "    - [Type Predictors](#type-predictors)\n",
    "- [Workflow](#workflow)\n",
    "- [Examples](#examples)\n",
    "- [Roadmap](#roadmap)\n",
    "- [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "Notes:\n",
    "- Earlier versions of DSPy involved tens of clients for different LM providers.(deprecated, and will be removed in DSPy 2.6) Starting from 2.5, use `dspy.LM` instead(using litellm under the hood)\n",
    "- Inspecting history\n",
    "- Adapters\n",
    "    - DSPy 2.5 introduces **Adapters** as a layer between Signatures and LMs, responsible for formatting these pieces (Signature I/O fields, instructions, and examples) as well as generating and parsing the outputs.\n",
    "- Using `dspy.configure` and `dspy.context` is thread-safe!\n",
    "- By default LMs in DSPy are cached. If you repeat the same call, you will get the same outputs. But you can turn off caching by setting `cache=False` while declaring `dspy.LM` object\n",
    "- Any OpenAI-compatible endpoint is easy to set up with an `openai/` prefix as well.\n",
    "\n",
    "\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/building-blocks/1-language_models/\n",
    "- source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/clients/lm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"gpt-4o-mini\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directly calling the LLM(not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! How can I assist you today?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chat LLMs\n",
    "lm(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": \"What is 2+2?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the llm with DSPy signatures and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient information to determine the number of floors in the castle David Gregory inherited.\n"
     ]
    }
   ],
   "source": [
    "# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# Run with the default LM configured with `dspy.configure` above.\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using multiple LLMs at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini: Insufficient information to determine the number of floors in the castle David Gregory inherited.\n",
      "gpt-4o: Unknown\n"
     ]
    }
   ],
   "source": [
    "# Run with the default LM configured above, i.e. GPT-4o-mini\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print('gpt-4o-mini:', response.answer)\n",
    "\n",
    "gpt_4o = dspy.LM(model='gpt-4o', max_tokens=300)\n",
    "\n",
    "# Run with GPT-4o instead\n",
    "with dspy.context(lm=gpt_4o):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "    print('gpt-4o:', response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuring llm attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini = dspy.LM(\n",
    "\t'gpt-4o-mini',\n",
    "\ttemperature=0.9,\n",
    "\tmax_tokens=3000,\n",
    "\tstop=None,\n",
    "\tcache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using locally hosted LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_port = 11434 \n",
    "ollama_url = f\"http://localhost:{ollama_port}\"\n",
    "ollama_llm = dspy.LM(model=\"ollama/llama3.2:1b\", api_base=ollama_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting llm output and usage metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: None\n",
      "messages: [{'role': 'system', 'content': 'Your input fields are:\\n1. `question` (str)\\n\\nYour output fields are:\\n1. `reasoning` (str)\\n2. `answer` (str)\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\n\\nIn adhering to this structure, your objective is: \\n        Given the fields `question`, produce the fields `answer`.'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nHow many floors are in the castle David Gregory inherited?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}]\n",
      "kwargs: {'temperature': 0.0, 'max_tokens': 1000}\n",
      "response: ModelResponse(id='chatcmpl-AP1gFYUpR7PAotKStTcGdeP89I0dM', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\\n\\n[[ ## answer ## ]]\\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None))], created=1730528023, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_0ba0d124f1', usage=Usage(completion_tokens=97, prompt_tokens=174, total_tokens=271, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), service_tier=None)\n",
      "outputs: ['[[ ## reasoning ## ]]\\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\\n\\n[[ ## answer ## ]]\\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\\n\\n[[ ## completed ## ]]']\n",
      "usage: {'completion_tokens': 97, 'prompt_tokens': 174, 'total_tokens': 271, 'completion_tokens_details': CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), 'prompt_tokens_details': PromptTokensDetails(audio_tokens=None, cached_tokens=0)}\n",
      "cost: 8.429999999999999e-05\n",
      "timestamp: 2024-11-02T14:31:31.584452\n",
      "uuid: 3772cf83-ec34-49e7-b8a6-ae90b7ab2992\n",
      "model: gpt-4o-mini\n",
      "model_type: chat\n"
     ]
    }
   ],
   "source": [
    "for k, v in lm.history[-1].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://dspy-docs.vercel.app/intro/\n",
    "- GitHub: https://github.com/stanfordnlp/dspy\n",
    "- Introduction by Author\n",
    "    - Video: https://www.youtube.com/live/JEMYuzrKLUw?si=iwAzhwobN52zgIZ_\n",
    "    - Slides: https://llmagents-learning.org/slides/dspy_lec.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-101-bGSlOmMx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
