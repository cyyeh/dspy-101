{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Concepts](#concepts)\n",
    "- [Building Blocks](#building-blocks)\n",
    "    - [Language Models](#language-models)\n",
    "    - [Signatures](#signatures)\n",
    "    - [Modules](#modules)\n",
    "    - [Data](#data)\n",
    "    - [Metrics](#metrics)\n",
    "    - [Optimizers](#optimizers)\n",
    "    - [Assertions](#assertions)\n",
    "    - [Type Predictors](#type-predictors)\n",
    "- [Workflow](#workflow)\n",
    "- [Examples](#examples)\n",
    "- [Roadmap](#roadmap)\n",
    "- [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "Notes:\n",
    "- Earlier versions of DSPy involved tens of clients for different LM providers.(deprecated, and will be removed in DSPy 2.6) Starting from 2.5, use `dspy.LM` instead(using litellm under the hood)\n",
    "- Inspecting history\n",
    "- Adapters\n",
    "    - DSPy 2.5 introduces **Adapters** as a layer between Signatures and LMs, responsible for formatting these pieces (Signature I/O fields, instructions, and examples) as well as generating and parsing the outputs.\n",
    "- Using `dspy.configure` and `dspy.context` is thread-safe!\n",
    "- By default LMs in DSPy are cached. If you repeat the same call, you will get the same outputs. But you can turn off caching by setting `cache=False` while declaring `dspy.LM` object\n",
    "- Any OpenAI-compatible endpoint is easy to set up with an `openai/` prefix as well.\n",
    "\n",
    "\n",
    "\n",
    "References:\n",
    "- documentation: https://dspy-docs.vercel.app/building-blocks/1-language_models/\n",
    "- source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/clients/lm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"gpt-4o-mini\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directly calling the LLM(not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! How can I assist you today?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chat LLMs\n",
    "lm(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": \"What is 2+2?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the llm with DSPy signatures and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient information to determine the number of floors in the castle David Gregory inherited.\n"
     ]
    }
   ],
   "source": [
    "# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# Run with the default LM configured with `dspy.configure` above.\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using multiple LLMs at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini: Insufficient information to determine the number of floors in the castle David Gregory inherited.\n",
      "gpt-4o: Unknown\n"
     ]
    }
   ],
   "source": [
    "# Run with the default LM configured above, i.e. GPT-4o-mini\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print('gpt-4o-mini:', response.answer)\n",
    "\n",
    "gpt_4o = dspy.LM(model='gpt-4o', max_tokens=300)\n",
    "\n",
    "# Run with GPT-4o instead\n",
    "with dspy.context(lm=gpt_4o):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "    print('gpt-4o:', response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuring llm attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini = dspy.LM(\n",
    "\t'gpt-4o-mini',\n",
    "\ttemperature=0.9,\n",
    "\tmax_tokens=3000,\n",
    "\tstop=None,\n",
    "\tcache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using locally hosted LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_port = 11434 \n",
    "ollama_url = f\"http://localhost:{ollama_port}\"\n",
    "ollama_llm = dspy.LM(model=\"ollama/llama3.2:1b\", api_base=ollama_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting llm output and usage metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: None\n",
      "messages: [{'role': 'system', 'content': 'Your input fields are:\\n1. `question` (str)\\n\\nYour output fields are:\\n1. `reasoning` (str)\\n2. `answer` (str)\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\n\\nIn adhering to this structure, your objective is: \\n        Given the fields `question`, produce the fields `answer`.'}, {'role': 'user', 'content': '[[ ## question ## ]]\\nHow many floors are in the castle David Gregory inherited?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}]\n",
      "kwargs: {'temperature': 0.0, 'max_tokens': 1000}\n",
      "response: ModelResponse(id='chatcmpl-AP1gFYUpR7PAotKStTcGdeP89I0dM', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\\n\\n[[ ## answer ## ]]\\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None))], created=1730528023, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_0ba0d124f1', usage=Usage(completion_tokens=97, prompt_tokens=174, total_tokens=271, completion_tokens_details=CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), service_tier=None)\n",
      "outputs: ['[[ ## reasoning ## ]]\\nThe question refers to a specific detail about a fictional or historical castle associated with David Gregory. However, without additional context or information about which David Gregory or which castle is being referenced, it is impossible to provide an accurate answer. The number of floors in a castle can vary widely depending on its design, size, and historical context.\\n\\n[[ ## answer ## ]]\\nInsufficient information to determine the number of floors in the castle David Gregory inherited.\\n\\n[[ ## completed ## ]]']\n",
      "usage: {'completion_tokens': 97, 'prompt_tokens': 174, 'total_tokens': 271, 'completion_tokens_details': CompletionTokensDetails(audio_tokens=None, reasoning_tokens=0), 'prompt_tokens_details': PromptTokensDetails(audio_tokens=None, cached_tokens=0)}\n",
      "cost: 8.429999999999999e-05\n",
      "timestamp: 2024-11-02T14:31:31.584452\n",
      "uuid: 3772cf83-ec34-49e7-b8a6-ae90b7ab2992\n",
      "model: gpt-4o-mini\n",
      "model_type: chat\n"
     ]
    }
   ],
   "source": [
    "for k, v in lm.history[-1].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating custom LLM class (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom LM class is quite straightforward in DSPy. You can inherit from the dspy.LM class or create a new class with a similar interface. You'll need to implement/override these three methods:\n",
    "\n",
    "- `__init__`: Initialize the LM with the given model and other keyword arguments.\n",
    "- `__call__`: Call the LM with the given input prompt and return a list of string outputs.\n",
    "- `inspect_history`: The history of interactions with the LM. This is optional but is needed by some optimizers in DSPy.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import dspy\n",
    "import google.generativeai as genai\n",
    "\n",
    "class GeminiLM(dspy.LM):\n",
    "    def __init__(self, model, api_key=None, endpoint=None, **kwargs):\n",
    "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"] or api_key)\n",
    "\n",
    "        self.endpoint = endpoint\n",
    "        self.history = []\n",
    "\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "\n",
    "    def __call__(self, prompt=None, messages=None, **kwargs):\n",
    "        # Custom chat model working for text completion model\n",
    "        prompt = '\\n\\n'.join([x['content'] for x in messages] + ['BEGIN RESPONSE:'])\n",
    "\n",
    "        completions = self.model.generate_content(prompt)\n",
    "        self.history.append({\"prompt\": prompt, \"completions\": completions})\n",
    "\n",
    "        # Must return a list of strings\n",
    "        return [completions.candidates[0].content.parts[0].text]\n",
    "\n",
    "    def inspect_history(self):\n",
    "        for interaction in self.history:\n",
    "            print(f\"Prompt: {interaction['prompt']} -> Completions: {interaction['completions']}\")\n",
    "\n",
    "lm = GeminiLM(\"gemini-1.5-flash\", temperature=0)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "qa = dspy.ChainOfThought(\"question->answer\")\n",
    "qa(question=\"What is the capital of France?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Structured LLM output with Adapters (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signatures\n",
    "\n",
    "Notes:\n",
    "- inline-based signature prompt creation\n",
    "![](https://dspy-docs.vercel.app/deep-dive/signature/img/prompt_creation.png)\n",
    "- class-based signature prompt creation\n",
    "![](https://dspy-docs.vercel.app/deep-dive/signature/img/class_based_prompt_creation.png)\n",
    "\n",
    "References:\n",
    "- documentation\n",
    "    - https://dspy-docs.vercel.app/building-blocks/2-signatures/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/understanding-signatures/\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/\n",
    "- source code: https://github.com/stanfordnlp/dspy/tree/main/dspy/signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature.\n",
    "\n",
    "A signature is a declarative specification of input/output behavior of a DSPy module. Signatures allow you to tell the LM what it needs to do, rather than specify how we should ask the LM to do it.\n",
    "\n",
    "You're probably familiar with function signatures, which specify the input and output arguments and their types. DSPy signatures are similar, but the differences are that:\n",
    "\n",
    "- While typical function signatures just describe things, DSPy Signatures define and control the behavior of modules.\n",
    "- The field names matter in DSPy Signatures. You express semantic roles in plain English: a question is different from an answer, a sql_query is different from python_code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should I use a DSPy Signature?\n",
    "\n",
    "tl;dr For modular and clean code, in which LM calls can be optimized into high-quality prompts (or automatic finetunes).\n",
    "\n",
    "Long Answer: Most people coerce LMs to do tasks by hacking long, brittle prompts. Or by collecting/generating data for fine-tuning.\n",
    "\n",
    "Writing signatures is far more modular, adaptive, and reproducible than hacking at prompts or finetunes. The DSPy compiler will figure out how to build a highly-optimized prompt for your LM (or finetune your small LM) for your signature, on your data, and within your pipeline. In many cases, we found that compiling leads to better prompts than humans write. Not because DSPy optimizers are more creative than humans, but simply because they can try more things and tune the metrics directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline DSPy Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signatures can be defined as a short string, with argument names that define semantic roles for inputs/outputs.\n",
    "\n",
    "1. `Question Answering: \"question -> answer\"`\n",
    "2. `Sentiment Classification: \"sentence -> sentiment\"`\n",
    "3. `Summarization: \"document -> summary\"`\n",
    "\n",
    "Your signatures can also have multiple input/output fields.\n",
    "\n",
    "1. `Retrieval-Augmented Question Answering: \"context, question -> answer\"`\n",
    "2. `Multiple-Choice Question Answering with Reasoning: \"question, choices -> reasoning, selection\"`\n",
    "\n",
    "Tip: For fields, any valid variable names work! Field names should be semantically meaningful, but start simple and don't prematurely optimize keywords! Leave that kind of hacking to the DSPy compiler. For example, for summarization, it's probably fine to say \"document -> summary\", \"text -> gist\", or \"long_context -> tldr\".\n",
    "\n",
    "Notes:\n",
    "- Many DSPy modules (except `dspy.Predict`) return auxiliary information by expanding your signature under the hood. For example, `dspy.ChainOfThought` also adds a rationale field that includes the LM's reasoning before it generates the output summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"it's a charming and often affecting journey.\"  # example from the SST-2 dataset.\n",
    "\n",
    "classify = dspy.Predict('sentence -> sentiment')\n",
    "classify(sentence=sentence).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee, a 21-year-old footballer, made seven appearances for the Hammers, scoring once in a Europa League match. He had loan spells at Blackpool and Colchester United, scoring twice for Colchester, but could not prevent their relegation. His contract details with the Tykes remain undisclosed.\n"
     ]
    }
   ],
   "source": [
    "# Example from the XSum dataset.\n",
    "document = \"\"\"The 21-year-old made seven appearances for the Hammers and netted his only goal for them in a Europa League qualification round match against Andorran side FC Lustrains last season. Lee had two loan spells in League One last term, with Blackpool and then Colchester United. He scored twice for the U's but was unable to save them from relegation. The length of Lee's contract with the promoted Tykes has not been revealed. Find all the latest football transfers on our dedicated page.\"\"\"\n",
    "\n",
    "summarize = dspy.ChainOfThought('document -> summary')\n",
    "response = summarize(document=document)\n",
    "\n",
    "print(response.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-based DSPy Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some advanced tasks, you need more verbose signatures. This is typically to:\n",
    "\n",
    "1. Clarify something about the nature of the task (expressed below as a docstring).\n",
    "2. Supply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.\n",
    "3. Supply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.\n",
    "\n",
    "Tips:\n",
    "- There's nothing wrong with specifying your requests to the LM more clearly. Class-based Signatures help you with that. However, don't prematurely tune the keywords of your signature by hand. The DSPy optimizers will likely do a better job (and will transfer better across LMs).\n",
    "- How `Predict` works:\n",
    "    - https://dspy-docs.vercel.app/deep-dive/signature/executing-signatures/#how-predict-works\n",
    "    - source code: https://github.com/stanfordnlp/dspy/blob/main/dspy/predict/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment='fear'\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Emotion(dspy.Signature):\n",
    "    \"\"\"Classify emotion among sadness, joy, love, anger, fear, surprise.\"\"\"\n",
    "\n",
    "    sentence = dspy.InputField()\n",
    "    sentiment = dspy.OutputField()\n",
    "\n",
    "sentence = \"i started feeling a little vulnerable when the giant spotlight started blinding me\"  # from dair-ai/emotion\n",
    "\n",
    "classify = dspy.Predict(Emotion)\n",
    "classify(sentence=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using signatures to build modules & compiling them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While signatures are convenient for prototyping with structured inputs/outputs, that's not the main reason to use them!\n",
    "\n",
    "You should compose multiple signatures into bigger DSPy modules and compile these modules into optimized prompts and finetunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://dspy-docs.vercel.app/intro/\n",
    "- GitHub: https://github.com/stanfordnlp/dspy\n",
    "- Introduction by Author\n",
    "    - Video: https://www.youtube.com/live/JEMYuzrKLUw?si=iwAzhwobN52zgIZ_\n",
    "    - Slides: https://llmagents-learning.org/slides/dspy_lec.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-101-bGSlOmMx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
